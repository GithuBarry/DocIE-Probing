Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 16.1339, Test loss: 3.5557
In this epoch 100/5000, Training loss: 0.1103, Test loss: 1.9037
In this epoch 150/5000, Training loss: 0.0050, Test loss: 1.8983
In this epoch 200/5000, Training loss: 0.0018, Test loss: 1.8999
In this epoch 250/5000, Training loss: 0.0010, Test loss: 1.9044
In this epoch 300/5000, Training loss: 0.0007, Test loss: 1.9080
In this epoch 350/5000, Training loss: 0.0006, Test loss: 1.9107
In this epoch 400/5000, Training loss: 0.0005, Test loss: 1.9125
In this epoch 450/5000, Training loss: 0.0005, Test loss: 1.9136
In this epoch 500/5000, Training loss: 0.0005, Test loss: 1.9143
In this epoch 550/5000, Training loss: 0.0004, Test loss: 1.9148
In this epoch 600/5000, Training loss: 0.0004, Test loss: 1.9151
In this epoch 650/5000, Training loss: 0.0004, Test loss: 1.9154
In this epoch 700/5000, Training loss: 0.0004, Test loss: 1.9156
In this epoch 750/5000, Training loss: 0.0004, Test loss: 1.9158
In this epoch 800/5000, Training loss: 0.0004, Test loss: 1.9159
In this epoch 850/5000, Training loss: 0.0004, Test loss: 1.9160
In this epoch 900/5000, Training loss: 0.0004, Test loss: 1.9162
In this epoch 950/5000, Training loss: 0.0004, Test loss: 1.9163
In this epoch 1000/5000, Training loss: 0.0004, Test loss: 1.9164
In this epoch 1050/5000, Training loss: 0.0004, Test loss: 1.9164
In this epoch 1100/5000, Training loss: 0.0004, Test loss: 1.9165
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 1.9165
In this epoch 1200/5000, Training loss: 0.0004, Test loss: 1.9166
In this epoch 1250/5000, Training loss: 0.0004, Test loss: 1.9166
In this epoch 1300/5000, Training loss: 0.0004, Test loss: 1.9166
In this epoch 1350/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1400/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1550/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1600/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1650/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1700/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1900/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 1950/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2000/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2050/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2250/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2300/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2350/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2400/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2450/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2600/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2650/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2700/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2750/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2800/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2850/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2900/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 2950/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3000/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3200/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3250/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3300/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3350/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3500/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3550/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3600/5000, Training loss: 0.0004, Test loss: 1.9167
In this epoch 3650/5000, Training loss: 0.0004, Test loss: 1.9169
In this epoch 3700/5000, Training loss: 60.0320, Test loss: 135.6703
In this epoch 3750/5000, Training loss: 0.0425, Test loss: 2.2485
In this epoch 3800/5000, Training loss: 0.0006, Test loss: 1.9366
In this epoch 3850/5000, Training loss: 0.0004, Test loss: 1.9293
In this epoch 3900/5000, Training loss: 0.0004, Test loss: 1.9299
In this epoch 3950/5000, Training loss: 0.0004, Test loss: 1.9299
In this epoch 4000/5000, Training loss: 0.0004, Test loss: 1.9299
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 1.9299
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 1.9299
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4200/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4350/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4450/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4500/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4600/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4650/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 1.9300
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 1.9299
In this epoch 5000/5000, Training loss: 0.0004, Test loss: 1.9299
train_acc 0.34911115916955016
test_acc 0.40939446366782006
{'y_pred': [0.9559603333473206, 0.10740701109170914, 0.8130027651786804, 0.956843912601471, 1.8208487033843994, 1.7797733545303345, 0.4848031997680664, 0.9329624772071838, 0.4419141709804535, 0.8166419863700867, 0.4346991181373596, 1.6751720905303955, 0.455024391412735, 1.0197798013687134, 0.006376306526362896, 1.4697744846343994, 0.8816139698028564, 1.5366873741149902, 0.8912253975868225, 1.4639198780059814, 1.4617465734481812, 1.0616658926010132, 0.9970974922180176, 0.9214463233947754, 0.562232494354248, 0.9465125203132629, 1.1389907598495483, 1.5831416845321655, 0.7640945911407471, 1.0474393367767334, 0.9959807991981506, 1.4285666942596436, -0.6680224537849426, 1.375826120376587, 0.5561749339103699, 0.4743983745574951, 2.046067476272583, 0.8620463013648987, 0.7712135314941406, 1.1967594623565674, 0.3209744393825531, 1.277909278869629, 1.5049558877944946, 1.01591157913208, 0.8556087613105774, 1.0406086444854736, 0.9200659990310669, 0.747573971748352, 0.35916826128959656, 0.4642108082771301, 0.6398624181747437, 1.0281964540481567, 0.5142515301704407, 1.5203274488449097, 1.4223722219467163, 1.0787994861602783, 0.21183046698570251, 1.4787455797195435, 1.5172432661056519, 0.5256589651107788, 0.5042018294334412, 1.2531853914260864, 1.212019920349121, 0.5625090003013611, 0.7631309628486633, 0.8982048630714417, 0.7192934155464172, 0.5299352407455444, 0.5452432036399841, 0.7225818037986755, 1.709812879562378, 1.231623649597168, 0.2954493761062622, 1.1883779764175415, 0.3499167263507843, 0.5712536573410034, 1.0357472896575928, 0.4949653446674347, 0.45700809359550476, 1.2507323026657104, 0.9703606367111206, 0.7666952610015869, 0.4560673236846924, 1.0493308305740356, 0.11371391266584396, 0.6835798025131226, 1.0645880699157715, 0.8841695189476013, 0.44381701946258545, 0.4293849468231201, 1.1088356971740723, 1.1649290323257446, 1.2035242319107056, 0.8631795644760132, 0.9710831642150879, 0.7571871876716614, 0.7936081290245056, 0.4099539816379547, 0.7085951566696167, 1.1852059364318848, 1.3713970184326172, 0.6337457895278931, 0.8830552101135254, 0.4210661053657532, 1.3882009983062744, 0.04455515742301941, 1.9947504997253418, 0.311968058347702, 0.9244095087051392, -0.7261894941329956, 0.40271079540252686, 0.32386109232902527, 0.4063290059566498, 1.4854249954223633, 0.03274519369006157, 1.417891502380371, 0.5095583200454712, 1.133178472518921, 0.881525456905365, 0.8392127156257629, 0.3165570795536041, 1.0177388191223145, 0.2251787781715393, 1.2084392309188843, 0.9413341283798218, 0.6442645788192749, 0.6569482684135437, 0.5808658003807068, -0.1293635368347168, 1.2440310716629028, 0.857280969619751, 1.4680122137069702, 0.786512017250061, 1.0305315256118774, 0.7588459253311157, 1.1267752647399902, 0.7514309287071228, 0.7326435446739197, 0.7981745004653931, -0.11539454758167267, 0.4381442368030548, 0.7674118280410767, 0.597571849822998, 0.04336083307862282, 1.1963119506835938, 0.4187704920768738, 0.780555248260498, 0.8127611875534058, 1.6573138236999512, 0.4940456449985504, 1.090188980102539, 1.1476471424102783, 0.75615394115448, 1.1869837045669556, 1.1173834800720215, 0.5983379483222961, 1.311395525932312, 1.5104444026947021, 0.6066795587539673, 1.5626071691513062, 1.2935738563537598, 0.7613135576248169, 0.7977039217948914, 0.7617688179016113, 0.7711283564567566, 1.1547765731811523, 0.9987488985061646, 1.0786205530166626, 0.7493209838867188, 0.37438857555389404, 0.9819639921188354, 1.2550380229949951, 0.15464773774147034, 1.377511978149414, 1.1159688234329224, 1.4739151000976562, 0.685737133026123, 0.8296171426773071, 0.6978069543838501, 1.4431540966033936, 0.7696113586425781, 0.716847836971283, 1.4914456605911255, 0.7695683240890503, 1.3076601028442383, 0.6985964775085449, 0.7759515643119812, 0.4018072783946991, 1.5500365495681763, 1.0418637990951538, 0.8490798473358154, 1.0619173049926758, 0.8365647792816162, 1.7876631021499634, 1.6468231678009033, 0.8247770667076111, 0.07355550676584244, -0.02159235253930092, -0.2658276855945587, 0.6664104461669922, 0.8333755135536194, 0.980758011341095, 1.070772647857666, 0.8190528154373169, 0.6635078191757202, 0.6508002281188965, 0.7786484956741333, 1.6968040466308594, 0.9340845346450806, 1.1097902059555054, 0.8635281920433044, 1.8215053081512451, 0.7182499766349792, 0.20737135410308838, 1.028282642364502, 1.3201696872711182, 0.5949558615684509, 1.2858171463012695, 1.1174530982971191, -0.08890257030725479, 0.36073485016822815, 1.1199736595153809, 1.383178472518921, 1.096573829650879, 0.473742812871933, 0.453196257352829, 0.8853862285614014, 0.7538413405418396, 1.5238195657730103, 0.2073211371898651, 1.2030420303344727, 1.217553734779358, 1.0070775747299194, -0.0362180694937706, 0.33833351731300354, 1.0984009504318237, 1.7458970546722412, 0.2789195477962494, 0.35115838050842285, 1.8871164321899414, 0.8313010334968567, 0.9394841194152832, -0.022264007478952408, 0.8946696519851685, 1.1376640796661377, 0.9811184406280518, 0.5262643694877625, 0.14847683906555176, 1.201770305633545, 0.3726489245891571, 1.3370585441589355, 0.9358079433441162, 0.8332675695419312, 0.4685351252555847, 1.0717480182647705, 0.9214621782302856, 0.9295374155044556, 0.7335951328277588, 0.8748283982276917, 0.7936885952949524, 0.1765507012605667, 1.4269391298294067, -0.2191278040409088, 1.3665517568588257, 1.1369693279266357, 0.5379124879837036, 1.2345515489578247, -0.07020943611860275, 0.40984880924224854, 0.7873001098632812, 1.024827480316162, 1.2674610614776611, 1.1745332479476929, 0.8918882608413696, 0.8790018558502197, 0.9733067154884338, 1.5707886219024658, 0.9938918352127075, 1.4750773906707764, 0.9633532166481018, 1.4957807064056396, 0.27406397461891174, 0.8579937219619751, 0.8292135000228882, 0.9879894852638245, -0.05857424810528755, 0.9750753045082092, 1.2291319370269775, 1.827985405921936, 1.1753745079040527, 1.212148904800415, 0.4055871069431305, 0.9859274625778198, 1.0211671590805054, 1.4167280197143555, 0.6175036430358887, 1.4424996376037598, 0.39618685841560364, 0.5101213455200195, 1.4642599821090698, 1.374436616897583, 0.8826825022697449, 0.8451367020606995, 1.0720629692077637, 0.9178509712219238, 0.709122896194458, 0.9423449635505676, 0.43247607350349426, 0.1797707974910736, 1.1789168119430542, 0.9023649096488953, 1.103202223777771, 0.8245705962181091, 0.23197247087955475, 1.2988027334213257, 0.48685529828071594, 1.444267988204956, 1.4054625034332275, 0.8824056386947632, 0.7669623494148254, 0.7913772463798523, 0.08164798468351364, 0.6344165205955505, 0.905207633972168, 0.34448015689849854, 1.5656670331954956, 0.8052242398262024, 0.6810985207557678, 1.40084707736969, 0.39612090587615967, 0.9369280338287354, -0.04130834713578224, 1.5686298608779907, 0.9095346927642822, 1.049616813659668, 0.8771635293960571, 0.8318480849266052, 0.8678213953971863, 1.5212337970733643, 1.0726416110992432], 'y_pred_int': [1, 0, 1, 1, 2, 2, 0, 1, 0, 1, 0, 2, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 2, 2, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 2, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 2, 1], 'y_true': [0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 4, 1, 3, 1, 1, 0, 2, 0, 2, 0, 1, 0, 2, 2, 2, 0, 1, 3, 1, 1, 2, 2, 1, 1, 2, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 3, 2, 0, 2, 1, 1, 1, 2, 0, 1, 1, 1, 0, 0, 1, 6, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 6, 0, 1, 3, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 3, 3, 3, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 2, 0, 1, 0, 0, 1, 3, 0, 0, 0, 1, 4, 0, 0, 2, 0, 0, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 0, 0, 0, 3, 0, 0, 0, 0, 6, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 3, 1, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 0, 2, 0, 3, 3, 1, 0, 0, 1, 3, 1, 1, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 2, 1, 5, 0, 0, 0, 1, 0, 14, 1, 0, 0, 1, 1, 0, 2, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 2, 0, 1, 0, 0, 0, 2, 3, 1, 3, 0, 0, 0, 1, 1, 1, 0, 0, 0, 3, 3, 1, 1, 0, 0, 2, 6, 0, 1, 0, 3, 0, 1, 0, 0, 0, 1, 2, 1, 1, 2, 3, 0, 1, 1, 0, 1, 0, 1, 0, 1, 2, 3, 1, 0, 1, 3, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 1, 1, 1, 0, 2, 1, 0, 2, 0, 2, 1, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 3, 1], 'train_acc': 0.34911115916955016, 'test_acc': 0.40939446366782006, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 21.7659, Test loss: 100.7825
In this epoch 100/5000, Training loss: 0.9098, Test loss: 97.6774
In this epoch 150/5000, Training loss: 0.2695, Test loss: 98.0236
In this epoch 200/5000, Training loss: 0.1453, Test loss: 98.1344
In this epoch 250/5000, Training loss: 0.1034, Test loss: 98.2737
In this epoch 300/5000, Training loss: 0.0836, Test loss: 98.4094
In this epoch 350/5000, Training loss: 0.0714, Test loss: 98.5258
In this epoch 400/5000, Training loss: 0.0619, Test loss: 98.6240
In this epoch 450/5000, Training loss: 0.0537, Test loss: 98.7101
In this epoch 500/5000, Training loss: 0.0466, Test loss: 98.7893
In this epoch 550/5000, Training loss: 0.0404, Test loss: 98.8644
In this epoch 600/5000, Training loss: 0.0350, Test loss: 98.9363
In this epoch 650/5000, Training loss: 0.0305, Test loss: 99.0054
In this epoch 700/5000, Training loss: 0.0268, Test loss: 99.0713
In this epoch 750/5000, Training loss: 0.0237, Test loss: 99.1336
In this epoch 800/5000, Training loss: 0.0212, Test loss: 99.1919
In this epoch 850/5000, Training loss: 0.0193, Test loss: 99.2460
In this epoch 900/5000, Training loss: 0.0177, Test loss: 99.2955
In this epoch 950/5000, Training loss: 0.0165, Test loss: 99.3407
In this epoch 1000/5000, Training loss: 0.0156, Test loss: 99.3812
In this epoch 1050/5000, Training loss: 0.0149, Test loss: 99.4175
In this epoch 1100/5000, Training loss: 0.0144, Test loss: 99.4495
In this epoch 1150/5000, Training loss: 0.0141, Test loss: 99.4776
In this epoch 1200/5000, Training loss: 0.0138, Test loss: 99.5020
In this epoch 1250/5000, Training loss: 0.0136, Test loss: 99.5230
In this epoch 1300/5000, Training loss: 0.0135, Test loss: 99.5409
In this epoch 1350/5000, Training loss: 0.0134, Test loss: 99.5561
In this epoch 1400/5000, Training loss: 0.0133, Test loss: 99.5688
In this epoch 1450/5000, Training loss: 6.3591, Test loss: 104.6099
In this epoch 1500/5000, Training loss: 0.0163, Test loss: 99.5377
In this epoch 1550/5000, Training loss: 0.0144, Test loss: 99.6077
In this epoch 1600/5000, Training loss: 3.2018, Test loss: 106.6151
In this epoch 1650/5000, Training loss: 0.0223, Test loss: 99.6236
In this epoch 1700/5000, Training loss: 0.0526, Test loss: 99.6621
In this epoch 1750/5000, Training loss: 0.8163, Test loss: 101.6981
In this epoch 1800/5000, Training loss: 0.0317, Test loss: 99.6331
In this epoch 1850/5000, Training loss: 0.0134, Test loss: 99.6134
In this epoch 1900/5000, Training loss: 22.4938, Test loss: 113.5064
In this epoch 1950/5000, Training loss: 0.0157, Test loss: 99.6347
In this epoch 2000/5000, Training loss: 0.0140, Test loss: 99.6171
In this epoch 2050/5000, Training loss: 0.0133, Test loss: 99.6177
In this epoch 2100/5000, Training loss: 6.8605, Test loss: 110.6913
In this epoch 2150/5000, Training loss: 0.9999, Test loss: 100.8011
In this epoch 2200/5000, Training loss: 0.0133, Test loss: 99.6222
In this epoch 2250/5000, Training loss: 0.0133, Test loss: 99.6195
In this epoch 2300/5000, Training loss: 0.0132, Test loss: 99.6193
In this epoch 2350/5000, Training loss: 19.4517, Test loss: 125.1054
In this epoch 2400/5000, Training loss: 0.5998, Test loss: 100.0841
In this epoch 2450/5000, Training loss: 0.0186, Test loss: 99.6257
In this epoch 2500/5000, Training loss: 0.0132, Test loss: 99.6203
In this epoch 2550/5000, Training loss: 2.7813, Test loss: 115.8350
In this epoch 2600/5000, Training loss: 0.0393, Test loss: 99.7145
In this epoch 2650/5000, Training loss: 0.0133, Test loss: 99.6210
In this epoch 2700/5000, Training loss: 0.0132, Test loss: 99.6209
In this epoch 2750/5000, Training loss: 0.0132, Test loss: 99.6209
In this epoch 2800/5000, Training loss: 0.0132, Test loss: 99.6209
In this epoch 2850/5000, Training loss: 27.6104, Test loss: 100.0679
In this epoch 2900/5000, Training loss: 0.3674, Test loss: 100.0208
In this epoch 2950/5000, Training loss: 0.0133, Test loss: 99.6215
In this epoch 3000/5000, Training loss: 0.0133, Test loss: 99.6211
In this epoch 3050/5000, Training loss: 0.0132, Test loss: 99.6210
In this epoch 3100/5000, Training loss: 0.0132, Test loss: 99.6211
In this epoch 3150/5000, Training loss: 0.4515, Test loss: 100.9238
In this epoch 3200/5000, Training loss: 11.3474, Test loss: 117.3340
In this epoch 3250/5000, Training loss: 0.4208, Test loss: 99.6285
In this epoch 3300/5000, Training loss: 0.0202, Test loss: 99.6233
In this epoch 3350/5000, Training loss: 0.0133, Test loss: 99.6210
In this epoch 3400/5000, Training loss: 0.0132, Test loss: 99.6211
In this epoch 3450/5000, Training loss: 138.4282, Test loss: 221.6315
In this epoch 3500/5000, Training loss: 0.6564, Test loss: 99.8852
In this epoch 3550/5000, Training loss: 0.0133, Test loss: 99.6230
In this epoch 3600/5000, Training loss: 0.0132, Test loss: 99.6211
In this epoch 3650/5000, Training loss: 0.0132, Test loss: 99.6211
In this epoch 3700/5000, Training loss: 0.0132, Test loss: 99.6212
In this epoch 3750/5000, Training loss: 20.0356, Test loss: 102.5385
In this epoch 3800/5000, Training loss: 0.1003, Test loss: 99.7601
In this epoch 3850/5000, Training loss: 0.0137, Test loss: 99.6210
In this epoch 3900/5000, Training loss: 0.0132, Test loss: 99.6211
In this epoch 3950/5000, Training loss: 0.0132, Test loss: 99.6211
In this epoch 4000/5000, Training loss: 4.8005, Test loss: 104.3717
In this epoch 4050/5000, Training loss: 14.7854, Test loss: 120.6657
In this epoch 4100/5000, Training loss: 0.5023, Test loss: 99.7543
In this epoch 4150/5000, Training loss: 0.0159, Test loss: 99.6216
In this epoch 4200/5000, Training loss: 0.0132, Test loss: 99.6211
In this epoch 4250/5000, Training loss: 104.4778, Test loss: 210.2240
In this epoch 4300/5000, Training loss: 0.5051, Test loss: 99.7640
In this epoch 4350/5000, Training loss: 0.0144, Test loss: 99.6224
In this epoch 4400/5000, Training loss: 0.0133, Test loss: 99.6211
In this epoch 4450/5000, Training loss: 0.0132, Test loss: 99.6211
In this epoch 4500/5000, Training loss: 113.6942, Test loss: 147.9443
In this epoch 4550/5000, Training loss: 0.1338, Test loss: 100.0139
In this epoch 4600/5000, Training loss: 0.0134, Test loss: 99.6219
In this epoch 4650/5000, Training loss: 0.0132, Test loss: 99.6210
In this epoch 4700/5000, Training loss: 0.0132, Test loss: 99.6211
In this epoch 4750/5000, Training loss: 0.0132, Test loss: 99.6191
In this epoch 4800/5000, Training loss: 46.7747, Test loss: 103.8215
In this epoch 4850/5000, Training loss: 0.2002, Test loss: 100.0369
In this epoch 4900/5000, Training loss: 0.0146, Test loss: 99.6177
In this epoch 4950/5000, Training loss: 0.0132, Test loss: 99.6207
In this epoch 5000/5000, Training loss: 0.0132, Test loss: 99.6211
train_acc 0.9985294117647059
test_acc 0.03529411764705882
{'y_pred': [12.191746711730957, 19.404678344726562, 17.980709075927734, 14.04697322845459, 4.655420303344727, 17.18098258972168, 8.47533893585205, 11.51771068572998, 14.259013175964355, 14.802239418029785, 14.243514060974121, 13.335978507995605, 11.831718444824219, 18.586185455322266, 13.917121887207031, 7.39296817779541, 11.681126594543457, 14.538695335388184, 16.086856842041016, 21.36892318725586, 13.091790199279785, 10.0761079788208, 12.206239700317383, 14.651373863220215, 9.80687427520752, 15.355147361755371, 18.293331146240234, 13.2742280960083, 12.697726249694824, 9.703059196472168, 12.959227561950684, 14.399211883544922, 11.154237747192383, 17.054607391357422, 12.700054168701172, 19.866186141967773, 17.941755294799805, 15.568532943725586, 11.072340965270996, 8.126568794250488, 14.038037300109863, 8.04530143737793, 12.633094787597656, 15.69851016998291, 10.931272506713867, 17.72649574279785, 13.120159149169922, 17.634746551513672, 10.368548393249512, 12.350874900817871, 16.383451461791992, 13.656295776367188, 12.012369155883789, 11.209269523620605, 11.976710319519043, 9.8511962890625, 15.40137767791748, 13.094037055969238, 18.69948387145996, 14.672881126403809, 13.914734840393066, 14.496807098388672, 13.661150932312012, 14.014050483703613, 12.929268836975098, 12.450394630432129, 13.881043434143066, 15.786517143249512, 12.057853698730469, 14.244357109069824, 11.57552433013916, 15.053886413574219, 8.472758293151855, 13.043288230895996, 16.567808151245117, 13.116957664489746, 10.594656944274902, 18.007244110107422, 14.710118293762207, 9.05944538116455, 9.282397270202637, 12.574089050292969, 15.112781524658203, 15.485047340393066, 11.939737319946289, 14.148005485534668, 17.86371612548828, 12.512680053710938, 14.937747955322266, 15.52280330657959, 20.76553726196289, 12.991625785827637, 17.82651138305664, 11.057374954223633, 17.205833435058594, 14.669055938720703, 12.91836166381836, 16.89167022705078, 16.462726593017578, 14.649571418762207, 16.49261474609375, 17.083045959472656, 13.140305519104004, 17.587499618530273, 19.352420806884766, 13.770618438720703, 15.868891716003418, 11.643930435180664, 12.451675415039062, 16.08119773864746, 8.693303108215332, 15.018290519714355, 13.11017894744873, 14.228960990905762, 13.126801490783691, 13.488388061523438, 15.88894271850586, 13.06437873840332, 13.645095825195312, 16.37628173828125, 13.9520902633667, 14.91053581237793, 16.8443603515625, 12.416059494018555, 8.166817665100098, 7.037844657897949, 13.566664695739746, 16.230497360229492, 18.665386199951172, 16.81955337524414, 15.300060272216797, 13.84481143951416, 17.396167755126953, 10.962994575500488, 15.44596004486084, 14.670358657836914, 9.420344352722168, 15.295584678649902, 13.37132740020752, 13.232525825500488, 13.917163848876953, 15.65149211883545, 13.93239974975586, 16.606367111206055, 15.42556095123291, 9.262858390808105, 20.420238494873047, 13.943251609802246, 12.8032808303833, 19.7346134185791, 14.604682922363281, 17.924524307250977, 13.320391654968262, 14.064789772033691, 11.599989891052246, 14.670939445495605, 14.446130752563477, 11.952778816223145, 12.519306182861328, 8.269682884216309, 18.14571189880371, 9.35115909576416, 9.26496410369873, 17.561336517333984, 15.016239166259766, 18.005088806152344, 9.408461570739746, 18.899288177490234, 14.948307991027832, 15.361163139343262, 17.926025390625, 11.399975776672363, 18.738924026489258, 12.48517894744873, 17.36124038696289, 12.946643829345703, 14.08004093170166, 12.056229591369629, 15.302611351013184, 13.002099990844727, 12.193277359008789, 11.158345222473145, 12.927125930786133, 15.577990531921387, 15.397429466247559, 12.584364891052246, 21.957029342651367, 17.4060115814209, 10.004834175109863, 10.993427276611328, 16.006847381591797, 14.061250686645508, 17.42667007446289, 16.309120178222656, 3.0003583431243896, 15.237580299377441, 9.667102813720703, 13.422924041748047, 13.094168663024902, 10.051374435424805, 14.1762056350708, 12.796390533447266, 10.915961265563965, 13.736563682556152, 12.629609107971191, 11.751055717468262, 18.626628875732422, 13.775225639343262, 9.16286563873291, 15.550432205200195, 11.138161659240723, 12.13032054901123, 16.021297454833984, 16.255592346191406, 12.617932319641113, 11.01130199432373, 9.336630821228027, 10.030461311340332, 15.86467456817627, 14.865938186645508, 10.785324096679688, 16.47473907470703, 15.523154258728027, 13.98004150390625, 18.224048614501953, 17.883953094482422, 13.40573787689209, 13.67050838470459, 15.512395858764648, 14.861319541931152, 11.667981147766113, 15.7017822265625, 13.140355110168457, 15.97209358215332, 14.441632270812988, 12.867093086242676, 13.574601173400879, 9.20197582244873, 17.16765594482422, 12.658061027526855, 18.752046585083008, 7.454437255859375, 10.209907531738281, 15.372000694274902, 15.527335166931152, 17.984792709350586, 13.305408477783203, 15.239726066589355, 14.494560241699219, 11.903590202331543, 18.105701446533203, 14.1575288772583, 12.496713638305664, 10.565080642700195, 20.0864200592041, 6.201028823852539, 11.839040756225586, 13.366454124450684, 16.17017936706543, 11.22374153137207, 19.837289810180664, 9.912922859191895, 17.160594940185547, 14.365792274475098, 13.03793716430664, 9.92935848236084, 11.933396339416504, 15.57075023651123, 17.835567474365234, 11.383585929870605, 10.68362045288086, 10.890213012695312, 14.517853736877441, 13.973761558532715, 36.330963134765625, 16.06151580810547, 12.93014907836914, 17.89700698852539, 14.57966136932373, 12.344226837158203, 15.851104736328125, 16.369789123535156, 11.570740699768066, 22.313274383544922, 11.210209846496582, 11.007936477661133, 13.171210289001465, 15.798961639404297, 14.967424392700195, 11.362160682678223, 12.02949047088623, 18.02448081970215, 15.248330116271973, 16.65796661376953, 7.860140323638916, 17.619552612304688, 17.767345428466797, 11.856124877929688, 14.164613723754883, 14.024079322814941, 9.52059268951416, 32.625732421875, 16.057764053344727, 12.164626121520996, 11.342376708984375, 16.27639389038086, 6.760512351989746, 14.143946647644043, 10.877453804016113, 14.29931926727295, 15.550399780273438, 16.68181800842285, 17.60805320739746, 12.344858169555664, 10.663369178771973, 14.631556510925293, 8.08487606048584, 13.223592758178711, 12.014962196350098, 11.608872413635254, 13.451722145080566, 14.63431167602539, 16.010286331176758, 16.619415283203125, 13.93139362335205, 12.596051216125488, 12.803976058959961, 13.137263298034668, 15.063681602478027, 9.89448356628418, 15.699301719665527, 9.623839378356934, 15.699755668640137, 12.977522850036621, 11.362332344055176, 9.849414825439453, 14.41707992553711, 15.236587524414062, 12.541633605957031, 14.004413604736328], 'y_pred_int': [12, 19, 18, 14, 5, 17, 8, 12, 14, 15, 14, 13, 12, 19, 14, 7, 12, 15, 16, 21, 13, 10, 12, 15, 10, 15, 18, 13, 13, 10, 13, 14, 11, 17, 13, 20, 18, 16, 11, 8, 14, 8, 13, 16, 11, 18, 13, 18, 10, 12, 16, 14, 12, 11, 12, 10, 15, 13, 19, 15, 14, 14, 14, 14, 13, 12, 14, 16, 12, 14, 12, 15, 8, 13, 17, 13, 11, 18, 15, 9, 9, 13, 15, 15, 12, 14, 18, 13, 15, 16, 21, 13, 18, 11, 17, 15, 13, 17, 16, 15, 16, 17, 13, 18, 19, 14, 16, 12, 12, 16, 9, 15, 13, 14, 13, 13, 16, 13, 14, 16, 14, 15, 17, 12, 8, 7, 14, 16, 19, 17, 15, 14, 17, 11, 15, 15, 9, 15, 13, 13, 14, 16, 14, 17, 15, 9, 20, 14, 13, 20, 15, 18, 13, 14, 12, 15, 14, 12, 13, 8, 18, 9, 9, 18, 15, 18, 9, 19, 15, 15, 18, 11, 19, 12, 17, 13, 14, 12, 15, 13, 12, 11, 13, 16, 15, 13, 22, 17, 10, 11, 16, 14, 17, 16, 3, 15, 10, 13, 13, 10, 14, 13, 11, 14, 13, 12, 19, 14, 9, 16, 11, 12, 16, 16, 13, 11, 9, 10, 16, 15, 11, 16, 16, 14, 18, 18, 13, 14, 16, 15, 12, 16, 13, 16, 14, 13, 14, 9, 17, 13, 19, 7, 10, 15, 16, 18, 13, 15, 14, 12, 18, 14, 12, 11, 20, 6, 12, 13, 16, 11, 20, 10, 17, 14, 13, 10, 12, 16, 18, 11, 11, 11, 15, 14, 36, 16, 13, 18, 15, 12, 16, 16, 12, 22, 11, 11, 13, 16, 15, 11, 12, 18, 15, 17, 8, 18, 18, 12, 14, 14, 10, 33, 16, 12, 11, 16, 7, 14, 11, 14, 16, 17, 18, 12, 11, 15, 8, 13, 12, 12, 13, 15, 16, 17, 14, 13, 13, 13, 15, 10, 16, 10, 16, 13, 11, 10, 14, 15, 13, 14], 'y_true': [7, 7, 6, 8, 6, 14, 8, 5, 8, 20, 9, 5, 22, 11, 3, 25, 11, 46, 4, 5, 9, 20, 10, 18, 6, 5, 23, 35, 13, 4, 17, 3, 13, 4, 20, 4, 6, 17, 5, 16, 5, 12, 9, 31, 9, 18, 14, 13, 2, 9, 8, 11, 35, 5, 16, 15, 20, 3, 11, 35, 13, 14, 12, 16, 15, 15, 6, 7, 12, 5, 12, 4, 18, 7, 5, 8, 22, 18, 17, 6, 4, 28, 6, 10, 7, 5, 14, 39, 20, 3, 41, 8, 24, 9, 17, 21, 46, 2, 20, 10, 12, 26, 30, 9, 32, 11, 32, 14, 9, 15, 12, 14, 7, 8, 15, 11, 13, 17, 7, 11, 8, 10, 4, 5, 4, 26, 8, 15, 6, 37, 7, 13, 18, 6, 25, 15, 7, 16, 20, 9, 10, 16, 17, 7, 6, 27, 13, 6, 15, 15, 6, 14, 5, 15, 4, 33, 8, 10, 9, 6, 15, 16, 16, 13, 27, 2, 3, 6, 9, 19, 15, 7, 11, 14, 19, 8, 4, 11, 22, 8, 5, 23, 8, 5, 5, 11, 49, 9, 9, 11, 12, 23, 10, 9, 5, 40, 8, 14, 6, 9, 9, 31, 6, 20, 26, 4, 3, 5, 5, 3, 17, 19, 17, 12, 3, 40, 11, 4, 21, 19, 10, 3, 38, 17, 8, 12, 15, 10, 9, 6, 12, 21, 13, 34, 8, 16, 19, 11, 9, 13, 4, 11, 10, 18, 25, 24, 6, 6, 6, 18, 9, 32, 10, 5, 19, 14, 6, 10, 18, 13, 3, 7, 13, 28, 15, 6, 28, 7, 4, 21, 3, 14, 9, 17, 8, 11, 6, 39, 39, 9, 7, 14, 21, 6, 8, 3, 6, 5, 43, 6, 14, 20, 1, 6, 31, 9, 8, 25, 19, 13, 34, 24, 25, 9, 6, 20, 23, 47, 17, 5, 8, 32, 8, 3, 18, 30, 25, 10, 1, 5, 13, 13, 15, 36, 3, 16, 5, 9, 35, 22, 26, 5, 17, 46, 10, 20, 13, 16, 8, 11], 'train_acc': 0.9985294117647059, 'test_acc': 0.03529411764705882, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 33125.3086, Test loss: 53649.0000
In this epoch 100/5000, Training loss: 18857.7656, Test loss: 52715.0469
In this epoch 150/5000, Training loss: 10376.0166, Test loss: 53331.9531
In this epoch 200/5000, Training loss: 5737.3779, Test loss: 53811.7773
In this epoch 250/5000, Training loss: 3269.7827, Test loss: 54174.6641
In this epoch 300/5000, Training loss: 1938.5690, Test loss: 54445.2617
In this epoch 350/5000, Training loss: 1197.8712, Test loss: 54652.5898
In this epoch 400/5000, Training loss: 771.9118, Test loss: 54816.9375
In this epoch 450/5000, Training loss: 519.7350, Test loss: 54950.7539
In this epoch 500/5000, Training loss: 366.8595, Test loss: 55061.6641
In this epoch 550/5000, Training loss: 272.3912, Test loss: 55154.5664
In this epoch 600/5000, Training loss: 213.0400, Test loss: 55232.7539
In this epoch 650/5000, Training loss: 175.1173, Test loss: 55298.6250
In this epoch 700/5000, Training loss: 150.3769, Test loss: 55354.0586
In this epoch 750/5000, Training loss: 133.7668, Test loss: 55400.6602
In this epoch 800/5000, Training loss: 122.1625, Test loss: 55439.8203
In this epoch 850/5000, Training loss: 113.6252, Test loss: 55472.7891
In this epoch 900/5000, Training loss: 106.9549, Test loss: 55500.6719
In this epoch 950/5000, Training loss: 101.4141, Test loss: 55524.4727
In this epoch 1000/5000, Training loss: 96.5550, Test loss: 55545.0430
In this epoch 1050/5000, Training loss: 92.1107, Test loss: 55563.1016
In this epoch 1100/5000, Training loss: 87.9253, Test loss: 55579.2617
In this epoch 1150/5000, Training loss: 83.9106, Test loss: 55594.0352
In this epoch 1200/5000, Training loss: 80.0192, Test loss: 55607.8242
In this epoch 1250/5000, Training loss: 76.2276, Test loss: 55620.9648
In this epoch 1300/5000, Training loss: 72.5255, Test loss: 55633.7305
In this epoch 1350/5000, Training loss: 68.9104, Test loss: 55646.2891
In this epoch 1400/5000, Training loss: 65.3835, Test loss: 55658.8008
In this epoch 1450/5000, Training loss: 61.9482, Test loss: 55671.3711
In this epoch 1500/5000, Training loss: 58.6084, Test loss: 55684.0898
In this epoch 1550/5000, Training loss: 55.3681, Test loss: 55697.0000
In this epoch 1600/5000, Training loss: 52.2312, Test loss: 55710.1289
In this epoch 1650/5000, Training loss: 49.2012, Test loss: 55723.4844
In this epoch 1700/5000, Training loss: 46.2809, Test loss: 55737.0898
In this epoch 1750/5000, Training loss: 43.4729, Test loss: 55750.9258
In this epoch 1800/5000, Training loss: 40.7789, Test loss: 55764.9883
In this epoch 1850/5000, Training loss: 38.2003, Test loss: 55779.2383
In this epoch 1900/5000, Training loss: 35.7380, Test loss: 55793.6758
In this epoch 1950/5000, Training loss: 33.3923, Test loss: 55808.2852
In this epoch 2000/5000, Training loss: 31.1631, Test loss: 55823.0117
In this epoch 2050/5000, Training loss: 29.0496, Test loss: 55837.8594
In this epoch 2100/5000, Training loss: 27.0507, Test loss: 55852.7852
In this epoch 2150/5000, Training loss: 25.1651, Test loss: 55867.7539
In this epoch 2200/5000, Training loss: 23.3907, Test loss: 55882.7383
In this epoch 2250/5000, Training loss: 21.7254, Test loss: 55897.7188
In this epoch 2300/5000, Training loss: 20.1665, Test loss: 55912.6523
In this epoch 2350/5000, Training loss: 18.7112, Test loss: 55927.5234
In this epoch 2400/5000, Training loss: 17.3562, Test loss: 55942.2773
In this epoch 2450/5000, Training loss: 16.0981, Test loss: 55956.9062
In this epoch 2500/5000, Training loss: 14.9334, Test loss: 55971.3789
In this epoch 2550/5000, Training loss: 13.8583, Test loss: 55985.6602
In this epoch 2600/5000, Training loss: 12.8687, Test loss: 55999.7266
In this epoch 2650/5000, Training loss: 11.9606, Test loss: 56013.5469
In this epoch 2700/5000, Training loss: 11.1299, Test loss: 56027.1016
In this epoch 2750/5000, Training loss: 10.3724, Test loss: 56040.3711
In this epoch 2800/5000, Training loss: 9.6839, Test loss: 56053.3086
In this epoch 2850/5000, Training loss: 9.0601, Test loss: 56065.9297
In this epoch 2900/5000, Training loss: 8.4969, Test loss: 56078.1836
In this epoch 2950/5000, Training loss: 7.9902, Test loss: 56090.0781
In this epoch 3000/5000, Training loss: 7.5359, Test loss: 56101.5820
In this epoch 3050/5000, Training loss: 7.1301, Test loss: 56112.6992
In this epoch 3100/5000, Training loss: 6.7689, Test loss: 56123.3906
In this epoch 3150/5000, Training loss: 6.4487, Test loss: 56133.6641
In this epoch 3200/5000, Training loss: 6.1659, Test loss: 56143.5078
In this epoch 3250/5000, Training loss: 5.9171, Test loss: 56152.9180
In this epoch 3300/5000, Training loss: 5.6992, Test loss: 56161.9023
In this epoch 3350/5000, Training loss: 5.5092, Test loss: 56170.4297
In this epoch 3400/5000, Training loss: 5.3441, Test loss: 56178.5352
In this epoch 3450/5000, Training loss: 5.2013, Test loss: 56186.1836
In this epoch 3500/5000, Training loss: 5.0784, Test loss: 56193.4258
In this epoch 3550/5000, Training loss: 4.9731, Test loss: 56200.2383
In this epoch 3600/5000, Training loss: 4.8834, Test loss: 56206.6250
In this epoch 3650/5000, Training loss: 4.8072, Test loss: 56212.5938
In this epoch 3700/5000, Training loss: 4.7428, Test loss: 56218.1875
In this epoch 3750/5000, Training loss: 4.6888, Test loss: 56223.3672
In this epoch 3800/5000, Training loss: 4.6436, Test loss: 56228.1836
In this epoch 3850/5000, Training loss: 4.6061, Test loss: 56232.6367
In this epoch 3900/5000, Training loss: 4.5751, Test loss: 56236.7422
In this epoch 3950/5000, Training loss: 4.5496, Test loss: 56240.5195
In this epoch 4000/5000, Training loss: 4.5288, Test loss: 56243.9766
In this epoch 4050/5000, Training loss: 4.5118, Test loss: 56247.1406
In this epoch 4100/5000, Training loss: 4.4982, Test loss: 56250.0078
In this epoch 4150/5000, Training loss: 4.4872, Test loss: 56252.6133
In this epoch 4200/5000, Training loss: 4.4785, Test loss: 56254.9648
In this epoch 4250/5000, Training loss: 4.4716, Test loss: 56257.1016
In this epoch 4300/5000, Training loss: 4.4661, Test loss: 56259.0078
In this epoch 4350/5000, Training loss: 4.4619, Test loss: 56260.6992
In this epoch 4400/5000, Training loss: 4.4586, Test loss: 56262.2266
In this epoch 4450/5000, Training loss: 4.4560, Test loss: 56263.5781
In this epoch 4500/5000, Training loss: 4.4541, Test loss: 56264.7656
In this epoch 4550/5000, Training loss: 4.4526, Test loss: 56265.8125
In this epoch 4600/5000, Training loss: 4.4515, Test loss: 56266.7305
In this epoch 4650/5000, Training loss: 4.4507, Test loss: 56267.5430
In this epoch 4700/5000, Training loss: 4.4501, Test loss: 56268.2500
In this epoch 4750/5000, Training loss: 4.4496, Test loss: 56268.8359
In this epoch 4800/5000, Training loss: 4.4493, Test loss: 56269.3555
In this epoch 4850/5000, Training loss: 4.4517, Test loss: 56266.8828
In this epoch 4900/5000, Training loss: 5.8393, Test loss: 56381.2539
In this epoch 4950/5000, Training loss: 4.4686, Test loss: 56280.6758
In this epoch 5000/5000, Training loss: 4.4488, Test loss: 56271.1602
train_acc 0.9985294117647059
test_acc 0.0
{'y_pred': [398.58245849609375, 315.38037109375, 424.1369323730469, 343.0034484863281, 329.79742431640625, 451.24591064453125, 348.1712951660156, 471.55712890625, 298.8463134765625, 365.3245544433594, 430.9061584472656, 260.6852722167969, 265.57403564453125, 317.5062561035156, 467.2273254394531, 320.3040466308594, 419.61737060546875, 323.8600769042969, 437.0122375488281, 475.03680419921875, 363.3202209472656, 463.77850341796875, 482.1034851074219, 365.967041015625, 387.2034912109375, 304.1869201660156, 317.102294921875, 354.1956481933594, 414.3225402832031, 360.3280334472656, 298.81854248046875, 366.2685546875, 376.0835266113281, 318.367431640625, 354.45428466796875, 333.23333740234375, 323.4139099121094, 395.7539978027344, 233.62901306152344, 404.3430480957031, 402.8002624511719, 330.40155029296875, 344.222900390625, 315.30438232421875, 335.73980712890625, 169.99501037597656, 383.968994140625, 402.1817321777344, 306.4539489746094, 403.87744140625, 376.4241027832031, 311.0028991699219, 342.6031494140625, 317.0921630859375, 383.9518737792969, 299.50384521484375, 311.1510009765625, 398.75128173828125, 388.47186279296875, 283.1291809082031, 284.9853210449219, 364.5682373046875, 191.39805603027344, 318.01812744140625, 259.90582275390625, 355.09686279296875, 376.08319091796875, 394.8445739746094, 417.0588684082031, 333.4898376464844, 379.4640197753906, 280.392822265625, 393.37884521484375, 472.0020751953125, 338.41131591796875, 311.16168212890625, 293.4122009277344, 433.0289001464844, 312.16314697265625, 412.0668029785156, 336.1707763671875, 391.4876403808594, 266.35406494140625, 361.90924072265625, 220.53858947753906, 351.35797119140625, 345.08734130859375, 356.5091552734375, 459.3675842285156, 331.8312683105469, 381.959228515625, 318.6043701171875, 317.45281982421875, 302.5273742675781, 362.5293884277344, 319.90576171875, 371.848876953125, 429.9294738769531, 428.1705627441406, 335.2386169433594, 329.15960693359375, 336.3870849609375, 410.2682189941406, 325.28704833984375, 393.27203369140625, 275.4736022949219, 361.1968078613281, 329.54473876953125, 299.92852783203125, 371.7680969238281, 299.613525390625, 366.00787353515625, 304.0976867675781, 386.97528076171875, 414.85308837890625, 375.8817138671875, 317.3138122558594, 261.03900146484375, 427.18658447265625, 384.7862243652344, 334.9666442871094, 337.9283447265625, 358.0429992675781, 411.5374755859375, 329.7800598144531, 336.2243347167969, 304.6568908691406, 386.2759704589844, 326.01007080078125, 354.5255432128906, 369.76922607421875, 279.4640197753906, 388.3224182128906, 321.32257080078125, 329.2416687011719, 475.2402038574219, 409.0505065917969, 341.30133056640625, 374.25396728515625, 289.4780578613281, 317.0782470703125, 388.216796875, 302.41754150390625, 273.7944030761719, 307.18310546875, 327.26739501953125, 397.9433898925781, 425.6329345703125, 379.2359924316406, 343.1565246582031, 326.39105224609375, 292.0093688964844, 415.3411560058594, 309.5161437988281, 390.59716796875, 316.278564453125, 381.18682861328125, 370.2248840332031, 358.36962890625, 399.38299560546875, 471.991455078125, 405.4324951171875, 389.15576171875, 343.20074462890625, 327.93194580078125, 337.64501953125, 380.7980041503906, 293.3734130859375, 270.4417419433594, 348.3874816894531, 376.68994140625, 312.41094970703125, 233.1663055419922, 196.06082153320312, 372.5317077636719, 321.11419677734375, 311.0761413574219, 369.10137939453125, 281.6161193847656, 466.4388427734375, 295.3314514160156, 355.5107116699219, 334.500244140625, 351.46435546875, 351.70849609375, 312.9620361328125, 333.7896728515625, 399.0594482421875, 379.50341796875, 470.50177001953125, 266.2484436035156, 327.0470275878906, 368.021728515625, 346.7930908203125, 330.859619140625, 308.5351257324219, 410.21337890625, 268.84527587890625, 272.9884033203125, 421.10040283203125, 410.5744934082031, 234.4940643310547, 328.9134521484375, 333.894287109375, 297.2276306152344, 371.7300720214844, 366.4888916015625, 299.95819091796875, 354.3265686035156, 254.0098419189453, 303.1961975097656, 271.5453186035156, 346.6349182128906, 330.2124328613281, 300.2218933105469, 394.1697082519531, 404.06414794921875, 347.7530822753906, 457.1746826171875, 408.2099609375, 336.2649230957031, 318.681396484375, 348.37750244140625, 360.27716064453125, 438.9496765136719, 423.4681396484375, 311.64776611328125, 371.89556884765625, 368.38763427734375, 328.4303283691406, 304.779052734375, 360.03515625, 314.95355224609375, 240.57054138183594, 322.9610290527344, 872.525146484375, 297.0848083496094, 391.90301513671875, 374.63690185546875, 316.8956298828125, 307.711181640625, 276.8280944824219, 409.00872802734375, 356.421875, 427.5613708496094, 273.3642883300781, 310.37603759765625, 284.0809326171875, 464.6019287109375, 354.4568176269531, 309.6165466308594, 417.74603271484375, 357.28643798828125, 420.6810302734375, 325.38671875, 384.679443359375, 295.38433837890625, 363.0200500488281, 278.82427978515625, 321.6087646484375, 352.9281005859375, 388.6885681152344, 151.42796325683594, 312.15228271484375, 340.8117980957031, 387.03509521484375, 472.9002990722656, 412.41229248046875, 425.3182067871094, 295.224853515625, 430.9015808105469, 381.4083251953125, 406.2195739746094, 378.5338134765625, 387.0533752441406, 324.1972961425781, 397.43548583984375, 422.5157470703125, 339.18060302734375, 372.5033264160156, 383.081298828125, 338.8184814453125, 285.52020263671875, 243.73097229003906, 353.59710693359375, 403.8070068359375, 346.62493896484375, 408.68939208984375, 258.65252685546875, 295.0343017578125, 472.941650390625, 344.0037841796875, 398.78399658203125, 466.16070556640625, 420.09552001953125, 310.05487060546875, 327.16400146484375, 392.44415283203125, 300.9295349121094, 373.02362060546875, 166.08827209472656, 293.674560546875, 264.65618896484375, 285.1731262207031, 363.8432312011719, 374.5093688964844, 564.0931396484375, 285.39434814453125, 365.7082824707031, 356.3001403808594, 359.1721496582031, 303.1802673339844, 339.5101318359375, 299.77935791015625, 337.05419921875, 300.6519775390625, 345.7521057128906, 278.51800537109375, 366.1431884765625, 337.68084716796875, 372.98095703125, 395.2771911621094, 333.74658203125, 365.3319091796875, 375.984375, 395.89520263671875, 273.5274963378906, 360.9588623046875, 335.819580078125, 366.60797119140625, 407.8896179199219, 338.1285400390625, 404.601806640625, 318.15478515625, 332.6322021484375, 362.0904541015625, 351.83135986328125, 385.18182373046875, 431.28204345703125, 397.16070556640625], 'y_pred_int': [399, 315, 424, 343, 330, 451, 348, 472, 299, 365, 431, 261, 266, 318, 467, 320, 420, 324, 437, 475, 363, 464, 482, 366, 387, 304, 317, 354, 414, 360, 299, 366, 376, 318, 354, 333, 323, 396, 234, 404, 403, 330, 344, 315, 336, 170, 384, 402, 306, 404, 376, 311, 343, 317, 384, 300, 311, 399, 388, 283, 285, 365, 191, 318, 260, 355, 376, 395, 417, 333, 379, 280, 393, 472, 338, 311, 293, 433, 312, 412, 336, 391, 266, 362, 221, 351, 345, 357, 459, 332, 382, 319, 317, 303, 363, 320, 372, 430, 428, 335, 329, 336, 410, 325, 393, 275, 361, 330, 300, 372, 300, 366, 304, 387, 415, 376, 317, 261, 427, 385, 335, 338, 358, 412, 330, 336, 305, 386, 326, 355, 370, 279, 388, 321, 329, 475, 409, 341, 374, 289, 317, 388, 302, 274, 307, 327, 398, 426, 379, 343, 326, 292, 415, 310, 391, 316, 381, 370, 358, 399, 472, 405, 389, 343, 328, 338, 381, 293, 270, 348, 377, 312, 233, 196, 373, 321, 311, 369, 282, 466, 295, 356, 335, 351, 352, 313, 334, 399, 380, 471, 266, 327, 368, 347, 331, 309, 410, 269, 273, 421, 411, 234, 329, 334, 297, 372, 366, 300, 354, 254, 303, 272, 347, 330, 300, 394, 404, 348, 457, 408, 336, 319, 348, 360, 439, 423, 312, 372, 368, 328, 305, 360, 315, 241, 323, 873, 297, 392, 375, 317, 308, 277, 409, 356, 428, 273, 310, 284, 465, 354, 310, 418, 357, 421, 325, 385, 295, 363, 279, 322, 353, 389, 151, 312, 341, 387, 473, 412, 425, 295, 431, 381, 406, 379, 387, 324, 397, 423, 339, 373, 383, 339, 286, 244, 354, 404, 347, 409, 259, 295, 473, 344, 399, 466, 420, 310, 327, 392, 301, 373, 166, 294, 265, 285, 364, 375, 564, 285, 366, 356, 359, 303, 340, 300, 337, 301, 346, 279, 366, 338, 373, 395, 334, 365, 376, 396, 274, 361, 336, 367, 408, 338, 405, 318, 333, 362, 352, 385, 431, 397], 'y_true': [797, 191, 205, 753, 165, 235, 98, 457, 387, 266, 197, 441, 493, 469, 494, 729, 669, 41, 904, 231, 198, 678, 126, 583, 586, 412, 440, 388, 171, 283, 232, 195, 73, 431, 253, 370, 342, 177, 921, 217, 523, 314, 155, 197, 401, 327, 226, 670, 323, 296, 1015, 55, 233, 182, 268, 201, 331, 481, 389, 599, 727, 296, 498, 375, 507, 291, 77, 753, 616, 215, 509, 463, 903, 563, 681, 328, 391, 154, 607, 175, 464, 752, 459, 78, 708, 643, 396, 324, 580, 657, 164, 406, 723, 610, 562, 173, 176, 358, 403, 453, 693, 434, 679, 158, 445, 256, 466, 88, 355, 911, 99, 638, 227, 344, 82, 205, 367, 577, 373, 459, 552, 244, 599, 249, 175, 559, 247, 778, 350, 1062, 113, 505, 197, 89, 401, 433, 814, 392, 257, 233, 210, 373, 207, 435, 89, 424, 189, 605, 529, 116, 156, 160, 176, 289, 487, 434, 433, 139, 280, 235, 104, 234, 677, 770, 266, 517, 323, 448, 934, 102, 142, 281, 214, 199, 339, 208, 840, 585, 175, 294, 741, 181, 201, 243, 156, 198, 167, 302, 438, 686, 194, 778, 145, 456, 701, 541, 243, 328, 1026, 446, 507, 148, 384, 172, 279, 663, 569, 180, 312, 80, 113, 165, 332, 569, 881, 158, 218, 139, 378, 629, 69, 255, 148, 343, 222, 617, 260, 790, 370, 500, 515, 603, 150, 460, 431, 256, 94, 659, 487, 485, 199, 378, 237, 326, 324, 621, 443, 404, 262, 588, 300, 235, 213, 101, 445, 480, 339, 164, 701, 758, 721, 476, 540, 370, 240, 210, 118, 276, 634, 141, 89, 123, 224, 112, 510, 370, 558, 633, 323, 375, 295, 590, 705, 680, 297, 607, 155, 216, 168, 431, 165, 102, 150, 1238, 689, 386, 397, 438, 396, 81, 598, 962, 446, 212, 249, 889, 1006, 603, 96, 109, 110, 645, 360, 658, 106, 188, 257, 279, 488, 172, 210, 314, 211, 54, 493, 351, 116, 441, 68, 765, 456, 131, 214, 471, 392, 500, 471, 154, 633, 260], 'train_acc': 0.9985294117647059, 'test_acc': 0.0, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 56.1789, Test loss: 26.4014
In this epoch 100/5000, Training loss: 0.7362, Test loss: 2.0329
In this epoch 150/5000, Training loss: 0.1943, Test loss: 1.8144
In this epoch 200/5000, Training loss: 0.0936, Test loss: 1.8429
In this epoch 250/5000, Training loss: 0.0446, Test loss: 1.8649
In this epoch 300/5000, Training loss: 0.0212, Test loss: 1.8809
In this epoch 350/5000, Training loss: 0.0102, Test loss: 1.8920
In this epoch 400/5000, Training loss: 0.0051, Test loss: 1.8996
In this epoch 450/5000, Training loss: 0.0028, Test loss: 1.9046
In this epoch 500/5000, Training loss: 0.0017, Test loss: 1.9078
In this epoch 550/5000, Training loss: 0.0013, Test loss: 1.9097
In this epoch 600/5000, Training loss: 0.0010, Test loss: 1.9108
In this epoch 650/5000, Training loss: 0.0009, Test loss: 1.9114
In this epoch 700/5000, Training loss: 0.0009, Test loss: 1.9117
In this epoch 750/5000, Training loss: 0.0009, Test loss: 1.9118
In this epoch 800/5000, Training loss: 0.0008, Test loss: 1.9118
In this epoch 850/5000, Training loss: 0.0008, Test loss: 1.9117
In this epoch 900/5000, Training loss: 0.0008, Test loss: 1.9116
In this epoch 950/5000, Training loss: 0.0008, Test loss: 1.9114
In this epoch 1000/5000, Training loss: 0.0008, Test loss: 1.9112
In this epoch 1050/5000, Training loss: 0.0007, Test loss: 1.9110
In this epoch 1100/5000, Training loss: 0.0007, Test loss: 1.9107
In this epoch 1150/5000, Training loss: 0.0007, Test loss: 1.9105
In this epoch 1200/5000, Training loss: 0.0007, Test loss: 1.9103
In this epoch 1250/5000, Training loss: 0.0007, Test loss: 1.9100
In this epoch 1300/5000, Training loss: 0.0007, Test loss: 1.9098
In this epoch 1350/5000, Training loss: 0.0007, Test loss: 1.9096
In this epoch 1400/5000, Training loss: 0.0006, Test loss: 1.9094
In this epoch 1450/5000, Training loss: 0.0006, Test loss: 1.9092
In this epoch 1500/5000, Training loss: 0.0006, Test loss: 1.9089
In this epoch 1550/5000, Training loss: 0.0006, Test loss: 1.9087
In this epoch 1600/5000, Training loss: 0.0006, Test loss: 1.9085
In this epoch 1650/5000, Training loss: 0.0006, Test loss: 1.9083
In this epoch 1700/5000, Training loss: 0.0006, Test loss: 1.9081
In this epoch 1750/5000, Training loss: 0.0006, Test loss: 1.9079
In this epoch 1800/5000, Training loss: 0.0006, Test loss: 1.9077
In this epoch 1850/5000, Training loss: 0.0006, Test loss: 1.9075
In this epoch 1900/5000, Training loss: 0.0005, Test loss: 1.9073
In this epoch 1950/5000, Training loss: 0.0005, Test loss: 1.9071
In this epoch 2000/5000, Training loss: 0.0005, Test loss: 1.9069
In this epoch 2050/5000, Training loss: 2.5181, Test loss: 19.4095
In this epoch 2100/5000, Training loss: 0.0718, Test loss: 1.9337
In this epoch 2150/5000, Training loss: 0.0014, Test loss: 1.9064
In this epoch 2200/5000, Training loss: 0.0005, Test loss: 1.9063
In this epoch 2250/5000, Training loss: 0.0005, Test loss: 1.9061
In this epoch 2300/5000, Training loss: 0.0005, Test loss: 1.9059
In this epoch 2350/5000, Training loss: 0.0005, Test loss: 1.9057
In this epoch 2400/5000, Training loss: 0.0005, Test loss: 1.9056
In this epoch 2450/5000, Training loss: 2.1228, Test loss: 6.0059
In this epoch 2500/5000, Training loss: 0.1534, Test loss: 3.6616
In this epoch 2550/5000, Training loss: 0.0174, Test loss: 1.9052
In this epoch 2600/5000, Training loss: 0.0006, Test loss: 1.9052
In this epoch 2650/5000, Training loss: 0.0005, Test loss: 1.9048
In this epoch 2700/5000, Training loss: 0.0005, Test loss: 1.9046
In this epoch 2750/5000, Training loss: 0.0005, Test loss: 1.9045
In this epoch 2800/5000, Training loss: 0.0005, Test loss: 1.9044
In this epoch 2850/5000, Training loss: 0.0008, Test loss: 1.9050
In this epoch 2900/5000, Training loss: 0.0776, Test loss: 13.1705
In this epoch 2950/5000, Training loss: 0.1123, Test loss: 1.9135
In this epoch 3000/5000, Training loss: 0.0011, Test loss: 1.9043
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 1.9037
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 1.9035
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 1.9034
In this epoch 3200/5000, Training loss: 0.0004, Test loss: 1.9032
In this epoch 3250/5000, Training loss: 0.0004, Test loss: 1.9031
In this epoch 3300/5000, Training loss: 0.0117, Test loss: 1.9248
In this epoch 3350/5000, Training loss: 8.2252, Test loss: 2.0003
In this epoch 3400/5000, Training loss: 0.0819, Test loss: 1.9392
In this epoch 3450/5000, Training loss: 0.0006, Test loss: 1.9032
In this epoch 3500/5000, Training loss: 0.0004, Test loss: 1.9025
In this epoch 3550/5000, Training loss: 0.0004, Test loss: 1.9024
In this epoch 3600/5000, Training loss: 0.0004, Test loss: 1.9022
In this epoch 3650/5000, Training loss: 0.0004, Test loss: 1.9021
In this epoch 3700/5000, Training loss: 0.0004, Test loss: 1.9020
In this epoch 3750/5000, Training loss: 13.8446, Test loss: 37.5536
In this epoch 3800/5000, Training loss: 0.0037, Test loss: 2.0189
In this epoch 3850/5000, Training loss: 0.0006, Test loss: 1.9016
In this epoch 3900/5000, Training loss: 0.0004, Test loss: 1.9016
In this epoch 3950/5000, Training loss: 0.0004, Test loss: 1.9014
In this epoch 4000/5000, Training loss: 0.0004, Test loss: 1.9013
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 1.9012
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 1.9011
In this epoch 4150/5000, Training loss: 0.2998, Test loss: 52.8917
In this epoch 4200/5000, Training loss: 0.0047, Test loss: 2.2007
In this epoch 4250/5000, Training loss: 0.0014, Test loss: 1.9031
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 1.9007
In this epoch 4350/5000, Training loss: 0.0004, Test loss: 1.9006
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 1.9005
In this epoch 4450/5000, Training loss: 0.0004, Test loss: 1.9004
In this epoch 4500/5000, Training loss: 0.0004, Test loss: 1.9003
In this epoch 4550/5000, Training loss: 27.4041, Test loss: 54.3393
In this epoch 4600/5000, Training loss: 1.3236, Test loss: 2.0227
In this epoch 4650/5000, Training loss: 0.0242, Test loss: 1.9085
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 1.9002
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 1.8999
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 1.8998
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 1.8997
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 1.8997
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 1.8996
In this epoch 5000/5000, Training loss: 101.1957, Test loss: 50.3862
train_acc 0.44033791089965396
test_acc 0.452439446366782
{'y_pred': [-6.701328754425049, -5.397085189819336, -6.777041912078857, -7.890933513641357, -6.094165802001953, -4.966113090515137, -0.35782918334007263, -6.040008544921875, -0.23656253516674042, -7.079566478729248, -6.212137222290039, -7.0050272941589355, -7.702608585357666, -7.764810085296631, -5.298178195953369, -6.502176761627197, -6.601032733917236, 1.788586139678955, -5.210399150848389, -7.864126682281494, -0.48962342739105225, -7.345360279083252, -1.090804934501648, -0.8746660351753235, -6.0039591789245605, -5.521557331085205, -6.581712245941162, -5.786287784576416, -7.13626766204834, -5.9960737228393555, -6.585602760314941, -7.5131001472473145, -0.9122262597084045, -5.486064434051514, -7.487509250640869, -5.812716007232666, -6.648413181304932, -5.4671101570129395, -7.249732494354248, -4.789614677429199, -5.488673686981201, -5.582700252532959, -6.803282260894775, -2.040785312652588, -7.815443515777588, -6.647345542907715, -6.608893871307373, -7.539437294006348, -5.818563461303711, -6.8693647384643555, -7.093436241149902, -6.026197910308838, -6.956884860992432, -7.224961280822754, -0.46174952387809753, -6.7216339111328125, -7.902590751647949, -6.6058454513549805, -6.661698341369629, -6.970280170440674, -4.955976486206055, -7.199312686920166, -0.1698905974626541, -5.735379219055176, -6.3666672706604, -5.509564399719238, -1.1961711645126343, -7.94531774520874, -6.326752185821533, -7.343472957611084, -5.665215969085693, -0.4104696214199066, -6.087490558624268, -5.697731971740723, -6.453126430511475, -7.065691947937012, -5.55444860458374, 0.17413237690925598, -5.5745625495910645, -6.660811901092529, -5.798847198486328, -7.7211384773254395, -6.34880256652832, -7.168466567993164, -5.670160293579102, -6.157106399536133, -6.808612823486328, -5.393404006958008, -0.14840291440486908, -6.367157936096191, -0.3323414623737335, -6.327333927154541, -6.004218101501465, -6.34031867980957, -6.648658275604248, -7.7212042808532715, -8.724690437316895, -5.859103679656982, -7.617619514465332, -7.999601364135742, -6.211446285247803, -7.029575824737549, -6.221922397613525, -5.895171642303467, -8.026501655578613, -5.98807954788208, 0.020154837518930435, -5.8795247077941895, -6.546668529510498, -5.601707935333252, -4.896702766418457, -6.574409484863281, -6.316614627838135, -6.81164026260376, -7.078288555145264, -6.819656848907471, -6.589227676391602, -6.385838985443115, -7.3223557472229, -5.422911167144775, -6.712939739227295, -5.56244421005249, -6.356508255004883, -5.973578929901123, -6.3110857009887695, -7.291616439819336, -6.099162578582764, 0.7562769055366516, -1.6064547300338745, -6.009160041809082, 0.051732081919908524, -8.270366668701172, -6.09757661819458, -6.438535213470459, -6.1532135009765625, -6.426035404205322, -6.482545375823975, -6.119365215301514, -5.153500556945801, -5.75826358795166, -6.133378028869629, -5.9832000732421875, -5.725550174713135, -6.20565938949585, -1.226198434829712, -1.7045097351074219, -5.81204080581665, -7.393369674682617, -4.741068363189697, -4.2004265785217285, -5.237057209014893, -6.179662227630615, -1.9423640966415405, -6.564821243286133, -7.667795181274414, -6.160885334014893, -7.6336188316345215, -1.9222379922866821, -2.0779428482055664, -6.521800518035889, -6.467705249786377, -7.251841068267822, -5.616922855377197, -4.353501796722412, -7.069063663482666, -7.286561489105225, -6.922391414642334, -8.273077964782715, -6.510220050811768, -5.690240383148193, -5.7570390701293945, -7.113734722137451, -7.868256092071533, -6.129326820373535, -6.305197715759277, -5.53692626953125, -6.547610759735107, -6.1412177085876465, -5.835093975067139, -6.414173603057861, -1.4003444910049438, -7.096768856048584, -7.858557224273682, -7.1307525634765625, -6.25271463394165, -6.1003265380859375, -8.643091201782227, -7.785492420196533, -5.029219150543213, -6.379469394683838, -6.362344741821289, -6.867347717285156, -6.543759822845459, -7.265813827514648, -5.828373432159424, -6.499906063079834, -7.259791851043701, -5.599833011627197, -7.219573974609375, -6.602769374847412, -5.778793811798096, -5.640511512756348, -7.254893779754639, -7.198246479034424, -5.353816509246826, 0.9864535927772522, -6.178786754608154, -6.452381134033203, -6.09089994430542, -6.536436080932617, -6.825516223907471, -7.326657772064209, -6.676599979400635, -7.977074146270752, -7.022486686706543, -6.344829559326172, -6.733121395111084, -1.3898472785949707, -6.136429309844971, -6.7178730964660645, -7.003574848175049, -6.2777323722839355, -7.014327526092529, -5.602932453155518, -4.995291233062744, -6.579387187957764, -6.970798492431641, -5.394526958465576, -5.303455829620361, -6.4696125984191895, -6.871414661407471, -7.633023262023926, -7.9408440589904785, -5.723110198974609, -6.007419586181641, -1.7186779975891113, -2.5118355751037598, -6.822268486022949, -6.361046314239502, -8.002167701721191, -6.755330562591553, -6.650049686431885, -5.373618125915527, -5.294525623321533, -6.932727813720703, -5.4665327072143555, -7.767563343048096, -6.263715744018555, -6.246631145477295, -8.030149459838867, -7.713856220245361, -7.76960563659668, -6.9891357421875, -5.993308067321777, -6.244603157043457, -8.680410385131836, -6.8816819190979, -6.622742652893066, -5.693935871124268, -6.946485996246338, -5.994263172149658, -5.155811309814453, -5.769888877868652, -5.744261741638184, -6.924347877502441, -7.389283657073975, -6.774686336517334, -7.373810768127441, -4.778782367706299, -6.949917316436768, -7.708564758300781, -6.297080993652344, -7.869391441345215, -6.053225994110107, -6.398858070373535, -0.8141548037528992, 1.4669098854064941, -6.507198333740234, -1.0393699407577515, -7.250368595123291, -1.7861584424972534, -6.169992446899414, 0.1376890242099762, -5.907289981842041, -6.516402244567871, -8.232521057128906, -6.780158996582031, -6.727941036224365, -7.296989917755127, -0.29464635252952576, -0.9784628748893738, -6.805290699005127, -0.7705496549606323, -5.330713748931885, -6.569818019866943, -7.21854829788208, -5.962952136993408, -1.6656949520111084, -7.014597415924072, -5.313418865203857, -7.224193572998047, -8.524930953979492, -8.192719459533691, -7.170853614807129, -0.20785902440547943, -5.547671318054199, -6.695737838745117, -1.5025575160980225, -6.4304728507995605, -4.758800029754639, -5.851409912109375, -5.203177452087402, -6.219022750854492, -6.189907073974609, -7.010650157928467, -6.666393756866455, -0.39901426434516907, -7.887406826019287, -5.304185390472412, -7.441717624664307, -4.9356160163879395, -7.223832607269287, -5.589055061340332, -6.2823710441589355, -5.822988033294678, -7.997315883636475, -6.30971097946167, -7.270144939422607, -8.135725975036621, -5.638280868530273, -6.259140968322754, -6.839352607727051, -1.3820221424102783, -6.478694915771484, -6.424519062042236, -7.130993366241455, -5.815884113311768, -6.861642360687256, -6.135598659515381, -5.729562759399414], 'y_pred_int': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'y_true': [1, 1, 0, 0, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 7, 0, 1, 2, 0, 0, 0, 1, 1, 1, 0, 1, 0, 2, 0, 1, 4, 1, 1, 1, 0, 1, 0, 3, 0, 0, 2, 0, 0, 2, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 3, 2, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 3, 1, 3, 1, 0, 2, 1, 0, 1, 3, 0, 0, 2, 1, 2, 1, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 3, 0, 1, 0, 2, 2, 0, 0, 3, 0, 1, 0, 0, 0, 0, 7, 0, 5, 1, 1, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2, 0, 1, 0, 0, 2, 1, 1, 1, 1, 1, 0, 14, 2, 0, 0, 1, 1, 0, 3, 0, 2, 1, 1, 1, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 2, 0, 2, 0, 1, 0, 4, 2, 0, 0, 0, 5, 0, 0, 1, 1, 0, 2, 2, 0, 1, 0, 2, 1, 0, 0, 2, 0, 2, 1, 1, 1, 1, 0, 1, 1, 3, 1, 1, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 3, 0, 1, 1, 1, 0, 0, 1, 0, 4, 4, 2, 0, 0, 1, 1, 1, 0, 1, 2, 0, 0, 1, 1, 0, 0, 3, 1, 1, 1, 4, 0, 1, 1, 1, 1, 1, 0, 1, 2, 0, 2, 1, 1, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 2, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 4, 0, 1, 1, 1, 0, 0, 1], 'train_acc': 0.44033791089965396, 'test_acc': 0.452439446366782, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 80.8362, Test loss: 153.0986
In this epoch 100/5000, Training loss: 31.0435, Test loss: 129.0108
In this epoch 150/5000, Training loss: 14.7955, Test loss: 133.3712
In this epoch 200/5000, Training loss: 6.9608, Test loss: 136.4200
In this epoch 250/5000, Training loss: 3.3352, Test loss: 138.6136
In this epoch 300/5000, Training loss: 1.6857, Test loss: 140.1047
In this epoch 350/5000, Training loss: 0.9393, Test loss: 141.0930
In this epoch 400/5000, Training loss: 0.6000, Test loss: 141.7361
In this epoch 450/5000, Training loss: 0.4432, Test loss: 142.1524
In this epoch 500/5000, Training loss: 0.3675, Test loss: 142.4260
In this epoch 550/5000, Training loss: 0.3277, Test loss: 142.6132
In this epoch 600/5000, Training loss: 0.3037, Test loss: 142.7498
In this epoch 650/5000, Training loss: 0.2864, Test loss: 142.8581
In this epoch 700/5000, Training loss: 0.2721, Test loss: 142.9513
In this epoch 750/5000, Training loss: 0.2591, Test loss: 143.0372
In this epoch 800/5000, Training loss: 0.2468, Test loss: 143.1203
In this epoch 850/5000, Training loss: 0.2350, Test loss: 143.2033
In this epoch 900/5000, Training loss: 0.2235, Test loss: 143.2874
In this epoch 950/5000, Training loss: 0.2124, Test loss: 143.3734
In this epoch 1000/5000, Training loss: 0.2016, Test loss: 143.4616
In this epoch 1050/5000, Training loss: 0.1912, Test loss: 143.5522
In this epoch 1100/5000, Training loss: 0.1813, Test loss: 143.6451
In this epoch 1150/5000, Training loss: 0.1717, Test loss: 143.7402
In this epoch 1200/5000, Training loss: 0.1626, Test loss: 143.8372
In this epoch 1250/5000, Training loss: 0.1539, Test loss: 143.9361
In this epoch 1300/5000, Training loss: 0.1457, Test loss: 144.0365
In this epoch 1350/5000, Training loss: 0.1378, Test loss: 144.1383
In this epoch 1400/5000, Training loss: 0.1305, Test loss: 144.2411
In this epoch 1450/5000, Training loss: 0.1235, Test loss: 144.3447
In this epoch 1500/5000, Training loss: 0.1169, Test loss: 144.4489
In this epoch 1550/5000, Training loss: 0.1107, Test loss: 144.5533
In this epoch 1600/5000, Training loss: 0.1049, Test loss: 144.6578
In this epoch 1650/5000, Training loss: 0.0995, Test loss: 144.7621
In this epoch 1700/5000, Training loss: 0.0944, Test loss: 144.8660
In this epoch 1750/5000, Training loss: 0.0939, Test loss: 144.7732
In this epoch 1800/5000, Training loss: 11.6080, Test loss: 148.4203
In this epoch 1850/5000, Training loss: 0.1334, Test loss: 145.2536
In this epoch 1900/5000, Training loss: 0.0792, Test loss: 145.1986
In this epoch 1950/5000, Training loss: 0.0755, Test loss: 145.3161
In this epoch 2000/5000, Training loss: 0.0725, Test loss: 145.4020
In this epoch 2050/5000, Training loss: 0.0696, Test loss: 145.4852
In this epoch 2100/5000, Training loss: 0.0669, Test loss: 145.5679
In this epoch 2150/5000, Training loss: 139.3084, Test loss: 424.6034
In this epoch 2200/5000, Training loss: 2.9168, Test loss: 144.3770
In this epoch 2250/5000, Training loss: 0.0601, Test loss: 145.6147
In this epoch 2300/5000, Training loss: 0.0582, Test loss: 145.8323
In this epoch 2350/5000, Training loss: 0.0563, Test loss: 145.9229
In this epoch 2400/5000, Training loss: 0.0545, Test loss: 145.9876
In this epoch 2450/5000, Training loss: 0.0527, Test loss: 146.0536
In this epoch 2500/5000, Training loss: 0.0510, Test loss: 146.1174
In this epoch 2550/5000, Training loss: 69.6111, Test loss: 241.8721
In this epoch 2600/5000, Training loss: 0.4249, Test loss: 146.4240
In this epoch 2650/5000, Training loss: 0.0500, Test loss: 146.1489
In this epoch 2700/5000, Training loss: 0.0454, Test loss: 146.3536
In this epoch 2750/5000, Training loss: 0.0442, Test loss: 146.4007
In this epoch 2800/5000, Training loss: 0.0429, Test loss: 146.4535
In this epoch 2850/5000, Training loss: 0.0417, Test loss: 146.5069
In this epoch 2900/5000, Training loss: 1.3811, Test loss: 145.3009
In this epoch 2950/5000, Training loss: 2.9540, Test loss: 157.5901
In this epoch 3000/5000, Training loss: 0.0720, Test loss: 146.2985
In this epoch 3050/5000, Training loss: 0.0378, Test loss: 146.6660
In this epoch 3100/5000, Training loss: 0.0367, Test loss: 146.7424
In this epoch 3150/5000, Training loss: 0.0358, Test loss: 146.7856
In this epoch 3200/5000, Training loss: 0.0349, Test loss: 146.8300
In this epoch 3250/5000, Training loss: 0.0340, Test loss: 146.8750
In this epoch 3300/5000, Training loss: 168.2854, Test loss: 167.5256
In this epoch 3350/5000, Training loss: 0.7793, Test loss: 145.5695
In this epoch 3400/5000, Training loss: 0.0351, Test loss: 147.1768
In this epoch 3450/5000, Training loss: 0.0310, Test loss: 147.0376
In this epoch 3500/5000, Training loss: 0.0303, Test loss: 147.0692
In this epoch 3550/5000, Training loss: 0.0297, Test loss: 147.1058
In this epoch 3600/5000, Training loss: 0.0290, Test loss: 147.1434
In this epoch 3650/5000, Training loss: 0.0284, Test loss: 147.1804
In this epoch 3700/5000, Training loss: 138.1873, Test loss: 242.4790
In this epoch 3750/5000, Training loss: 0.8401, Test loss: 146.0403
In this epoch 3800/5000, Training loss: 0.0292, Test loss: 147.3035
In this epoch 3850/5000, Training loss: 0.0262, Test loss: 147.3116
In this epoch 3900/5000, Training loss: 0.0257, Test loss: 147.3451
In this epoch 3950/5000, Training loss: 0.0252, Test loss: 147.3778
In this epoch 4000/5000, Training loss: 0.0247, Test loss: 147.4101
In this epoch 4050/5000, Training loss: 0.0242, Test loss: 147.4427
In this epoch 4100/5000, Training loss: 309.3499, Test loss: 609.9261
In this epoch 4150/5000, Training loss: 0.1714, Test loss: 146.0345
In this epoch 4200/5000, Training loss: 0.0280, Test loss: 147.5250
In this epoch 4250/5000, Training loss: 0.0226, Test loss: 147.5390
In this epoch 4300/5000, Training loss: 0.0221, Test loss: 147.5859
In this epoch 4350/5000, Training loss: 0.0218, Test loss: 147.6158
In this epoch 4400/5000, Training loss: 0.0214, Test loss: 147.6443
In this epoch 4450/5000, Training loss: 0.0211, Test loss: 147.6566
In this epoch 4500/5000, Training loss: 33.0882, Test loss: 146.5238
In this epoch 4550/5000, Training loss: 0.0692, Test loss: 146.8510
In this epoch 4600/5000, Training loss: 0.0215, Test loss: 147.6910
In this epoch 4650/5000, Training loss: 0.0198, Test loss: 147.7760
In this epoch 4700/5000, Training loss: 0.0196, Test loss: 147.7994
In this epoch 4750/5000, Training loss: 0.0193, Test loss: 147.8239
In this epoch 4800/5000, Training loss: 0.0190, Test loss: 147.8490
In this epoch 4850/5000, Training loss: 0.0188, Test loss: 147.8983
In this epoch 4900/5000, Training loss: 18.5310, Test loss: 148.1173
In this epoch 4950/5000, Training loss: 0.0183, Test loss: 148.7360
In this epoch 5000/5000, Training loss: 0.0188, Test loss: 147.8604
train_acc 0.9970588235294118
test_acc 0.04077854671280277
{'y_pred': [10.978765487670898, 5.042705535888672, 10.162392616271973, 9.74756145477295, 10.337735176086426, 13.16203784942627, 19.14952278137207, 11.183531761169434, 10.371196746826172, 20.647581100463867, 12.666447639465332, 11.823037147521973, 16.671472549438477, 14.43118953704834, 10.418124198913574, 11.213499069213867, 10.770377159118652, 13.821067810058594, 11.15355396270752, 10.02079963684082, 9.984972953796387, 11.844853401184082, 20.9593563079834, 16.707334518432617, 20.46467399597168, 13.714959144592285, 13.87128734588623, 14.538146018981934, 13.771222114562988, 15.205753326416016, 21.35005760192871, 6.292933464050293, 15.09334659576416, 10.97697639465332, 7.67369270324707, 15.848922729492188, 9.419922828674316, 13.181218147277832, 17.17386817932129, 20.669816970825195, 12.343376159667969, 10.353277206420898, 10.530658721923828, 18.691043853759766, 17.548681259155273, 8.77287769317627, 17.447439193725586, 12.999844551086426, 18.944740295410156, 13.34721565246582, 13.632989883422852, 6.913522720336914, 20.606164932250977, 13.317782402038574, 10.745879173278809, 0.9397850036621094, 17.401302337646484, 9.03115177154541, 16.6047420501709, 14.776219367980957, 7.321052074432373, 9.334548950195312, 13.361251831054688, 9.94400405883789, 18.808589935302734, 8.527129173278809, 20.927148818969727, 16.206254959106445, 18.703622817993164, 16.165225982666016, 16.336200714111328, 15.66059398651123, 11.533552169799805, 8.18213939666748, 10.489293098449707, 11.156891822814941, 14.389595985412598, 15.765936851501465, 17.459932327270508, 15.590977668762207, 15.384427070617676, 20.679357528686523, 8.390356063842773, 15.05633544921875, 21.126890182495117, 18.499292373657227, 6.1994757652282715, 13.712279319763184, 24.889013290405273, 7.959610939025879, 21.468080520629883, 23.606534957885742, 14.972238540649414, 18.818073272705078, 16.629817962646484, 15.415894508361816, 8.907548904418945, 1.1081116199493408, 8.863090515136719, 14.163681983947754, 14.815491676330566, 10.509237289428711, 17.42018699645996, 3.553067922592163, 13.998082160949707, 6.965847492218018, 13.985203742980957, 1.917525053024292, 8.917523384094238, 10.307291030883789, 13.550623893737793, 19.83307647705078, 8.343677520751953, 9.044292449951172, 14.175814628601074, 16.072782516479492, 11.886100769042969, 15.468548774719238, 12.867255210876465, 14.231414794921875, 15.777568817138672, 10.382102012634277, 18.046232223510742, 16.695697784423828, 18.989402770996094, 13.812472343444824, 10.99990177154541, 10.195114135742188, 11.886595726013184, 6.5605268478393555, 11.108386039733887, 7.332401752471924, 2.537631034851074, 8.131939888000488, 22.665861129760742, 17.336994171142578, 19.88105010986328, 13.03394889831543, 17.998151779174805, 13.794264793395996, 11.662940979003906, 17.548690795898438, 22.891908645629883, 13.285837173461914, 15.562745094299316, 22.17879295349121, 11.900895118713379, 10.68651294708252, 10.874523162841797, 19.201913833618164, 19.849201202392578, 16.6602725982666, 13.687724113464355, 12.308923721313477, 13.913520812988281, 11.768918991088867, 20.3732852935791, 11.622593879699707, 17.06462860107422, 17.18484878540039, 11.310260772705078, 2.279449701309204, 10.8682222366333, 4.209868907928467, 10.01352310180664, 16.4119815826416, 2.7599658966064453, 18.982675552368164, 10.497151374816895, 10.6249361038208, 8.304494857788086, 7.421126842498779, 22.023164749145508, 6.918736457824707, 17.509456634521484, 13.558575630187988, 12.167990684509277, 19.937952041625977, 10.70840072631836, 11.142422676086426, 22.708789825439453, 14.247945785522461, 14.900505065917969, 16.088144302368164, 13.376166343688965, 9.573381423950195, 18.09280776977539, 7.80118989944458, 14.208516120910645, 0.019903799518942833, -1.8298441171646118, 8.91124439239502, 6.159974098205566, 13.544051170349121, 14.201725006103516, 14.118188858032227, 14.358307838439941, 9.589756965637207, 7.7759552001953125, 4.565003871917725, 16.59825325012207, 7.613316059112549, 0.8868904113769531, 14.655022621154785, 22.017866134643555, 14.203341484069824, 24.19285774230957, 4.162374973297119, 21.215879440307617, 12.959315299987793, 7.485079288482666, 6.013195991516113, 15.123393058776855, 12.7349214553833, 4.544262409210205, 8.82559585571289, 14.96016788482666, 13.410609245300293, 13.92629337310791, 10.890406608581543, 12.883816719055176, 5.933065891265869, 17.612272262573242, 16.48430633544922, 11.573503494262695, 15.216902732849121, 20.079071044921875, 13.73950481414795, 12.656825065612793, 10.04380989074707, 13.83161449432373, 17.91209602355957, 14.685014724731445, 13.959976196289062, 18.91343879699707, 19.022686004638672, 23.61398696899414, 12.678872108459473, 3.041395902633667, 15.57856273651123, 5.827263355255127, 10.186186790466309, 16.547849655151367, 13.093941688537598, 22.33017921447754, 7.008497714996338, 16.987224578857422, 9.189199447631836, 18.916492462158203, 12.171902656555176, 13.140685081481934, 18.750499725341797, 15.917248725891113, 10.870359420776367, 12.789421081542969, 10.734503746032715, 17.39612579345703, 9.418496131896973, 14.337409973144531, 5.369715690612793, 16.443662643432617, 17.144454956054688, 14.707558631896973, 15.054464340209961, 14.68132209777832, 13.182456970214844, 13.665596961975098, 17.26202392578125, 23.03912353515625, 12.961033821105957, 8.159367561340332, 11.877579689025879, 7.101660251617432, 12.32547664642334, 11.602903366088867, 13.209965705871582, 16.296655654907227, 9.739226341247559, 15.208324432373047, 10.40152645111084, 16.536542892456055, 12.379667282104492, 10.295126914978027, 11.9110107421875, 8.969365119934082, 11.975552558898926, 12.880324363708496, 11.093789100646973, 16.297706604003906, 18.57784652709961, 6.9358110427856445, 13.457327842712402, 17.9008846282959, 11.398189544677734, 16.627052307128906, 17.32579803466797, 7.85440731048584, 5.48314905166626, 12.847414016723633, 14.841567039489746, 11.225834846496582, 6.310730457305908, 12.328185081481934, 20.092327117919922, 9.309486389160156, 18.04227638244629, 21.26204490661621, 14.189522743225098, 11.47451114654541, 13.25212574005127, 21.769277572631836, 10.271060943603516, 16.53711700439453, 14.997883796691895, 14.377287864685059, 17.088665008544922, 13.399139404296875, 12.951712608337402, 11.564577102661133, 14.694059371948242, 14.154940605163574, 13.765631675720215, 22.5960636138916, 17.150468826293945, 9.70041561126709, 9.971464157104492, 4.525121688842773, 14.92383098602295, 13.652752876281738, 11.006628036499023, 18.627851486206055, 3.0363574028015137, 17.81077003479004, 18.60760498046875, 17.18978500366211, 13.969862937927246, 18.862388610839844, 21.541337966918945, 12.922640800476074, 16.942598342895508], 'y_pred_int': [11, 5, 10, 10, 10, 13, 19, 11, 10, 21, 13, 12, 17, 14, 10, 11, 11, 14, 11, 10, 10, 12, 21, 17, 20, 14, 14, 15, 14, 15, 21, 6, 15, 11, 8, 16, 9, 13, 17, 21, 12, 10, 11, 19, 18, 9, 17, 13, 19, 13, 14, 7, 21, 13, 11, 1, 17, 9, 17, 15, 7, 9, 13, 10, 19, 9, 21, 16, 19, 16, 16, 16, 12, 8, 10, 11, 14, 16, 17, 16, 15, 21, 8, 15, 21, 18, 6, 14, 25, 8, 21, 24, 15, 19, 17, 15, 9, 1, 9, 14, 15, 11, 17, 4, 14, 7, 14, 2, 9, 10, 14, 20, 8, 9, 14, 16, 12, 15, 13, 14, 16, 10, 18, 17, 19, 14, 11, 10, 12, 7, 11, 7, 3, 8, 23, 17, 20, 13, 18, 14, 12, 18, 23, 13, 16, 22, 12, 11, 11, 19, 20, 17, 14, 12, 14, 12, 20, 12, 17, 17, 11, 2, 11, 4, 10, 16, 3, 19, 10, 11, 8, 7, 22, 7, 18, 14, 12, 20, 11, 11, 23, 14, 15, 16, 13, 10, 18, 8, 14, 0, 0, 9, 6, 14, 14, 14, 14, 10, 8, 5, 17, 8, 1, 15, 22, 14, 24, 4, 21, 13, 7, 6, 15, 13, 5, 9, 15, 13, 14, 11, 13, 6, 18, 16, 12, 15, 20, 14, 13, 10, 14, 18, 15, 14, 19, 19, 24, 13, 3, 16, 6, 10, 17, 13, 22, 7, 17, 9, 19, 12, 13, 19, 16, 11, 13, 11, 17, 9, 14, 5, 16, 17, 15, 15, 15, 13, 14, 17, 23, 13, 8, 12, 7, 12, 12, 13, 16, 10, 15, 10, 17, 12, 10, 12, 9, 12, 13, 11, 16, 19, 7, 13, 18, 11, 17, 17, 8, 5, 13, 15, 11, 6, 12, 20, 9, 18, 21, 14, 11, 13, 22, 10, 17, 15, 14, 17, 13, 13, 12, 15, 14, 14, 23, 17, 10, 10, 5, 15, 14, 11, 19, 3, 18, 19, 17, 14, 19, 22, 13, 17], 'y_true': [5, 8, 20, 6, 4, 11, 7, 7, 13, 28, 5, 34, 26, 8, 23, 12, 5, 13, 16, 13, 10, 25, 7, 63, 15, 21, 9, 9, 6, 31, 20, 43, 8, 22, 3, 6, 9, 26, 31, 8, 14, 7, 23, 8, 6, 26, 13, 19, 6, 31, 5, 10, 14, 5, 15, 20, 35, 3, 6, 59, 7, 10, 44, 9, 8, 4, 20, 9, 13, 8, 2, 6, 7, 25, 4, 17, 14, 27, 7, 36, 12, 5, 10, 6, 48, 10, 8, 17, 13, 11, 6, 15, 27, 14, 5, 12, 4, 22, 9, 7, 15, 16, 12, 24, 7, 14, 13, 3, 18, 28, 8, 49, 6, 28, 6, 12, 34, 3, 6, 10, 12, 8, 20, 18, 18, 35, 42, 25, 15, 13, 7, 11, 38, 4, 5, 28, 4, 16, 17, 7, 9, 6, 10, 14, 11, 28, 7, 24, 18, 22, 11, 11, 12, 6, 20, 5, 17, 11, 15, 13, 25, 14, 33, 11, 29, 8, 4, 5, 17, 35, 12, 41, 1, 4, 15, 5, 19, 22, 14, 11, 17, 51, 54, 20, 5, 27, 51, 10, 32, 13, 12, 4, 9, 19, 35, 21, 10, 47, 8, 16, 11, 3, 17, 5, 12, 10, 11, 7, 13, 15, 9, 6, 16, 9, 13, 6, 14, 32, 23, 29, 19, 10, 22, 14, 9, 1, 20, 10, 13, 41, 8, 13, 9, 13, 15, 31, 13, 23, 6, 12, 6, 6, 14, 4, 11, 7, 16, 11, 7, 3, 8, 11, 12, 8, 13, 6, 9, 9, 6, 3, 10, 11, 4, 6, 5, 13, 6, 9, 8, 6, 13, 9, 25, 39, 20, 4, 3, 4, 19, 29, 8, 11, 22, 17, 12, 5, 2, 10, 16, 20, 20, 8, 15, 16, 8, 28, 13, 8, 7, 17, 3, 19, 4, 4, 34, 3, 8, 13, 5, 29, 5, 44, 6, 6, 5, 10, 22, 7, 12, 3, 21, 23, 31, 8, 9, 15, 62, 15, 7, 5, 10, 5, 14, 40, 16, 10, 14, 18, 15, 4], 'train_acc': 0.9970588235294118, 'test_acc': 0.04077854671280277, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 44860.0352, Test loss: 47909.7188
In this epoch 100/5000, Training loss: 38343.2656, Test loss: 46735.6992
In this epoch 150/5000, Training loss: 32447.2422, Test loss: 46845.9844
In this epoch 200/5000, Training loss: 27106.7305, Test loss: 47105.4023
In this epoch 250/5000, Training loss: 22422.2891, Test loss: 47403.7344
In this epoch 300/5000, Training loss: 18394.5352, Test loss: 47730.8828
In this epoch 350/5000, Training loss: 14982.1738, Test loss: 48081.0898
In this epoch 400/5000, Training loss: 12125.8252, Test loss: 48446.4258
In this epoch 450/5000, Training loss: 9759.2197, Test loss: 48817.3086
In this epoch 500/5000, Training loss: 7815.5474, Test loss: 49184.3672
In this epoch 550/5000, Training loss: 6231.3330, Test loss: 49539.7852
In this epoch 600/5000, Training loss: 4948.6836, Test loss: 49877.7773
In this epoch 650/5000, Training loss: 3916.3049, Test loss: 50194.5586
In this epoch 700/5000, Training loss: 3089.7610, Test loss: 50487.9414
In this epoch 750/5000, Training loss: 2431.2017, Test loss: 50757.0352
In this epoch 800/5000, Training loss: 1908.8273, Test loss: 51001.9258
In this epoch 850/5000, Training loss: 1496.2050, Test loss: 51223.3086
In this epoch 900/5000, Training loss: 1171.5654, Test loss: 51422.3203
In this epoch 950/5000, Training loss: 917.1188, Test loss: 51600.3828
In this epoch 1000/5000, Training loss: 718.4183, Test loss: 51759.0312
In this epoch 1050/5000, Training loss: 563.8043, Test loss: 51899.8438
In this epoch 1100/5000, Training loss: 443.9131, Test loss: 52024.4023
In this epoch 1150/5000, Training loss: 351.2600, Test loss: 52134.2188
In this epoch 1200/5000, Training loss: 279.8911, Test loss: 52230.7227
In this epoch 1250/5000, Training loss: 225.0899, Test loss: 52315.2891
In this epoch 1300/5000, Training loss: 183.1350, Test loss: 52389.1367
In this epoch 1350/5000, Training loss: 151.1045, Test loss: 52453.4258
In this epoch 1400/5000, Training loss: 126.7126, Test loss: 52509.2227
In this epoch 1450/5000, Training loss: 108.1785, Test loss: 52557.4727
In this epoch 1500/5000, Training loss: 94.1211, Test loss: 52599.0586
In this epoch 1550/5000, Training loss: 83.4733, Test loss: 52634.8086
In this epoch 1600/5000, Training loss: 75.4141, Test loss: 52665.4062
In this epoch 1650/5000, Training loss: 69.3140, Test loss: 52691.5312
In this epoch 1700/5000, Training loss: 64.6921, Test loss: 52713.7734
In this epoch 1750/5000, Training loss: 61.1828, Test loss: 52732.6641
In this epoch 1800/5000, Training loss: 58.5083, Test loss: 52748.6836
In this epoch 1850/5000, Training loss: 56.4586, Test loss: 52762.2656
In this epoch 1900/5000, Training loss: 54.8752, Test loss: 52773.7773
In this epoch 1950/5000, Training loss: 53.6388, Test loss: 52783.5781
In this epoch 2000/5000, Training loss: 52.6596, Test loss: 52791.9180
In this epoch 2050/5000, Training loss: 51.8708, Test loss: 52799.0938
In this epoch 2100/5000, Training loss: 51.2218, Test loss: 52805.3086
In this epoch 2150/5000, Training loss: 50.6752, Test loss: 52810.7422
In this epoch 2200/5000, Training loss: 50.2028, Test loss: 52815.5703
In this epoch 2250/5000, Training loss: 49.7840, Test loss: 52819.9297
In this epoch 2300/5000, Training loss: 49.4030, Test loss: 52823.9414
In this epoch 2350/5000, Training loss: 49.0484, Test loss: 52827.6875
In this epoch 2400/5000, Training loss: 48.7117, Test loss: 52831.2617
In this epoch 2450/5000, Training loss: 48.3867, Test loss: 52834.7070
In this epoch 2500/5000, Training loss: 48.0685, Test loss: 52838.0938
In this epoch 2550/5000, Training loss: 47.7539, Test loss: 52841.4844
In this epoch 2600/5000, Training loss: 47.4403, Test loss: 52844.8789
In this epoch 2650/5000, Training loss: 47.1260, Test loss: 52848.3125
In this epoch 2700/5000, Training loss: 46.8097, Test loss: 52851.7969
In this epoch 2750/5000, Training loss: 46.4903, Test loss: 52855.3789
In this epoch 2800/5000, Training loss: 46.1672, Test loss: 52859.0469
In this epoch 2850/5000, Training loss: 45.8400, Test loss: 52862.8125
In this epoch 2900/5000, Training loss: 45.5083, Test loss: 52866.6875
In this epoch 2950/5000, Training loss: 45.1720, Test loss: 52870.6719
In this epoch 3000/5000, Training loss: 44.8309, Test loss: 52874.7773
In this epoch 3050/5000, Training loss: 44.4848, Test loss: 52879.0000
In this epoch 3100/5000, Training loss: 44.1340, Test loss: 52883.3555
In this epoch 3150/5000, Training loss: 43.7782, Test loss: 52887.8320
In this epoch 3200/5000, Training loss: 43.4174, Test loss: 52892.4180
In this epoch 3250/5000, Training loss: 43.0518, Test loss: 52897.1289
In this epoch 3300/5000, Training loss: 42.6815, Test loss: 52901.9883
In this epoch 3350/5000, Training loss: 42.3064, Test loss: 52906.9648
In this epoch 3400/5000, Training loss: 41.9268, Test loss: 52912.0703
In this epoch 3450/5000, Training loss: 41.5425, Test loss: 52917.3203
In this epoch 3500/5000, Training loss: 41.1538, Test loss: 52922.6953
In this epoch 3550/5000, Training loss: 40.7607, Test loss: 52928.1992
In this epoch 3600/5000, Training loss: 40.3635, Test loss: 52933.8477
In this epoch 3650/5000, Training loss: 39.9621, Test loss: 52939.6250
In this epoch 3700/5000, Training loss: 39.5568, Test loss: 52945.5312
In this epoch 3750/5000, Training loss: 39.1476, Test loss: 52951.5938
In this epoch 3800/5000, Training loss: 38.7348, Test loss: 52957.7891
In this epoch 3850/5000, Training loss: 38.3184, Test loss: 52964.1406
In this epoch 3900/5000, Training loss: 37.8987, Test loss: 52970.6367
In this epoch 3950/5000, Training loss: 37.4757, Test loss: 52977.2734
In this epoch 4000/5000, Training loss: 37.0498, Test loss: 52984.0586
In this epoch 4050/5000, Training loss: 36.6210, Test loss: 52991.0000
In this epoch 4100/5000, Training loss: 36.1896, Test loss: 52998.0938
In this epoch 4150/5000, Training loss: 35.8497, Test loss: 52988.0820
In this epoch 4200/5000, Training loss: 41.6283, Test loss: 53045.8242
In this epoch 4250/5000, Training loss: 35.0051, Test loss: 53009.8125
In this epoch 4300/5000, Training loss: 34.5722, Test loss: 53025.9883
In this epoch 4350/5000, Training loss: 34.1791, Test loss: 53032.9883
In this epoch 4400/5000, Training loss: 33.7839, Test loss: 53040.1953
In this epoch 4450/5000, Training loss: 33.3867, Test loss: 53047.5352
In this epoch 4500/5000, Training loss: 32.9881, Test loss: 53053.9883
In this epoch 4550/5000, Training loss: 59.5116, Test loss: 53217.9727
In this epoch 4600/5000, Training loss: 32.3964, Test loss: 53059.8438
In this epoch 4650/5000, Training loss: 31.8980, Test loss: 53075.4375
In this epoch 4700/5000, Training loss: 31.5392, Test loss: 53083.7305
In this epoch 4750/5000, Training loss: 31.1796, Test loss: 53091.0469
In this epoch 4800/5000, Training loss: 30.8186, Test loss: 53098.5820
In this epoch 4850/5000, Training loss: 30.4564, Test loss: 53106.3672
In this epoch 4900/5000, Training loss: 117.8767, Test loss: 53442.2852
In this epoch 4950/5000, Training loss: 29.8907, Test loss: 53117.9531
In this epoch 5000/5000, Training loss: 29.4476, Test loss: 53130.0938
train_acc 0.9955882352941177
test_acc 0.0
{'y_pred': [450.5802001953125, 274.470947265625, 296.8561706542969, 241.60572814941406, 346.4604797363281, 453.3390808105469, 262.94036865234375, 409.87652587890625, 440.6011047363281, 165.37559509277344, 513.4794311523438, 454.3469543457031, 207.56370544433594, 313.7759704589844, 357.77764892578125, 336.43896484375, 360.9194030761719, 397.5132141113281, 118.61611938476562, 383.3471374511719, 306.3560791015625, 223.54132080078125, 408.0690612792969, 276.74737548828125, 363.09771728515625, 376.219970703125, 351.28076171875, 359.679931640625, 501.779541015625, 348.1692810058594, 529.3375854492188, 378.8927001953125, 333.90057373046875, 423.07275390625, 401.5269775390625, 308.97601318359375, 308.8032531738281, 447.7637939453125, 359.6668701171875, 382.3858642578125, 244.1529541015625, 326.638916015625, 470.03955078125, 353.2011413574219, 408.4808044433594, 355.91839599609375, 319.088623046875, 516.7913208007812, 444.2569885253906, 279.7900695800781, 309.787353515625, 405.65020751953125, 388.3492126464844, 305.33966064453125, 484.4416809082031, 435.45281982421875, 335.055908203125, 92.28085327148438, 435.34112548828125, 284.0963134765625, 381.3356628417969, 232.44891357421875, 517.7536010742188, 269.8980407714844, 223.65696716308594, 321.37939453125, 348.8809814453125, 414.48529052734375, 384.3712158203125, 254.17164611816406, 442.947509765625, 340.8501892089844, 343.496826171875, 311.06298828125, 386.88128662109375, 522.3418579101562, 275.731689453125, 339.521240234375, 353.8070373535156, 252.93399047851562, 236.68426513671875, 418.2376403808594, 253.0518341064453, 493.1753234863281, 171.2200469970703, 361.204833984375, 570.3499145507812, 247.60665893554688, 533.2140502929688, 396.821044921875, 427.6126708984375, 492.3037109375, 379.74151611328125, 516.1927490234375, 287.31549072265625, 225.61570739746094, 405.89398193359375, 461.6845703125, 491.3280029296875, 337.8736877441406, 437.04583740234375, 181.5613250732422, 474.1021728515625, 358.94329833984375, 411.3605041503906, 335.9759216308594, 302.0272216796875, 309.72772216796875, 452.181640625, 372.1016845703125, 405.5708312988281, 277.679931640625, 393.3924560546875, 425.93658447265625, 324.1654968261719, 358.46466064453125, 321.8583984375, 365.663330078125, 416.96624755859375, 370.5425720214844, 416.6002197265625, 461.1723937988281, 207.77474975585938, 141.6363067626953, 277.6561584472656, 273.2590637207031, 270.9560546875, 592.2985229492188, 453.2151794433594, 268.85186767578125, 289.7412109375, 441.41082763671875, 370.3297119140625, 233.0659637451172, 421.85247802734375, 454.9388427734375, 579.2333374023438, 473.258544921875, 409.97430419921875, 435.70281982421875, 301.6733093261719, 407.9246520996094, 268.02783203125, 408.865478515625, 568.5922241210938, 356.3495178222656, 337.3544006347656, 358.91497802734375, 267.29278564453125, 269.3136901855469, 239.3446807861328, 426.3057556152344, 411.5714416503906, 368.30926513671875, 358.4985656738281, 442.56280517578125, 553.3506469726562, 761.3743896484375, 151.6887969970703, 315.0611267089844, 342.09149169921875, 507.8928527832031, 451.4975891113281, 311.0108947753906, 221.78945922851562, 341.9012145996094, 335.2720031738281, 321.8817138671875, 422.60980224609375, 344.51483154296875, 454.10235595703125, 228.7989501953125, 410.130126953125, 749.9676513671875, 300.4416809082031, 497.338134765625, 204.58555603027344, 368.94073486328125, 410.12066650390625, 359.2152404785156, 327.8951110839844, 355.7818603515625, 391.0967712402344, 327.0299072265625, 315.18212890625, 457.93536376953125, 492.05242919921875, 362.6424560546875, 212.91415405273438, 376.0028991699219, 421.6482849121094, 195.0334014892578, 384.5220947265625, 449.4592590332031, 470.1760559082031, 348.0326232910156, 273.662353515625, 380.92889404296875, 554.1790771484375, 510.26544189453125, 281.0431213378906, 426.422607421875, 349.57733154296875, 277.9559020996094, 302.80755615234375, 391.5356750488281, 511.8976135253906, 441.2501525878906, 231.1761932373047, 262.25506591796875, 348.5176696777344, 526.3217163085938, 140.78973388671875, 294.62213134765625, 382.49725341796875, 331.207763671875, 297.5655517578125, 428.8548889160156, 474.65557861328125, 432.8499755859375, 341.356689453125, 405.58685302734375, 397.7899169921875, 353.6417236328125, 367.28143310546875, 312.2904968261719, 382.1695556640625, 435.0235595703125, 277.64874267578125, 250.4843292236328, 470.7549743652344, 370.8988342285156, 242.2540740966797, 313.00390625, 388.0841369628906, 462.7029724121094, 391.5744934082031, 196.70709228515625, 520.4395751953125, 490.925048828125, 466.5451354980469, 359.7688903808594, 378.4248046875, 300.81036376953125, 402.1556701660156, 511.03387451171875, 370.6877746582031, 521.6698608398438, 368.3730773925781, 405.7004699707031, 434.2839050292969, 261.5653991699219, 425.6457824707031, 397.60894775390625, 433.7950439453125, 300.602294921875, 395.62066650390625, 386.52178955078125, 263.99444580078125, 388.500244140625, 398.85650634765625, 356.9281005859375, 416.246826171875, 284.3145751953125, 438.0113220214844, 435.75286865234375, 351.6533508300781, 354.793212890625, 398.38568115234375, 362.1697998046875, 351.2090759277344, 151.08792114257812, 285.17205810546875, 319.7575378417969, 412.549072265625, 283.89697265625, 307.06329345703125, 414.0613708496094, 243.09732055664062, 316.59930419921875, 341.340576171875, 546.01611328125, 353.94854736328125, 326.114013671875, 333.663330078125, 108.33910369873047, 343.1050109863281, 317.19061279296875, 308.3360900878906, 498.3619079589844, 349.7889099121094, 380.7168273925781, 398.4940490722656, 295.0518798828125, 307.4487609863281, 357.8607482910156, 461.4683837890625, 330.6062316894531, 484.93536376953125, 439.1259765625, 415.24560546875, 235.72833251953125, 240.80755615234375, 301.76287841796875, 447.410888671875, 482.76666259765625, 115.87346649169922, 649.4801025390625, 362.588134765625, 466.19354248046875, 348.947509765625, 361.0064697265625, 434.2918701171875, 380.5689697265625, 359.5718994140625, 467.7774658203125, 195.17465209960938, 357.417724609375, 456.4295349121094, 221.90989685058594, 200.8562774658203, 265.35888671875, 315.29693603515625, 478.4560241699219, 386.4242248535156, 444.13604736328125, 220.06436157226562, 307.4173278808594, 373.3143310546875, 342.75872802734375, 479.0052185058594, 284.4147644042969, 335.486083984375, 262.3299560546875, 518.4771728515625, 495.8001708984375, 326.79644775390625, 378.3480529785156, 397.81060791015625, 276.12762451171875], 'y_pred_int': [451, 274, 297, 242, 346, 453, 263, 410, 441, 165, 513, 454, 208, 314, 358, 336, 361, 398, 119, 383, 306, 224, 408, 277, 363, 376, 351, 360, 502, 348, 529, 379, 334, 423, 402, 309, 309, 448, 360, 382, 244, 327, 470, 353, 408, 356, 319, 517, 444, 280, 310, 406, 388, 305, 484, 435, 335, 92, 435, 284, 381, 232, 518, 270, 224, 321, 349, 414, 384, 254, 443, 341, 343, 311, 387, 522, 276, 340, 354, 253, 237, 418, 253, 493, 171, 361, 570, 248, 533, 397, 428, 492, 380, 516, 287, 226, 406, 462, 491, 338, 437, 182, 474, 359, 411, 336, 302, 310, 452, 372, 406, 278, 393, 426, 324, 358, 322, 366, 417, 371, 417, 461, 208, 142, 278, 273, 271, 592, 453, 269, 290, 441, 370, 233, 422, 455, 579, 473, 410, 436, 302, 408, 268, 409, 569, 356, 337, 359, 267, 269, 239, 426, 412, 368, 358, 443, 553, 761, 152, 315, 342, 508, 451, 311, 222, 342, 335, 322, 423, 345, 454, 229, 410, 750, 300, 497, 205, 369, 410, 359, 328, 356, 391, 327, 315, 458, 492, 363, 213, 376, 422, 195, 385, 449, 470, 348, 274, 381, 554, 510, 281, 426, 350, 278, 303, 392, 512, 441, 231, 262, 349, 526, 141, 295, 382, 331, 298, 429, 475, 433, 341, 406, 398, 354, 367, 312, 382, 435, 278, 250, 471, 371, 242, 313, 388, 463, 392, 197, 520, 491, 467, 360, 378, 301, 402, 511, 371, 522, 368, 406, 434, 262, 426, 398, 434, 301, 396, 387, 264, 389, 399, 357, 416, 284, 438, 436, 352, 355, 398, 362, 351, 151, 285, 320, 413, 284, 307, 414, 243, 317, 341, 546, 354, 326, 334, 108, 343, 317, 308, 498, 350, 381, 398, 295, 307, 358, 461, 331, 485, 439, 415, 236, 241, 302, 447, 483, 116, 649, 363, 466, 349, 361, 434, 381, 360, 468, 195, 357, 456, 222, 201, 265, 315, 478, 386, 444, 220, 307, 373, 343, 479, 284, 335, 262, 518, 496, 327, 378, 398, 276], 'y_true': [535, 831, 408, 218, 221, 189, 656, 249, 233, 457, 378, 911, 106, 292, 215, 1027, 109, 258, 599, 266, 174, 577, 76, 86, 758, 230, 123, 187, 172, 370, 367, 241, 512, 680, 89, 286, 738, 706, 621, 157, 326, 352, 211, 278, 173, 130, 126, 327, 241, 227, 343, 280, 135, 344, 222, 376, 434, 397, 878, 181, 433, 100, 215, 78, 453, 682, 167, 659, 436, 84, 185, 232, 226, 80, 633, 466, 769, 202, 607, 478, 116, 455, 279, 373, 243, 601, 173, 656, 493, 124, 284, 545, 232, 264, 289, 114, 370, 419, 158, 295, 249, 175, 156, 675, 217, 212, 778, 350, 431, 133, 169, 122, 786, 415, 331, 110, 505, 236, 209, 198, 135, 434, 200, 260, 367, 486, 312, 158, 163, 403, 402, 254, 393, 45, 543, 561, 572, 77, 239, 372, 778, 212, 546, 189, 200, 211, 212, 424, 177, 141, 143, 401, 464, 259, 771, 97, 105, 237, 244, 874, 250, 176, 241, 80, 223, 88, 55, 62, 258, 113, 104, 753, 113, 256, 53, 316, 203, 161, 583, 79, 375, 791, 543, 164, 605, 96, 334, 510, 155, 212, 240, 81, 99, 31, 1062, 676, 443, 115, 790, 446, 392, 247, 75, 435, 580, 204, 569, 423, 676, 350, 76, 659, 372, 155, 365, 111, 215, 117, 463, 276, 607, 343, 99, 295, 126, 269, 412, 243, 182, 379, 236, 169, 59, 194, 795, 540, 842, 566, 148, 132, 456, 239, 387, 325, 87, 361, 92, 590, 898, 649, 260, 108, 901, 607, 207, 93, 582, 239, 220, 205, 447, 330, 164, 351, 244, 310, 138, 367, 514, 428, 212, 180, 452, 373, 111, 370, 263, 80, 193, 546, 281, 550, 185, 206, 461, 302, 539, 372, 792, 200, 445, 396, 768, 92, 572, 242, 588, 127, 269, 556, 155, 441, 425, 161, 122, 601, 201, 470, 126, 185, 453, 324, 487, 381, 715, 579, 350, 502, 343, 94, 723, 348, 407, 279, 335, 230, 455, 139, 558, 373, 840, 64, 319, 600, 716, 391, 547, 629, 309, 252], 'train_acc': 0.9955882352941177, 'test_acc': 0.0, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 496.0425, Test loss: 2174.3459
In this epoch 100/5000, Training loss: 18.2955, Test loss: 82.8209
In this epoch 150/5000, Training loss: 0.0207, Test loss: 71.5896
In this epoch 200/5000, Training loss: 0.0008, Test loss: 72.0689
In this epoch 250/5000, Training loss: 0.0001, Test loss: 72.0696
In this epoch 300/5000, Training loss: 0.0000, Test loss: 72.0758
In this epoch 350/5000, Training loss: 0.0000, Test loss: 72.0750
In this epoch 400/5000, Training loss: 0.0000, Test loss: 72.0748
In this epoch 450/5000, Training loss: 0.0000, Test loss: 72.0747
In this epoch 500/5000, Training loss: 0.0000, Test loss: 72.0746
In this epoch 550/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 600/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 650/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 700/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 750/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 800/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 850/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 900/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 950/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 72.0745
In this epoch 2700/5000, Training loss: 49298.3203, Test loss: 10892.2617
In this epoch 2750/5000, Training loss: 127.4279, Test loss: 157.6416
In this epoch 2800/5000, Training loss: 1.1199, Test loss: 72.4209
In this epoch 2850/5000, Training loss: 0.0022, Test loss: 73.3021
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 73.3323
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 73.3486
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 73.3489
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 73.3488
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 73.3488
train_acc 0.34621215397923877
test_acc 0.2894463667820069
{'y_pred': [-3.435753345489502, -13.595154762268066, -13.957161903381348, -1.2387341260910034, -12.821853637695312, 9.91515827178955, 11.747817993164062, 10.132681846618652, -3.448699951171875, 14.913055419921875, 5.697526454925537, 9.697837829589844, -6.425107955932617, -19.117197036743164, 0.12060633301734924, -7.070453643798828, 15.235631942749023, 3.047197103500366, -4.331263542175293, 3.907457113265991, 15.771427154541016, -2.9513444900512695, -16.10675621032715, -5.636013984680176, -10.447098731994629, 5.72515869140625, 3.5195953845977783, -5.027002334594727, 9.849213600158691, 3.2142677307128906, 5.236550807952881, 2.015019416809082, 2.952460765838623, 5.435236930847168, -5.1657233238220215, 8.05759334564209, 2.971177577972412, -2.9690146446228027, 10.180680274963379, -15.066076278686523, 6.115537166595459, -3.0391554832458496, -0.0752612054347992, -16.631803512573242, 22.701244354248047, -14.174002647399902, 0.4868406355381012, 5.7080230712890625, 2.5411336421966553, 2.3818001747131348, 2.612381935119629, 3.773042917251587, -11.822672843933105, -2.2454090118408203, -7.842668056488037, -0.9730619192123413, -0.37349018454551697, 5.310807228088379, 0.08385959267616272, 7.4336347579956055, 12.317792892456055, -3.4876155853271484, -4.664823532104492, 0.773453414440155, -9.861367288976908e-06, -4.151968002319336, 2.5522685050964355, -3.3287668228149414, -5.844691276550293, 1.7442399263381958, -7.256064414978027, -2.553711414337158, 3.428704261779785, -16.032686233520508, -5.542416572570801, -3.679694414138794, 4.787747383117676, 6.833954334259033, -2.8529083728790283, -0.8903365731239319, 2.962754726409912, -4.116610527038574, -10.26484203338623, 8.22834587097168, 3.7242186069488525, -12.228484153747559, 1.8238571882247925, 3.5041990280151367, 1.491094946861267, -11.32203483581543, 17.116910934448242, 5.747282028198242, 6.3820037841796875, 9.853288650512695, 6.453993320465088, -0.7648945450782776, -4.769010543823242, 0.28054800629615784, 5.818521022796631, 2.830848217010498, -5.419343948364258, 4.10618782043457, -7.353468894958496, -4.253286838531494, 2.043341875076294, -14.839386940002441, -1.394729495048523, -0.3868647515773773, 11.12714672088623, 7.711872100830078, 3.9711990356445312, -8.158333778381348, -15.493803977966309, 3.260740041732788, 2.8157553672790527, 4.701613426208496, 0.308467298746109, 11.053519248962402, 1.2205404043197632, -6.431973457336426, -4.533963203430176, -4.925373077392578, -8.842541694641113, 12.519408226013184, 17.1790828704834, 7.773377895355225, -4.650256156921387, -12.963517189025879, -21.037979125976562, 2.0734357833862305, -6.254748821258545, -4.524035930633545, 4.919493675231934, 2.24277925491333, 12.721806526184082, 5.383260726928711, 0.6295170187950134, 8.180331230163574, 0.899135172367096, -0.39268311858177185, -5.4269232749938965, -17.141714096069336, 5.502081871032715, -2.474672317504883, 0.7576512694358826, 9.239144325256348, -0.8934749960899353, -4.294150352478027, 2.495088577270508, -5.7963128089904785, -1.4911937713623047, -4.848914623260498, -3.044097900390625, -7.99180269241333, -9.435464859008789, -3.685774803161621, 17.18899917602539, -5.4512128829956055, -12.789041519165039, -5.954482078552246, -9.479756355285645, -1.5873886346817017, -10.6935396194458, 14.59861946105957, -3.202756404876709, 7.334396839141846, -3.390531063079834, -5.504549980163574, -7.576603889465332, -4.729486465454102, 12.699409484863281, 2.4118380546569824, 1.0342957973480225, 1.2475899457931519, 2.8424224853515625, -27.8892879486084, 1.7785884141921997, -7.070964813232422, -16.822593688964844, -1.2507165670394897, -9.955265045166016, -14.031830787658691, 3.9114184379577637, -1.4253578186035156, 4.083290100097656, 5.088397026062012, -8.347396850585938, 6.761013984680176, 2.915456771850586, -13.011456489562988, -12.708395004272461, -7.859649658203125, -9.23874568939209, 9.2924165725708, 8.960972785949707, -5.515007019042969, -8.434831619262695, -12.921319007873535, -13.015599250793457, -12.934979438781738, -3.31510066986084, 12.103663444519043, -4.149203300476074, 1.0459736585617065, 6.446797847747803, -11.983722686767578, -15.760564804077148, 2.233262538909912, -4.647627353668213, -8.406268119812012, 11.778397560119629, -4.061824321746826, 1.2475155591964722, 6.7910614013671875, 3.34977388381958, -1.1821891069412231, -2.9244775772094727, -16.92280387878418, -1.015708565711975, 8.949246406555176, -2.56703519821167, 6.793680667877197, -3.8207883834838867, 8.081682205200195, 0.755727231502533, -1.1228911876678467, 12.563210487365723, 4.885715484619141, -1.91336190700531, -4.90080451965332, 4.541797637939453, -1.5691627264022827, -9.184399604797363, -2.6606650352478027, 1.5123289823532104, 17.47132682800293, -8.922587394714355, -2.271644353866577, 1.8320287466049194, -1.727821946144104, 3.644343137741089, 9.053446769714355, 0.04308024421334267, -3.203939914703369, -7.041902542114258, 1.7090338468551636, 3.062175750732422, -5.841026782989502, -9.463648796081543, 2.580287456512451, 6.736916542053223, -0.551725447177887, -9.265454292297363, 12.772115707397461, -15.474987030029297, -3.2680118083953857, -3.0341334342956543, 23.916091918945312, -1.4613856077194214, -3.653104782104492, -16.994491577148438, -0.03866681084036827, -4.38560676574707, -6.067023277282715, 8.774435997009277, -9.676310539245605, 1.424603819847107, -2.794602870941162, 6.211179256439209, -15.714723587036133, -6.10404634475708, 6.43008279800415, -7.333527565002441, 8.870682716369629, 9.979473114013672, -7.721902370452881, -5.9000420570373535, -15.495254516601562, -4.601086616516113, -2.202775001525879, -3.1508798599243164, -2.9033141136169434, 4.737485885620117, -1.2525900602340698, 0.7817577719688416, 3.3417911529541016, 0.8888452053070068, 2.2219061851501465, 3.0829198360443115, -8.951828956604004, -5.428282260894775, -0.8668544888496399, -8.692623138427734, -3.218069076538086, -4.2509260177612305, 5.9706878662109375, -0.7435174584388733, -8.623043060302734, 0.24870672821998596, -17.73177146911621, -5.522974967956543, 11.088467597961426, -4.3545145988464355, -0.9364100098609924, 7.49671745300293, -6.778126239776611, 2.08327579498291, -8.414905548095703, -7.38367223739624, -17.379804611206055, -26.098722457885742, 2.4963958263397217, -6.100582122802734, -5.616702079772949, 1.8370376825332642, -8.004725456237793, 8.469653129577637, -1.401169776916504, 9.044785499572754, 13.829878807067871, 0.9428836703300476, 6.244789123535156, -8.734169960021973, 14.38164234161377, -0.7223854660987854, 4.5926513671875, -2.5544378757476807, 9.902654647827148, -1.4821752309799194, -5.11414098739624, 12.160219192504883, -13.07539176940918, -2.9742345809936523, -14.535022735595703, -10.654926300048828, 10.2402982711792, -12.28526496887207, 12.790020942687988, -0.984406054019928, 10.931519508361816], 'y_pred_int': [0, 0, 0, 0, 0, 10, 12, 10, 0, 15, 6, 10, 0, 0, 0, 0, 15, 3, 0, 4, 16, 0, 0, 0, 0, 6, 4, 0, 10, 3, 5, 2, 3, 5, 0, 8, 3, 0, 10, 0, 6, 0, 0, 0, 23, 0, 0, 6, 3, 2, 3, 4, 0, 0, 0, 0, 0, 5, 0, 7, 12, 0, 0, 1, 0, 0, 3, 0, 0, 2, 0, 0, 3, 0, 0, 0, 5, 7, 0, 0, 3, 0, 0, 8, 4, 0, 2, 4, 1, 0, 17, 6, 6, 10, 6, 0, 0, 0, 6, 3, 0, 4, 0, 0, 2, 0, 0, 0, 11, 8, 4, 0, 0, 3, 3, 5, 0, 11, 1, 0, 0, 0, 0, 13, 17, 8, 0, 0, 0, 2, 0, 0, 5, 2, 13, 5, 1, 8, 1, 0, 0, 0, 6, 0, 1, 9, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 15, 0, 7, 0, 0, 0, 0, 13, 2, 1, 1, 3, 0, 2, 0, 0, 0, 0, 0, 4, 0, 4, 5, 0, 7, 3, 0, 0, 0, 0, 9, 9, 0, 0, 0, 0, 0, 0, 12, 0, 1, 6, 0, 0, 2, 0, 0, 12, 0, 1, 7, 3, 0, 0, 0, 0, 9, 0, 7, 0, 8, 1, 0, 13, 5, 0, 0, 5, 0, 0, 0, 2, 17, 0, 0, 2, 0, 4, 9, 0, 0, 0, 2, 3, 0, 0, 3, 7, 0, 0, 13, 0, 0, 0, 24, 0, 0, 0, 0, 0, 0, 9, 0, 1, 0, 6, 0, 0, 6, 0, 9, 10, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 3, 1, 2, 3, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 11, 0, 0, 7, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 8, 0, 9, 14, 1, 6, 0, 14, 0, 5, 0, 10, 0, 0, 12, 0, 0, 0, 0, 10, 0, 13, 0, 11], 'y_true': [1, 3, 1, 7, 0, 1, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 0, 2, 0, 1, 0, 1, 0, 1, 0, 0, 3, 2, 0, 1, 0, 0, 1, 3, 0, 0, 1, 1, 1, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 1, 0, 5, 1, 3, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 3, 1, 2, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 2, 6, 1, 1, 2, 1, 1, 1, 0, 0, 2, 0, 0, 1, 7, 6, 0, 0, 0, 7, 1, 3, 0, 2, 1, 0, 2, 3, 0, 1, 0, 1, 4, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 1, 1, 0, 0, 0, 1, 0, 2, 3, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 2, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 4, 1, 1, 2, 2, 0, 0, 3, 1, 1, 0, 1, 1, 0, 0, 0, 0, 2, 1, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 8, 1, 3, 1, 1, 1, 0, 1, 2, 2, 2, 1, 1, 0, 1, 2, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 3, 0, 0, 1, 1, 0, 0, 0, 1, 2, 1, 1, 0, 0, 1, 0, 2, 1, 1, 1, 0, 2, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 3, 2, 0, 0, 2, 0, 0, 1, 1, 2, 3, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 5, 1, 0, 3, 0, 0, 1], 'train_acc': 0.34621215397923877, 'test_acc': 0.2894463667820069, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 586.1301, Test loss: 1909.2012
In this epoch 100/5000, Training loss: 19.0915, Test loss: 158.5355
In this epoch 150/5000, Training loss: 0.0335, Test loss: 150.7581
In this epoch 200/5000, Training loss: 0.0005, Test loss: 151.6805
In this epoch 250/5000, Training loss: 0.0000, Test loss: 151.7502
In this epoch 300/5000, Training loss: 0.0000, Test loss: 151.7541
In this epoch 350/5000, Training loss: 0.0000, Test loss: 151.7543
In this epoch 400/5000, Training loss: 0.0000, Test loss: 151.7543
In this epoch 450/5000, Training loss: 0.0000, Test loss: 151.7544
In this epoch 500/5000, Training loss: 0.0000, Test loss: 151.7544
In this epoch 550/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 600/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 650/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 700/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 750/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 800/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 850/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 900/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 950/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 151.7545
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 151.7449
In this epoch 2250/5000, Training loss: 4004.9353, Test loss: 2521.8171
In this epoch 2300/5000, Training loss: 31.4511, Test loss: 177.3771
In this epoch 2350/5000, Training loss: 0.1458, Test loss: 151.0872
In this epoch 2400/5000, Training loss: 0.0008, Test loss: 151.7307
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 151.7979
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 151.7935
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 151.7938
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 151.7934
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 151.7937
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 151.7937
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 151.7937
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 151.7937
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 151.7937
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 151.7936
In this epoch 4200/5000, Training loss: 0.1619, Test loss: 149.5635
In this epoch 4250/5000, Training loss: 1216.4479, Test loss: 1833.1304
In this epoch 4300/5000, Training loss: 7.4192, Test loss: 146.3542
In this epoch 4350/5000, Training loss: 0.0144, Test loss: 149.0561
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 148.1190
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 148.1498
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 148.1543
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 148.1544
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 148.1544
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 148.1544
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 148.1544
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 148.1544
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 148.1544
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 148.1544
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 148.1544
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 148.1544
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 148.1544
train_acc 1.0
test_acc 0.026366782006920414
{'y_pred': [20.635549545288086, -0.8834838271141052, 32.70682144165039, 12.442802429199219, 13.482312202453613, 37.63298797607422, 20.906160354614258, 6.218688488006592, 5.535210609436035, 34.7768440246582, 27.28139877319336, 13.967293739318848, 6.218438625335693, 21.37415313720703, 8.934090614318848, 2.80393648147583, 6.640537738800049, 13.044136047363281, 37.119972229003906, 36.58292007446289, 0.6295590996742249, 22.195852279663086, -3.8929409980773926, 45.69307327270508, -8.260760307312012, 19.50531005859375, 16.554271697998047, 15.952300071716309, -9.842645645141602, -12.067728996276855, 13.796996116638184, 29.97084617614746, 7.094395637512207, 44.68614959716797, 18.207416534423828, -17.135190963745117, 26.43641471862793, 31.060670852661133, 19.66416358947754, 17.483800888061523, 19.159116744995117, 23.082910537719727, 27.07164764404297, 6.133011817932129, 15.534919738769531, 2.457939624786377, 14.813081741333008, 15.337053298950195, -9.40652084350586, 14.626568794250488, 13.304381370544434, 1.0680633783340454, 10.772436141967773, 10.258641242980957, 17.22323989868164, 3.4661660194396973, 5.712218761444092, 5.21904182434082, 20.605558395385742, 32.19464874267578, 25.785951614379883, 20.817127227783203, 27.384061813354492, 7.280539035797119, 27.905611038208008, 20.508678436279297, 18.83033561706543, 7.401209354400635, 22.449932098388672, 3.154421329498291, 12.813493728637695, 8.550538063049316, 14.788092613220215, 34.060909271240234, 7.7338786125183105, 17.260128021240234, 23.753679275512695, 15.427428245544434, 23.658830642700195, 15.029740333557129, 9.608154296875, 20.483186721801758, 10.487532615661621, 22.118852615356445, -2.0127758979797363, 4.035926342010498, 5.602131366729736, 18.860897064208984, 21.721158981323242, 16.53132438659668, 28.693679809570312, 20.164087295532227, 12.468897819519043, 12.269536018371582, 6.4534592628479, 6.975794315338135, 26.3547420501709, 17.014095306396484, 18.013463973999023, 25.986480712890625, 6.268883228302002, 6.332467555999756, 23.985143661499023, 32.93752670288086, 18.585540771484375, -12.300154685974121, 10.483781814575195, 1.8156895637512207, 11.24219036102295, 2.5327606201171875, 0.5978513360023499, -6.465742111206055, -15.087481498718262, 19.17508316040039, 16.505990982055664, 6.08845853805542, 21.62509536743164, 2.162391185760498, 18.574235916137695, 22.862947463989258, 15.206068992614746, 18.17942237854004, 28.676668167114258, 17.110515594482422, 27.19492530822754, 17.039735794067383, -5.521910190582275, 20.60498046875, 9.413538932800293, -1.6080207824707031, 30.386018753051758, 19.678064346313477, 5.05000638961792, 12.98046588897705, 35.38986587524414, 1.342843770980835, 9.616800308227539, 21.802490234375, 13.369933128356934, 6.845496654510498, 1.1525590419769287, 17.438060760498047, 9.951956748962402, 22.929763793945312, -2.7589056491851807, 3.8598251342773438, 21.264005661010742, 8.000718116760254, 16.605058670043945, 3.555635690689087, 13.853882789611816, 21.598188400268555, 21.46926498413086, 24.204607009887695, 10.0886869430542, 10.287267684936523, 22.69174575805664, 16.016014099121094, 21.48902702331543, -2.314495801925659, -10.881795883178711, 14.073348045349121, 13.921736717224121, 7.839240074157715, 18.526968002319336, 28.813732147216797, 11.14593505859375, 22.913372039794922, 9.38152027130127, 22.308704376220703, 13.426942825317383, 37.57603454589844, -0.27374595403671265, 7.837739944458008, 3.5960564613342285, 24.219741821289062, 21.287092208862305, 2.029679775238037, -6.643548011779785, 13.197639465332031, 16.355300903320312, 8.017074584960938, 29.000003814697266, 12.032827377319336, 16.102664947509766, 6.222728252410889, 7.58793306350708, 35.70446014404297, 1.0285224914550781, 10.63823413848877, 23.657718658447266, 23.69364356994629, 20.822938919067383, 14.916712760925293, 42.84705352783203, 10.55125904083252, 26.037023544311523, 17.523237228393555, 7.93715238571167, -11.023892402648926, 4.718122959136963, 10.175293922424316, 33.12150192260742, 0.8079343438148499, 8.47785472869873, 6.324728012084961, 25.739301681518555, 20.12224769592285, 10.217555046081543, 17.86445426940918, 5.188613414764404, 18.302013397216797, 23.774982452392578, 30.59755516052246, 19.247655868530273, 33.2528190612793, 36.1567497253418, 31.76246452331543, 19.296659469604492, 4.859432697296143, 10.660538673400879, 10.341261863708496, 25.416406631469727, 13.62467098236084, 13.696511268615723, 20.784414291381836, 28.259565353393555, 10.109989166259766, 17.174560546875, 25.480985641479492, 13.255305290222168, 16.236902236938477, -6.4594244956970215, 30.200490951538086, 25.757631301879883, 25.34340476989746, 20.143596649169922, 20.543621063232422, -9.287520408630371, 27.802831649780273, 7.34216833114624, -9.473322868347168, 19.812292098999023, 15.404515266418457, 14.972123146057129, 17.65471076965332, 6.456017017364502, 9.169072151184082, 11.840848922729492, 0.8321104645729065, 20.726890563964844, -9.4480562210083, 1.903677225112915, 30.58514976501465, 23.435556411743164, 8.504253387451172, 17.429264068603516, 23.445344924926758, 26.732919692993164, 14.077906608581543, 18.87969207763672, 27.28131675720215, 15.737685203552246, 21.030351638793945, 28.092012405395508, 24.82962989807129, 14.764822959899902, 11.624492645263672, -1.6850190162658691, 23.332693099975586, 12.860811233520508, 17.387048721313477, 28.055377960205078, -15.374276161193848, 8.211197853088379, 12.63986873626709, 12.926335334777832, 29.02854347229004, 4.078955173492432, 15.826531410217285, 7.5460004806518555, 20.02216148376465, -0.6931790709495544, -0.756624162197113, 8.400914192199707, -10.757853507995605, 24.859718322753906, -5.531713485717773, 13.520332336425781, 23.9680118560791, 3.888878345489502, 24.55124282836914, 33.16128921508789, 17.55225944519043, 9.437026023864746, 19.98491668701172, 22.880155563354492, 12.552998542785645, 0.8896241784095764, -0.3164066672325134, 21.09715461730957, 16.20448875427246, 25.363746643066406, -2.0496039390563965, 11.997979164123535, -8.965001106262207, 17.688570022583008, 26.954721450805664, -4.127777099609375, 12.5448579788208, 36.37553024291992, 8.482972145080566, 9.982869148254395, 11.712257385253906, 12.210058212280273, 15.984210968017578, 36.24050521850586, 32.2385368347168, 24.807130813598633, 23.01775550842285, -2.1216797828674316, -12.09716510772705, 23.395362854003906, 15.95913028717041, 15.780230522155762, 13.916293144226074, 29.491552352905273, 1.2709965705871582, 18.793949127197266, -1.0310478210449219, 18.473587036132812, 22.63997459411621, 30.909269332885742, 14.976747512817383, 3.2163186073303223, 26.835050582885742, 11.303192138671875, 26.727964401245117, 10.366862297058105, 2.563267230987549], 'y_pred_int': [21, 0, 33, 12, 13, 38, 21, 6, 6, 35, 27, 14, 6, 21, 9, 3, 7, 13, 37, 37, 1, 22, 0, 46, 0, 20, 17, 16, 0, 0, 14, 30, 7, 45, 18, 0, 26, 31, 20, 17, 19, 23, 27, 6, 16, 2, 15, 15, 0, 15, 13, 1, 11, 10, 17, 3, 6, 5, 21, 32, 26, 21, 27, 7, 28, 21, 19, 7, 22, 3, 13, 9, 15, 34, 8, 17, 24, 15, 24, 15, 10, 20, 10, 22, 0, 4, 6, 19, 22, 17, 29, 20, 12, 12, 6, 7, 26, 17, 18, 26, 6, 6, 24, 33, 19, 0, 10, 2, 11, 3, 1, 0, 0, 19, 17, 6, 22, 2, 19, 23, 15, 18, 29, 17, 27, 17, 0, 21, 9, 0, 30, 20, 5, 13, 35, 1, 10, 22, 13, 7, 1, 17, 10, 23, 0, 4, 21, 8, 17, 4, 14, 22, 21, 24, 10, 10, 23, 16, 21, 0, 0, 14, 14, 8, 19, 29, 11, 23, 9, 22, 13, 38, 0, 8, 4, 24, 21, 2, 0, 13, 16, 8, 29, 12, 16, 6, 8, 36, 1, 11, 24, 24, 21, 15, 43, 11, 26, 18, 8, 0, 5, 10, 33, 1, 8, 6, 26, 20, 10, 18, 5, 18, 24, 31, 19, 33, 36, 32, 19, 5, 11, 10, 25, 14, 14, 21, 28, 10, 17, 25, 13, 16, 0, 30, 26, 25, 20, 21, 0, 28, 7, 0, 20, 15, 15, 18, 6, 9, 12, 1, 21, 0, 2, 31, 23, 9, 17, 23, 27, 14, 19, 27, 16, 21, 28, 25, 15, 12, 0, 23, 13, 17, 28, 0, 8, 13, 13, 29, 4, 16, 8, 20, 0, 0, 8, 0, 25, 0, 14, 24, 4, 25, 33, 18, 9, 20, 23, 13, 1, 0, 21, 16, 25, 0, 12, 0, 18, 27, 0, 13, 36, 8, 10, 12, 12, 16, 36, 32, 25, 23, 0, 0, 23, 16, 16, 14, 29, 1, 19, 0, 18, 23, 31, 15, 3, 27, 11, 27, 10, 3], 'y_true': [7, 8, 9, 7, 7, 16, 12, 12, 12, 76, 16, 20, 10, 15, 16, 13, 10, 6, 25, 6, 8, 18, 3, 41, 3, 11, 5, 31, 3, 6, 15, 16, 13, 6, 29, 5, 21, 16, 6, 22, 5, 25, 40, 5, 11, 8, 22, 7, 3, 29, 5, 3, 19, 17, 5, 6, 14, 18, 42, 49, 11, 7, 35, 11, 21, 12, 10, 14, 11, 10, 7, 9, 14, 13, 22, 17, 20, 7, 16, 13, 14, 18, 9, 3, 15, 10, 9, 18, 19, 7, 6, 8, 7, 45, 14, 19, 16, 7, 25, 20, 18, 9, 13, 18, 17, 3, 5, 9, 12, 6, 13, 4, 3, 6, 32, 16, 7, 10, 9, 22, 39, 3, 10, 23, 8, 12, 4, 10, 19, 5, 4, 13, 10, 6, 20, 9, 17, 21, 8, 5, 8, 23, 11, 12, 1, 11, 5, 5, 12, 18, 8, 8, 7, 53, 5, 11, 5, 34, 10, 7, 3, 60, 7, 4, 11, 4, 5, 24, 11, 20, 11, 19, 6, 15, 14, 23, 12, 15, 5, 21, 7, 9, 29, 20, 4, 7, 9, 30, 9, 10, 7, 5, 10, 6, 30, 11, 11, 5, 26, 2, 10, 12, 12, 9, 11, 26, 6, 15, 11, 14, 7, 9, 14, 26, 17, 14, 3, 25, 10, 19, 14, 15, 8, 4, 17, 22, 21, 8, 10, 15, 9, 7, 3, 6, 23, 14, 4, 18, 3, 38, 16, 3, 20, 35, 23, 38, 9, 25, 14, 15, 23, 2, 3, 26, 6, 9, 40, 25, 13, 5, 4, 7, 5, 6, 28, 5, 11, 10, 1, 34, 42, 10, 18, 4, 15, 11, 17, 46, 15, 7, 8, 50, 11, 5, 10, 4, 5, 4, 6, 35, 9, 11, 23, 8, 11, 16, 25, 6, 8, 4, 17, 5, 5, 8, 16, 12, 15, 40, 8, 5, 19, 8, 27, 17, 8, 12, 18, 23, 25, 7, 11, 4, 5, 14, 8, 11, 22, 6, 9, 5, 17, 46, 15, 9, 8, 2, 5, 15, 24, 8], 'train_acc': 1.0, 'test_acc': 0.026366782006920414, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 835.5320, Test loss: 17844.4238
In this epoch 100/5000, Training loss: 1.8087, Test loss: 18202.8184
In this epoch 150/5000, Training loss: 0.0345, Test loss: 18219.9102
In this epoch 200/5000, Training loss: 0.0006, Test loss: 18220.6270
In this epoch 250/5000, Training loss: 0.0004, Test loss: 18220.9219
In this epoch 300/5000, Training loss: 0.0004, Test loss: 18220.9629
In this epoch 350/5000, Training loss: 0.0004, Test loss: 18220.9609
In this epoch 400/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 450/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 500/5000, Training loss: 0.0004, Test loss: 18220.9609
In this epoch 550/5000, Training loss: 0.0004, Test loss: 18220.9609
In this epoch 600/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 650/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 700/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 750/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 800/5000, Training loss: 0.0004, Test loss: 18220.9629
In this epoch 850/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 900/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 950/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 1000/5000, Training loss: 0.0004, Test loss: 18220.9609
In this epoch 1050/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 1100/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 1200/5000, Training loss: 0.0006, Test loss: 18220.6523
In this epoch 1250/5000, Training loss: 739.8385, Test loss: 20002.2031
In this epoch 1300/5000, Training loss: 0.8960, Test loss: 18183.1934
In this epoch 1350/5000, Training loss: 0.0946, Test loss: 18218.4805
In this epoch 1400/5000, Training loss: 0.0011, Test loss: 18221.3027
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 18220.9297
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 18220.9551
In this epoch 1550/5000, Training loss: 17.2636, Test loss: 18167.4746
In this epoch 1600/5000, Training loss: 158.0263, Test loss: 18872.7832
In this epoch 1650/5000, Training loss: 0.3194, Test loss: 18228.0215
In this epoch 1700/5000, Training loss: 0.0064, Test loss: 18222.5508
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 18220.9102
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 18220.9512
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 18220.9570
In this epoch 1900/5000, Training loss: 0.5133, Test loss: 18235.1055
In this epoch 1950/5000, Training loss: 85.2100, Test loss: 18443.3438
In this epoch 2000/5000, Training loss: 1.5279, Test loss: 18252.0938
In this epoch 2050/5000, Training loss: 0.0011, Test loss: 18221.8496
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 18221.0879
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 18220.9707
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 18220.9570
In this epoch 2250/5000, Training loss: 0.0004, Test loss: 18220.9551
In this epoch 2300/5000, Training loss: 50333.2734, Test loss: 66394.6094
In this epoch 2350/5000, Training loss: 3.7425, Test loss: 18489.7656
In this epoch 2400/5000, Training loss: 0.4954, Test loss: 18217.2734
In this epoch 2450/5000, Training loss: 0.0046, Test loss: 18220.9648
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 18220.9453
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 18220.9551
In this epoch 2600/5000, Training loss: 0.0004, Test loss: 18220.9629
In this epoch 2650/5000, Training loss: 0.0004, Test loss: 18220.9609
In this epoch 2700/5000, Training loss: 0.0004, Test loss: 18220.9609
In this epoch 2750/5000, Training loss: 0.0004, Test loss: 18220.9629
In this epoch 2800/5000, Training loss: 0.0004, Test loss: 18220.9922
In this epoch 2850/5000, Training loss: 2149.9626, Test loss: 22652.9355
In this epoch 2900/5000, Training loss: 137.8400, Test loss: 18206.5840
In this epoch 2950/5000, Training loss: 0.0033, Test loss: 18214.6836
In this epoch 3000/5000, Training loss: 0.0023, Test loss: 18220.0625
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 18220.8848
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 18220.9570
In this epoch 3150/5000, Training loss: 0.4753, Test loss: 18209.7852
In this epoch 3200/5000, Training loss: 833.6940, Test loss: 18579.7852
In this epoch 3250/5000, Training loss: 1.7365, Test loss: 18221.4766
In this epoch 3300/5000, Training loss: 0.0010, Test loss: 18220.0703
In this epoch 3350/5000, Training loss: 0.0006, Test loss: 18221.1172
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 18220.9805
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 18220.9609
In this epoch 3500/5000, Training loss: 0.4296, Test loss: 18209.7910
In this epoch 3550/5000, Training loss: 788.5840, Test loss: 18406.2812
In this epoch 3600/5000, Training loss: 4.7347, Test loss: 18233.8262
In this epoch 3650/5000, Training loss: 0.0326, Test loss: 18219.4121
In this epoch 3700/5000, Training loss: 0.0005, Test loss: 18220.7910
In this epoch 3750/5000, Training loss: 0.0004, Test loss: 18220.9492
In this epoch 3800/5000, Training loss: 0.0004, Test loss: 18220.9629
In this epoch 3850/5000, Training loss: 0.0004, Test loss: 18220.9648
In this epoch 3900/5000, Training loss: 0.0004, Test loss: 18220.9414
In this epoch 3950/5000, Training loss: 822.1371, Test loss: 19781.2754
In this epoch 4000/5000, Training loss: 6.8101, Test loss: 18176.3594
In this epoch 4050/5000, Training loss: 0.3557, Test loss: 18225.2676
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 18220.6738
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 18220.9883
In this epoch 4200/5000, Training loss: 0.0004, Test loss: 18220.9590
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 18220.9570
In this epoch 4300/5000, Training loss: 0.9325, Test loss: 18178.2109
In this epoch 4350/5000, Training loss: 3630.9097, Test loss: 21145.2793
In this epoch 4400/5000, Training loss: 18.5148, Test loss: 18320.6797
In this epoch 4450/5000, Training loss: 0.2055, Test loss: 18214.7070
In this epoch 4500/5000, Training loss: 0.0031, Test loss: 18221.8398
In this epoch 4550/5000, Training loss: 3635.7244, Test loss: 19859.1270
In this epoch 4600/5000, Training loss: 0.7977, Test loss: 18273.3613
In this epoch 4650/5000, Training loss: 0.0781, Test loss: 18225.7422
In this epoch 4700/5000, Training loss: 0.0010, Test loss: 18221.1660
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 18220.9863
In this epoch 4800/5000, Training loss: 0.1028, Test loss: 18215.6973
In this epoch 4850/5000, Training loss: 1003.4092, Test loss: 19839.1270
In this epoch 4900/5000, Training loss: 7.0764, Test loss: 18261.8945
In this epoch 4950/5000, Training loss: 0.0176, Test loss: 18218.4863
In this epoch 5000/5000, Training loss: 0.0007, Test loss: 18221.1836
train_acc 0.9992647058823529
test_acc 0.011764705882352941
{'y_pred': [385.63055419921875, 88.41983795166016, 128.23814392089844, 439.0267333984375, 432.3808898925781, 534.6110229492188, 466.4134216308594, 526.4996337890625, 147.30035400390625, 181.50436401367188, 509.82977294921875, 116.80601501464844, 593.077880859375, 422.4515380859375, 199.94482421875, 488.36956787109375, 476.4171447753906, 421.7351989746094, 214.90713500976562, 297.1177673339844, 524.00390625, 545.2344360351562, 450.5985107421875, 534.860107421875, 492.56719970703125, 347.01214599609375, 514.0421752929688, 417.0442810058594, 170.17694091796875, 295.58447265625, 77.8504409790039, 471.73260498046875, 240.89427185058594, 481.73736572265625, 223.74435424804688, 478.78216552734375, 481.67852783203125, 187.43785095214844, 544.0198364257812, 345.5220031738281, 180.9977569580078, 532.373046875, 539.0217895507812, 198.3966522216797, 486.11865234375, 211.9407196044922, 98.30110168457031, 60.49763488769531, 81.0932846069336, 476.17083740234375, 274.6116638183594, 216.6903533935547, 183.07090759277344, 386.19384765625, 512.5499267578125, 188.84315490722656, 126.33106994628906, 449.7695007324219, 480.2001953125, 522.3025512695312, 506.9527587890625, 507.39215087890625, 240.4722442626953, 522.1554565429688, 221.65785217285156, 455.26629638671875, 219.8149871826172, 414.8115234375, 173.85203552246094, 520.1090087890625, 525.8988037109375, 453.85400390625, 110.67423248291016, 458.02801513671875, 504.17926025390625, 275.5011901855469, 470.24176025390625, 372.3648986816406, 411.7447509765625, 458.99029541015625, 456.5211486816406, 497.1019287109375, 283.1701965332031, 173.2113800048828, 186.0998077392578, 525.3148803710938, 469.6405029296875, 323.6092224121094, 336.4812927246094, 125.0024185180664, 167.52386474609375, 464.7457275390625, 152.08128356933594, 187.47088623046875, 182.5292205810547, 454.3409118652344, 165.1700897216797, 536.1654052734375, 174.47357177734375, 280.67559814453125, 155.40945434570312, 88.39888763427734, 130.9312286376953, 559.5847778320312, 134.45603942871094, 412.81793212890625, 491.69091796875, 141.6489715576172, 498.04803466796875, 555.1231079101562, 156.59815979003906, 520.7431640625, 201.61770629882812, 203.491455078125, 460.0734558105469, 416.08929443359375, 100.4057388305664, 231.66297912597656, 210.88723754882812, 523.2444458007812, 418.3175964355469, 175.98031616210938, 262.61688232421875, 499.8106689453125, 167.6348419189453, 54.2331428527832, 556.4124145507812, 63.650211334228516, 464.9048767089844, 447.76025390625, 554.162841796875, 148.5176239013672, 500.57958984375, 349.036376953125, 436.42352294921875, 494.80029296875, 553.9518432617188, 577.2362670898438, 456.4430847167969, 223.12733459472656, 111.39237213134766, 331.5247802734375, 184.91043090820312, 256.5829772949219, 145.5315704345703, 505.4266052246094, 107.63155364990234, 454.7642822265625, 578.4063110351562, 551.7067260742188, 179.65414428710938, 443.5235290527344, 481.11761474609375, 139.04212951660156, 57.657493591308594, 187.09341430664062, 141.89767456054688, 308.58148193359375, 136.1219940185547, 434.0009765625, 472.4330139160156, 590.3875122070312, 470.6753845214844, 214.2456817626953, 372.9861755371094, 271.087646484375, 485.6823425292969, 104.10248565673828, 148.1868133544922, 207.74256896972656, 520.9993286132812, 526.7021484375, 462.16064453125, 175.66741943359375, 447.0186767578125, 164.52044677734375, 507.63104248046875, 536.1810302734375, 28.485836029052734, 197.24407958984375, 472.252197265625, 445.6396484375, 510.1063537597656, 568.8637084960938, 289.4075622558594, 276.7725524902344, 386.4029846191406, 581.0448608398438, 451.8846740722656, 507.5885009765625, 505.3843994140625, 512.023681640625, 157.8699951171875, 109.98405456542969, 166.48143005371094, 278.00439453125, 454.95947265625, 141.53073120117188, 234.6144256591797, 86.17630767822266, 150.0869903564453, 286.2939147949219, 176.6689910888672, 578.2022705078125, 115.62799835205078, 513.2730102539062, 430.8523864746094, 538.8838500976562, 510.89910888671875, 460.5499572753906, 59.06850051879883, 362.7672119140625, 438.22174072265625, 142.79469299316406, 519.1145629882812, 472.89190673828125, 475.5799255371094, 502.0987548828125, 153.84298706054688, 118.58956146240234, 434.1103820800781, 127.37540435791016, 187.912109375, 176.99964904785156, 471.69158935546875, 510.0560302734375, 556.0984497070312, 506.21148681640625, 492.2763671875, 493.37091064453125, 259.81036376953125, 556.7457275390625, 466.30859375, 285.3072509765625, 304.290283203125, 429.16143798828125, 288.451171875, 254.36244201660156, 287.965087890625, 517.776123046875, 502.6485290527344, 165.639404296875, 275.2854309082031, 511.0791015625, 416.9605712890625, 525.65771484375, 202.8263397216797, 454.08905029296875, 207.68504333496094, 467.5599060058594, 494.99615478515625, 113.8733139038086, 400.223388671875, 605.029296875, 275.4076232910156, 194.81350708007812, 274.4529724121094, 517.7184448242188, 528.7528686523438, 384.13763427734375, 534.9153442382812, 53.57523727416992, 495.3327941894531, 143.60963439941406, 154.45108032226562, 482.6505432128906, 238.06529235839844, 525.9351196289062, 447.37530517578125, 108.03620147705078, 482.0473937988281, 230.77108764648438, 518.3236694335938, 574.9326782226562, 276.76220703125, 439.54473876953125, 505.5145263671875, 439.829833984375, 455.8550720214844, 523.9182739257812, 280.9593505859375, 480.3736267089844, 243.9147491455078, 191.44776916503906, 228.7610321044922, 303.6679382324219, 446.86279296875, 422.5633544921875, 347.48541259765625, 396.218017578125, 192.77330017089844, 465.46917724609375, 481.55609130859375, 154.97967529296875, 539.4558715820312, 487.9764404296875, 222.21241760253906, 528.9320068359375, 462.5828857421875, 212.78079223632812, 464.4143981933594, 232.2522735595703, 233.24684143066406, 132.79747009277344, 483.3231201171875, 237.33729553222656, 236.19007873535156, 479.93011474609375, 295.07244873046875, 52.47117233276367, 489.2882080078125, 498.7563781738281, 238.65977478027344, 575.8751220703125, 523.719970703125, 116.56793975830078, 132.92808532714844, 159.89295959472656, 539.622314453125, 354.91259765625, 461.3927001953125, 462.1285400390625, 130.00477600097656, 223.965576171875, 240.2198028564453, 338.0653991699219, 526.3038330078125, 158.90464782714844, 261.34405517578125, 118.6834487915039, 98.59232330322266, 533.7061157226562, 179.89720153808594, 391.1059875488281, 483.06402587890625, 102.3104019165039, 108.36341094970703, 504.4991455078125, 97.56952667236328, 444.8499755859375], 'y_pred_int': [386, 88, 128, 439, 432, 535, 466, 526, 147, 182, 510, 117, 593, 422, 200, 488, 476, 422, 215, 297, 524, 545, 451, 535, 493, 347, 514, 417, 170, 296, 78, 472, 241, 482, 224, 479, 482, 187, 544, 346, 181, 532, 539, 198, 486, 212, 98, 60, 81, 476, 275, 217, 183, 386, 513, 189, 126, 450, 480, 522, 507, 507, 240, 522, 222, 455, 220, 415, 174, 520, 526, 454, 111, 458, 504, 276, 470, 372, 412, 459, 457, 497, 283, 173, 186, 525, 470, 324, 336, 125, 168, 465, 152, 187, 183, 454, 165, 536, 174, 281, 155, 88, 131, 560, 134, 413, 492, 142, 498, 555, 157, 521, 202, 203, 460, 416, 100, 232, 211, 523, 418, 176, 263, 500, 168, 54, 556, 64, 465, 448, 554, 149, 501, 349, 436, 495, 554, 577, 456, 223, 111, 332, 185, 257, 146, 505, 108, 455, 578, 552, 180, 444, 481, 139, 58, 187, 142, 309, 136, 434, 472, 590, 471, 214, 373, 271, 486, 104, 148, 208, 521, 527, 462, 176, 447, 165, 508, 536, 28, 197, 472, 446, 510, 569, 289, 277, 386, 581, 452, 508, 505, 512, 158, 110, 166, 278, 455, 142, 235, 86, 150, 286, 177, 578, 116, 513, 431, 539, 511, 461, 59, 363, 438, 143, 519, 473, 476, 502, 154, 119, 434, 127, 188, 177, 472, 510, 556, 506, 492, 493, 260, 557, 466, 285, 304, 429, 288, 254, 288, 518, 503, 166, 275, 511, 417, 526, 203, 454, 208, 468, 495, 114, 400, 605, 275, 195, 274, 518, 529, 384, 535, 54, 495, 144, 154, 483, 238, 526, 447, 108, 482, 231, 518, 575, 277, 440, 506, 440, 456, 524, 281, 480, 244, 191, 229, 304, 447, 423, 347, 396, 193, 465, 482, 155, 539, 488, 222, 529, 463, 213, 464, 232, 233, 133, 483, 237, 236, 480, 295, 52, 489, 499, 239, 576, 524, 117, 133, 160, 540, 355, 461, 462, 130, 224, 240, 338, 526, 159, 261, 119, 99, 534, 180, 391, 483, 102, 108, 504, 98, 445], 'y_true': [349, 84, 142, 292, 333, 617, 598, 521, 156, 176, 489, 139, 753, 362, 168, 497, 778, 350, 195, 296, 446, 601, 373, 354, 458, 307, 371, 346, 131, 277, 76, 309, 256, 486, 239, 463, 765, 175, 585, 317, 178, 778, 523, 199, 661, 197, 89, 73, 59, 448, 239, 209, 181, 286, 512, 208, 133, 367, 443, 481, 450, 292, 203, 531, 198, 351, 212, 294, 154, 495, 447, 539, 102, 358, 494, 258, 756, 353, 331, 675, 447, 656, 217, 203, 179, 433, 385, 322, 291, 116, 144, 468, 167, 148, 154, 371, 181, 1027, 201, 212, 173, 100, 121, 552, 123, 340, 412, 146, 1695, 526, 156, 440, 207, 202, 600, 387, 108, 234, 200, 833, 376, 164, 266, 842, 168, 62, 434, 53, 380, 510, 828, 121, 355, 309, 428, 406, 423, 550, 382, 193, 101, 284, 161, 235, 153, 591, 93, 560, 787, 461, 204, 286, 391, 122, 58, 204, 141, 279, 144, 624, 474, 1099, 469, 238, 373, 276, 359, 90, 129, 279, 706, 1238, 441, 173, 380, 194, 327, 655, 41, 203, 367, 635, 773, 721, 276, 257, 324, 517, 438, 421, 389, 607, 158, 126, 153, 256, 481, 122, 240, 88, 156, 205, 176, 399, 102, 484, 296, 464, 1027, 347, 65, 248, 588, 132, 414, 338, 384, 681, 169, 138, 624, 128, 169, 141, 370, 846, 634, 544, 726, 542, 235, 748, 608, 258, 235, 297, 249, 247, 292, 535, 491, 166, 252, 611, 313, 465, 212, 634, 237, 446, 498, 99, 297, 680, 294, 222, 250, 350, 507, 331, 659, 54, 373, 135, 148, 815, 221, 392, 690, 106, 422, 216, 453, 425, 300, 373, 513, 339, 398, 556, 258, 801, 234, 195, 192, 269, 699, 386, 314, 296, 203, 449, 288, 155, 352, 350, 208, 477, 387, 217, 340, 189, 200, 143, 539, 210, 247, 723, 291, 57, 369, 492, 213, 903, 716, 84, 118, 164, 783, 331, 395, 448, 89, 226, 260, 327, 802, 154, 258, 92, 111, 401, 183, 315, 629, 131, 148, 1006, 103, 588], 'train_acc': 0.9992647058823529, 'test_acc': 0.011764705882352941, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_1_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 29896.4590, Test loss: 13500.6445
In this epoch 100/5000, Training loss: 187.5714, Test loss: 214.5329
In this epoch 150/5000, Training loss: 0.0078, Test loss: 145.9072
In this epoch 200/5000, Training loss: 0.0022, Test loss: 142.1455
In this epoch 250/5000, Training loss: 0.0003, Test loss: 142.0432
In this epoch 300/5000, Training loss: 0.0002, Test loss: 142.0489
In this epoch 350/5000, Training loss: 0.0002, Test loss: 142.0512
In this epoch 400/5000, Training loss: 0.0001, Test loss: 142.0540
In this epoch 450/5000, Training loss: 0.0001, Test loss: 142.0566
In this epoch 500/5000, Training loss: 0.0001, Test loss: 142.0591
In this epoch 550/5000, Training loss: 0.0001, Test loss: 142.0614
In this epoch 600/5000, Training loss: 0.0000, Test loss: 142.0635
In this epoch 650/5000, Training loss: 0.0000, Test loss: 142.0653
In this epoch 700/5000, Training loss: 0.0000, Test loss: 142.0668
In this epoch 750/5000, Training loss: 0.0000, Test loss: 142.0682
In this epoch 800/5000, Training loss: 0.0000, Test loss: 142.0694
In this epoch 850/5000, Training loss: 0.0000, Test loss: 142.0704
In this epoch 900/5000, Training loss: 0.0000, Test loss: 142.0712
In this epoch 950/5000, Training loss: 0.0000, Test loss: 142.0719
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 142.0724
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 142.0729
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 142.0732
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 142.0735
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 142.0737
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 142.0739
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 142.0741
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 142.0742
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 142.0742
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 142.0743
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 142.0743
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 142.0743
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 142.0744
In this epoch 3950/5000, Training loss: 152042.7500, Test loss: 338675.6875
In this epoch 4000/5000, Training loss: 1771.0302, Test loss: 1627.9971
In this epoch 4050/5000, Training loss: 5.8429, Test loss: 140.9576
In this epoch 4100/5000, Training loss: 0.0340, Test loss: 141.1095
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 142.1268
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 142.0575
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 142.0618
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 142.0610
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 142.0608
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 142.0607
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 142.0606
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 142.0605
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 142.0605
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 142.0604
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 142.0604
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 142.0603
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 142.0603
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 142.0602
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 142.0602
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 142.0602
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 142.0601
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 142.0601
train_acc 0.3458034169550173
test_acc 0.2036764705882353
{'y_pred': [-0.11521992087364197, 2.417395830154419, 16.882413864135742, 1.8108254671096802, 2.7116777896881104, 15.550658226013184, 9.56884765625, 15.767230033874512, 16.99309730529785, -13.988539695739746, -4.488970756530762, 4.266366958618164, -1.933113694190979, 12.618474960327148, 14.060503005981445, 5.864212989807129, 7.01587438583374, -7.350070953369141, 5.245052814483643, 29.130016326904297, 4.916406154632568, -2.77051043510437, 25.523435592651367, 1.284917950630188, -10.941819190979004, -10.416828155517578, 0.5911606550216675, 4.313576698303223, -0.4843228757381439, -0.8367260694503784, 9.681821823120117, 2.082458734512329, 12.086941719055176, 0.023835808038711548, 21.13283920288086, 8.156320571899414, 10.302849769592285, -8.791662216186523, 7.123928070068359, -1.3092429637908936, -2.350665807723999, 15.380867004394531, 11.283244132995605, -0.14889606833457947, -8.891706466674805, 1.0249351263046265, 3.6936585903167725, 10.655034065246582, 5.02107048034668, -4.521794319152832, -22.42693328857422, 16.757253646850586, -1.8457902669906616, 11.739761352539062, -17.90450668334961, 33.29828643798828, 1.7829760313034058, 11.344747543334961, -5.029733657836914, 16.16874122619629, 7.857892990112305, 7.49201774597168, -3.6069676876068115, -3.996443271636963, -4.5436787605285645, -10.140340805053711, 8.221638679504395, -15.31484317779541, 18.27410888671875, 1.8767000436782837, 10.818880081176758, 15.565279960632324, -12.598771095275879, -20.242918014526367, 1.3094693422317505, 8.8965425491333, 14.215764045715332, 3.8367269039154053, 5.6904754638671875, -9.764626502990723, 6.8521857261657715, -2.1491029262542725, 19.573902130126953, 9.308585166931152, 21.291240692138672, 23.569046020507812, -12.31045150756836, 17.809175491333008, 11.883115768432617, -2.1356494426727295, -2.201120615005493, -6.731085300445557, -28.17178726196289, -1.5959142446517944, 16.923480987548828, 7.986193656921387, -0.5810307264328003, -4.793062210083008, 21.01719093322754, -3.666616976261139e-06, -1.3313803672790527, 2.1145341396331787, -13.388768196105957, 12.28235912322998, -14.165087699890137, 16.7630672454834, -4.1955671310424805, 9.523313522338867, 32.77479553222656, 14.030303001403809, -6.755777835845947, 8.072165489196777, 9.967158317565918, 6.57177734375, -0.6915768384933472, 1.5933457612991333, -2.58339524269104, 7.698358058929443, -6.602104187011719, 15.54394245147705, -8.617490768432617, 9.161142349243164, -1.0335415601730347, 7.017859935760498, -4.642451763153076, 17.395824432373047, 7.276176452636719, 14.463945388793945, -1.5311535596847534, 19.552330017089844, 3.6968600749969482, 11.44555377960205, 21.20197296142578, 16.806886672973633, 16.696502685546875, 7.761602401733398, -2.0847699642181396, -12.273858070373535, 16.24600601196289, -12.016916275024414, 5.229668140411377, -8.105408668518066, -0.33426031470298767, -10.445368766784668, 4.671200752258301, 12.356880187988281, -13.513254165649414, -18.2232666015625, 8.241273880004883, 5.2271409034729, 15.652737617492676, 20.389312744140625, -2.3464906215667725, 4.576749801635742, 2.5655064582824707, 19.3802433013916, -9.20307445526123, -6.045750617980957, -12.67561149597168, -22.485918045043945, 11.37841510772705, -3.039513349533081, 12.957226753234863, 1.5779553651809692, 22.658588409423828, 11.36473560333252, 13.48000431060791, 2.4585072994232178, 34.297950744628906, -9.064959526062012, 0.020765453577041626, 4.458776473999023, 5.794285297393799, -20.15755844116211, 3.107081174850464, 2.7833797931671143, 4.0504961013793945, 1.0111762285232544, -0.5287612676620483, -15.468534469604492, -7.680551052093506, 8.25248908996582, -15.610454559326172, 11.09207534790039, -8.81074047088623, -5.298001289367676, 3.777705430984497, 16.486045837402344, -0.5578240156173706, 7.818028450012207, 21.120534896850586, 11.21193790435791, -1.0774716138839722, -3.8239266872406006, -14.358259201049805, 4.892275810241699, 5.288957595825195, -0.10163339972496033, 7.165584564208984, -16.383304595947266, 0.42880645394325256, 6.464966297149658, 16.959789276123047, 13.827153205871582, -5.524999618530273, 9.891148567199707, -4.412787914276123, 14.498549461364746, 5.863124847412109, 2.900683641433716, 3.1523430347442627, -4.383339881896973, 1.6943202018737793, 9.44411563873291, 9.37299919128418, 9.89973258972168, 2.809959888458252, 13.242059707641602, 12.743993759155273, 19.81865692138672, -0.8712567090988159, 15.038531303405762, -7.0175371170043945, -9.092116355895996, 4.313254356384277, -13.040654182434082, 19.36779022216797, 2.2268478870391846, -2.05908465385437, 7.989829063415527, 1.9754825830459595, 4.492161750793457, 15.609155654907227, 9.870163917541504, -3.6445252895355225, -0.5885595083236694, 1.828332781791687, 41.61153793334961, 0.9430371522903442, -3.8431546688079834, -0.7811211347579956, -8.308359146118164, 13.807706832885742, -16.832666397094727, 9.84444808959961, -9.160155296325684, -2.35404109954834, 24.305423736572266, -15.248705863952637, 3.757202386856079, 15.948419570922852, -0.43569931387901306, 24.94379425048828, 21.055810928344727, -5.395514965057373, 7.007670879364014, 4.856438636779785, -2.693530797958374, 17.68939971923828, 0.712938666343689, 6.268686771392822, 20.8447265625, 8.758674621582031, 27.438846588134766, 2.775745153427124, -1.0091720819473267, 7.191948890686035, 1.3785244226455688, -16.420425415039062, -11.393585205078125, -4.214714050292969, 7.453642845153809, 8.985841751098633, 1.1837931871414185, 17.103647232055664, 0.2673622667789459, 3.495469808578491, -25.309371948242188, -1.6491869688034058, -10.623869895935059, 6.469089508056641, -9.65956974029541, 35.24489212036133, -16.99633026123047, -26.61966323852539, 11.80035400390625, 11.590677261352539, -6.943235397338867, -23.58282470703125, -1.261006236076355, 18.59308624267578, 18.604450225830078, 9.880658149719238, -2.555804967880249, -6.882927894592285, 7.610753059387207, 2.524244546890259, 5.132203102111816, 4.52006721496582, 17.455978393554688, 19.638288497924805, 15.84909725189209, 10.296401023864746, 8.048152923583984, 10.304754257202148, 15.882241249084473, -4.768799781799316, 7.012260913848877, -16.637224197387695, -5.682810306549072, 23.076461791992188, -0.163991779088974, -6.056155204772949, 3.595952272415161, -7.917719841003418, -4.6496992111206055, -9.339858055114746, 10.215804100036621, -3.65692138671875, 3.9602725505828857, -15.105290412902832, -16.371124267578125, 17.164283752441406, 0.21380963921546936, 30.670957565307617, 16.224559783935547, -11.980676651000977, 16.161203384399414, -10.738412857055664, 4.022077560424805, 3.7631871700286865, -13.713412284851074, -3.122633218765259, 3.2846627235412598, -14.573269844055176, -3.544482946395874, 1.0009316205978394, 9.978299140930176, 0.18624606728553772, -4.245034694671631], 'y_pred_int': [0, 2, 17, 2, 3, 16, 10, 16, 17, 0, 0, 4, 0, 13, 14, 6, 7, 0, 5, 29, 5, 0, 26, 1, 0, 0, 1, 4, 0, 0, 10, 2, 12, 0, 21, 8, 10, 0, 7, 0, 0, 15, 11, 0, 0, 1, 4, 11, 5, 0, 0, 17, 0, 12, 0, 33, 2, 11, 0, 16, 8, 7, 0, 0, 0, 0, 8, 0, 18, 2, 11, 16, 0, 0, 1, 9, 14, 4, 6, 0, 7, 0, 20, 9, 21, 24, 0, 18, 12, 0, 0, 0, 0, 0, 17, 8, 0, 0, 21, 0, 0, 2, 0, 12, 0, 17, 0, 10, 33, 14, 0, 8, 10, 7, 0, 2, 0, 8, 0, 16, 0, 9, 0, 7, 0, 17, 7, 14, 0, 20, 4, 11, 21, 17, 17, 8, 0, 0, 16, 0, 5, 0, 0, 0, 5, 12, 0, 0, 8, 5, 16, 20, 0, 5, 3, 19, 0, 0, 0, 0, 11, 0, 13, 2, 23, 11, 13, 2, 34, 0, 0, 4, 6, 0, 3, 3, 4, 1, 0, 0, 0, 8, 0, 11, 0, 0, 4, 16, 0, 8, 21, 11, 0, 0, 0, 5, 5, 0, 7, 0, 0, 6, 17, 14, 0, 10, 0, 14, 6, 3, 3, 0, 2, 9, 9, 10, 3, 13, 13, 20, 0, 15, 0, 0, 4, 0, 19, 2, 0, 8, 2, 4, 16, 10, 0, 0, 2, 42, 1, 0, 0, 0, 14, 0, 10, 0, 0, 24, 0, 4, 16, 0, 25, 21, 0, 7, 5, 0, 18, 1, 6, 21, 9, 27, 3, 0, 7, 1, 0, 0, 0, 7, 9, 1, 17, 0, 3, 0, 0, 0, 6, 0, 35, 0, 0, 12, 12, 0, 0, 0, 19, 19, 10, 0, 0, 8, 3, 5, 5, 17, 20, 16, 10, 8, 10, 16, 0, 7, 0, 0, 23, 0, 0, 4, 0, 0, 0, 10, 0, 4, 0, 0, 17, 0, 31, 16, 0, 16, 0, 4, 4, 0, 0, 3, 0, 0, 1, 10, 0, 0], 'y_true': [2, 0, 0, 1, 0, 0, 1, 0, 1, 3, 0, 1, 2, 1, 0, 2, 1, 0, 0, 2, 2, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 3, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 3, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 1, 2, 0, 2, 1, 0, 1, 2, 0, 0, 2, 0, 0, 1, 0, 0, 1, 1, 3, 1, 1, 0, 3, 1, 0, 0, 1, 1, 3, 1, 2, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 2, 1, 0, 0, 1, 0, 0, 1, 1, 3, 0, 0, 1, 0, 0, 0, 1, 0, 1, 3, 1, 0, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 4, 1, 0, 2, 0, 0, 2, 0, 0, 2, 1, 1, 1, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 2, 1, 2, 1, 1, 1, 1, 1, 3, 0, 1, 9, 3, 0, 7, 0, 1, 2, 0, 1, 1, 1, 1, 0, 2, 1, 0, 0, 0, 3, 1, 1, 0, 2, 1, 0, 1, 3, 0, 1, 1, 1, 2, 0, 0, 0, 1, 0, 0, 1, 1, 1, 2, 0, 1, 0, 2, 1, 1, 2, 1, 1, 2, 2, 0, 3, 0, 3, 1, 0, 0, 1, 2, 0, 8, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 7, 2, 1, 0, 0, 2, 2, 2, 1, 0, 0, 3, 0, 0, 0, 6, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 2, 0, 1, 0, 1, 1, 0, 1, 0, 2, 0, 1], 'train_acc': 0.3458034169550173, 'test_acc': 0.2036764705882353, 'X': 'X_TANL_layer_1_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_2_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 8970.7354, Test loss: 30665.5293
In this epoch 100/5000, Training loss: 220.2737, Test loss: 428.5803
In this epoch 150/5000, Training loss: 0.0121, Test loss: 430.9962
In this epoch 200/5000, Training loss: 0.0063, Test loss: 431.6240
In this epoch 250/5000, Training loss: 0.0002, Test loss: 431.7690
In this epoch 300/5000, Training loss: 0.0001, Test loss: 431.7827
In this epoch 350/5000, Training loss: 0.0001, Test loss: 431.7830
In this epoch 400/5000, Training loss: 0.0001, Test loss: 431.7838
In this epoch 450/5000, Training loss: 0.0001, Test loss: 431.7845
In this epoch 500/5000, Training loss: 0.0000, Test loss: 431.7850
In this epoch 550/5000, Training loss: 0.0000, Test loss: 431.7855
In this epoch 600/5000, Training loss: 0.0000, Test loss: 431.7862
In this epoch 650/5000, Training loss: 0.0000, Test loss: 431.7870
In this epoch 700/5000, Training loss: 0.0000, Test loss: 431.7876
In this epoch 750/5000, Training loss: 0.0000, Test loss: 431.7881
In this epoch 800/5000, Training loss: 0.0000, Test loss: 431.7886
In this epoch 850/5000, Training loss: 0.0000, Test loss: 431.7891
In this epoch 900/5000, Training loss: 0.0000, Test loss: 431.7894
In this epoch 950/5000, Training loss: 0.0000, Test loss: 431.7897
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 431.7901
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 431.7904
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 431.7906
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 431.7908
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 431.7908
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 431.7910
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 431.7910
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 431.7911
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 431.7911
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 431.7911
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 431.7911
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 431.7911
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 431.7911
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 431.7912
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 431.7912
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 431.7912
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 431.7912
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 431.7912
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 431.7912
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 431.7912
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 431.7912
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 431.7914
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 431.7913
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 431.7921
In this epoch 4250/5000, Training loss: 264768.6250, Test loss: 1193.4650
In this epoch 4300/5000, Training loss: 667.3612, Test loss: 2101.8242
In this epoch 4350/5000, Training loss: 1.8259, Test loss: 432.6057
In this epoch 4400/5000, Training loss: 0.0294, Test loss: 425.4126
In this epoch 4450/5000, Training loss: 0.0001, Test loss: 425.5713
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 425.5431
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 425.5470
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 425.5469
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 425.5467
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 425.5466
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 425.5465
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 425.5463
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 425.5463
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 425.5461
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 425.5461
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 425.5460
train_acc 0.3461040224913495
test_acc 0.244878892733564
{'y_pred': [-33.74647903442383, 34.37283706665039, 16.098817825317383, 41.83698654174805, 46.17995834350586, -6.487342357635498, 30.20772933959961, -13.990046501159668, 4.6551079750061035, -14.641510963439941, 8.558060646057129, -0.7039948105812073, 11.675082206726074, 1.2634080648422241, 5.060586929321289, -21.298290252685547, 28.982778549194336, -0.6488624215126038, -4.136970043182373, 8.879948616027832, 8.305146217346191, 2.74769926071167, 23.127262115478516, -32.25402069091797, -6.112072467803955, 24.312793731689453, 46.71394729614258, 38.91873550415039, -21.315048217773438, 30.159313201904297, -20.1739501953125, 7.9092206954956055, -5.348276138305664, 18.589860916137695, 27.28985595703125, 10.487114906311035, -6.462955951690674, 7.781832218170166, -16.989532470703125, 37.34708786010742, -33.98044204711914, 32.08810806274414, -18.92096710205078, -3.7746806144714355, -9.33281135559082, -8.737194061279297, 14.009369850158691, 5.812581539154053, -1.6517878770828247, 26.575885772705078, -6.639103412628174, -15.027562141418457, -10.919173240661621, 13.525248527526855, 8.228703498840332, -26.284757614135742, -7.378040790557861, 23.230087280273438, -6.019693851470947, 1.6741231679916382, 37.71657180786133, 6.721681594848633, 46.449981689453125, -39.17832565307617, -19.814516067504883, -31.313051223754883, 31.2782039642334, -29.609378814697266, 0.7301573157310486, 49.00437545776367, -4.738129138946533, -20.775957107543945, 0.6288122534751892, -7.754985332489014, 1.5112923383712769, 4.07700777053833, 12.414308547973633, -39.296546936035156, 15.740500450134277, -5.145799160003662, -17.214998245239258, 4.215692043304443, 40.43741226196289, 8.896916389465332, -33.90454864501953, -0.27790600061416626, -18.55759620666504, 1.7829715013504028, -12.316404342651367, -0.20129354298114777, -55.647098541259766, -11.771637916564941, -0.6058850884437561, 13.742080688476562, 28.823904037475586, 27.94721221923828, 11.032923698425293, 6.865433216094971, 33.92316436767578, -0.944155752658844, 13.978517532348633, -12.241536140441895, 0.9993109107017517, -32.19358444213867, -15.089847564697266, -0.4399791359901428, 6.6040425300598145, -8.240862846374512, 13.927515983581543, 6.626536846160889, 13.081131935119629, -22.74433708190918, -11.069376945495605, 19.17584991455078, -7.383594989776611, 11.484013557434082, 15.398905754089355, 30.828346252441406, -37.040287017822266, -16.677701950073242, 20.016708374023438, 9.304459571838379, -1.6288954019546509, 15.403406143188477, -5.177031993865967, 32.627445220947266, 22.939075469970703, -16.33991050720215, 11.629368782043457, -0.9175267815589905, -0.1281038075685501, 19.984806060791016, -11.051006317138672, -13.907382011413574, -17.316295623779297, -0.3152037262916565, 11.45460033416748, -14.43839168548584, 9.1276273727417, -4.8497395515441895, -3.702702045440674, -9.826632499694824, 10.880755424499512, 0.908223569393158, 10.772268295288086, -7.97051761765033e-05, -6.066610336303711, 14.805840492248535, -25.047691345214844, -1.863756775856018, 9.473311424255371, 34.22825622558594, 10.095799446105957, -19.313661575317383, -57.317893981933594, -41.012977600097656, 31.81437110900879, 3.05491304397583, 1.4339431524276733, -25.891958236694336, 31.967761993408203, 9.948314666748047, -3.4754223823547363, 4.228282451629639, 0.8973860144615173, -6.939272403717041, -25.14558219909668, -20.219036102294922, 56.09207534790039, -13.97538948059082, 19.151363372802734, 11.600662231445312, -27.144439697265625, -1.3733612298965454, 1.915274977684021, -11.212079048156738, -6.904045581817627, 2.1046433448791504, 5.198922634124756, -19.28424644470215, -31.87118148803711, -8.521440505981445, 12.75576400756836, 16.982873916625977, 17.20738410949707, 32.28266525268555, 17.35553741455078, 6.9607253074646, -3.6258816719055176, -18.347164154052734, 6.691366195678711, 11.062094688415527, 18.009212493896484, 3.2325682640075684, -21.59274673461914, 7.053403377532959, 24.385089874267578, 22.732967376708984, 25.185270309448242, 0.7291063666343689, 21.135574340820312, 2.3909201622009277, -0.3355427384376526, -0.6816907525062561, 23.63251495361328, 7.745691776275635, 15.71013355255127, -38.96163558959961, 47.66293716430664, 36.56095886230469, -5.7289299964904785, -9.261895179748535, 11.929219245910645, 2.8649697303771973, 16.97002601623535, 24.098575592041016, -14.54948616027832, -13.455400466918945, 41.227455139160156, -2.4147229194641113, -8.008963584899902, 14.704445838928223, 2.928835868835449, -7.170344829559326, -36.395301818847656, -11.651497840881348, 14.440651893615723, -49.09326934814453, -7.8859148025512695, -0.18453557789325714, 43.9503059387207, 4.554371356964111, -8.361152648925781, 15.615086555480957, -6.527164936065674, 21.743953704833984, 30.67518424987793, 25.504098892211914, 6.7849812507629395, 24.720752716064453, -7.467735767364502, -0.5957685112953186, 0.5239424109458923, 18.55076026916504, 33.47526931762695, -5.258360385894775, 23.267057418823242, -0.6056533455848694, 0.22972528636455536, -11.704371452331543, -41.66056823730469, 0.8216142058372498, -13.89575481414795, 50.852054595947266, 6.495471477508545, 8.678018569946289, 16.223833084106445, -10.267447471618652, -6.788218021392822, -12.304064750671387, -20.695499420166016, -26.427406311035156, 30.482059478759766, -29.716533660888672, 16.253211975097656, -15.151009559631348, -15.116020202636719, 3.011946201324463, -9.166997909545898, -15.384888648986816, 9.141575813293457, -11.594680786132812, 3.7877840995788574, -10.744519233703613, 47.85609817504883, 41.58701705932617, -10.412009239196777, 33.75486755371094, -13.944517135620117, -16.244314193725586, -20.275508880615234, 20.925586700439453, 11.401890754699707, 32.02887725830078, 13.24680233001709, -17.006568908691406, 16.55105972290039, 3.693664073944092, -4.186212539672852, 30.059961318969727, 10.054971694946289, 34.78583908081055, 22.33299446105957, -36.63783645629883, -4.299757957458496, -24.90576934814453, 1.1156076192855835, -11.4636869430542, 21.00800132751465, -0.7712121605873108, -21.70990562438965, 3.5570902824401855, 16.330589294433594, 13.12298583984375, -9.471955299377441, 11.249338150024414, -7.558108806610107, -24.633140563964844, -35.51552963256836, 30.797775268554688, -30.935218811035156, 25.89004898071289, -10.12985897064209, 10.672602653503418, 34.80873107910156, 1.380316138267517, 9.319547653198242, -8.624991416931152, -11.118626594543457, 8.757657051086426, -24.063488006591797, 7.8711256980896, -2.7585959434509277, 15.310586929321289, 0.5121802687644958, 31.584293365478516, -43.65304183959961, 19.252483367919922, -0.12267883121967316, -13.0695161819458, 17.259302139282227, 8.502630233764648, 42.84043502807617, 50.88395690917969, -13.47341537475586, -2.3446426391601562, -24.351572036743164, -10.995726585388184, -15.986324310302734, 16.457612991333008], 'y_pred_int': [0, 34, 16, 42, 46, 0, 30, 0, 5, 0, 9, 0, 12, 1, 5, 0, 29, 0, 0, 9, 8, 3, 23, 0, 0, 24, 47, 39, 0, 30, 0, 8, 0, 19, 27, 10, 0, 8, 0, 37, 0, 32, 0, 0, 0, 0, 14, 6, 0, 27, 0, 0, 0, 14, 8, 0, 0, 23, 0, 2, 38, 7, 46, 0, 0, 0, 31, 0, 1, 49, 0, 0, 1, 0, 2, 4, 12, 0, 16, 0, 0, 4, 40, 9, 0, 0, 0, 2, 0, 0, 0, 0, 0, 14, 29, 28, 11, 7, 34, 0, 14, 0, 1, 0, 0, 0, 7, 0, 14, 7, 13, 0, 0, 19, 0, 11, 15, 31, 0, 0, 20, 9, 0, 15, 0, 33, 23, 0, 12, 0, 0, 20, 0, 0, 0, 0, 11, 0, 9, 0, 0, 0, 11, 1, 11, 0, 0, 15, 0, 0, 9, 34, 10, 0, 0, 0, 32, 3, 1, 0, 32, 10, 0, 4, 1, 0, 0, 0, 56, 0, 19, 12, 0, 0, 2, 0, 0, 2, 5, 0, 0, 0, 13, 17, 17, 32, 17, 7, 0, 0, 7, 11, 18, 3, 0, 7, 24, 23, 25, 1, 21, 2, 0, 0, 24, 8, 16, 0, 48, 37, 0, 0, 12, 3, 17, 24, 0, 0, 41, 0, 0, 15, 3, 0, 0, 0, 14, 0, 0, 0, 44, 5, 0, 16, 0, 22, 31, 26, 7, 25, 0, 0, 1, 19, 33, 0, 23, 0, 0, 0, 0, 1, 0, 51, 6, 9, 16, 0, 0, 0, 0, 0, 30, 0, 16, 0, 0, 3, 0, 0, 9, 0, 4, 0, 48, 42, 0, 34, 0, 0, 0, 21, 11, 32, 13, 0, 17, 4, 0, 30, 10, 35, 22, 0, 0, 0, 1, 0, 21, 0, 0, 4, 16, 13, 0, 11, 0, 0, 0, 31, 0, 26, 0, 11, 35, 1, 9, 0, 0, 9, 0, 8, 0, 15, 1, 32, 0, 19, 0, 0, 17, 9, 43, 51, 0, 0, 0, 0, 0, 16], 'y_true': [0, 0, 1, 0, 1, 0, 4, 0, 0, 0, 1, 1, 2, 1, 1, 2, 1, 0, 1, 0, 0, 0, 2, 2, 0, 1, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 1, 3, 0, 0, 1, 0, 1, 1, 1, 3, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 8, 0, 3, 3, 2, 0, 0, 2, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 3, 0, 3, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 3, 1, 2, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 3, 4, 2, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 7, 2, 2, 2, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 3, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 2, 0, 0, 0, 2, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 3, 0, 1, 0, 0, 3, 1, 1, 2, 1, 0, 2, 0, 1, 1, 0, 1, 0, 1, 0, 5, 0, 1, 1, 0, 1, 2, 1, 1, 0, 0, 0, 3, 0, 1, 1, 1, 2, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 6, 0, 0, 3, 0, 0, 1, 0, 2, 2, 2, 2, 0, 0, 1, 0, 2, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 1, 1, 2, 3, 0, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 2, 0, 2, 2, 1, 3, 0, 0, 1, 1, 1, 1, 1, 0, 0, 3, 0, 0, 0, 1, 0, 2, 0, 3, 0, 3, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 2], 'train_acc': 0.3461040224913495, 'test_acc': 0.244878892733564, 'X': 'X_TANL_layer_2_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_3_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 6413.8184, Test loss: 30413.0742
In this epoch 100/5000, Training loss: 393.4911, Test loss: 802.3221
In this epoch 150/5000, Training loss: 1.0179, Test loss: 566.5090
In this epoch 200/5000, Training loss: 0.0126, Test loss: 566.4623
In this epoch 250/5000, Training loss: 0.0004, Test loss: 566.1364
In this epoch 300/5000, Training loss: 0.0004, Test loss: 566.1141
In this epoch 350/5000, Training loss: 0.0003, Test loss: 566.1201
In this epoch 400/5000, Training loss: 0.0003, Test loss: 566.1238
In this epoch 450/5000, Training loss: 0.0002, Test loss: 566.1279
In this epoch 500/5000, Training loss: 0.0002, Test loss: 566.1320
In this epoch 550/5000, Training loss: 0.0001, Test loss: 566.1355
In this epoch 600/5000, Training loss: 0.0001, Test loss: 566.1392
In this epoch 650/5000, Training loss: 0.0001, Test loss: 566.1425
In this epoch 700/5000, Training loss: 0.0001, Test loss: 566.1457
In this epoch 750/5000, Training loss: 0.0000, Test loss: 566.1487
In this epoch 800/5000, Training loss: 0.0000, Test loss: 566.1514
In this epoch 850/5000, Training loss: 0.0000, Test loss: 566.1537
In this epoch 900/5000, Training loss: 0.0000, Test loss: 566.1560
In this epoch 950/5000, Training loss: 0.0000, Test loss: 566.1578
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 566.1595
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 566.1608
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 566.1620
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 566.1631
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 566.1639
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 566.1647
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 566.1653
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 566.1658
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 566.1663
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 566.1667
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 566.1669
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 566.1670
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 566.1671
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 566.1672
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 566.1674
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 566.1674
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 566.1675
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 566.1675
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 566.1675
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 566.1675
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 566.1675
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 566.1676
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 566.1676
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 566.1676
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 566.1675
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 566.1676
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 566.1677
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 566.1826
In this epoch 4400/5000, Training loss: 25743.9395, Test loss: 349745.7812
In this epoch 4450/5000, Training loss: 5.6040, Test loss: 1927.8967
In this epoch 4500/5000, Training loss: 1.2367, Test loss: 627.9590
In this epoch 4550/5000, Training loss: 0.0624, Test loss: 614.9678
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 614.9981
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 615.0063
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 615.0078
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 615.0081
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 615.0085
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 615.0089
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 615.0093
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 615.0097
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 615.0101
train_acc 0.3507644896193772
test_acc 0.22046712802768167
{'y_pred': [47.35678482055664, -26.278976440429688, -29.967187881469727, 6.378720760345459, -18.01106834411621, -35.564144134521484, -3.6248316764831543, 7.1318888664245605, -63.983951568603516, 9.92572021484375, 47.02339553833008, 14.053058624267578, -27.12701988220215, -26.834108352661133, 14.87289047241211, -9.928509712219238, 3.581514835357666, -45.29538345336914, 25.371036529541016, 6.409317493438721, 16.79234504699707, 5.99407434463501, -14.54274845123291, -37.99541473388672, -2.442624568939209, 12.820725440979004, 11.31942367553711, -36.01042556762695, 32.492313385009766, -40.594749450683594, 38.34402847290039, -25.320125579833984, -22.459823608398438, -60.8472785949707, -14.46563720703125, 30.93008804321289, 63.14492416381836, -41.68687438964844, 23.1489315032959, -48.21101760864258, 5.1004509925842285, -14.099336624145508, 11.49586296081543, 8.261979103088379, 13.31898307800293, 9.907408714294434, 0.3865464925765991, 31.731035232543945, -18.104537963867188, -9.794015884399414, -6.9965620040893555, 4.376619338989258, 55.205135345458984, 3.4597840309143066, -22.743362426757812, 46.32410430908203, -16.618181228637695, 3.4533047676086426, -5.413978099822998, 11.968457221984863, -32.21628952026367, 15.588756561279297, 22.27815818786621, 23.161069869995117, -10.041667938232422, 8.5804443359375, -18.54019546508789, -39.36939239501953, -43.81992721557617, -10.668153762817383, 16.659408569335938, -17.785404205322266, -5.380149841308594, -26.12095069885254, 17.344863891601562, -26.57491683959961, 37.889286041259766, -24.207761764526367, -25.47844696044922, 7.0378546714782715, 11.105321884155273, -0.5506836175918579, 6.613204479217529, -36.52964782714844, 14.431353569030762, -3.331935405731201, -12.814420700073242, 21.63953971862793, 16.254709243774414, -49.92739486694336, -30.0334415435791, -26.108539581298828, 36.19057083129883, 5.013756275177002, 15.927396774291992, -16.263446807861328, -36.80232238769531, 23.03273582458496, 17.273366928100586, 8.192953109741211, 3.3089823722839355, 14.308399200439453, 32.76518630981445, 20.93570899963379, 35.95364761352539, 3.6577858924865723, 8.619853973388672, -7.955822467803955, 33.5671272277832, -10.935267448425293, -8.487981796264648, -11.789800643920898, -10.887572288513184, -51.56277084350586, 14.533554077148438, 20.99000358581543, -25.283254623413086, 43.14646530151367, 1.6480239629745483, 21.626983642578125, -36.03266525268555, -4.627919673919678, -51.22703170776367, 30.470388412475586, 22.429719924926758, -6.1604228019714355, -6.8882575035095215, -19.91031837463379, 0.02870786562561989, -23.269027709960938, 18.31624412536621, -19.451162338256836, 35.68291473388672, -41.267974853515625, 17.615209579467773, 23.003664016723633, -14.111885070800781, 16.753347396850586, 0.08783948421478271, -9.614060401916504, 0.5138648748397827, -6.656198024749756, -6.245815277099609, -18.120698928833008, 7.685343265533447, 17.54728889465332, 19.91119384765625, 10.290796279907227, -9.696304321289062, 3.3687710762023926, -14.49772834777832, 37.77109909057617, 9.651888847351074, 33.28656005859375, -17.603849411010742, -31.730554580688477, 53.71493148803711, -25.20757484436035, -24.083589553833008, 28.96202850341797, -45.714298248291016, -45.10783767700195, -0.8838154077529907, -10.144441604614258, -7.2164835929870605, 33.9561882019043, -6.525589942932129, 11.515938758850098, -9.492083549499512, -22.343320846557617, 0.01544225588440895, -9.44982624053955, 35.865989685058594, -16.07128143310547, 27.60811424255371, -15.292381286621094, 6.914349555969238, 5.3831048011779785, -29.49810218811035, -38.33518600463867, -10.556899070739746, -25.4710636138916, 2.6651010513305664, -6.405731678009033, 20.768840789794922, -40.55693435668945, 16.14997673034668, 32.2559814453125, 8.853668212890625, 31.732423782348633, -14.447052001953125, 29.810840606689453, 29.34395408630371, 15.916468620300293, 24.707225799560547, 5.996962547302246, -8.13588809967041, -28.549480438232422, -3.837751626968384, -41.64458465576172, -21.569517135620117, 45.792354583740234, 8.092130661010742, 11.680061340332031, 28.190858840942383, -41.50818634033203, -16.42351722717285, 27.650985717773438, -32.636512756347656, -2.3289201259613037, 2.0020432472229004, 51.0140266418457, 4.284796237945557, -29.304473876953125, 36.358028411865234, 1.53180992603302, -42.999515533447266, -12.705867767333984, -20.07164764404297, 37.30708694458008, 2.237154483795166, -4.2926859855651855, 28.583833694458008, -8.168716430664062, 15.083730697631836, -22.473474502563477, 3.1213278770446777, -29.718042373657227, 2.276449680328369, 13.12260627746582, -13.257739067077637, -37.662193298339844, -14.343857765197754, 14.476661682128906, -0.8639055490493774, 27.388792037963867, -13.031789779663086, -31.36319923400879, 24.913742065429688, -2.793971538543701, 14.587015151977539, 2.623405933380127, 20.54058074951172, -5.067907333374023, 19.3072452545166, 18.096607208251953, 23.134855270385742, 61.87015914916992, -20.43825340270996, -27.882640838623047, 7.470865726470947, 44.852603912353516, 19.5101375579834, 28.93311882019043, 10.464690208435059, 21.178909301757812, -30.212738037109375, -1.8138161897659302, 5.157890796661377, -25.629085540771484, 11.891590118408203, 14.365274429321289, 9.757286071777344, 12.596220016479492, 19.067041397094727, 17.37510108947754, 5.862768650054932, 2.532106876373291, 7.2112717628479, 3.1052651405334473, 29.829639434814453, -22.7541561126709, -8.351179122924805, -8.007803916931152, -1.4489606618881226, 4.025867938995361, -1.4726330041885376, -17.090438842773438, -18.192073822021484, 1.8497165441513062, -28.295461654663086, -26.69341278076172, -28.781831741333008, -44.06467056274414, 34.256385803222656, -1.0931583642959595, 7.606573581695557, -6.66056489944458, -17.86672592163086, -18.30590057373047, 62.34355926513672, 28.760984420776367, 3.778128147125244, -18.324359893798828, 16.37751579284668, -15.485437393188477, 27.492904663085938, -41.596038818359375, 6.600972652435303, -23.623613357543945, -37.612083435058594, 33.271114349365234, -60.38718795776367, -4.03674840927124, 17.737688064575195, 6.210997104644775, 9.845815658569336, 1.600570559501648, -27.128808975219727, 13.171937942504883, 30.443456649780273, -4.081033229827881, 21.559547424316406, 18.049556732177734, -31.70415687561035, 12.490949630737305, -19.16815757751465, 7.0221943855285645, 57.63051986694336, 1.3037513494491577, -9.7032470703125, 15.069001197814941, -4.080471992492676, 7.6200127601623535, 23.443086624145508, 26.261390686035156, 4.359492301940918, 37.99014663696289, 20.031997680664062, -10.078123092651367, 57.22014617919922, 51.095149993896484, -9.112211227416992, 21.230850219726562, 1.9292806386947632, 30.249813079833984, -38.35002517700195, 23.92618179321289, 35.50876235961914, -9.379956245422363], 'y_pred_int': [47, 0, 0, 6, 0, 0, 0, 7, 0, 10, 47, 14, 0, 0, 15, 0, 4, 0, 25, 6, 17, 6, 0, 0, 0, 13, 11, 0, 32, 0, 38, 0, 0, 0, 0, 31, 63, 0, 23, 0, 5, 0, 11, 8, 13, 10, 0, 32, 0, 0, 0, 4, 55, 3, 0, 46, 0, 3, 0, 12, 0, 16, 22, 23, 0, 9, 0, 0, 0, 0, 17, 0, 0, 0, 17, 0, 38, 0, 0, 7, 11, 0, 7, 0, 14, 0, 0, 22, 16, 0, 0, 0, 36, 5, 16, 0, 0, 23, 17, 8, 3, 14, 33, 21, 36, 4, 9, 0, 34, 0, 0, 0, 0, 0, 15, 21, 0, 43, 2, 22, 0, 0, 0, 30, 22, 0, 0, 0, 0, 0, 18, 0, 36, 0, 18, 23, 0, 17, 0, 0, 1, 0, 0, 0, 8, 18, 20, 10, 0, 3, 0, 38, 10, 33, 0, 0, 54, 0, 0, 29, 0, 0, 0, 0, 0, 34, 0, 12, 0, 0, 0, 0, 36, 0, 28, 0, 7, 5, 0, 0, 0, 0, 3, 0, 21, 0, 16, 32, 9, 32, 0, 30, 29, 16, 25, 6, 0, 0, 0, 0, 0, 46, 8, 12, 28, 0, 0, 28, 0, 0, 2, 51, 4, 0, 36, 2, 0, 0, 0, 37, 2, 0, 29, 0, 15, 0, 3, 0, 2, 13, 0, 0, 0, 14, 0, 27, 0, 0, 25, 0, 15, 3, 21, 0, 19, 18, 23, 62, 0, 0, 7, 45, 20, 29, 10, 21, 0, 0, 5, 0, 12, 14, 10, 13, 19, 17, 6, 3, 7, 3, 30, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 0, 34, 0, 8, 0, 0, 0, 62, 29, 4, 0, 16, 0, 27, 0, 7, 0, 0, 33, 0, 0, 18, 6, 10, 2, 0, 13, 30, 0, 22, 18, 0, 12, 0, 7, 58, 1, 0, 15, 0, 8, 23, 26, 4, 38, 20, 0, 57, 51, 0, 21, 2, 30, 0, 24, 36, 0], 'y_true': [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 2, 2, 3, 2, 1, 2, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 3, 0, 0, 2, 0, 1, 0, 1, 1, 2, 3, 1, 0, 0, 1, 0, 1, 0, 2, 0, 1, 1, 1, 4, 0, 2, 1, 0, 1, 0, 0, 0, 2, 1, 1, 5, 1, 0, 0, 3, 0, 6, 0, 0, 1, 1, 0, 2, 1, 3, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 2, 2, 5, 1, 0, 1, 1, 8, 2, 1, 0, 1, 1, 0, 1, 0, 1, 2, 0, 2, 0, 0, 1, 5, 1, 0, 1, 0, 0, 1, 1, 4, 1, 5, 1, 1, 0, 1, 7, 0, 3, 2, 0, 1, 1, 3, 2, 1, 2, 1, 0, 1, 1, 1, 0, 0, 2, 4, 0, 1, 0, 1, 0, 0, 1, 3, 0, 0, 2, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 4, 2, 1, 0, 1, 0, 2, 1, 0, 3, 0, 0, 1, 0, 4, 0, 1, 1, 1, 1, 2, 1, 0, 3, 3, 0, 1, 1, 0, 0, 1, 1, 1, 0, 2, 6, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 2, 1, 0, 0, 1, 1, 0, 0, 0, 1, 2, 2, 1, 0, 1, 0, 0, 0, 0, 3, 1, 0, 0, 0, 1, 3, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 3, 1, 1, 1, 1, 4, 4, 1, 1, 1, 0, 0, 0, 0, 2, 3, 1, 1, 4, 1, 2, 1, 0, 1, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 2, 2, 0, 1, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 3], 'train_acc': 0.3507644896193772, 'test_acc': 0.22046712802768167, 'X': 'X_TANL_layer_3_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_4_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 111257.2031, Test loss: 53438.6250
In this epoch 100/5000, Training loss: 645.9967, Test loss: 926.9658
In this epoch 150/5000, Training loss: 0.2406, Test loss: 457.5844
In this epoch 200/5000, Training loss: 0.0142, Test loss: 454.0860
In this epoch 250/5000, Training loss: 0.0017, Test loss: 454.0588
In this epoch 300/5000, Training loss: 0.0013, Test loss: 454.0515
In this epoch 350/5000, Training loss: 0.0010, Test loss: 454.0471
In this epoch 400/5000, Training loss: 0.0007, Test loss: 454.0437
In this epoch 450/5000, Training loss: 0.0005, Test loss: 454.0409
In this epoch 500/5000, Training loss: 0.0004, Test loss: 454.0382
In this epoch 550/5000, Training loss: 0.0003, Test loss: 454.0359
In this epoch 600/5000, Training loss: 0.0002, Test loss: 454.0343
In this epoch 650/5000, Training loss: 0.0001, Test loss: 454.0327
In this epoch 700/5000, Training loss: 0.0001, Test loss: 454.0315
In this epoch 750/5000, Training loss: 0.0001, Test loss: 454.0305
In this epoch 800/5000, Training loss: 0.0000, Test loss: 454.0296
In this epoch 850/5000, Training loss: 0.0000, Test loss: 454.0289
In this epoch 900/5000, Training loss: 0.0000, Test loss: 454.0284
In this epoch 950/5000, Training loss: 0.0000, Test loss: 454.0280
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 454.0275
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 454.0272
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 454.0271
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 454.0270
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 454.0269
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 454.0268
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 454.0267
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 454.0266
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 454.0266
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 454.0264
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 454.0264
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 454.0264
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 454.0264
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 454.0264
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 454.0264
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 454.0264
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 454.0264
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 454.0264
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 454.0264
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 454.0264
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 454.0265
In this epoch 3000/5000, Training loss: 10729758.0000, Test loss: 40117536.0000
In this epoch 3050/5000, Training loss: 126593.6484, Test loss: 148495.8125
In this epoch 3100/5000, Training loss: 1037.0588, Test loss: 634.2260
In this epoch 3150/5000, Training loss: 0.1236, Test loss: 518.3939
In this epoch 3200/5000, Training loss: 0.0098, Test loss: 516.8383
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 516.8957
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 516.8918
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 516.8927
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 516.8926
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 516.8927
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 516.8929
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 516.8928
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 516.8928
train_acc 0.35527465397923874
test_acc 0.20512110726643598
{'y_pred': [28.229066848754883, 7.200586795806885, 6.985743045806885, -19.593400955200195, 45.112831115722656, 21.235496520996094, -7.5857062339782715, 16.352663040161133, 30.0827579498291, 7.531872272491455, 27.111724853515625, -22.747886657714844, 18.340856552124023, -6.458714962005615, 2.332012891769409, 9.464641571044922, 23.97283363342285, -31.385364532470703, -63.77529525756836, 30.499841690063477, 18.92743682861328, -13.869954109191895, 9.306288719177246, -41.611942291259766, -0.09335264563560486, 17.30436897277832, 24.117765426635742, -31.29938316345215, 21.808195114135742, -40.947593688964844, -8.911534678190947e-06, -37.338470458984375, 0.11617627739906311, -28.780424118041992, 33.165611267089844, 5.015585422515869, -44.526039123535156, -23.897661209106445, 8.740479469299316, 18.569480895996094, 14.135931968688965, -31.422609329223633, 8.587258338928223, -5.908128261566162, 2.4629409313201904, -15.247260093688965, -29.330106735229492, 5.670760631561279, 26.167673110961914, 14.69559383392334, -65.55719757080078, 31.676965713500977, 18.707019805908203, 13.665217399597168, -9.168684959411621, 7.667666912078857, 28.06194305419922, -8.73630142211914, 19.085573196411133, -27.545093536376953, -33.38687515258789, -35.832454681396484, 13.039034843444824, 42.470069885253906, -19.949832916259766, 9.315730094909668, 7.088476181030273, -23.83833885192871, -4.925848484039307, 69.64324188232422, 1.663964867591858, -17.472312927246094, -0.20071539282798767, 7.980724811553955, 36.00572204589844, 34.850337982177734, 22.50714683532715, -41.306236267089844, 10.993396759033203, -3.352996826171875, 15.947900772094727, -4.300255298614502, -32.854549407958984, 8.309868812561035, -12.174466133117676, 34.06169128417969, 7.8563618659973145, -12.746444702148438, -41.17549133300781, -42.005279541015625, 7.859426975250244, 27.93084144592285, 36.65471649169922, -32.69331741333008, -38.377960205078125, -14.62645435333252, 28.554746627807617, -9.220078468322754, 9.682967185974121, 2.766252279281616, 9.854291915893555, -12.271870613098145, -5.429280757904053, -1.4267696142196655, 10.947513580322266, 35.6589241027832, -5.253439426422119, -4.101531505584717, 21.337873458862305, -1.7489408254623413, 0.08844152092933655, 11.555743217468262, 37.047569274902344, -3.387822389602661, 1.5668329000473022, 40.444705963134766, -14.685802459716797, -14.811914443969727, -16.958717346191406, 37.322853088378906, 2.9313504695892334, -8.236144065856934, -20.039655685424805, -4.425906658172607, -37.507720947265625, 5.186190128326416, -25.71100425720215, 14.866777420043945, -10.705750465393066, -10.408310890197754, -1.3540104627609253, -8.975675582885742, -16.548093795776367, 30.5897159576416, 32.21720886230469, -27.697669982910156, -13.658387184143066, 13.020085334777832, 17.709178924560547, 13.620360374450684, 27.638853073120117, 42.643760681152344, -20.6851863861084, -3.63899302482605, 3.079535722732544, 17.404741287231445, -28.974828720092773, 4.699615478515625, -29.49191665649414, -4.766591548919678, -17.08229637145996, -5.504868984222412, -6.098959445953369, 15.665976524353027, 58.21653366088867, 6.738241063430905e-05, -2.060992479324341, 3.8998610973358154, -4.332988262176514, -17.7220401763916, 0.9398571252822876, 7.790619373321533, -21.479520797729492, 1.5203653573989868, -4.221130847930908, -15.434462547302246, 3.3779170513153076, 9.276841163635254, -16.037240982055664, -29.204744338989258, -29.513124465942383, 27.441085815429688, -3.8085453510284424, -31.51453971862793, 13.047439575195312, 24.729103088378906, 14.960583686828613, -1.9480880498886108, -37.74725341796875, 9.026537895202637, 3.0945627689361572, -63.56556701660156, -12.38447380065918, -26.651708602905273, 26.67173194885254, 41.10831832885742, 4.515724182128906, 10.816760063171387, 39.2998161315918, -3.0457675457000732, 4.901522159576416, 1.6652313470840454, 2.6383464336395264, 21.389524459838867, -1.1718343496322632, 21.898815155029297, -2.9010097980499268, -3.7671024799346924, 21.61245346069336, 20.37189292907715, -26.925817489624023, 27.236116409301758, -18.05815887451172, -11.77397632598877, 9.637534141540527, -24.793500900268555, -6.610091686248779, -21.103540420532227, 2.4114816188812256, -6.729302883148193, 12.81098461151123, 22.999845504760742, 38.289527893066406, -13.624204635620117, 40.815895080566406, 8.996959686279297, 24.530643463134766, 7.134653568267822, -31.108457565307617, 28.86236572265625, 18.712146759033203, -2.129042863845825, 37.526145935058594, 15.423394203186035, 7.112852573394775, -2.3513576984405518, -6.832931041717529, -3.2988674640655518, 23.47882652282715, 22.12532615661621, 44.109336853027344, -43.950660705566406, 10.033686637878418, -19.684904098510742, 13.27890682220459, 11.93256664276123, -32.12089920043945, 15.437323570251465, -9.621443748474121, -21.853023529052734, 17.205839157104492, -18.81496810913086, 21.825511932373047, 36.711158752441406, -18.909029006958008, 33.70026779174805, 2.6192848682403564, -27.232654571533203, 14.536275863647461, 22.025869369506836, 11.026593208312988, 8.10942268371582, -11.923876762390137, -27.22191619873047, -1.8748725652694702, -40.64556884765625, 4.794294834136963, 11.782197952270508, -17.22757911682129, -21.007017135620117, -6.447509288787842, -17.969091415405273, 13.85740852355957, -6.199324131011963, -9.790121078491211, -17.700075149536133, -8.173834800720215, -39.557945251464844, 6.888554573059082, 4.000790119171143, -18.192842483520508, 25.159414291381836, 1.2271238565444946, -17.1978759765625, 13.620108604431152, 0.4081369936466217, 12.24644947052002, 10.143427848815918, -42.057334899902344, 52.35673904418945, 15.22989273071289, 27.6126766204834, 21.998775482177734, -51.301719665527344, 4.768255710601807, -6.8354620933532715, -34.454437255859375, 34.281429290771484, -11.17945384979248, -9.189203262329102, -21.811765670776367, -47.304386138916016, -30.211437225341797, -7.674645900726318, 3.3725993633270264, -16.27342987060547, -1.985878348350525, 35.528587341308594, 9.804278373718262, 12.402825355529785, -13.001086235046387, -14.511534690856934, 17.366117477416992, -13.689238548278809, 18.71837615966797, -31.943891525268555, 29.413740158081055, 47.83802795410156, -7.615914821624756, -41.145511627197266, 8.507991790771484, 54.632408142089844, -7.5368876457214355, 23.017772674560547, 14.605401039123535, 4.8549981117248535, -17.991193771362305, -15.507609367370605, 9.695435523986816, -0.319110244512558, 15.045252799987793, 1.965802788734436, 3.533843755722046, 8.715479850769043, 23.2923641204834, 1.1577192544937134, 7.221363544464111, -23.20564842224121, -19.618331909179688, -9.649643898010254, -9.069682121276855, -16.108898162841797, 22.572160720825195, 34.71411895751953, -8.842113494873047, -27.457555770874023, -0.5883086919784546, 4.60032320022583, 20.23355484008789, 31.60902214050293], 'y_pred_int': [28, 7, 7, 0, 45, 21, 0, 16, 30, 8, 27, 0, 18, 0, 2, 9, 24, 0, 0, 30, 19, 0, 9, 0, 0, 17, 24, 0, 22, 0, 0, 0, 0, 0, 33, 5, 0, 0, 9, 19, 14, 0, 9, 0, 2, 0, 0, 6, 26, 15, 0, 32, 19, 14, 0, 8, 28, 0, 19, 0, 0, 0, 13, 42, 0, 9, 7, 0, 0, 70, 2, 0, 0, 8, 36, 35, 23, 0, 11, 0, 16, 0, 0, 8, 0, 34, 8, 0, 0, 0, 8, 28, 37, 0, 0, 0, 29, 0, 10, 3, 10, 0, 0, 0, 11, 36, 0, 0, 21, 0, 0, 12, 37, 0, 2, 40, 0, 0, 0, 37, 3, 0, 0, 0, 0, 5, 0, 15, 0, 0, 0, 0, 0, 31, 32, 0, 0, 13, 18, 14, 28, 43, 0, 0, 3, 17, 0, 5, 0, 0, 0, 0, 0, 16, 58, 0, 0, 4, 0, 0, 1, 8, 0, 2, 0, 0, 3, 9, 0, 0, 0, 27, 0, 0, 13, 25, 15, 0, 0, 9, 3, 0, 0, 0, 27, 41, 5, 11, 39, 0, 5, 2, 3, 21, 0, 22, 0, 0, 22, 20, 0, 27, 0, 0, 10, 0, 0, 0, 2, 0, 13, 23, 38, 0, 41, 9, 25, 7, 0, 29, 19, 0, 38, 15, 7, 0, 0, 0, 23, 22, 44, 0, 10, 0, 13, 12, 0, 15, 0, 0, 17, 0, 22, 37, 0, 34, 3, 0, 15, 22, 11, 8, 0, 0, 0, 0, 5, 12, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 7, 4, 0, 25, 1, 0, 14, 0, 12, 10, 0, 52, 15, 28, 22, 0, 5, 0, 0, 34, 0, 0, 0, 0, 0, 0, 3, 0, 0, 36, 10, 12, 0, 0, 17, 0, 19, 0, 29, 48, 0, 0, 9, 55, 0, 23, 15, 5, 0, 0, 10, 0, 15, 2, 4, 9, 23, 1, 7, 0, 0, 0, 0, 0, 23, 35, 0, 0, 0, 5, 20, 32], 'y_true': [1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 3, 1, 0, 1, 0, 1, 0, 0, 0, 2, 1, 8, 1, 0, 0, 0, 1, 1, 1, 0, 6, 2, 2, 1, 0, 1, 1, 1, 2, 3, 2, 1, 2, 1, 4, 1, 1, 0, 0, 4, 1, 0, 2, 1, 1, 0, 0, 1, 0, 3, 1, 0, 1, 1, 1, 2, 0, 1, 0, 0, 1, 0, 1, 2, 2, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 2, 1, 2, 2, 0, 1, 0, 1, 0, 1, 1, 2, 0, 2, 0, 1, 0, 0, 1, 0, 3, 0, 2, 1, 1, 3, 0, 1, 1, 2, 1, 1, 3, 0, 9, 1, 0, 3, 1, 1, 0, 2, 1, 3, 0, 5, 2, 1, 4, 4, 0, 1, 0, 0, 3, 1, 0, 0, 0, 1, 0, 1, 0, 2, 1, 0, 3, 0, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 3, 2, 0, 0, 2, 3, 0, 1, 0, 0, 0, 1, 0, 3, 2, 1, 1, 4, 0, 1, 1, 1, 0, 7, 0, 1, 0, 1, 0, 0, 1, 2, 1, 1, 1, 0, 0, 4, 0, 0, 1, 3, 0, 0, 4, 0, 0, 0, 0, 0, 0, 3, 0, 2, 2, 1, 1, 5, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 1, 14, 0, 0, 3, 3, 1, 2, 0, 0, 3, 2, 3, 2, 0, 0, 0, 1, 1, 2, 1, 0, 3, 0, 1, 0, 0, 1, 1, 2, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 2, 0, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 2, 1, 1, 0, 0, 2, 1, 1, 2], 'train_acc': 0.35527465397923874, 'test_acc': 0.20512110726643598, 'X': 'X_TANL_layer_4_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_5_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 57715.2656, Test loss: 2196.2827
In this epoch 100/5000, Training loss: 117.0392, Test loss: 1222.0415
In this epoch 150/5000, Training loss: 2.7944, Test loss: 933.8246
In this epoch 200/5000, Training loss: 0.0051, Test loss: 935.7785
In this epoch 250/5000, Training loss: 0.0010, Test loss: 937.0215
In this epoch 300/5000, Training loss: 0.0008, Test loss: 936.9078
In this epoch 350/5000, Training loss: 0.0007, Test loss: 936.9050
In this epoch 400/5000, Training loss: 0.0006, Test loss: 936.9044
In this epoch 450/5000, Training loss: 0.0005, Test loss: 936.9049
In this epoch 500/5000, Training loss: 0.0004, Test loss: 936.9053
In this epoch 550/5000, Training loss: 0.0003, Test loss: 936.9053
In this epoch 600/5000, Training loss: 0.0003, Test loss: 936.9053
In this epoch 650/5000, Training loss: 0.0002, Test loss: 936.9055
In this epoch 700/5000, Training loss: 0.0002, Test loss: 936.9057
In this epoch 750/5000, Training loss: 0.0001, Test loss: 936.9055
In this epoch 800/5000, Training loss: 0.0001, Test loss: 936.9055
In this epoch 850/5000, Training loss: 0.0001, Test loss: 936.9057
In this epoch 900/5000, Training loss: 0.0001, Test loss: 936.9062
In this epoch 950/5000, Training loss: 0.0000, Test loss: 936.9060
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 936.9057
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 936.9055
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 936.9057
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 936.9057
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 936.9060
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 936.9060
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 936.9060
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 936.9061
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 936.9061
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 936.9062
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 936.9062
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 936.9061
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 936.9061
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 936.9057
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 936.9057
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 936.9057
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 936.9057
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 936.9058
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 936.9061
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 936.9060
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 936.9059
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 936.9061
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 936.9060
In this epoch 4300/5000, Training loss: 969682.2500, Test loss: 32105454.0000
In this epoch 4350/5000, Training loss: 149183.6406, Test loss: 2582.5549
In this epoch 4400/5000, Training loss: 827.0478, Test loss: 1664.8066
In this epoch 4450/5000, Training loss: 0.7738, Test loss: 909.8241
In this epoch 4500/5000, Training loss: 0.0033, Test loss: 922.6790
In this epoch 4550/5000, Training loss: 0.0001, Test loss: 923.9106
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 924.0148
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 924.0178
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 924.0176
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 924.0180
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 924.0184
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 924.0187
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 924.0192
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 924.0199
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 924.0201
train_acc 0.3539738321799308
test_acc 0.1931401384083045
{'y_pred': [19.888410568237305, 32.24393844604492, 10.180747032165527, -6.074705600738525, -5.1276936531066895, 9.094860076904297, -19.12293815612793, 20.512786865234375, -8.654544830322266, -35.07218551635742, 14.623286247253418, -7.784553527832031, -5.662930965423584, 5.45236873626709, -12.04188060760498, 22.431673049926758, 14.844075202941895, -47.85095977783203, 10.181693077087402, -33.249366760253906, 4.662281036376953, 17.153614044189453, 20.913894653320312, 48.31551742553711, -20.525211334228516, 15.986019134521484, 8.94822883605957, 47.345481872558594, -36.839473724365234, -12.009033203125, -32.166751861572266, -8.616449356079102, 10.72746753692627, 3.783127784729004, 58.537994384765625, 47.58517074584961, 0.17734625935554504, 24.417320251464844, -15.569488525390625, 24.972965240478516, 8.30766487121582, 56.093650817871094, 36.00310516357422, 54.02379608154297, 36.10210037231445, 9.890073776245117, -24.859350204467773, 37.08759307861328, -40.338130950927734, -21.926713943481445, 54.06068801879883, 3.2954225540161133, 24.79604721069336, -39.12968826293945, 35.99940490722656, -47.87532424926758, -19.54161262512207, -43.96558380126953, -11.751224517822266, 2.1036911010742188, 19.025558471679688, 32.68830490112305, 62.204524993896484, -0.4893054664134979, 5.711559295654297, -3.6837291717529297, -25.860837936401367, 0.026713641360402107, 46.37630844116211, -26.78680992126465, 25.894262313842773, 3.102123737335205, 14.891569137573242, 30.15277671813965, -75.44996643066406, 27.825965881347656, 7.917219638824463, -18.85331153869629, 41.619510650634766, 19.35636329650879, -34.3090705871582, -2.0051937103271484, 28.034297943115234, -0.22069165110588074, 20.48404312133789, 11.877760887145996, 9.277739524841309, 12.207996368408203, 45.35905075073242, -57.25152587890625, 14.805472373962402, 15.674125671386719, 69.13956451416016, -40.19648742675781, 49.17619705200195, -23.3797607421875, -5.030431747436523, -8.939729690551758, 5.724996566772461, -73.36297607421875, 16.113792419433594, -3.218855857849121, -5.919710159301758, -37.82407760620117, 78.91616821289062, -18.895679473876953, 27.657997131347656, 8.334806442260742, -17.812923431396484, -3.707411766052246, 8.744651794433594, 8.640252113342285, -7.75228214263916, -62.44325637817383, -8.715677261352539, -4.427377700805664, 1.4346504211425781, -14.141225814819336, 76.0062255859375, 48.68495559692383, -12.242575645446777, -41.705623626708984, -38.63371276855469, 9.749811172485352, 4.144669532775879, -27.322486877441406, -10.336507797241211, -0.7766761183738708, -16.515884399414062, -25.958009719848633, -34.706336975097656, -9.86477279663086, 43.99909973144531, -6.398101806640625, -19.939313888549805, 24.894657135009766, 56.3128776550293, 23.997318267822266, 67.19219970703125, 33.963130950927734, -0.050157513469457626, -8.819255828857422, 32.180946350097656, -7.4315185546875, -9.499833106994629, 18.489118576049805, 28.23357582092285, -5.303599834442139, 10.79379653930664, 2.8063392639160156, 32.55268478393555, -77.2296371459961, -9.364845275878906, 34.27248764038086, -56.727935791015625, 38.35451126098633, 18.347904205322266, 3.253808245062828e-08, 5.118057727813721, -17.300804138183594, -5.4788641929626465, -40.47201919555664, -1.9223599433898926, -25.07337188720703, -10.777175903320312, 13.965456008911133, 32.41817092895508, -31.97903823852539, -15.697672843933105, -14.941930770874023, -42.13582992553711, -2.9328432083129883, 37.6766242980957, 14.692817687988281, -11.20296859741211, 20.562942504882812, 3.9912939071655273, 57.6112060546875, -19.406539916992188, 52.994991302490234, 12.883129119873047, -18.23499870300293, -33.794925689697266, 27.52610206604004, -14.750202178955078, 47.89292907714844, 34.92656707763672, 53.168731689453125, 17.081979751586914, -33.54594039916992, -46.02280044555664, 20.368606567382812, -14.941646575927734, 4.236141204833984, -14.824737548828125, -9.256309509277344, -29.448328018188477, 11.728813171386719, -28.13689422607422, 21.34069061279297, -14.37759017944336, -39.07010269165039, 49.650936126708984, 53.661865234375, 0.1016106903553009, -15.182760238647461, -45.223106384277344, -45.127994537353516, 20.691932678222656, 37.16681671142578, -3.4291224479675293, -87.977783203125, 29.955032348632812, 28.047157287597656, 58.7846794128418, -8.891732215881348, 50.51813888549805, -38.59497833251953, -22.731937408447266, 11.673465728759766, 28.0764217376709, -12.940298080444336, -11.375109672546387, 26.348560333251953, -13.633110046386719, -23.213706970214844, -25.145668029785156, -29.23484992980957, -7.195242404937744, 19.909889221191406, 36.740020751953125, 20.30270004272461, 34.02693176269531, -21.149747848510742, -52.71697998046875, 38.5532341003418, -16.624893188476562, 12.304262161254883, 24.144817352294922, 7.356802940368652, -29.929157257080078, 16.10818099975586, -8.491665840148926, 47.980709075927734, -4.3466644287109375, 74.02922821044922, 27.332067489624023, 57.41374206542969, -3.9408674240112305, -23.193702697753906, -60.553489685058594, -33.560909271240234, -8.363992691040039, 3.1484432220458984, 25.0036678314209, 51.50365447998047, 20.446393966674805, 2.5460243225097656, 29.963539123535156, 24.23211097717285, 7.632648339495063e-05, 11.181379318237305, -12.629993438720703, 5.118711948394775, -27.464996337890625, 63.78838348388672, 8.785770416259766, 21.678579330444336, 23.344680786132812, 60.24772262573242, -49.84151077270508, 11.851879119873047, -94.628173828125, 14.670011520385742, -48.2656364440918, 52.68058395385742, -0.6467346549034119, 6.814359664916992, -46.73490524291992, -17.523019790649414, -2.2470932006835938, 14.893345832824707, -5.455992221832275, 0.5072260499000549, 4.779548645019531, 43.62237548828125, -1.834920883178711, 37.68815231323242, 10.206127166748047, -5.966287612915039, 12.137415885925293, 79.54157257080078, 14.851873397827148, -22.362869262695312, 24.50701141357422, 16.037181854248047, 20.95269012451172, 14.866118431091309, 13.277358055114746, -23.82147216796875, 0.2961597740650177, -9.052233695983887, 24.67811393737793, 25.010406494140625, 16.386890411376953, -8.426814079284668, 10.889358520507812, 39.14468002319336, -9.737909317016602, -12.851289749145508, 27.567241668701172, 58.02989196777344, -30.19292449951172, 12.82922077178955, 18.231996536254883, 27.95585823059082, 37.25950241088867, 34.683231353759766, 18.446189880371094, -30.721141815185547, -9.01150131225586, 3.4677162170410156, -3.9748363494873047, 8.908565521240234, 12.16946029663086, 4.6422119140625, 12.336135864257812, 67.13359069824219, -34.633522033691406, -20.83518409729004, -7.2033305168151855, 40.94674301147461, -4.945664405822754, 13.415953636169434, 21.134071350097656, 64.57369232177734, 17.443614959716797, 18.781557083129883, 4.690472602844238, 3.5264930725097656], 'y_pred_int': [20, 32, 10, 0, 0, 9, 0, 21, 0, 0, 15, 0, 0, 5, 0, 22, 15, 0, 10, 0, 5, 17, 21, 48, 0, 16, 9, 47, 0, 0, 0, 0, 11, 4, 59, 48, 0, 24, 0, 25, 8, 56, 36, 54, 36, 10, 0, 37, 0, 0, 54, 3, 25, 0, 36, 0, 0, 0, 0, 2, 19, 33, 62, 0, 6, 0, 0, 0, 46, 0, 26, 3, 15, 30, 0, 28, 8, 0, 42, 19, 0, 0, 28, 0, 20, 12, 9, 12, 45, 0, 15, 16, 69, 0, 49, 0, 0, 0, 6, 0, 16, 0, 0, 0, 79, 0, 28, 8, 0, 0, 9, 9, 0, 0, 0, 0, 1, 0, 76, 49, 0, 0, 0, 10, 4, 0, 0, 0, 0, 0, 0, 0, 44, 0, 0, 25, 56, 24, 67, 34, 0, 0, 32, 0, 0, 18, 28, 0, 11, 3, 33, 0, 0, 34, 0, 38, 18, 0, 5, 0, 0, 0, 0, 0, 0, 14, 32, 0, 0, 0, 0, 0, 38, 15, 0, 21, 4, 58, 0, 53, 13, 0, 0, 28, 0, 48, 35, 53, 17, 0, 0, 20, 0, 4, 0, 0, 0, 12, 0, 21, 0, 0, 50, 54, 0, 0, 0, 0, 21, 37, 0, 0, 30, 28, 59, 0, 51, 0, 0, 12, 28, 0, 0, 26, 0, 0, 0, 0, 0, 20, 37, 20, 34, 0, 0, 39, 0, 12, 24, 7, 0, 16, 0, 48, 0, 74, 27, 57, 0, 0, 0, 0, 0, 3, 25, 52, 20, 3, 30, 24, 0, 11, 0, 5, 0, 64, 9, 22, 23, 60, 0, 12, 0, 15, 0, 53, 0, 7, 0, 0, 0, 15, 0, 1, 5, 44, 0, 38, 10, 0, 12, 80, 15, 0, 25, 16, 21, 15, 13, 0, 0, 0, 25, 25, 16, 0, 11, 39, 0, 0, 28, 58, 0, 13, 18, 28, 37, 35, 18, 0, 0, 3, 0, 9, 12, 5, 12, 67, 0, 0, 0, 41, 0, 13, 21, 65, 17, 19, 5, 4], 'y_true': [1, 2, 1, 0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 3, 4, 0, 0, 1, 0, 0, 1, 3, 2, 0, 2, 3, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 1, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1, 2, 3, 0, 3, 0, 1, 1, 3, 1, 1, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 3, 1, 1, 0, 0, 7, 1, 1, 1, 8, 1, 0, 5, 3, 2, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 3, 2, 1, 0, 1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 1, 0, 1, 4, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 5, 0, 0, 2, 0, 2, 1, 2, 1, 2, 0, 1, 1, 0, 0, 0, 2, 0, 3, 1, 0, 0, 1, 1, 2, 0, 3, 1, 1, 1, 4, 2, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 1, 0, 0, 1, 2, 2, 1, 3, 0, 1, 2, 1, 1, 2, 2, 2, 0, 2, 2, 1, 0, 3, 3, 0, 0, 0, 0, 0, 1, 2, 1, 3, 0, 1, 1, 4, 3, 0, 1, 1, 3, 1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 2, 0, 0, 3, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 1, 1, 0, 4, 0, 3, 2, 0, 2, 1, 1, 2, 1, 1, 0, 2, 7, 1, 3, 0, 1, 1, 0, 0, 1, 0, 2, 0, 2, 0, 1, 0, 1, 3, 0, 1, 1, 1, 0, 1, 1, 0, 2, 0, 1, 1, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0], 'train_acc': 0.3539738321799308, 'test_acc': 0.1931401384083045, 'X': 'X_TANL_layer_5_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_6_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 191426.0781, Test loss: 81214.3047
In this epoch 100/5000, Training loss: 1221.7155, Test loss: 1497.2837
In this epoch 150/5000, Training loss: 0.4331, Test loss: 787.6787
In this epoch 200/5000, Training loss: 0.0141, Test loss: 780.1862
In this epoch 250/5000, Training loss: 0.0041, Test loss: 779.8741
In this epoch 300/5000, Training loss: 0.0032, Test loss: 779.8708
In this epoch 350/5000, Training loss: 0.0025, Test loss: 779.8506
In this epoch 400/5000, Training loss: 0.0019, Test loss: 779.8311
In this epoch 450/5000, Training loss: 0.0014, Test loss: 779.8136
In this epoch 500/5000, Training loss: 0.0010, Test loss: 779.7972
In this epoch 550/5000, Training loss: 0.0007, Test loss: 779.7829
In this epoch 600/5000, Training loss: 0.0005, Test loss: 779.7701
In this epoch 650/5000, Training loss: 0.0004, Test loss: 779.7596
In this epoch 700/5000, Training loss: 0.0002, Test loss: 779.7500
In this epoch 750/5000, Training loss: 0.0002, Test loss: 779.7421
In this epoch 800/5000, Training loss: 0.0001, Test loss: 779.7349
In this epoch 850/5000, Training loss: 0.0001, Test loss: 779.7299
In this epoch 900/5000, Training loss: 0.0000, Test loss: 779.7247
In this epoch 950/5000, Training loss: 0.0000, Test loss: 779.7211
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 779.7184
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 779.7162
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 779.7145
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 779.7131
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 779.7118
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 779.7109
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 779.7105
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 779.7100
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 779.7100
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 779.7098
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 779.7096
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 779.7093
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 779.7093
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 779.7093
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 779.7091
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 779.7091
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 779.7090
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 779.7090
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 779.7091
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 779.7091
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 779.7091
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 779.7092
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 779.7091
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 779.7091
In this epoch 2800/5000, Training loss: 5156211.5000, Test loss: 1390051.7500
In this epoch 2850/5000, Training loss: 21891.6973, Test loss: 14868.3721
In this epoch 2900/5000, Training loss: 139.6075, Test loss: 844.5701
In this epoch 2950/5000, Training loss: 0.2244, Test loss: 801.6703
In this epoch 3000/5000, Training loss: 0.0040, Test loss: 800.6003
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 800.6335
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 800.6357
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 800.6359
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 800.6364
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 800.6364
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 800.6370
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 800.6373
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 800.6375
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 800.6376
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 800.6379
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 800.6384
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 800.6385
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 800.6389
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 800.6392
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 800.6393
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 800.6395
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 800.6395
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 800.6396
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 800.6397
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 800.6399
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 800.6398
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 800.6399
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 800.6401
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 800.6401
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 800.6401
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 800.6400
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 800.6401
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 800.6401
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 800.6402
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 800.6401
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 800.6401
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 800.6401
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 800.6402
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 800.6403
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 800.6403
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 800.6403
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 800.6403
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 800.6403
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 800.6403
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 800.6403
train_acc 0.3535867214532872
test_acc 0.21298442906574394
{'y_pred': [-18.46519660949707, -0.07802698761224747, 4.58497428894043, -28.077726364135742, -24.68592643737793, -15.744702339172363, -78.51990509033203, 52.29777526855469, -46.25703048706055, 3.194063901901245, 29.772493362426758, -15.275345802307129, -6.650862693786621, -4.090399742126465, -13.034618377685547, -70.00462341308594, 1.0779192447662354, -17.274559020996094, 5.690190315246582, 32.656951904296875, -11.790708541870117, -21.90983772277832, 32.018821716308594, 27.055355072021484, -5.330541610717773, 8.914376258850098, 15.827666282653809, -30.105024337768555, 50.34553146362305, 41.96057891845703, -15.185357093811035, -16.150026321411133, 17.859590530395508, 42.54124069213867, 33.352203369140625, 16.54120445251465, 36.7104377746582, -30.659910202026367, 7.5409135818481445, -38.495670318603516, 11.099690437316895, 22.896398544311523, 40.44060516357422, -13.353285789489746, 24.55594825744629, -50.312339782714844, 13.866501808166504, 15.813660621643066, -10.765771865844727, -17.283723831176758, 2.4888923168182373, 3.9628703594207764, -16.905454635620117, 4.080304145812988, -16.301801681518555, -1.4235413074493408, 25.37565803527832, 30.741804122924805, -2.4161179065704346, 27.461572647094727, -38.64791488647461, -4.636811256408691, -51.478912353515625, -25.531051635742188, -15.383904457092285, -34.73850631713867, 27.312313079833984, 19.753664016723633, 10.3145170211792, -11.211146354675293, 46.10050582885742, -57.50077438354492, 20.17616844177246, 21.496686935424805, 4.758338928222656, -19.5120792388916, -1.0532000064849854, 46.34382629394531, 10.759448051452637, 26.96974754333496, 27.574363708496094, 37.67036437988281, -35.78065872192383, -76.0155258178711, -9.787164688110352, 2.6620285511016846, 34.534706115722656, 0.37065866589546204, -10.005452156066895, -22.43846321105957, 9.924036979675293, -45.75578689575195, 15.478306770324707, -33.09346389770508, 25.155221939086914, 5.947079181671143, 64.34516143798828, 3.697804640978575e-05, 74.29017639160156, -7.144589424133301, -17.738370895385742, 27.70825958251953, -18.548200607299805, -46.479713439941406, -28.42133903503418, 7.312047004699707, 29.950712203979492, 22.34531593322754, -14.381882667541504, 34.445796966552734, -38.387969970703125, 7.208351135253906, 34.41743850708008, -18.74888801574707, -2.986163377761841, 21.193845748901367, -20.118480682373047, -6.327700614929199, 2.0325992107391357, -6.322270393371582, -35.79652786254883, -5.886282920837402, -3.8896725177764893, 16.37721061706543, 31.837932586669922, 50.24094009399414, 20.520578384399414, 2.7974956035614014, 23.394084930419922, 30.60978889465332, -52.91983413696289, -48.85868835449219, 19.306869506835938, 23.131803512573242, 6.333881378173828, 16.618635177612305, -8.916543006896973, -75.50508880615234, -49.78401184082031, -2.0192768573760986, 4.709578514099121, -16.2298526763916, 7.5173139572143555, 14.639948844909668, -17.839351654052734, -10.676679611206055, -37.71320343017578, 16.040842056274414, 11.51173210144043, 63.0206413269043, 11.600160598754883, -20.6448917388916, 2.5429065227508545, 36.65700149536133, -51.166812896728516, -1.0118486881256104, 42.616817474365234, 14.126620292663574, -5.337984085083008, 14.145867347717285, 9.330986022949219, -8.142720222473145, -6.151001930236816, -4.356320381164551, 35.58316421508789, 32.45792770385742, -15.747967720031738, 52.21858596801758, -14.421984672546387, 21.793088912963867, 13.12060832977295, 6.215133190155029, 5.774104118347168, -13.661166191101074, 2.4146201610565186, 15.3320951461792, 39.001461029052734, -27.513463973999023, 17.895692825317383, -24.664730072021484, -36.2068977355957, -14.381775856018066, 22.03767204284668, -15.802035331726074, 9.365293502807617, 11.657744407653809, -3.3386728763580322, 18.980247497558594, 1.0124499797821045, -23.638185501098633, 19.365739822387695, -0.011487222276628017, 45.34090042114258, -37.4573974609375, -16.174577713012695, 2.0064098834991455, -25.541643142700195, -67.10224914550781, -8.634150505065918, -13.555423736572266, -11.571686744689941, 5.343257427215576, 21.732576370239258, -18.681013107299805, 9.993788719177246, 42.30201721191406, -28.16605567932129, 0.8756253719329834, 42.28607940673828, -25.81208038330078, 18.977075576782227, -5.254265785217285, 3.7324492931365967, 11.24938678741455, 10.455721855163574, -6.048748970031738, 15.763086318969727, -17.768369674682617, 16.356109619140625, 20.888132095336914, 27.53974151611328, -4.104327201843262, -10.62879467010498, -2.9853851795196533, -8.054142951965332, -13.274249076843262, 49.753170013427734, 11.623085021972656, 24.14556121826172, 20.070810317993164, -11.469947814941406, 17.8731632232666, -11.131876945495605, -13.332711219787598, -5.469446182250977, -13.692572593688965, 9.20927906036377, 1.003258466720581, 19.085817337036133, 34.10213851928711, 19.918947219848633, 32.855010986328125, -6.716729164123535, 29.192026138305664, 1.497063398361206, -10.909441947937012, 22.555328369140625, -31.015647888183594, -8.40609073638916, 63.20014572143555, -8.856961250305176, 12.497946739196777, 16.133819580078125, -7.871169090270996, -51.21561813354492, -30.452133178710938, -6.75046443939209, -39.27711868286133, -47.23830795288086, 4.6073102951049805, 36.14236068725586, 10.593077659606934, 44.8216552734375, -21.603666305541992, -1.7057063579559326, -52.304534912109375, -46.12663269042969, 6.117053031921387, 27.819286346435547, -30.646329879760742, 7.918780326843262, 17.128149032592773, -2.316371202468872, -10.077152252197266, -18.83237648010254, 49.479759216308594, -26.202730178833008, 1.3722217082977295, 15.324992179870605, -6.534714698791504, 21.931995391845703, -44.14780044555664, -29.72605323791504, 7.137062072753906, -8.396965980529785, 30.706836700439453, -8.748820304870605, 33.28391647338867, 42.52473449707031, -58.323482513427734, -18.380672454833984, 38.17631149291992, -10.01174259185791, 16.428319931030273, -4.592061519622803, 29.338884353637695, 12.209928512573242, -43.79084396362305, 32.34896469116211, 26.806570053100586, -26.924959182739258, 4.024480819702148, -8.156035423278809, 76.26365661621094, -44.54263687133789, -6.530661582946777, 61.69203186035156, 1.6521785259246826, 14.166234016418457, -11.8278226852417, 7.527113437652588, -2.2913029193878174, 3.624809503555298, -34.57925796508789, -15.864164352416992, -46.84661102294922, 12.961645126342773, -84.3626480102539, -22.98206901550293, -4.3227434158325195, -9.15284252166748, 78.83380889892578, 34.54960632324219, -61.75088119506836, -40.779075622558594, -30.31479263305664, 46.973751068115234, 26.983718872070312, 4.508715629577637, -10.605488777160645, 19.848587036132812, 22.06667709350586, -36.52397537231445, -17.3209285736084, 19.077041625976562, -9.7176513671875, 2.765752077102661, 14.471219062805176, -32.055301666259766, -52.3981819152832], 'y_pred_int': [0, 0, 5, 0, 0, 0, 0, 52, 0, 3, 30, 0, 0, 0, 0, 0, 1, 0, 6, 33, 0, 0, 32, 27, 0, 9, 16, 0, 50, 42, 0, 0, 18, 43, 33, 17, 37, 0, 8, 0, 11, 23, 40, 0, 25, 0, 14, 16, 0, 0, 2, 4, 0, 4, 0, 0, 25, 31, 0, 27, 0, 0, 0, 0, 0, 0, 27, 20, 10, 0, 46, 0, 20, 21, 5, 0, 0, 46, 11, 27, 28, 38, 0, 0, 0, 3, 35, 0, 0, 0, 10, 0, 15, 0, 25, 6, 64, 0, 74, 0, 0, 28, 0, 0, 0, 7, 30, 22, 0, 34, 0, 7, 34, 0, 0, 21, 0, 0, 2, 0, 0, 0, 0, 16, 32, 50, 21, 3, 23, 31, 0, 0, 19, 23, 6, 17, 0, 0, 0, 0, 5, 0, 8, 15, 0, 0, 0, 16, 12, 63, 12, 0, 3, 37, 0, 0, 43, 14, 0, 14, 9, 0, 0, 0, 36, 32, 0, 52, 0, 22, 13, 6, 6, 0, 2, 15, 39, 0, 18, 0, 0, 0, 22, 0, 9, 12, 0, 19, 1, 0, 19, 0, 45, 0, 0, 2, 0, 0, 0, 0, 0, 5, 22, 0, 10, 42, 0, 1, 42, 0, 19, 0, 4, 11, 10, 0, 16, 0, 16, 21, 28, 0, 0, 0, 0, 0, 50, 12, 24, 20, 0, 18, 0, 0, 0, 0, 9, 1, 19, 34, 20, 33, 0, 29, 1, 0, 23, 0, 0, 63, 0, 12, 16, 0, 0, 0, 0, 0, 0, 5, 36, 11, 45, 0, 0, 0, 0, 6, 28, 0, 8, 17, 0, 0, 0, 49, 0, 1, 15, 0, 22, 0, 0, 7, 0, 31, 0, 33, 43, 0, 0, 38, 0, 16, 0, 29, 12, 0, 32, 27, 0, 4, 0, 76, 0, 0, 62, 2, 14, 0, 8, 0, 4, 0, 0, 0, 13, 0, 0, 0, 0, 79, 35, 0, 0, 0, 47, 27, 5, 0, 20, 22, 0, 0, 19, 0, 3, 14, 0, 0], 'y_true': [1, 2, 1, 0, 0, 0, 1, 0, 2, 1, 3, 7, 1, 7, 2, 7, 1, 1, 0, 1, 0, 3, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 1, 1, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 5, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 0, 1, 1, 4, 1, 0, 5, 4, 1, 3, 1, 1, 0, 2, 0, 1, 1, 2, 0, 1, 1, 2, 1, 3, 1, 0, 0, 3, 1, 4, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 4, 0, 0, 0, 1, 1, 1, 1, 0, 3, 1, 2, 0, 1, 2, 0, 7, 2, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 1, 0, 2, 1, 1, 0, 2, 0, 3, 4, 1, 2, 3, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 1, 3, 0, 2, 1, 0, 2, 1, 0, 0, 2, 1, 2, 0, 2, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 2, 3, 2, 0, 0, 0, 0, 1, 5, 1, 1, 1, 0, 0, 0, 2, 1, 3, 3, 1, 0, 0, 0, 1, 3, 0, 1, 1, 0, 1, 1, 0, 2, 3, 4, 3, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 2, 6, 9, 1, 2, 2, 2, 3, 0, 1, 0, 0, 0, 1, 0, 5, 1, 0, 3, 0, 2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 7, 1, 1, 2, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 2, 0, 2, 2, 1, 2, 1], 'train_acc': 0.3535867214532872, 'test_acc': 0.21298442906574394, 'X': 'X_TANL_layer_6_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_7_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 203290.3750, Test loss: 157204.3594
In this epoch 100/5000, Training loss: 1259.7018, Test loss: 3325.2063
In this epoch 150/5000, Training loss: 4.2666, Test loss: 2354.2747
In this epoch 200/5000, Training loss: 0.0503, Test loss: 2324.2507
In this epoch 250/5000, Training loss: 0.0053, Test loss: 2325.0771
In this epoch 300/5000, Training loss: 0.0046, Test loss: 2324.8684
In this epoch 350/5000, Training loss: 0.0039, Test loss: 2324.8408
In this epoch 400/5000, Training loss: 0.0033, Test loss: 2324.8137
In this epoch 450/5000, Training loss: 0.0027, Test loss: 2324.7869
In this epoch 500/5000, Training loss: 0.0022, Test loss: 2324.7595
In this epoch 550/5000, Training loss: 0.0018, Test loss: 2324.7339
In this epoch 600/5000, Training loss: 0.0014, Test loss: 2324.7070
In this epoch 650/5000, Training loss: 0.0011, Test loss: 2324.6843
In this epoch 700/5000, Training loss: 0.0008, Test loss: 2324.6628
In this epoch 750/5000, Training loss: 0.0006, Test loss: 2324.6443
In this epoch 800/5000, Training loss: 0.0005, Test loss: 2324.6279
In this epoch 850/5000, Training loss: 0.0003, Test loss: 2324.6111
In this epoch 900/5000, Training loss: 0.0003, Test loss: 2324.5989
In this epoch 950/5000, Training loss: 0.0002, Test loss: 2324.5891
In this epoch 1000/5000, Training loss: 0.0001, Test loss: 2324.5764
In this epoch 1050/5000, Training loss: 0.0001, Test loss: 2324.5681
In this epoch 1100/5000, Training loss: 0.0001, Test loss: 2324.5603
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 2324.5530
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 2324.5474
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 2324.5410
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 2324.5376
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 2324.5342
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 2324.5334
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 2324.5317
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 2324.5300
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 2324.5283
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 2324.5261
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 2324.5254
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 2324.5239
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 2324.5237
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 2324.5229
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 2324.5227
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 2324.5222
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 2324.5222
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 2324.5220
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 2324.5217
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 2324.5217
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 2324.5220
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 2324.5215
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 2324.5215
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 2324.5215
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 2324.5215
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 2324.5217
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 2324.5222
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 2324.5217
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 2324.5215
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 2324.5220
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 2324.5215
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 2324.5215
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 2324.5222
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 2324.5217
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 2324.5220
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 2324.5222
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 2324.5217
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 2324.5222
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 2324.5220
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 2324.5217
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 2324.5222
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 2324.5220
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 2324.5217
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 2324.5215
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 2324.5222
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 2324.5215
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 2324.5220
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 2324.5215
In this epoch 3550/5000, Training loss: 4451280.0000, Test loss: 751078.4375
In this epoch 3600/5000, Training loss: 23560.7031, Test loss: 8641.8223
In this epoch 3650/5000, Training loss: 89.6244, Test loss: 2352.8428
In this epoch 3700/5000, Training loss: 0.0347, Test loss: 2345.5320
In this epoch 3750/5000, Training loss: 0.0021, Test loss: 2351.9729
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 2351.9158
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 2351.9048
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 2351.9048
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 2351.9048
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 2351.9045
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 2351.9045
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 2351.9043
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 2351.9041
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 2351.9038
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 2351.9041
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 2351.9041
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 2351.9045
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 2351.9041
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 2351.9038
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 2351.9038
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 2351.9038
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 2351.9036
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 2351.9038
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 2351.9033
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 2351.9038
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 2351.9033
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 2351.9031
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 2351.9033
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 2351.9033
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 2351.9033
train_acc 0.34829801038062286
test_acc 0.2557006920415225
{'y_pred': [22.7520694732666, -36.378936767578125, -31.02195167541504, -8.50657844543457, -63.725074768066406, -54.920982360839844, -76.46664428710938, -11.632574081420898, -116.8825912475586, -9.523443222045898, 69.10467529296875, -15.646106719970703, 27.223657608032227, -11.06800651550293, -30.00178337097168, 41.62236022949219, 78.24200439453125, 6.6444411277771, 72.93193817138672, -16.329172134399414, 3.754434108734131, -24.8371639251709, -33.16253662109375, 3.6852355003356934, -30.0017147064209, 41.22679901123047, -1.9995331764221191, 26.948881149291992, -31.211477279663086, -52.12117004394531, -54.68795394897461, 15.250951766967773, -55.73162841796875, 37.06487274169922, -77.45418548583984, 19.97202491760254, -37.089256286621094, 9.110251426696777, -72.35624694824219, -61.60841751098633, -2.7244553565979004, -2.6938462257385254, -75.22323608398438, -14.68931770324707, 57.48267364501953, -4.090647220611572, 34.1522216796875, -47.934967041015625, 77.32757568359375, 17.02744483947754, -40.543212890625, -6.061537265777588, -22.337255477905273, 53.02829360961914, 87.11663055419922, 3.8959708213806152, -90.894287109375, -3.9922776222229004, 7.552667140960693, -78.33336639404297, 98.75285339355469, -11.49321174621582, -32.367794036865234, 45.050148010253906, 33.646270751953125, 104.5442123413086, -77.51338195800781, -36.775089263916016, 14.593057632446289, 86.25245666503906, -28.473020553588867, -12.461629867553711, 75.70047760009766, 44.46910095214844, 69.4105453491211, -23.034379959106445, -3.66925960406661e-05, 15.494112014770508, -16.46034049987793, 61.7595100402832, -26.820585250854492, -86.01470184326172, 105.22186279296875, 29.9692325592041, 36.29192352294922, -64.90579986572266, -31.400575637817383, -40.696250915527344, -7.310621738433838, -7.684027194976807, -47.28492736816406, -48.735286712646484, 35.20441818237305, 29.024126052856445, -0.9875779151916504, 55.86154556274414, 47.706764221191406, -9.938692092895508, -75.13427734375, 0.9619307518005371, 17.216611862182617, 110.32293701171875, -17.7222843170166, 61.25669860839844, 5.086942195892334, 58.02000427246094, 112.67021179199219, -32.75450897216797, -32.677249908447266, 27.082468032836914, 6.175111293792725, -52.88654327392578, -46.62196350097656, 50.371307373046875, -71.81278991699219, 22.176179885864258, -35.14009094238281, -66.62692260742188, 4.105157375335693, 44.11378479003906, 96.8381118774414, -8.635602951049805, 6.459470272064209, 5.877542018890381, 17.0880069732666, -31.486900329589844, -50.679527282714844, -12.399412155151367, -54.41884231567383, -5.864527225494385, -12.477499008178711, 20.376977920532227, 3.1291708946228027, -15.08900260925293, -24.640188217163086, -51.81871795654297, -26.148813247680664, -50.91396713256836, 35.889930725097656, -65.3157730102539, 58.77941131591797, -64.91020965576172, 34.42265319824219, 5.509232997894287, 13.615610122680664, 34.329986572265625, 46.1102294921875, 11.252286911010742, -5.676565647125244, -43.17821502685547, 56.885215759277344, 60.572052001953125, -39.61019515991211, 7.973198413848877, -10.563004493713379, 50.474159240722656, 70.86627960205078, 8.963825225830078, 63.65889358520508, -70.68067932128906, 101.47258758544922, -29.952402114868164, 96.4368896484375, -49.90190887451172, -85.96460723876953, 38.24456024169922, 28.054513931274414, -43.5705451965332, -60.734642028808594, -155.97984313964844, -9.763132095336914, -24.822622299194336, 34.730098724365234, 35.765254974365234, 33.144248962402344, -65.79345703125, 24.448965072631836, -32.522003173828125, 24.751619338989258, -39.70998001098633, -3.85999446734786e-05, -122.06159973144531, -39.434715270996094, -0.8931870460510254, -66.80938720703125, -8.924406051635742, -10.303495407104492, -4.8338799476623535, 42.181861877441406, -43.25438690185547, -56.420875549316406, 4.539224147796631, -68.52201080322266, -41.80645751953125, -18.2215518951416, 14.623891830444336, -66.10911560058594, -48.30333709716797, 8.292964935302734, 36.751182556152344, -9.453657150268555, 8.260690689086914, 64.6295166015625, -45.2413444519043, -22.332216262817383, 29.241098403930664, 46.41973876953125, -16.55806541442871, -0.4743179976940155, 1.4552741050720215, 12.643167495727539, 18.764036178588867, 121.8195571899414, 2.3833861351013184, -17.812963485717773, -29.451261520385742, -92.61236572265625, -4.405386447906494, -13.317472457885742, -34.152687072753906, -2.004263401031494, 73.34536743164062, 43.30021667480469, -1.521456241607666, 39.51958465576172, -4.297262668609619, -69.90714263916016, 62.45970153808594, -50.49491500854492, 48.10395812988281, 80.52521514892578, -17.57172966003418, 100.10052490234375, -89.51802062988281, -8.117177963256836, 82.12159729003906, -72.00070190429688, 53.156646728515625, -26.623151779174805, -11.232587814331055, 36.486961364746094, 73.03634643554688, -33.52268600463867, -13.61878776550293, -12.827779769897461, 0.148561030626297, -1.7832741737365723, -29.77924919128418, 9.99583625793457, 19.905969619750977, -23.334243774414062, 24.297243118286133, -57.553070068359375, 66.70564270019531, 25.576799392700195, 23.47221565246582, 19.286405563354492, -8.148252487182617, -6.51922082901001, 51.203739166259766, -14.356626510620117, -52.57870101928711, -81.94856262207031, -15.044179916381836, -148.12905883789062, -45.28932571411133, 18.286455154418945, -6.4783501625061035, -98.54605102539062, -0.5358262062072754, -21.031755447387695, -0.0001206159358844161, -17.846403121948242, 40.473941802978516, -52.0096435546875, -110.2755355834961, 69.07901763916016, 56.11046600341797, -50.949493408203125, 12.53056526184082, -38.85987854003906, -7.319601535797119, 21.606035232543945, -47.65277862548828, -13.00877571105957, 47.91073226928711, 25.549150466918945, -60.602149963378906, -1.129164218902588, -7.344324588775635, 28.09827995300293, -54.55738830566406, -39.71478271484375, 40.80961608886719, 25.099802017211914, -10.345987319946289, -18.352001190185547, 80.18854522705078, 5.308614253997803, -17.454614639282227, -58.259437561035156, 15.609537124633789, -46.743675231933594, -71.91339111328125, 54.690673828125, -20.45460319519043, -12.708587646484375, -81.34152221679688, -30.29104232788086, 32.229122161865234, 0.8620696067810059, 18.6641902923584, -72.79658508300781, -21.274553298950195, -48.36434555053711, -44.896873474121094, 30.722379684448242, 14.756967544555664, -25.90607261657715, -8.928970336914062, 71.70402526855469, -15.502668380737305, -31.87916374206543, -56.02272033691406, -70.56037139892578, 22.80534553527832, -33.41925048828125, -169.99769592285156, -62.13429260253906, 31.664644241333008, 48.53559112548828, -9.951055526733398, 7.143357753753662, 3.102578639984131, 30.00105094909668, 36.91811752319336, 31.90390968322754, -7.387640476226807, 61.422019958496094, 39.25343322753906], 'y_pred_int': [23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 69, 0, 27, 0, 0, 42, 78, 7, 73, 0, 4, 0, 0, 4, 0, 41, 0, 27, 0, 0, 0, 15, 0, 37, 0, 20, 0, 9, 0, 0, 0, 0, 0, 0, 57, 0, 34, 0, 77, 17, 0, 0, 0, 53, 87, 4, 0, 0, 8, 0, 99, 0, 0, 45, 34, 105, 0, 0, 15, 86, 0, 0, 76, 44, 69, 0, 0, 15, 0, 62, 0, 0, 105, 30, 36, 0, 0, 0, 0, 0, 0, 0, 35, 29, 0, 56, 48, 0, 0, 1, 17, 110, 0, 61, 5, 58, 113, 0, 0, 27, 6, 0, 0, 50, 0, 22, 0, 0, 4, 44, 97, 0, 6, 6, 17, 0, 0, 0, 0, 0, 0, 20, 3, 0, 0, 0, 0, 0, 36, 0, 59, 0, 34, 6, 14, 34, 46, 11, 0, 0, 57, 61, 0, 8, 0, 50, 71, 9, 64, 0, 101, 0, 96, 0, 0, 38, 28, 0, 0, 0, 0, 0, 35, 36, 33, 0, 24, 0, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 5, 0, 0, 0, 15, 0, 0, 8, 37, 0, 8, 65, 0, 0, 29, 46, 0, 0, 1, 13, 19, 122, 2, 0, 0, 0, 0, 0, 0, 0, 73, 43, 0, 40, 0, 0, 62, 0, 48, 81, 0, 100, 0, 0, 82, 0, 53, 0, 0, 36, 73, 0, 0, 0, 0, 0, 0, 10, 20, 0, 24, 0, 67, 26, 23, 19, 0, 0, 51, 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 40, 0, 0, 69, 56, 0, 13, 0, 0, 22, 0, 0, 48, 26, 0, 0, 0, 28, 0, 0, 41, 25, 0, 0, 80, 5, 0, 0, 16, 0, 0, 55, 0, 0, 0, 0, 32, 1, 19, 0, 0, 0, 0, 31, 15, 0, 0, 72, 0, 0, 0, 0, 23, 0, 0, 0, 32, 49, 0, 7, 3, 30, 37, 32, 0, 61, 39], 'y_true': [2, 1, 0, 6, 3, 0, 0, 0, 3, 1, 3, 0, 0, 5, 9, 0, 1, 0, 1, 1, 0, 1, 0, 0, 2, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 4, 1, 0, 1, 4, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 3, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 3, 2, 0, 2, 1, 0, 0, 0, 1, 0, 0, 1, 1, 2, 1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 4, 0, 14, 1, 0, 4, 0, 0, 0, 1, 1, 3, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 3, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 1, 2, 0, 1, 0, 1, 1, 0, 0, 0, 0, 3, 1, 0, 0, 1, 2, 0, 0, 1, 0, 1, 3, 1, 0, 1, 6, 0, 2, 2, 0, 0, 1, 2, 1, 0, 0, 0, 3, 0, 0, 1, 0, 1, 1, 1, 2, 0, 1, 0, 1, 3, 1, 1, 3, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 4, 0, 4, 2, 0, 1, 2, 0, 0, 1, 1, 1, 1, 0, 1, 2, 1, 2, 2, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 2, 0, 1, 0, 1, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 1, 4, 0, 1, 1, 2, 2, 0, 2, 2, 0, 1, 0, 1, 3, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 1, 0, 0, 6, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 0, 1, 1, 2, 0, 2, 2, 0], 'train_acc': 0.34829801038062286, 'test_acc': 0.2557006920415225, 'X': 'X_TANL_layer_7_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_8_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 251049.7031, Test loss: 36432.6562
In this epoch 100/5000, Training loss: 1328.4210, Test loss: 2255.0010
In this epoch 150/5000, Training loss: 13.3565, Test loss: 2202.8423
In this epoch 200/5000, Training loss: 0.0775, Test loss: 2207.8376
In this epoch 250/5000, Training loss: 0.0004, Test loss: 2208.8833
In this epoch 300/5000, Training loss: 0.0000, Test loss: 2208.8860
In this epoch 350/5000, Training loss: 0.0000, Test loss: 2208.8882
In this epoch 400/5000, Training loss: 0.0000, Test loss: 2208.8884
In this epoch 450/5000, Training loss: 0.0000, Test loss: 2208.8894
In this epoch 500/5000, Training loss: 0.0000, Test loss: 2208.8909
In this epoch 550/5000, Training loss: 0.0000, Test loss: 2208.8918
In this epoch 600/5000, Training loss: 0.0000, Test loss: 2208.8931
In this epoch 650/5000, Training loss: 0.0000, Test loss: 2208.8938
In this epoch 700/5000, Training loss: 0.0000, Test loss: 2208.8948
In this epoch 750/5000, Training loss: 0.0000, Test loss: 2208.8958
In this epoch 800/5000, Training loss: 0.0000, Test loss: 2208.8967
In this epoch 850/5000, Training loss: 0.0000, Test loss: 2208.8972
In this epoch 900/5000, Training loss: 0.0000, Test loss: 2208.8975
In this epoch 950/5000, Training loss: 0.0000, Test loss: 2208.8975
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 2208.8982
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 2208.8984
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 2208.8992
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 2208.8994
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 2208.8994
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 2208.8997
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 2208.8999
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 2208.8999
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 2208.8997
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 2208.9006
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 2208.9006
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 2208.9006
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 2208.9001
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 2208.9009
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 2208.9006
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 2208.9006
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 2208.9004
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 2208.9009
In this epoch 4150/5000, Training loss: 52255004.0000, Test loss: 61823416.0000
In this epoch 4200/5000, Training loss: 675891.3750, Test loss: 216265.5625
In this epoch 4250/5000, Training loss: 2913.0962, Test loss: 2339.5696
In this epoch 4300/5000, Training loss: 0.1357, Test loss: 2263.8994
In this epoch 4350/5000, Training loss: 0.0787, Test loss: 2267.7354
In this epoch 4400/5000, Training loss: 0.0005, Test loss: 2267.8252
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 2267.7810
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 2267.7854
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 2267.7859
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 2267.7878
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 2267.7886
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 2267.7893
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 2267.7908
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 2267.7917
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 2267.7922
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 2267.7930
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 2267.7937
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 2267.7942
train_acc 0.3536256487889273
test_acc 0.1867128027681661
{'y_pred': [-37.49983215332031, 32.740516662597656, 45.581844329833984, -27.508638381958008, -37.4797248840332, -46.70216369628906, 30.55012321472168, 9.741095542907715, 12.427721977233887, 78.49142456054688, -41.126258850097656, -21.788198471069336, 27.944887161254883, -64.47491455078125, 23.7893009185791, 3.3191897869110107, 30.341848373413086, -6.664584636688232, 29.6562442779541, 60.15486526489258, 8.62049388885498, 28.211530685424805, 11.189607620239258, 26.866058349609375, 27.74376678466797, -38.96649932861328, 71.22631072998047, 2.4182441234588623, 75.78868865966797, -26.55402183532715, -60.958587646484375, -8.487037658691406, -57.41498565673828, 14.252423286437988, 61.65454864501953, -51.789344787597656, 35.852108001708984, 74.81208038330078, -25.20001792907715, 2.43152117729187, 23.39253807067871, 0.18821968138217926, 9.388127326965332, 6.084599018096924, 18.15334129333496, 17.662450790405273, -14.044914245605469, -0.5265897512435913, 86.77301025390625, -10.000203132629395, 136.78860473632812, -35.09850311279297, 6.913465976715088, 6.633223056793213, 27.721454620361328, -47.4157829284668, -77.95145416259766, -12.923548698425293, -16.275794982910156, 21.331010818481445, 79.74882507324219, 29.527748107910156, -21.48907470703125, 19.72176742553711, 29.662710189819336, 70.75135040283203, 76.18216705322266, 12.045511245727539, 21.82997703552246, -1.4568113088607788, 45.749603271484375, -22.431320190429688, -62.628929138183594, 121.56844329833984, -12.736113548278809, 20.37474250793457, 21.20979881286621, 40.405094146728516, -15.13548755645752, -4.808671474456787, 12.39282512664795, 1.5021463632583618, -60.73577117919922, 4.03301477432251, -40.11438751220703, -24.7788143157959, -38.374908447265625, 57.9141845703125, 23.499189376831055, -4.625062465667725, 73.29991912841797, -8.54838752746582, -9.157795906066895, 40.57004165649414, 80.9725112915039, 72.71247100830078, -28.200712203979492, 11.975813865661621, 46.0200309753418, -7.210931301116943, 26.002538681030273, -58.10528564453125, 25.981002807617188, 55.48283386230469, -102.02373504638672, 66.93049621582031, 52.43388366699219, -73.28577423095703, -25.232830047607422, 52.981239318847656, -35.84761047363281, 68.6301498413086, -50.197383880615234, -0.20420967042446136, 11.847825050354004, -36.71448516845703, -97.25318908691406, 12.978907585144043, 32.056610107421875, 14.448004722595215, 12.240809440612793, -31.11017608642578, 77.7102279663086, -19.727157592773438, 95.25033569335938, -3.900578737258911, -68.29618072509766, -7.950135707855225, 15.316834449768066, -7.024242877960205, 53.498252868652344, 34.99309539794922, -85.483642578125, -65.34146881103516, 16.5992374420166, -16.166540145874023, -97.33850860595703, 31.625057220458984, -10.388489723205566, 11.987170219421387, 63.29869842529297, -23.638517379760742, 49.09307861328125, -77.74078369140625, 72.59464263916016, 36.687469482421875, 1.05813467502594, 12.80687141418457, -59.39384078979492, -52.82088088989258, -53.41899871826172, -48.9079475402832, 40.32890319824219, -108.92147064208984, 43.02818298339844, -28.09971809387207, 10.013440132141113, 32.54616928100586, -0.26483094692230225, 61.80656051635742, -23.73647117614746, -16.081674575805664, 54.62918472290039, -31.0346622467041, -27.942392349243164, -33.122711181640625, 72.0563735961914, 25.999183654785156, 2.3929812908172607, 7.153431415557861, -67.06199645996094, -3.011267900466919, 9.670463562011719, -12.835076332092285, 4.63064432144165, 37.908023834228516, -97.60857391357422, -67.58174896240234, 16.783361434936523, 20.035682678222656, 8.651618003845215, 10.274514198303223, -43.90157699584961, 11.063952445983887, -2.4043657779693604, -3.41072154045105, 2.4396426677703857, 3.5351569652557373, -13.175320625305176, -20.680696487426758, -24.02113151550293, -27.13971519470215, -10.70754337310791, 37.03578567504883, 28.097944259643555, -12.180428504943848, -13.092949867248535, -68.8861312866211, -9.049675941467285, 12.505690574645996, -22.62471580505371, 21.518285751342773, 52.82592010498047, -64.28231048583984, -82.66775512695312, 20.04119873046875, 20.56993865966797, -72.69244384765625, -64.9378890991211, 57.01047897338867, -12.820477485656738, 26.75713539123535, -3.462941884994507, -14.887635231018066, -54.16355514526367, -24.49601936340332, 21.742599487304688, 30.622329711914062, 60.54668426513672, 41.983909606933594, 90.3328628540039, -20.583921432495117, 23.397241592407227, 68.54842376708984, 24.979284286499023, 20.983449935913086, 12.552008628845215, -68.27207946777344, -7.971787929534912, -59.157867431640625, 34.43482971191406, 33.598793029785156, 103.86087799072266, -83.77738189697266, -114.60116577148438, 14.16926097869873, 48.618011474609375, 60.07798767089844, 32.64215087890625, 62.812931060791016, 54.933433532714844, 32.415504455566406, 40.12678527832031, 8.687129020690918, -44.09825897216797, -0.46540868282318115, 24.880826950073242, 51.13652801513672, 40.95740509033203, 77.64842224121094, 20.435348510742188, -10.637014389038086, -47.95923614501953, -44.00785827636719, 5.082945346832275, 7.56276273727417, -112.75293731689453, 46.250186920166016, 73.4443130493164, 36.08146286010742, -112.697509765625, -12.411910057067871, 24.47111701965332, 68.392578125, -53.868377685546875, 24.323184967041016, -79.75059509277344, 17.659318923950195, 24.990488052368164, 155.18455505371094, -35.120792388916016, 76.69395446777344, -42.235382080078125, 101.72821044921875, -23.899019241333008, -94.57467651367188, 44.805355072021484, 36.151268005371094, -11.14018726348877, -54.813194274902344, 46.56454849243164, 1.96230947971344, 71.87228393554688, 27.65788459777832, 36.35003662109375, -8.356717109680176, 21.35965347290039, 31.877466201782227, -84.35416412353516, 55.4780158996582, -12.83376407623291, -31.03353500366211, -19.996320724487305, 37.63801956176758, 35.13805389404297, -86.1745376586914, 109.04545593261719, -56.28395462036133, -65.86248016357422, 13.802767753601074, -55.432003021240234, -56.639488220214844, 34.31107711791992, 47.06131362915039, 40.17658233642578, -42.18451690673828, 44.470794677734375, -55.695343017578125, 11.233963966369629, 22.658498764038086, -28.970958709716797, -58.20561218261719, 0.15458358824253082, 1.5913225412368774, -29.850732803344727, -8.297642707824707, -33.63311767578125, -153.65310668945312, 30.61041831970215, 61.80656051635742, -7.517286777496338, -6.229224681854248, 62.59480285644531, -79.32717895507812, -14.368974685668945, 28.114164352416992, 11.990702629089355, -61.79329299926758, 55.614009857177734, 16.749998092651367, 57.47991943359375, -55.265594482421875, -85.00048065185547, 38.1112060546875, 24.409433364868164, -43.9874267578125, -21.652490615844727, 56.280799865722656, 8.498557090759277, 33.56554412841797], 'y_pred_int': [0, 33, 46, 0, 0, 0, 31, 10, 12, 78, 0, 0, 28, 0, 24, 3, 30, 0, 30, 60, 9, 28, 11, 27, 28, 0, 71, 2, 76, 0, 0, 0, 0, 14, 62, 0, 36, 75, 0, 2, 23, 0, 9, 6, 18, 18, 0, 0, 87, 0, 137, 0, 7, 7, 28, 0, 0, 0, 0, 21, 80, 30, 0, 20, 30, 71, 76, 12, 22, 0, 46, 0, 0, 122, 0, 20, 21, 40, 0, 0, 12, 2, 0, 4, 0, 0, 0, 58, 23, 0, 73, 0, 0, 41, 81, 73, 0, 12, 46, 0, 26, 0, 26, 55, 0, 67, 52, 0, 0, 53, 0, 69, 0, 0, 12, 0, 0, 13, 32, 14, 12, 0, 78, 0, 95, 0, 0, 0, 15, 0, 53, 35, 0, 0, 17, 0, 0, 32, 0, 12, 63, 0, 49, 0, 73, 37, 1, 13, 0, 0, 0, 0, 40, 0, 43, 0, 10, 33, 0, 62, 0, 0, 55, 0, 0, 0, 72, 26, 2, 7, 0, 0, 10, 0, 5, 38, 0, 0, 17, 20, 9, 10, 0, 11, 0, 0, 2, 4, 0, 0, 0, 0, 0, 37, 28, 0, 0, 0, 0, 13, 0, 22, 53, 0, 0, 20, 21, 0, 0, 57, 0, 27, 0, 0, 0, 0, 22, 31, 61, 42, 90, 0, 23, 69, 25, 21, 13, 0, 0, 0, 34, 34, 104, 0, 0, 14, 49, 60, 33, 63, 55, 32, 40, 9, 0, 0, 25, 51, 41, 78, 20, 0, 0, 0, 5, 8, 0, 46, 73, 36, 0, 0, 24, 68, 0, 24, 0, 18, 25, 155, 0, 77, 0, 102, 0, 0, 45, 36, 0, 0, 47, 2, 72, 28, 36, 0, 21, 32, 0, 55, 0, 0, 0, 38, 35, 0, 109, 0, 0, 14, 0, 0, 34, 47, 40, 0, 44, 0, 11, 23, 0, 0, 0, 2, 0, 0, 0, 0, 31, 62, 0, 0, 63, 0, 0, 28, 12, 0, 56, 17, 57, 0, 0, 38, 24, 0, 0, 56, 8, 34], 'y_true': [0, 0, 1, 1, 1, 1, 2, 2, 0, 2, 3, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 5, 1, 0, 0, 2, 2, 1, 0, 1, 0, 0, 2, 1, 0, 1, 1, 2, 0, 0, 6, 0, 5, 0, 0, 0, 1, 0, 1, 0, 0, 3, 1, 1, 0, 0, 4, 0, 1, 1, 4, 1, 3, 0, 6, 0, 1, 1, 1, 0, 0, 0, 0, 1, 2, 2, 0, 2, 0, 2, 1, 3, 3, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 0, 1, 0, 1, 0, 0, 1, 3, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 3, 1, 8, 0, 0, 0, 1, 0, 1, 0, 1, 0, 3, 0, 3, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 3, 1, 0, 5, 0, 1, 1, 2, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 2, 1, 1, 1, 2, 1, 0, 0, 1, 1, 2, 0, 0, 4, 0, 2, 0, 1, 1, 2, 1, 0, 1, 1, 4, 0, 0, 1, 2, 0, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 0, 0, 2, 0, 1, 0, 3, 1, 1, 1, 0, 0, 4, 2, 3, 0, 0, 0, 1, 2, 1, 1, 1, 1, 3, 1, 0, 2, 1, 1, 1, 1, 3, 0, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 3, 1, 2, 0, 0, 1, 0, 2, 0, 2, 3, 0, 0, 2, 6, 2, 2, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 3, 1, 0, 2, 1, 1, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0, 1, 1, 0, 2, 1, 0, 0, 0, 3, 0], 'train_acc': 0.3536256487889273, 'test_acc': 0.1867128027681661, 'X': 'X_TANL_layer_8_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_9_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 502126.2812, Test loss: 151989.5938
In this epoch 100/5000, Training loss: 3465.7979, Test loss: 7969.2217
In this epoch 150/5000, Training loss: 8.7763, Test loss: 6585.5327
In this epoch 200/5000, Training loss: 0.0330, Test loss: 6612.9868
In this epoch 250/5000, Training loss: 0.0052, Test loss: 6617.0957
In this epoch 300/5000, Training loss: 0.0041, Test loss: 6616.7632
In this epoch 350/5000, Training loss: 0.0033, Test loss: 6616.6826
In this epoch 400/5000, Training loss: 0.0026, Test loss: 6616.6201
In this epoch 450/5000, Training loss: 0.0021, Test loss: 6616.5591
In this epoch 500/5000, Training loss: 0.0016, Test loss: 6616.5054
In this epoch 550/5000, Training loss: 0.0012, Test loss: 6616.4561
In this epoch 600/5000, Training loss: 0.0009, Test loss: 6616.4121
In this epoch 650/5000, Training loss: 0.0006, Test loss: 6616.3677
In this epoch 700/5000, Training loss: 0.0004, Test loss: 6616.3311
In this epoch 750/5000, Training loss: 0.0003, Test loss: 6616.2979
In this epoch 800/5000, Training loss: 0.0002, Test loss: 6616.2661
In this epoch 850/5000, Training loss: 0.0001, Test loss: 6616.2441
In this epoch 900/5000, Training loss: 0.0001, Test loss: 6616.2222
In this epoch 950/5000, Training loss: 0.0001, Test loss: 6616.2061
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 6616.1899
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 6616.1787
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 6616.1670
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 6616.1597
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 6616.1538
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 6616.1494
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 6616.1450
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 6616.1411
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 6616.1382
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 6616.1367
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 6616.1338
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 6616.1323
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 6616.1323
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 6616.1323
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 6616.1309
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 6616.1309
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 6616.1304
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 6616.1304
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 6616.1294
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 6616.1294
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 6616.1294
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 6616.1279
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 6616.1279
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 6616.1279
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 6616.1279
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 6616.1294
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 6616.1279
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 6616.1279
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 6616.1279
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 6616.1294
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 6616.1279
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 6616.1294
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 6616.1294
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 6616.1289
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 6616.1294
In this epoch 3450/5000, Training loss: 180092416.0000, Test loss: 121072352.0000
In this epoch 3500/5000, Training loss: 4561.1792, Test loss: 558149.6875
In this epoch 3550/5000, Training loss: 3677.5908, Test loss: 11607.0078
In this epoch 3600/5000, Training loss: 6.0132, Test loss: 6618.7471
In this epoch 3650/5000, Training loss: 0.1327, Test loss: 6582.3965
In this epoch 3700/5000, Training loss: 0.0007, Test loss: 6583.2773
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 6583.3691
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 6583.3833
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 6583.3647
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 6583.3896
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 6583.3940
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 6583.3979
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 6583.4023
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 6583.4092
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 6583.4136
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 6583.4165
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 6583.4194
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 6583.4229
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 6583.4268
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 6583.4282
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 6583.4297
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 6583.4312
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 6583.4341
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 6583.4341
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 6583.4355
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 6583.4360
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 6583.4370
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 6583.4375
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 6583.4385
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 6583.4399
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 6583.4399
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 6583.4399
train_acc 0.3529660467128028
test_acc 0.23349480968858133
{'y_pred': [41.88755416870117, 34.43773651123047, -18.625207901000977, 38.05851745605469, 164.6389923095703, -76.02153778076172, -70.43976593017578, -20.99467658996582, 68.12598419189453, -34.95619583129883, -33.137813568115234, -113.80929565429688, -22.319738388061523, 12.570706367492676, -23.682043075561523, 52.68874740600586, 9.073147773742676, -0.23592813313007355, -52.20967483520508, 62.032081604003906, 144.47537231445312, -30.016721725463867, -20.779069900512695, -57.31598663330078, 277.6184387207031, 93.33724975585938, 5.0944043323397636e-05, -47.05408477783203, -145.03123474121094, 77.38399505615234, -81.6800308227539, -56.56044387817383, 16.101438522338867, -69.5699691772461, 55.5277099609375, -18.587377548217773, 39.962772369384766, -9.380976676940918, -37.430938720703125, -138.6848907470703, -83.93177032470703, -66.56945037841797, -68.33320617675781, -2.348921298980713, 24.479387283325195, -56.777645111083984, 54.298885345458984, 58.26260757446289, 5.629136562347412, 82.96977996826172, 14.0160493850708, -154.74888610839844, -92.33802795410156, 40.01945877075195, 141.64761352539062, 76.8343276977539, 145.39352416992188, 99.61231231689453, -128.6127471923828, -196.2970428466797, 51.21042251586914, 63.1777458190918, -103.6436996459961, 8.979035377502441, 30.244871139526367, -22.655481338500977, 21.260345458984375, 29.964601516723633, -43.43857192993164, -68.79507446289062, 15.31194019317627, -16.8603572845459, 92.84593963623047, 165.138916015625, -0.0007081807125359774, -31.032617568969727, 6.6558051109313965, 31.057424545288086, -37.09022521972656, -74.53787231445312, 53.46446990966797, 67.67997741699219, -111.23539733886719, -108.28655242919922, 58.03517532348633, -181.37513732910156, 51.1967887878418, -67.32072448730469, 18.513320922851562, -63.410099029541016, -55.135494232177734, 64.87078094482422, -6.4294114112854, 37.97978210449219, 42.340885162353516, -50.95488357543945, 5.550145626068115, -119.75480651855469, -4.4568352699279785, 69.20777130126953, -3.842134952545166, -55.37495803833008, -115.8685531616211, 22.857213973999023, -10.446825981140137, -59.04924774169922, -172.82334899902344, 14.107264518737793, -57.57466125488281, 185.13902282714844, 118.90171813964844, 16.541696548461914, -112.63175201416016, 5.408135890960693, 117.67388916015625, -56.8398323059082, -6.930206775665283, 60.43564987182617, 90.66203308105469, 31.3382625579834, -61.91775894165039, -167.8070831298828, 9.777050971984863, -26.71021270751953, 12.319430351257324, -154.30636596679688, -76.23695373535156, -7.402115345001221, -15.406630516052246, -18.098756790161133, -33.40450668334961, -96.82759857177734, -35.101993560791016, 76.17231750488281, 43.31804275512695, -140.21119689941406, 27.1356258392334, 8.940308570861816, 46.856998443603516, -171.1152801513672, 1.3463948965072632, -56.33464431762695, -64.22875213623047, 168.00238037109375, -49.70864486694336, 18.242586135864258, 40.215572357177734, -85.47518920898438, 93.97879028320312, -124.49211883544922, 22.670869827270508, 85.71376037597656, 30.76520347595215, -18.649354934692383, 60.5649528503418, -29.171628952026367, -52.67467498779297, -74.88724517822266, -32.24300003051758, 4.551370143890381, -22.292943954467773, 11.378216743469238, -39.779842376708984, 20.780475616455078, -60.10655975341797, 40.45263671875, -48.12574005126953, 5.614952564239502, -165.67987060546875, -28.7095890045166, 25.690673828125, 55.64607238769531, -31.40656852722168, 30.956418991088867, -114.9183349609375, -155.19956970214844, 233.1215057373047, 49.13124465942383, 28.782869338989258, 17.10933494567871, 8.902321815490723, -11.195902824401855, -49.80417251586914, -30.975889205932617, 94.33692932128906, 183.7077178955078, -27.322710037231445, 248.60244750976562, -68.54234313964844, 76.58049011230469, -76.25592803955078, -81.7113265991211, -50.85331344604492, -38.984806060791016, 18.413747787475586, -0.26479586958885193, 45.477413177490234, 28.01222038269043, -20.67450523376465, -40.570533752441406, -163.85092163085938, 12.738385200500488, 37.13434600830078, 42.9577522277832, 1.7237123250961304, -27.55101203918457, 66.17285919189453, -31.274763107299805, -117.63018035888672, -24.866609573364258, 15.826390266418457, -24.488191604614258, 57.414825439453125, 72.27637481689453, -16.07526969909668, 100.49310302734375, -25.76527214050293, 183.08187866210938, -13.372157096862793, 11.653607368469238, -36.95722579956055, -49.790218353271484, -120.0562973022461, 36.110477447509766, -160.50721740722656, -95.45155334472656, 26.170686721801758, -33.37809371948242, -17.905637741088867, -73.74925994873047, 49.62290573120117, -119.26155090332031, -63.135231018066406, 21.871618270874023, 26.177453994750977, -103.3991470336914, -195.53054809570312, 77.3089828491211, 62.101539611816406, 189.80804443359375, -92.69851684570312, 63.396366119384766, -123.46497344970703, -63.94104766845703, -92.7755355834961, -116.70697784423828, -47.629093170166016, 78.47196197509766, 139.80271911621094, 73.36571502685547, 34.14625549316406, 62.324920654296875, -23.37488555908203, 12.150715827941895, -9.70264720916748, 17.32419776916504, 25.651872634887695, -41.806827545166016, 8.726571083068848, -72.6541976928711, 39.98743438720703, -34.58192443847656, -96.44194030761719, 26.37775230407715, 70.06566619873047, 60.85215377807617, -23.435911178588867, -95.59007263183594, -8.318140983581543, -51.752864837646484, 79.6749038696289, 76.7266616821289, 33.287235260009766, 25.61271095275879, -42.144039154052734, 7.714822292327881, -62.18339157104492, -23.619661331176758, -56.170249938964844, -50.997188568115234, -27.378061294555664, 182.29141235351562, 6.1850810050964355, 90.72560119628906, 4.193933010101318, -8.21264934539795, -20.2273006439209, 39.98831558227539, -38.853431701660156, 47.89939880371094, 42.52595901489258, 21.735923767089844, 43.26406478881836, 65.7567138671875, -49.331695556640625, -53.7767219543457, -163.0135040283203, 65.25856018066406, -170.1849822998047, 85.47359466552734, 55.5857048034668, -134.63063049316406, 179.11927795410156, -67.21986389160156, 73.12150573730469, 55.10748291015625, -80.79512786865234, -68.10720825195312, 54.72530746459961, 26.54319190979004, -25.794065475463867, 42.68094253540039, 35.16123580932617, -61.78945541381836, -115.5034408569336, 101.82367706298828, -29.071809768676758, 168.2932891845703, 14.401978492736816, 53.03982925415039, -55.5047721862793, 41.47532272338867, -51.078758239746094, -209.1747283935547, -78.52137756347656, -140.5568389892578, -92.78030395507812, 146.67572021484375, -26.467058181762695, -7.878614902496338, 100.89115905761719, -24.91362953186035, -56.816593170166016, 107.85215759277344, 17.740652084350586, -3.0535950660705566, -118.76471710205078, 8.02214527130127, 280.3908386230469, -61.30429458618164], 'y_pred_int': [42, 34, 0, 38, 165, 0, 0, 0, 68, 0, 0, 0, 0, 13, 0, 53, 9, 0, 0, 62, 144, 0, 0, 0, 278, 93, 0, 0, 0, 77, 0, 0, 16, 0, 56, 0, 40, 0, 0, 0, 0, 0, 0, 0, 24, 0, 54, 58, 6, 83, 14, 0, 0, 40, 142, 77, 145, 100, 0, 0, 51, 63, 0, 9, 30, 0, 21, 30, 0, 0, 15, 0, 93, 165, 0, 0, 7, 31, 0, 0, 53, 68, 0, 0, 58, 0, 51, 0, 19, 0, 0, 65, 0, 38, 42, 0, 6, 0, 0, 69, 0, 0, 0, 23, 0, 0, 0, 14, 0, 185, 119, 17, 0, 5, 118, 0, 0, 60, 91, 31, 0, 0, 10, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 76, 43, 0, 27, 9, 47, 0, 1, 0, 0, 168, 0, 18, 40, 0, 94, 0, 23, 86, 31, 0, 61, 0, 0, 0, 0, 5, 0, 11, 0, 21, 0, 40, 0, 6, 0, 0, 26, 56, 0, 31, 0, 0, 233, 49, 29, 17, 9, 0, 0, 0, 94, 184, 0, 249, 0, 77, 0, 0, 0, 0, 18, 0, 45, 28, 0, 0, 0, 13, 37, 43, 2, 0, 66, 0, 0, 0, 16, 0, 57, 72, 0, 100, 0, 183, 0, 12, 0, 0, 0, 36, 0, 0, 26, 0, 0, 0, 50, 0, 0, 22, 26, 0, 0, 77, 62, 190, 0, 63, 0, 0, 0, 0, 0, 78, 140, 73, 34, 62, 0, 12, 0, 17, 26, 0, 9, 0, 40, 0, 0, 26, 70, 61, 0, 0, 0, 0, 80, 77, 33, 26, 0, 8, 0, 0, 0, 0, 0, 182, 6, 91, 4, 0, 0, 40, 0, 48, 43, 22, 43, 66, 0, 0, 0, 65, 0, 85, 56, 0, 179, 0, 73, 55, 0, 0, 55, 27, 0, 43, 35, 0, 0, 102, 0, 168, 14, 53, 0, 41, 0, 0, 0, 0, 0, 147, 0, 0, 101, 0, 0, 108, 18, 0, 0, 8, 280, 0], 'y_true': [2, 2, 1, 0, 0, 4, 0, 1, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 3, 3, 4, 0, 0, 1, 1, 0, 1, 0, 0, 3, 1, 1, 2, 0, 0, 2, 1, 2, 1, 1, 1, 2, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 3, 1, 2, 0, 1, 3, 2, 1, 1, 1, 2, 1, 3, 1, 0, 0, 1, 0, 3, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 3, 5, 1, 2, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 2, 2, 1, 0, 1, 14, 1, 1, 3, 1, 1, 0, 0, 1, 2, 1, 1, 0, 0, 1, 4, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 7, 0, 0, 1, 3, 3, 4, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 2, 0, 0, 1, 0, 1, 1, 2, 1, 0, 1, 3, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 5, 7, 0, 3, 0, 4, 0, 0, 0, 2, 0, 0, 1, 0, 6, 1, 1, 2, 1, 1, 0, 2, 0, 2, 1, 2, 1, 0, 2, 2, 1, 1, 0, 2, 5, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 3, 1, 0, 1, 2, 1, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 1, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 3, 0, 2, 0, 2, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 3, 1, 3, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 3, 1, 5, 0, 1, 0, 2, 4, 1, 1, 0, 0, 1, 1, 1, 0, 0, 4], 'train_acc': 0.3529660467128028, 'test_acc': 0.23349480968858133, 'X': 'X_TANL_layer_9_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_10_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 102331.8828, Test loss: 50883.8828
In this epoch 100/5000, Training loss: 2149.3486, Test loss: 20245.8301
In this epoch 150/5000, Training loss: 19.1951, Test loss: 18301.4004
In this epoch 200/5000, Training loss: 0.1197, Test loss: 18306.6465
In this epoch 250/5000, Training loss: 0.0006, Test loss: 18310.7910
In this epoch 300/5000, Training loss: 0.0000, Test loss: 18310.5996
In this epoch 350/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 400/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 450/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 500/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 550/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 600/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 650/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 700/5000, Training loss: 0.0000, Test loss: 18310.6230
In this epoch 750/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 800/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 850/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 900/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 950/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 18310.6230
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 18310.6191
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 18310.6211
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 18310.6172
In this epoch 3250/5000, Training loss: 0.0634, Test loss: 18272.7246
In this epoch 3300/5000, Training loss: 2539868.7500, Test loss: 4849666.0000
In this epoch 3350/5000, Training loss: 9606.3203, Test loss: 19673.7871
In this epoch 3400/5000, Training loss: 143.6817, Test loss: 18392.6465
In this epoch 3450/5000, Training loss: 0.7755, Test loss: 18485.7773
In this epoch 3500/5000, Training loss: 0.0023, Test loss: 18476.9668
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 18476.7266
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 18476.7969
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 18476.8535
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 18476.7910
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 18476.8535
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 18476.8125
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 18476.7949
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 18476.8242
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 18476.8223
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 18476.8223
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 18476.8223
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 18476.8184
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 18476.8223
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 18476.8242
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 18476.8223
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 18476.8242
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 18476.8203
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 18476.8223
train_acc 0.35488538062283737
test_acc 0.23798442906574394
{'y_pred': [-41.96446228027344, 7.64266300201416, -42.352149963378906, -57.852989196777344, 103.60577392578125, -27.45281982421875, -99.73409271240234, -188.69308471679688, -72.48442840576172, -102.47896575927734, -95.5272445678711, -259.056396484375, -96.10972595214844, 42.53223419189453, 88.6301040649414, -28.034881591796875, -84.5730972290039, -194.69078063964844, 21.337501525878906, 398.26544189453125, -103.66722106933594, 350.5407409667969, 22.987213134765625, -476.8242492675781, 83.50450897216797, 211.04525756835938, -32.4309196472168, -121.57549285888672, -185.7802734375, 285.8121337890625, -10.72376537322998, -101.77909088134766, -25.805274963378906, -156.97703552246094, -55.466148376464844, 96.01465606689453, 35.13984680175781, 155.99476623535156, 94.5317153930664, -1.6603232622146606, -37.827964782714844, 382.1635437011719, -4.295218467712402, -42.80339813232422, -243.87477111816406, 131.1323699951172, 70.9162826538086, -77.68616485595703, 26.487037658691406, 46.7740592956543, 33.01685333251953, -6.3805952072143555, 94.7259750366211, 19.46493148803711, -112.81536865234375, -90.16377258300781, -12.58750057220459, 80.82523345947266, -225.37747192382812, 72.63224029541016, -68.5627670288086, 50.34766387939453, -95.95700073242188, 73.41259002685547, -68.21097564697266, -236.07411193847656, 298.1361389160156, -76.4330062866211, 113.79219818115234, -86.03205108642578, -236.1551055908203, -122.53583526611328, -209.38461303710938, -113.30645751953125, 64.4426498413086, 3.24878191947937, -69.75031280517578, -60.329994201660156, -58.543514251708984, -89.86833953857422, 118.1377944946289, 14.360611915588379, -226.6693572998047, 362.3279724121094, -33.246734619140625, 61.183563232421875, 12.375245094299316, -84.9100112915039, 109.0063247680664, 57.353492736816406, 109.02640533447266, -264.2370910644531, 8.533791542053223, -72.80388641357422, 189.6019287109375, -223.62242126464844, 356.9667663574219, 101.05804443359375, -22.49413299560547, 96.72949981689453, 47.88580322265625, 580.4437255859375, 117.9257583618164, 38.38251495361328, -27.15019989013672, -39.237449645996094, 70.53889465332031, -52.22725296020508, 46.43671417236328, -73.21576690673828, -87.2734603881836, 24.81679916381836, -201.17044067382812, 88.45561981201172, 87.56086730957031, 79.56444549560547, -30.168846130371094, -90.29623413085938, 282.3434143066406, -146.32420349121094, -229.10499572753906, -98.6980209350586, 94.08960723876953, 38.945396423339844, -24.746604919433594, -28.283348083496094, 24.816566467285156, 6.517086982727051, 46.83000946044922, 28.098312377929688, 4.977889060974121, 61.247650146484375, 59.97730255126953, 38.58324432373047, 51.26258087158203, -21.688453674316406, 141.11692810058594, 108.21517944335938, -256.2779846191406, -110.09530639648438, 33.060546875, 37.929351806640625, -38.321014404296875, -10.119135856628418, -142.2360076904297, -95.43614196777344, -16.159080505371094, 142.15354919433594, 98.60636138916016, -95.55219268798828, -124.5163345336914, -28.963706970214844, -21.789894104003906, 18.64733123779297, -200.634033203125, -5.261228943709284e-05, -380.5109558105469, -26.42116928100586, 154.44541931152344, -37.116493225097656, -143.2532501220703, -119.04874420166016, 90.62557220458984, 39.30670928955078, 27.211631774902344, 56.625, -9.758549690246582, -19.995460510253906, -10.473158836364746, 122.2494888305664, 19.44269561767578, -247.68194580078125, -177.4949188232422, 248.06605529785156, 50.356719970703125, 4.388428688049316, 79.44219207763672, -150.80125427246094, -8.633744239807129, 91.87063598632812, 95.61124420166016, -237.07418823242188, 71.65560150146484, 167.19314575195312, 34.01582336425781, -387.4348449707031, -170.4869384765625, -122.20890808105469, -78.95932006835938, -40.149269104003906, -40.98182678222656, 80.15807342529297, 106.45648956298828, 52.13218688964844, -34.756507873535156, 107.39568328857422, 113.85550689697266, -162.3201904296875, 57.40830993652344, -55.21379089355469, -94.10432434082031, 126.58924102783203, -313.2237243652344, -182.6596221923828, 72.0685043334961, -20.94988250732422, 210.84796142578125, -258.2594299316406, -50.00045394897461, -17.82482147216797, 54.41285705566406, -245.47840881347656, 40.14396667480469, -41.79662322998047, 106.88915252685547, -36.639705657958984, -74.24134063720703, -195.52146911621094, 218.71554565429688, -47.59016799926758, 39.303062438964844, 37.40485382080078, -111.62749481201172, -62.34085464477539, 19.902603149414062, -155.7272491455078, 56.9981689453125, 305.02813720703125, -154.31446838378906, 24.636817932128906, -74.0536117553711, -18.478271484375, -308.5396423339844, -55.74677276611328, -72.19847869873047, 12.90590763092041, 190.2924346923828, -115.38945770263672, -20.29205322265625, 65.63251495361328, -286.6228332519531, -68.43729400634766, -98.95600128173828, 115.3034439086914, 17.608806610107422, 60.676597595214844, -57.04115295410156, 2.920395612716675, -134.34335327148438, 82.48951721191406, -69.2898178100586, -123.96416473388672, 27.23609733581543, -321.0994567871094, -61.46175765991211, -42.78675842285156, -71.41100311279297, 127.29652404785156, -150.09327697753906, 210.4373321533203, -67.9687271118164, -79.54249572753906, 55.593421936035156, 127.4144058227539, 233.70278930664062, 5.743454933166504, 95.96587371826172, -203.1194610595703, -294.1591796875, -111.4546127319336, -3.821582078933716, 27.000709533691406, -18.704132080078125, 12.878575325012207, -52.011505126953125, 18.10900115966797, -9.14621639251709, -67.10897064208984, -117.48072052001953, 51.63745880126953, 126.0959701538086, -5.819239616394043, -14.56717586517334, 136.0670623779297, 104.90178680419922, 70.42144775390625, -68.35562133789062, -61.67677307128906, -94.9360580444336, 92.13707733154297, 77.84253692626953, 74.62236785888672, 139.7310028076172, -154.70738220214844, -264.22509765625, -68.75431823730469, -175.1646270751953, 261.6134033203125, 78.83521270751953, -82.23450469970703, -158.2887420654297, -83.77200317382812, 8.822361946105957, 222.20448303222656, -61.698570251464844, 20.89783477783203, -55.71465301513672, -85.24066162109375, 11.364090919494629, 4.607590675354004, 115.30941009521484, -93.24922943115234, -8.635184288024902, -108.2081298828125, -57.11921310424805, 24.80249786376953, -95.04798126220703, -9.929312705993652, 29.493690490722656, 43.01294708251953, 264.6577453613281, 107.16326141357422, -148.5403594970703, -105.07868957519531, -124.96501922607422, 301.1028137207031, -102.42269134521484, -19.579933166503906, 114.61626434326172, 202.5045928955078, 32.25115966796875, -122.15123748779297, 50.3800048828125, -8.200858116149902, -182.10403442382812, -4.21115779876709, 178.00904846191406, 108.69043731689453, -308.8735046386719, -6.99155330657959], 'y_pred_int': [0, 8, 0, 0, 104, 0, 0, 0, 0, 0, 0, 0, 0, 43, 89, 0, 0, 0, 21, 398, 0, 351, 23, 0, 84, 211, 0, 0, 0, 286, 0, 0, 0, 0, 0, 96, 35, 156, 95, 0, 0, 382, 0, 0, 0, 131, 71, 0, 26, 47, 33, 0, 95, 19, 0, 0, 0, 81, 0, 73, 0, 50, 0, 73, 0, 0, 298, 0, 114, 0, 0, 0, 0, 0, 64, 3, 0, 0, 0, 0, 118, 14, 0, 362, 0, 61, 12, 0, 109, 57, 109, 0, 9, 0, 190, 0, 357, 101, 0, 97, 48, 580, 118, 38, 0, 0, 71, 0, 46, 0, 0, 25, 0, 88, 88, 80, 0, 0, 282, 0, 0, 0, 94, 39, 0, 0, 25, 7, 47, 28, 5, 61, 60, 39, 51, 0, 141, 108, 0, 0, 33, 38, 0, 0, 0, 0, 0, 142, 99, 0, 0, 0, 0, 19, 0, 0, 0, 0, 154, 0, 0, 0, 91, 39, 27, 57, 0, 0, 0, 122, 19, 0, 0, 248, 50, 4, 79, 0, 0, 92, 96, 0, 72, 167, 34, 0, 0, 0, 0, 0, 0, 80, 106, 52, 0, 107, 114, 0, 57, 0, 0, 127, 0, 0, 72, 0, 211, 0, 0, 0, 54, 0, 40, 0, 107, 0, 0, 0, 219, 0, 39, 37, 0, 0, 20, 0, 57, 305, 0, 25, 0, 0, 0, 0, 0, 13, 190, 0, 0, 66, 0, 0, 0, 115, 18, 61, 0, 3, 0, 82, 0, 0, 27, 0, 0, 0, 0, 127, 0, 210, 0, 0, 56, 127, 234, 6, 96, 0, 0, 0, 0, 27, 0, 13, 0, 18, 0, 0, 0, 52, 126, 0, 0, 136, 105, 70, 0, 0, 0, 92, 78, 75, 140, 0, 0, 0, 0, 262, 79, 0, 0, 0, 9, 222, 0, 21, 0, 0, 11, 5, 115, 0, 0, 0, 0, 25, 0, 0, 29, 43, 265, 107, 0, 0, 0, 301, 0, 0, 115, 203, 32, 0, 50, 0, 0, 0, 178, 109, 0, 0], 'y_true': [0, 0, 1, 0, 8, 1, 1, 0, 4, 5, 2, 1, 2, 0, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 2, 2, 1, 3, 0, 0, 0, 1, 2, 3, 1, 1, 1, 0, 1, 0, 0, 2, 0, 3, 2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 0, 1, 1, 1, 4, 0, 0, 1, 0, 0, 0, 0, 3, 1, 1, 3, 1, 0, 1, 1, 0, 3, 2, 4, 1, 0, 1, 0, 0, 1, 2, 2, 1, 3, 2, 0, 1, 1, 2, 1, 0, 0, 0, 2, 1, 0, 1, 0, 1, 0, 1, 2, 14, 3, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 5, 3, 1, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 1, 0, 1, 2, 3, 1, 0, 1, 0, 1, 0, 0, 2, 2, 0, 1, 0, 0, 1, 2, 1, 0, 0, 0, 2, 0, 1, 1, 4, 1, 0, 0, 0, 3, 0, 0, 0, 2, 1, 0, 0, 4, 1, 2, 0, 2, 3, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 1, 0, 1, 0, 0, 1, 1, 4, 0, 2, 0, 0, 1, 8, 1, 0, 0, 5, 1, 2, 0, 3, 1, 1, 2, 1, 1, 0, 0, 1, 3, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 3, 2, 1, 2, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 0, 1, 1, 2, 0, 0, 1, 1, 1, 6, 1, 0, 0, 2, 0, 5, 1, 2, 2, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 3, 2, 3, 0, 1, 0, 0, 1, 3, 2, 1, 2, 1, 1, 2, 0, 0, 7, 1, 0, 1], 'train_acc': 0.35488538062283737, 'test_acc': 0.23798442906574394, 'X': 'X_TANL_layer_10_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_11_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 668360.1250, Test loss: 78589.0859
In this epoch 100/5000, Training loss: 2366.4966, Test loss: 33272.9023
In this epoch 150/5000, Training loss: 40.9170, Test loss: 32844.2344
In this epoch 200/5000, Training loss: 0.1700, Test loss: 32775.5469
In this epoch 250/5000, Training loss: 0.0004, Test loss: 32775.5273
In this epoch 300/5000, Training loss: 0.0002, Test loss: 32775.1406
In this epoch 350/5000, Training loss: 0.0001, Test loss: 32775.1289
In this epoch 400/5000, Training loss: 0.0001, Test loss: 32775.2109
In this epoch 450/5000, Training loss: 0.0001, Test loss: 32775.2695
In this epoch 500/5000, Training loss: 0.0001, Test loss: 32775.3281
In this epoch 550/5000, Training loss: 0.0001, Test loss: 32775.3984
In this epoch 600/5000, Training loss: 0.0000, Test loss: 32775.4414
In this epoch 650/5000, Training loss: 0.0000, Test loss: 32775.4844
In this epoch 700/5000, Training loss: 0.0000, Test loss: 32775.5117
In this epoch 750/5000, Training loss: 0.0000, Test loss: 32775.5547
In this epoch 800/5000, Training loss: 0.0000, Test loss: 32775.5859
In this epoch 850/5000, Training loss: 0.0000, Test loss: 32775.6133
In this epoch 900/5000, Training loss: 0.0000, Test loss: 32775.6328
In this epoch 950/5000, Training loss: 0.0000, Test loss: 32775.6523
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 32775.6719
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 32775.6875
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 32775.6992
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 32775.7109
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 32775.7227
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 32775.7227
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 32775.7305
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 32775.7344
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 32775.7383
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 32775.7383
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 32775.7344
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 32775.7344
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 32775.7383
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 32775.7383
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 32775.7383
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 32775.7461
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 32775.7461
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 32775.7461
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 32775.7344
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 32775.7383
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 32775.7344
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 32775.7383
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 32775.7344
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 32775.7422
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 32775.7344
In this epoch 3600/5000, Training loss: 1228840.5000, Test loss: 63405104.0000
In this epoch 3650/5000, Training loss: 4893339.5000, Test loss: 1056528.3750
In this epoch 3700/5000, Training loss: 13611.3330, Test loss: 51048.6641
In this epoch 3750/5000, Training loss: 34.6122, Test loss: 33251.2070
In this epoch 3800/5000, Training loss: 0.4425, Test loss: 32855.3555
In this epoch 3850/5000, Training loss: 0.0033, Test loss: 32873.4062
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 32873.1836
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 32873.0664
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 32873.0664
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 32873.0703
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 32873.0586
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 32873.0547
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 32873.0508
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 32873.0469
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 32873.0430
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 32873.0430
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 32873.0430
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 32873.0391
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 32873.0352
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 32873.0352
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 32873.0352
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 32873.0352
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 32873.0312
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 32873.0312
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 32873.0312
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 32873.0312
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 32873.0312
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 32873.0312
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 32873.0312
train_acc 0.344113321799308
test_acc 0.20871107266435987
{'y_pred': [86.97938537597656, -0.9880993366241455, -21.233240127563477, 425.8609924316406, -3.7531445026397705, 25.4145565032959, 433.9664306640625, -85.94877624511719, -248.95028686523438, -31.489320755004883, 176.37191772460938, -31.114126205444336, -3.8482601642608643, 34.15258026123047, 57.66522979736328, -38.633872985839844, -70.54313659667969, 175.74468994140625, 162.7677459716797, 43.480018615722656, 82.87322235107422, -96.66852569580078, 41.315589904785156, 79.45935821533203, 146.77059936523438, 39.589393615722656, -380.42803955078125, -32.34922790527344, 30.193822860717773, -37.983970642089844, 276.7659912109375, -156.4205322265625, -147.69461059570312, -486.6459655761719, 70.53729248046875, 188.8162841796875, -42.05431365966797, -43.392303466796875, 17.534521102905273, -3.7323009967803955, 118.92642974853516, 257.3105163574219, -134.71929931640625, -304.5201416015625, 65.52452850341797, 8.371085166931152, -204.9893035888672, -18.30368995666504, 34.953651428222656, -200.8896026611328, 47.31271743774414, -15.032357215881348, -322.18975830078125, 105.64038848876953, 105.82452392578125, 152.59939575195312, -87.47480010986328, -85.8816146850586, 145.400634765625, 111.66829681396484, 496.4371337890625, 89.68360137939453, 441.29754638671875, 189.99566650390625, 51.83631134033203, 155.19839477539062, 71.74059295654297, -32.04991912841797, -109.81795501708984, 143.6966552734375, -27.28032875061035, 253.56640625, -26.075082778930664, 21.930517196655273, -60.127662658691406, 4.072988986968994, -207.169677734375, 158.37588500976562, -89.58790588378906, 120.7601089477539, 72.32892608642578, 44.598670959472656, 59.98219299316406, -93.90459442138672, -43.82410430908203, 52.688194274902344, 78.9983901977539, 55.38355255126953, 188.793212890625, -655.5000610351562, -274.24920654296875, -18.631479263305664, 259.023193359375, 57.01593017578125, 48.60437774658203, -54.47985076904297, -112.4579849243164, -161.99404907226562, -106.47309112548828, 92.78150939941406, -108.26428985595703, 65.51250457763672, -50.1893310546875, 88.56346893310547, -150.75465393066406, 86.42388916015625, -162.3333740234375, -365.0558776855469, 33.164215087890625, -224.56329345703125, 26.383859634399414, 140.45945739746094, 48.05567169189453, 92.27701568603516, -33.485076904296875, 33.797645568847656, 104.3260269165039, 44.22435760498047, 25.927648544311523, 6.838964939117432, -32.16908264160156, -0.00010801677126437426, -166.41558837890625, -199.82447814941406, 57.14891815185547, 57.460594177246094, 27.32491111755371, -301.71405029296875, 194.71063232421875, 75.54296875, -21.83707618713379, 356.7377014160156, -46.840576171875, 147.10455322265625, -3.819749116897583, 20.42143440246582, -352.3459777832031, -310.32000732421875, 105.72456359863281, 139.72836303710938, 97.80014038085938, 77.36071014404297, -290.1397705078125, -66.31005096435547, 8.695830345153809, -80.67615509033203, -227.51759338378906, -107.41624450683594, 59.98760223388672, 59.797508239746094, 144.63235473632812, -297.8538818359375, 11.45963191986084, -465.712646484375, -224.5235595703125, -197.62852478027344, -361.52471923828125, 50.58470916748047, 122.2003173828125, 43.10338592529297, -345.72344970703125, -424.64398193359375, -185.51507568359375, 189.84555053710938, 293.1868591308594, 79.33739471435547, -221.12197875976562, -144.25428771972656, 218.26991271972656, -233.1585693359375, -210.84303283691406, 48.873817443847656, 96.9383316040039, 75.30728912353516, -219.15267944335938, -46.02989959716797, -287.1950378417969, 130.46705627441406, 117.62297821044922, 83.06733703613281, -204.39993286132812, 17.89881706237793, 216.4161834716797, 209.2259979248047, 6.397818088531494, -2.8757946491241455, -76.3082046508789, 254.18865966796875, -316.30389404296875, -175.54183959960938, -72.45893096923828, -384.6474609375, 202.23912048339844, 169.44174194335938, -195.22265625, 14.055060386657715, 110.49359893798828, -77.87883758544922, 10.796561241149902, -327.09234619140625, -84.84890747070312, 100.16877746582031, 132.54010009765625, -53.88621520996094, -51.59666442871094, 104.11722564697266, -65.34990692138672, 243.94012451171875, 356.5906982421875, -209.89187622070312, 18.250505447387695, -153.9172821044922, 3.0957553386688232, 65.63782501220703, 172.06283569335938, -152.2987518310547, -74.66846466064453, -239.94647216796875, 197.67550659179688, 2.6725223064422607, 114.01189422607422, 45.14555358886719, 185.63876342773438, 440.7472839355469, 118.79972076416016, -264.5570373535156, -26.682283401489258, -53.136314392089844, 195.87600708007812, 110.91506958007812, 92.19947052001953, -129.78817749023438, 81.09452056884766, 77.29138946533203, -211.17996215820312, 20.083723068237305, 55.96918487548828, 90.62902069091797, 245.25369262695312, -375.8179931640625, 119.50265502929688, 226.22515869140625, 49.11180877685547, 229.3385467529297, 349.7667236328125, -86.01477813720703, 54.00987243652344, -525.7298583984375, -179.58453369140625, -253.47305297851562, 226.61236572265625, -116.48900604248047, -23.552412033081055, 69.02924346923828, -91.88833618164062, -139.1339111328125, -86.6520004272461, 162.86172485351562, -125.331787109375, -29.127443313598633, 289.9729309082031, 94.29901885986328, 170.02133178710938, 13.918814659118652, 61.08921813964844, 46.74073028564453, 91.91486358642578, -135.04684448242188, -260.89886474609375, -63.644126892089844, 144.25982666015625, -524.774169921875, -31.273401260375977, 139.3089599609375, -104.41155242919922, 33.08161926269531, -301.07421875, 13.481986045837402, 357.19403076171875, -339.83526611328125, -212.2586669921875, 94.75858306884766, 26.23592185974121, -145.38504028320312, -0.7862789630889893, 238.8442840576172, 167.35049438476562, -262.42236328125, 188.23126220703125, -447.04339599609375, 92.84280395507812, -71.22823333740234, 110.5305404663086, -122.22703552246094, -550.7243041992188, -2.8175137042999268, -264.19512939453125, 12.74362850189209, -263.8013000488281, 299.5308532714844, -134.46478271484375, 69.6672134399414, 85.7568359375, -22.665735244750977, 43.24671173095703, -8.410325050354004, 112.62096405029297, 173.9332275390625, 117.0810317993164, 103.59136962890625, -108.78002166748047, 46.903282165527344, 287.3404235839844, 133.96551513671875, -137.92298889160156, -13.670853614807129, 4.233633518218994, -32.960479736328125, -265.50091552734375, -98.72806549072266, -23.253084182739258, 5.485716342926025, 289.29547119140625, -53.75153350830078, 37.48778533935547, -295.465087890625, -260.85601806640625, -376.478271484375, 44.31049346923828, 256.475830078125, -235.1857452392578, 123.84264373779297, 139.5701446533203, 6.220327854156494, 285.43414306640625, -132.04104614257812, -182.87815856933594, -6.531838893890381, 2.557142972946167, -146.8642578125], 'y_pred_int': [87, 0, 0, 426, 0, 25, 434, 0, 0, 0, 176, 0, 0, 34, 58, 0, 0, 176, 163, 43, 83, 0, 41, 79, 147, 40, 0, 0, 30, 0, 277, 0, 0, 0, 71, 189, 0, 0, 18, 0, 119, 257, 0, 0, 66, 8, 0, 0, 35, 0, 47, 0, 0, 106, 106, 153, 0, 0, 145, 112, 496, 90, 441, 190, 52, 155, 72, 0, 0, 144, 0, 254, 0, 22, 0, 4, 0, 158, 0, 121, 72, 45, 60, 0, 0, 53, 79, 55, 189, 0, 0, 0, 259, 57, 49, 0, 0, 0, 0, 93, 0, 66, 0, 89, 0, 86, 0, 0, 33, 0, 26, 140, 48, 92, 0, 34, 104, 44, 26, 7, 0, 0, 0, 0, 57, 57, 27, 0, 195, 76, 0, 357, 0, 147, 0, 20, 0, 0, 106, 140, 98, 77, 0, 0, 9, 0, 0, 0, 60, 60, 145, 0, 11, 0, 0, 0, 0, 51, 122, 43, 0, 0, 0, 190, 293, 79, 0, 0, 218, 0, 0, 49, 97, 75, 0, 0, 0, 130, 118, 83, 0, 18, 216, 209, 6, 0, 0, 254, 0, 0, 0, 0, 202, 169, 0, 14, 110, 0, 11, 0, 0, 100, 133, 0, 0, 104, 0, 244, 357, 0, 18, 0, 3, 66, 172, 0, 0, 0, 198, 3, 114, 45, 186, 441, 119, 0, 0, 0, 196, 111, 92, 0, 81, 77, 0, 20, 56, 91, 245, 0, 120, 226, 49, 229, 350, 0, 54, 0, 0, 0, 227, 0, 0, 69, 0, 0, 0, 163, 0, 0, 290, 94, 170, 14, 61, 47, 92, 0, 0, 0, 144, 0, 0, 139, 0, 33, 0, 13, 357, 0, 0, 95, 26, 0, 0, 239, 167, 0, 188, 0, 93, 0, 111, 0, 0, 0, 0, 13, 0, 300, 0, 70, 86, 0, 43, 0, 113, 174, 117, 104, 0, 47, 287, 134, 0, 0, 4, 0, 0, 0, 0, 5, 289, 0, 37, 0, 0, 0, 44, 256, 0, 124, 140, 6, 285, 0, 0, 0, 3, 0], 'y_true': [1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 2, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 4, 1, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 4, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 4, 4, 2, 1, 0, 0, 2, 0, 7, 0, 1, 0, 0, 1, 1, 2, 1, 0, 1, 1, 0, 1, 0, 2, 1, 0, 1, 1, 1, 2, 2, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 3, 1, 8, 2, 0, 0, 1, 0, 0, 2, 1, 0, 2, 0, 0, 0, 2, 0, 0, 7, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 1, 0, 2, 3, 1, 0, 1, 0, 0, 2, 1, 0, 1, 4, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 3, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 2, 0, 2, 2, 2, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 3, 1, 0, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 2, 1, 0, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0, 3, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 9, 1, 2, 1, 0, 2, 0, 1, 2, 0, 0, 1, 2, 0, 3, 1, 1, 1, 1, 1, 3, 1, 0, 3, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 3, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 2, 2, 1, 0, 1, 1, 0, 2, 1, 0, 0, 1, 1, 1, 1], 'train_acc': 0.344113321799308, 'test_acc': 0.20871107266435987, 'X': 'X_TANL_layer_11_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 0.6308, Test loss: 1.7044
In this epoch 100/5000, Training loss: 0.0030, Test loss: 1.7040
In this epoch 150/5000, Training loss: 0.0000, Test loss: 1.6884
In this epoch 200/5000, Training loss: 0.0000, Test loss: 1.6896
In this epoch 250/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 300/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 350/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 400/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 450/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 500/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 550/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 600/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 650/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 700/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 750/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 800/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 850/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 900/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 950/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 1.6897
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 1.6898
In this epoch 3500/5000, Training loss: 16.2336, Test loss: 122.5883
In this epoch 3550/5000, Training loss: 0.4441, Test loss: 2.4476
In this epoch 3600/5000, Training loss: 0.0036, Test loss: 1.6896
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 1.6886
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 1.6890
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 1.6890
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 1.6890
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 1.6890
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 1.6890
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 1.6890
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 1.6890
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 1.6891
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 1.6891
train_acc 0.3505633650519031
test_acc 0.4003460207612457
{'y_pred': [0.5569930672645569, 0.35486793518066406, 0.7979573011398315, 0.7571597695350647, 0.8428214192390442, 1.4400298595428467, 1.704667091369629, 0.4592519998550415, 0.2491040974855423, 1.7347259521484375, 1.5101118087768555, 1.3282220363616943, 0.10771723091602325, 0.9900360703468323, 0.647104799747467, 0.8060917258262634, 1.0526591539382935, 1.0131036043167114, 0.46067091822624207, 1.3394438028335571, 1.1704132556915283, 1.3694236278533936, -0.08640957623720169, 0.5421842932701111, 0.661378800868988, 0.6614838242530823, 0.34983307123184204, 0.8672128319740295, 1.4787477254867554, 0.30142080783843994, 0.8476396799087524, 0.4139728546142578, 1.5364444255828857, 0.6887957453727722, 1.1921141147613525, 0.35181155800819397, 0.6270126700401306, 0.07885465770959854, 0.5512728691101074, 0.21672841906547546, 0.4460180997848511, 1.1115376949310303, 0.5787277221679688, 0.49045610427856445, 0.3134936988353729, 0.7223303914070129, 0.4931257367134094, 0.4990968704223633, 0.5333219170570374, 1.5923465490341187, 0.6946461796760559, 1.28896164894104, 0.17921029031276703, 1.5899803638458252, 0.8584754467010498, 1.8706938028335571, 1.2623032331466675, -0.001777420286089182, 1.1693592071533203, 0.4700166583061218, 1.8403517007827759, 0.8859476447105408, 0.6405152082443237, 0.652307391166687, 2.010047674179077, 0.9111838936805725, 0.7832011580467224, 0.7789771556854248, 0.5654428005218506, 1.5510979890823364, 0.14591310918331146, 1.412276268005371, 0.8575147986412048, 0.5117179155349731, 2.013416290283203, 0.12582291662693024, 1.2828149795532227, 1.319356918334961, 1.261216163635254, 1.4148659706115723, 1.0329327583312988, 0.5638160705566406, 0.5919997096061707, 0.7238994240760803, 0.6575444936752319, 0.43215012550354004, 1.866943359375, 0.47541573643684387, 1.3118855953216553, 0.8787163496017456, 0.6634151339530945, 0.913859486579895, 1.6564676761627197, 1.1311075687408447, 0.954628050327301, 0.40192103385925293, 0.3566409945487976, 1.0527302026748657, 1.2388612031936646, 0.5078997015953064, 0.6381378769874573, 1.3026708364486694, 1.2597929239273071, -0.20692631602287292, 0.29009345173835754, -0.6712700724601746, 1.1438711881637573, 0.11499568819999695, 1.0884575843811035, 0.9669783115386963, 0.6450986266136169, 0.8944118022918701, 1.4244446754455566, 0.4885670244693756, 1.3608615398406982, 0.7179874777793884, 1.4244871139526367, 0.7404240369796753, 1.259227991104126, 0.6180327534675598, 0.46401098370552063, 1.1921122074127197, 1.8179447650909424, 0.784397542476654, 0.39479732513427734, 1.0599291324615479, 1.5778491497039795, 0.774400532245636, 0.9893084168434143, 1.1377991437911987, 0.2999856173992157, 0.5917056202888489, 0.970093309879303, 0.35687732696533203, 1.1269491910934448, 0.7271491289138794, 0.20943544805049896, 1.4986777305603027, 1.1064283847808838, 1.4505424499511719, 0.7341729998588562, 0.580363929271698, 0.23567670583724976, 0.10296805948019028, 0.9238700270652771, 1.2173378467559814, -0.11108992248773575, 1.1150585412979126, 1.0790352821350098, 0.9840832352638245, 0.4808647334575653, 0.9741094708442688, 0.5347935557365417, 1.4279100894927979, 0.3691616952419281, 0.900242030620575, 0.29034119844436646, 1.4889506101608276, 1.0994176864624023, 0.9547883868217468, 0.27588334679603577, 0.9049676060676575, 0.23536700010299683, 0.7249791026115417, 1.148733377456665, 1.026991605758667, 0.38739150762557983, 1.0146758556365967, 1.273362636566162, 0.3024793267250061, 0.7265095114707947, 0.6263551115989685, 1.0072481632232666, 0.1259693205356598, 0.8391327261924744, 1.2348768711090088, 0.6053073406219482, 1.7339725494384766, 1.0826205015182495, 0.6461140513420105, 0.8408256769180298, 0.6482505798339844, 0.7904707193374634, 1.3074475526809692, 0.4368877410888672, 0.665494978427887, 1.2038912773132324, 1.3045450448989868, 0.7771170735359192, 0.03713616728782654, 0.4922752380371094, 0.610109269618988, 0.9869489669799805, 1.1165926456451416, 1.5120124816894531, 0.20697492361068726, 1.5280632972717285, 0.9335139989852905, 0.566092848777771, 1.1987756490707397, 1.111160159111023, 1.2654805183410645, 1.6541675329208374, 0.3828766345977783, 0.6348315477371216, 1.1146003007888794, 0.771920919418335, 0.5120064616203308, 1.4232561588287354, 0.6208400726318359, 0.15319226682186127, 0.510686457157135, 1.1739963293075562, 0.4448504149913788, 0.4727843999862671, 0.45378023386001587, 1.762062907218933, 1.1317068338394165, 0.4069664776325226, 0.7191668748855591, 0.8648682832717896, 0.5760582685470581, 1.7171934843063354, 0.8974820971488953, 1.3673887252807617, 0.8689582347869873, 1.560263752937317, 1.3418350219726562, 1.2993744611740112, 1.3331186771392822, 0.6737653613090515, 0.5828172564506531, 0.38714367151260376, 0.6407938599586487, 0.9850814938545227, 0.7168038487434387, 0.38474446535110474, 0.8407721519470215, 0.4216577708721161, 2.1149368286132812, 1.6885926723480225, 0.4816664755344391, 1.2486200332641602, -0.17217719554901123, 2.0954208374023438, 0.5578267574310303, 1.365335464477539, 0.9006226658821106, 0.8119630217552185, 1.0791583061218262, 1.1630859375, 1.457627296447754, 2.043372392654419, 1.4002654552459717, 0.6797187924385071, 0.5421052575111389, 1.0518858432769775, 0.5626205801963806, 0.9890398383140564, 1.4661659002304077, 1.1452702283859253, 1.6161768436431885, 0.23638683557510376, 1.0873160362243652, 0.6622682213783264, 0.6522125601768494, 1.6232280731201172, 0.5434735417366028, 0.3088385760784149, 1.162628412246704, 1.316803216934204, 1.3025275468826294, 1.5800623893737793, 0.8963924050331116, 0.8379516005516052, 0.6483821272850037, 1.150879144668579, 0.743854820728302, 1.5151690244674683, 0.08872216194868088, 0.8519918322563171, 0.9660001397132874, 1.0947074890136719, 1.6555372476577759, 0.2921748161315918, 1.2191096544265747, 1.3974175453186035, 0.8532688021659851, 1.5464332103729248, 0.1889793574810028, 0.6675553321838379, 1.0441051721572876, 0.386203408241272, 1.1361579895019531, 0.39926159381866455, 1.3421106338500977, 0.3269423544406891, 0.5667445659637451, 0.45843222737312317, 1.1987438201904297, 1.4007973670959473, 0.26722580194473267, 0.9357449412345886, -0.01074842270463705, -0.2600969672203064, 0.6504257917404175, 1.7304041385650635, 0.9501412510871887, 0.38524240255355835, 0.1365119069814682, 0.8098850846290588, 1.1893796920776367, 1.1841704845428467, 1.3009623289108276, 1.2645183801651, 0.4521699845790863, 0.1744013875722885, 0.5438430905342102, 0.7057285308837891, 0.7981011271476746, 0.417836457490921, 0.9279588460922241, 0.7326346039772034, 1.7630139589309692, 0.6511533260345459, 0.9549825191497803, 0.6178849935531616, 0.9437902569770813, 0.6868608593940735, 0.4667928218841553, 0.6965258121490479, 1.3246842622756958, 0.5543547868728638, 0.983205258846283, 0.17108610272407532, 1.1114416122436523, 1.1529548168182373, 0.609954833984375, 1.7782888412475586, 0.2703043818473816], 'y_pred_int': [1, 0, 1, 1, 1, 1, 2, 0, 0, 2, 2, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 2, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 2, 1, 1, 0, 2, 1, 2, 1, 0, 1, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 2, 0, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 2, 2, 0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 2, 0, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0], 'y_true': [0, 2, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 3, 1, 1, 1, 2, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 3, 0, 2, 2, 1, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 2, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 2, 2, 5, 1, 0, 1, 1, 2, 0, 1, 0, 4, 3, 0, 1, 1, 2, 0, 1, 0, 1, 3, 1, 2, 0, 1, 3, 0, 1, 0, 1, 0, 1, 1, 1, 0, 4, 1, 0, 0, 3, 1, 1, 2, 1, 1, 0, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 1, 0, 2, 2, 1, 1, 3, 0, 7, 0, 3, 0, 1, 1, 1, 0, 2, 0, 0, 2, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 2, 0, 5, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 1, 2, 0, 9, 2, 0, 0, 0, 0, 1, 1, 3, 0, 0, 0, 0, 1, 3, 0, 0, 0, 2, 0, 0, 0, 1, 3, 1, 2, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 4, 0, 0, 1, 0, 2, 0, 2, 0, 1, 1, 3, 2, 1, 1, 0, 1, 0, 1, 0, 0, 1, 2, 1, 2, 0, 1, 2, 0, 1, 1, 4, 0, 1, 2, 0, 0, 1, 2, 0, 1, 2, 2, 1, 1, 0, 5, 1, 2, 1, 0, 0, 6, 0, 1, 0, 0, 1, 2, 0, 2, 1, 1, 2, 0, 0, 1, 4, 1, 1, 0, 1, 1, 1, 3, 3, 0, 0, 2, 0, 0, 0, 0, 1, 14, 2, 1, 0, 1, 0, 1, 1, 3, 1, 0, 1, 0, 3, 2, 1, 0], 'train_acc': 0.3505633650519031, 'test_acc': 0.4003460207612457, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 0.7112, Test loss: 36.7326
In this epoch 100/5000, Training loss: 0.0076, Test loss: 36.5142
In this epoch 150/5000, Training loss: 0.0000, Test loss: 36.4985
In this epoch 200/5000, Training loss: 0.0000, Test loss: 36.4973
In this epoch 250/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 300/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 350/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 400/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 450/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 500/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 550/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 600/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 650/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 700/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 750/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 800/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 850/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 900/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 950/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 36.4972
In this epoch 1500/5000, Training loss: 279.2226, Test loss: 41.6763
In this epoch 1550/5000, Training loss: 0.3771, Test loss: 37.1641
In this epoch 1600/5000, Training loss: 0.0024, Test loss: 36.4956
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 36.5024
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 36.5015
In this epoch 3950/5000, Training loss: 15.5961, Test loss: 36.9757
In this epoch 4000/5000, Training loss: 0.0484, Test loss: 36.5161
In this epoch 4050/5000, Training loss: 0.0001, Test loss: 36.4991
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 36.5020
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 36.5021
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 36.5021
train_acc 1.0
test_acc 0.03410899653979239
{'y_pred': [18.88721466064453, 11.919169425964355, 19.74959373474121, 6.638027191162109, 16.487722396850586, 30.560474395751953, 21.12413787841797, 23.62909698486328, 9.413911819458008, 7.957118511199951, 4.608095645904541, 29.0, 8.404156684875488, 17.272724151611328, 13.122234344482422, 19.35392951965332, 22.072635650634766, 4.349997043609619, 6.130335807800293, 20.803659439086914, 16.492158889770508, 22.54242706298828, 6.775391101837158, 8.284902572631836, 3.9389615058898926, 18.105030059814453, 4.910020351409912, 23.099393844604492, 10.765961647033691, 14.776626586914062, 3.793548345565796, 22.587600708007812, 7.102036952972412, 20.30522918701172, 22.143081665039062, 3.9991507530212402, 22.502277374267578, 5.796231269836426, 23.167470932006836, 4.562281131744385, 3.6025032997131348, 15.216721534729004, 22.782909393310547, 7.7938313484191895, 21.963233947753906, 18.845544815063477, 18.897069931030273, 13.120314598083496, 20.052629470825195, 23.730497360229492, 8.349863052368164, 21.71945571899414, 6.481025695800781, 5.6704792976379395, 19.466968536376953, 19.930660247802734, 8.004057884216309, 19.282182693481445, 6.703192234039307, 18.198169708251953, 16.443145751953125, 13.747366905212402, 14.435168266296387, 8.245012283325195, 9.322003364562988, 20.408592224121094, 22.39706039428711, 16.125743865966797, 6.360273838043213, 5.887505054473877, 3.5836904048919678, 17.883594512939453, 9.257416725158691, 1.9086014032363892, 9.081830024719238, 20.455678939819336, 7.674525737762451, 22.627975463867188, 4.776827335357666, 11.534111022949219, 7.567058086395264, 2.9427807331085205, 11.087178230285645, 12.202269554138184, 22.55559730529785, 27.174945831298828, 18.52737045288086, 8.732247352600098, 8.627470016479492, 23.839218139648438, 17.683137893676758, 11.811034202575684, 23.68309211730957, 25.070297241210938, 6.193334102630615, 3.707059383392334, 13.331652641296387, 10.39240550994873, 18.370460510253906, 26.069534301757812, 13.25537395477295, 20.875593185424805, 16.487812042236328, 16.055273056030273, 1.7647799253463745, 11.565735816955566, 9.372963905334473, 25.102062225341797, 14.581469535827637, 16.08761215209961, 9.698304176330566, 17.111526489257812, 10.14434814453125, 17.919151306152344, 22.185752868652344, 8.38587760925293, 10.004168510437012, 19.3800048828125, 15.3535737991333, 23.408016204833984, 1.307515263557434, 20.142581939697266, 15.655925750732422, 21.127275466918945, 23.783926010131836, 1.1330695152282715, 18.96436882019043, 5.874457836151123, 19.63931655883789, 6.509374141693115, 18.96883773803711, 4.829265117645264, 5.3689494132995605, 16.445953369140625, 26.21222686767578, 19.600481033325195, 2.6451761722564697, 12.302474021911621, 11.434127807617188, 3.070218086242676, 4.274753093719482, 11.939509391784668, 2.2921125888824463, 29.021076202392578, 22.043384552001953, 25.1369686126709, 20.925678253173828, 18.075820922851562, 5.404745578765869, 10.496683120727539, 26.813989639282227, 19.144588470458984, 22.33871078491211, 21.559904098510742, 3.249995708465576, 13.604642868041992, 15.662507057189941, 23.551166534423828, 6.900241374969482, 14.278164863586426, 6.8490118980407715, 7.130037784576416, 18.338546752929688, 18.293712615966797, 17.999130249023438, 6.462188243865967, 5.249256610870361, 19.297447204589844, 20.692127227783203, 6.2096638679504395, 23.977998733520508, 20.222482681274414, 21.537694931030273, 7.015231132507324, 8.506194114685059, 18.693889617919922, 10.46413803100586, 8.117854118347168, 17.17009925842285, 16.002147674560547, 10.358061790466309, 24.23708152770996, 19.305471420288086, -1.7638875246047974, 13.125785827636719, 20.05330467224121, 18.52973175048828, 6.110595226287842, 12.381401062011719, 20.192527770996094, 15.897892951965332, 10.638251304626465, 6.3770527839660645, 7.7723517417907715, 9.28799819946289, 9.074213027954102, 9.558513641357422, 12.974642753601074, 21.150760650634766, 13.256707191467285, 19.1552734375, 7.236779689788818, 17.551301956176758, 2.3029427528381348, 17.65629005432129, 16.790802001953125, 20.212501525878906, 20.810504913330078, 21.362049102783203, 21.433902740478516, 8.462861061096191, 6.778675556182861, 22.62030792236328, 24.26645278930664, 8.782195091247559, 21.318500518798828, 18.65560531616211, 23.47110366821289, 15.825858116149902, 22.81475830078125, 4.123250484466553, 20.248252868652344, 5.674232482910156, 5.805647373199463, 18.081188201904297, 4.04727029800415, 19.166378021240234, 6.677826404571533, 24.916423797607422, 22.786746978759766, 4.899365425109863, 18.111461639404297, 18.81707000732422, 24.366256713867188, 7.622286796569824, 23.726974487304688, 22.646268844604492, 19.038589477539062, 25.245494842529297, 20.444807052612305, 18.492813110351562, 8.336668968200684, 21.560815811157227, 27.27739906311035, 16.240497589111328, 5.206775188446045, 7.672348499298096, 17.918251037597656, 22.4555721282959, 18.58394432067871, 19.286375045776367, 3.994664430618286, 13.668967247009277, 4.6416096687316895, 23.571380615234375, 17.862783432006836, 17.39651870727539, 3.0429039001464844, 16.182857513427734, 3.2773830890655518, 8.593746185302734, 7.986993312835693, 20.05984878540039, 17.37132453918457, 23.42137336730957, 7.636631011962891, 20.499191284179688, 3.0730721950531006, 16.554981231689453, 22.559240341186523, 9.392853736877441, 26.10150909423828, 0.06822129338979721, 19.358768463134766, 16.610363006591797, 9.593005180358887, 9.62487506866455, 28.15947914123535, 21.463905334472656, 10.611328125, 6.827613353729248, 18.319683074951172, 21.03961753845215, 5.720438003540039, 11.12678050994873, 8.333341598510742, 5.061913967132568, 12.330243110656738, 5.566431045532227, 21.055503845214844, 7.601187229156494, 6.0845537185668945, 6.052912712097168, 7.80102014541626, 2.7585630416870117, 17.893484115600586, 19.31844711303711, 21.85848617553711, 7.066546440124512, 21.367591857910156, 3.217784881591797, 9.60877513885498, 7.0490803718566895, 10.179277420043945, 8.100624084472656, 24.108034133911133, 16.76613998413086, 4.2635087966918945, 18.607208251953125, 0.30460432171821594, 9.371723175048828, 26.429203033447266, 17.367008209228516, 20.804046630859375, 6.87409782409668, 5.591114521026611, 26.00709342956543, 8.468100547790527, 12.545649528503418, 19.323963165283203, 7.409407138824463, 3.5541300773620605, 22.521671295166016, 0.7003167271614075, 5.603932857513428, 15.350743293762207, 21.75699806213379, 20.073970794677734, 20.383886337280273, 3.4547977447509766, 19.08439064025879, 19.152666091918945, 5.921428203582764, 6.176947593688965, 10.343531608581543, 9.33396053314209, 4.3648457527160645, 4.631435871124268, 6.8931450843811035, 21.432607650756836], 'y_pred_int': [19, 12, 20, 7, 16, 31, 21, 24, 9, 8, 5, 29, 8, 17, 13, 19, 22, 4, 6, 21, 16, 23, 7, 8, 4, 18, 5, 23, 11, 15, 4, 23, 7, 20, 22, 4, 23, 6, 23, 5, 4, 15, 23, 8, 22, 19, 19, 13, 20, 24, 8, 22, 6, 6, 19, 20, 8, 19, 7, 18, 16, 14, 14, 8, 9, 20, 22, 16, 6, 6, 4, 18, 9, 2, 9, 20, 8, 23, 5, 12, 8, 3, 11, 12, 23, 27, 19, 9, 9, 24, 18, 12, 24, 25, 6, 4, 13, 10, 18, 26, 13, 21, 16, 16, 2, 12, 9, 25, 15, 16, 10, 17, 10, 18, 22, 8, 10, 19, 15, 23, 1, 20, 16, 21, 24, 1, 19, 6, 20, 7, 19, 5, 5, 16, 26, 20, 3, 12, 11, 3, 4, 12, 2, 29, 22, 25, 21, 18, 5, 10, 27, 19, 22, 22, 3, 14, 16, 24, 7, 14, 7, 7, 18, 18, 18, 6, 5, 19, 21, 6, 24, 20, 22, 7, 9, 19, 10, 8, 17, 16, 10, 24, 19, 0, 13, 20, 19, 6, 12, 20, 16, 11, 6, 8, 9, 9, 10, 13, 21, 13, 19, 7, 18, 2, 18, 17, 20, 21, 21, 21, 8, 7, 23, 24, 9, 21, 19, 23, 16, 23, 4, 20, 6, 6, 18, 4, 19, 7, 25, 23, 5, 18, 19, 24, 8, 24, 23, 19, 25, 20, 18, 8, 22, 27, 16, 5, 8, 18, 22, 19, 19, 4, 14, 5, 24, 18, 17, 3, 16, 3, 9, 8, 20, 17, 23, 8, 20, 3, 17, 23, 9, 26, 0, 19, 17, 10, 10, 28, 21, 11, 7, 18, 21, 6, 11, 8, 5, 12, 6, 21, 8, 6, 6, 8, 3, 18, 19, 22, 7, 21, 3, 10, 7, 10, 8, 24, 17, 4, 19, 0, 9, 26, 17, 21, 7, 6, 26, 8, 13, 19, 7, 4, 23, 1, 6, 15, 22, 20, 20, 3, 19, 19, 6, 6, 10, 9, 4, 5, 7, 21], 'y_true': [31, 14, 13, 10, 10, 31, 17, 13, 10, 8, 3, 29, 6, 10, 25, 15, 18, 4, 6, 18, 17, 22, 1, 6, 5, 35, 4, 14, 8, 17, 7, 38, 6, 13, 18, 5, 26, 7, 11, 4, 3, 13, 9, 6, 13, 20, 25, 10, 32, 38, 11, 24, 5, 5, 14, 23, 7, 24, 12, 17, 23, 6, 11, 8, 9, 20, 14, 10, 8, 7, 6, 19, 8, 5, 14, 29, 4, 24, 4, 9, 9, 3, 11, 8, 35, 46, 25, 7, 8, 23, 12, 15, 26, 21, 9, 6, 11, 7, 34, 26, 10, 11, 11, 11, 3, 10, 10, 23, 14, 17, 8, 29, 15, 13, 17, 6, 11, 10, 21, 34, 6, 11, 13, 15, 14, 6, 19, 5, 15, 7, 37, 5, 5, 15, 35, 15, 3, 13, 10, 5, 5, 9, 6, 41, 34, 14, 10, 15, 6, 12, 34, 11, 32, 32, 3, 10, 13, 28, 8, 10, 8, 7, 13, 13, 16, 6, 4, 15, 13, 6, 16, 16, 27, 7, 8, 28, 10, 7, 7, 9, 11, 26, 16, 1, 9, 42, 28, 7, 11, 21, 17, 10, 8, 6, 8, 7, 14, 13, 10, 19, 11, 7, 10, 2, 22, 23, 17, 16, 15, 21, 10, 8, 26, 26, 8, 31, 25, 28, 16, 27, 7, 14, 7, 8, 12, 4, 27, 5, 39, 12, 8, 10, 14, 35, 9, 14, 25, 17, 44, 13, 11, 10, 36, 39, 20, 5, 6, 16, 34, 15, 19, 5, 10, 6, 23, 14, 12, 7, 15, 4, 7, 5, 20, 15, 22, 7, 10, 2, 16, 22, 8, 39, 2, 15, 11, 14, 11, 33, 32, 9, 5, 14, 14, 5, 13, 11, 6, 8, 4, 33, 8, 5, 7, 6, 2, 8, 29, 21, 6, 42, 4, 7, 6, 7, 7, 31, 13, 3, 10, 2, 8, 23, 10, 9, 8, 7, 25, 7, 10, 13, 6, 5, 21, 3, 9, 17, 51, 15, 16, 5, 29, 26, 7, 5, 8, 5, 4, 6, 9, 17], 'train_acc': 1.0, 'test_acc': 0.03410899653979239, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 13952.9561, Test loss: 15322.5234
In this epoch 100/5000, Training loss: 7762.7754, Test loss: 13525.9443
In this epoch 150/5000, Training loss: 4441.6665, Test loss: 13123.6328
In this epoch 200/5000, Training loss: 2424.3689, Test loss: 12959.1309
In this epoch 250/5000, Training loss: 1266.4114, Test loss: 12902.3164
In this epoch 300/5000, Training loss: 637.0958, Test loss: 12895.5381
In this epoch 350/5000, Training loss: 310.8774, Test loss: 12908.5439
In this epoch 400/5000, Training loss: 148.3512, Test loss: 12926.3281
In this epoch 450/5000, Training loss: 69.9088, Test loss: 12942.5195
In this epoch 500/5000, Training loss: 32.9042, Test loss: 12955.1738
In this epoch 550/5000, Training loss: 15.6635, Test loss: 12964.2842
In this epoch 600/5000, Training loss: 7.6346, Test loss: 12970.5088
In this epoch 650/5000, Training loss: 3.8485, Test loss: 12974.6123
In this epoch 700/5000, Training loss: 2.0177, Test loss: 12977.2441
In this epoch 750/5000, Training loss: 1.1008, Test loss: 12978.8896
In this epoch 800/5000, Training loss: 0.6225, Test loss: 12979.8984
In this epoch 850/5000, Training loss: 0.3628, Test loss: 12980.5146
In this epoch 900/5000, Training loss: 0.2166, Test loss: 12980.8828
In this epoch 950/5000, Training loss: 0.1319, Test loss: 12981.1152
In this epoch 1000/5000, Training loss: 0.0817, Test loss: 12981.2617
In this epoch 1050/5000, Training loss: 0.0516, Test loss: 12981.3594
In this epoch 1100/5000, Training loss: 0.0335, Test loss: 12981.4307
In this epoch 1150/5000, Training loss: 0.0224, Test loss: 12981.4854
In this epoch 1200/5000, Training loss: 0.0157, Test loss: 12981.5283
In this epoch 1250/5000, Training loss: 0.0116, Test loss: 12981.5625
In this epoch 1300/5000, Training loss: 0.0091, Test loss: 12981.5898
In this epoch 1350/5000, Training loss: 0.0075, Test loss: 12981.6064
In this epoch 1400/5000, Training loss: 0.0066, Test loss: 12981.6211
In this epoch 1450/5000, Training loss: 0.0060, Test loss: 12981.6270
In this epoch 1500/5000, Training loss: 0.0056, Test loss: 12981.6357
In this epoch 1550/5000, Training loss: 0.0053, Test loss: 12981.6387
In this epoch 1600/5000, Training loss: 0.0051, Test loss: 12981.6396
In this epoch 1650/5000, Training loss: 0.0049, Test loss: 12981.6387
In this epoch 1700/5000, Training loss: 0.0048, Test loss: 12981.6387
In this epoch 1750/5000, Training loss: 0.0047, Test loss: 12981.6328
In this epoch 1800/5000, Training loss: 0.0045, Test loss: 12981.6299
In this epoch 1850/5000, Training loss: 0.0044, Test loss: 12981.6240
In this epoch 1900/5000, Training loss: 0.0043, Test loss: 12981.6211
In this epoch 1950/5000, Training loss: 0.0042, Test loss: 12981.6123
In this epoch 2000/5000, Training loss: 0.0040, Test loss: 12981.6094
In this epoch 2050/5000, Training loss: 0.0039, Test loss: 12981.6006
In this epoch 2100/5000, Training loss: 0.0038, Test loss: 12981.5957
In this epoch 2150/5000, Training loss: 0.0037, Test loss: 12981.5898
In this epoch 2200/5000, Training loss: 0.0036, Test loss: 12981.5830
In this epoch 2250/5000, Training loss: 0.0035, Test loss: 12981.5742
In this epoch 2300/5000, Training loss: 0.0033, Test loss: 12981.5684
In this epoch 2350/5000, Training loss: 0.0032, Test loss: 12981.5605
In this epoch 2400/5000, Training loss: 0.0031, Test loss: 12981.5547
In this epoch 2450/5000, Training loss: 0.0030, Test loss: 12981.5498
In this epoch 2500/5000, Training loss: 0.0029, Test loss: 12981.5410
In this epoch 2550/5000, Training loss: 0.0028, Test loss: 12981.5381
In this epoch 2600/5000, Training loss: 0.0027, Test loss: 12981.5283
In this epoch 2650/5000, Training loss: 0.0026, Test loss: 12981.5234
In this epoch 2700/5000, Training loss: 0.0025, Test loss: 12981.5146
In this epoch 2750/5000, Training loss: 0.0023, Test loss: 12981.5088
In this epoch 2800/5000, Training loss: 0.0022, Test loss: 12981.4990
In this epoch 2850/5000, Training loss: 0.0021, Test loss: 12981.4971
In this epoch 2900/5000, Training loss: 0.0020, Test loss: 12981.4883
In this epoch 2950/5000, Training loss: 0.0019, Test loss: 12981.4795
In this epoch 3000/5000, Training loss: 0.0019, Test loss: 12981.4727
In this epoch 3050/5000, Training loss: 0.0018, Test loss: 12981.4668
In this epoch 3100/5000, Training loss: 0.0017, Test loss: 12981.4590
In this epoch 3150/5000, Training loss: 0.0016, Test loss: 12981.4502
In this epoch 3200/5000, Training loss: 0.0015, Test loss: 12981.4414
In this epoch 3250/5000, Training loss: 0.0014, Test loss: 12981.4326
In this epoch 3300/5000, Training loss: 0.0013, Test loss: 12981.4268
In this epoch 3350/5000, Training loss: 0.0013, Test loss: 12981.4219
In this epoch 3400/5000, Training loss: 0.0012, Test loss: 12981.4160
In this epoch 3450/5000, Training loss: 0.0011, Test loss: 12981.4092
In this epoch 3500/5000, Training loss: 0.0010, Test loss: 12981.4043
In this epoch 3550/5000, Training loss: 0.0010, Test loss: 12981.3955
In this epoch 3600/5000, Training loss: 0.0009, Test loss: 12981.3896
In this epoch 3650/5000, Training loss: 0.0008, Test loss: 12981.3828
In this epoch 3700/5000, Training loss: 0.0008, Test loss: 12981.3740
In this epoch 3750/5000, Training loss: 0.0007, Test loss: 12981.3682
In this epoch 3800/5000, Training loss: 0.0007, Test loss: 12981.3623
In this epoch 3850/5000, Training loss: 0.0006, Test loss: 12981.3564
In this epoch 3900/5000, Training loss: 0.0006, Test loss: 12981.3486
In this epoch 3950/5000, Training loss: 0.0005, Test loss: 12981.3418
In this epoch 4000/5000, Training loss: 0.0005, Test loss: 12981.3359
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 12981.3330
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 12981.3281
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 12981.3213
In this epoch 4200/5000, Training loss: 0.0003, Test loss: 12981.3184
In this epoch 4250/5000, Training loss: 0.0003, Test loss: 12981.3135
In this epoch 4300/5000, Training loss: 0.0003, Test loss: 12981.3086
In this epoch 4350/5000, Training loss: 0.0003, Test loss: 12981.3027
In this epoch 4400/5000, Training loss: 0.0002, Test loss: 12981.2969
In this epoch 4450/5000, Training loss: 0.0002, Test loss: 12981.2930
In this epoch 4500/5000, Training loss: 0.0002, Test loss: 12981.2871
In this epoch 4550/5000, Training loss: 0.0002, Test loss: 12981.2822
In this epoch 4600/5000, Training loss: 0.0001, Test loss: 12981.2793
In this epoch 4650/5000, Training loss: 0.0001, Test loss: 12981.2734
In this epoch 4700/5000, Training loss: 0.0001, Test loss: 12981.2705
In this epoch 4750/5000, Training loss: 0.0001, Test loss: 12981.2676
In this epoch 4800/5000, Training loss: 0.0001, Test loss: 12981.2617
In this epoch 4850/5000, Training loss: 0.0001, Test loss: 12981.2588
In this epoch 4900/5000, Training loss: 0.0001, Test loss: 12981.2529
In this epoch 4950/5000, Training loss: 0.0001, Test loss: 12981.2471
In this epoch 5000/5000, Training loss: 0.0001, Test loss: 12981.2471
train_acc 1.0
test_acc 0.0014878892733564014
{'y_pred': [541.6895141601562, 522.1982421875, 585.5171508789062, 99.83259582519531, 241.5144805908203, 288.4505615234375, 153.76197814941406, 561.6172485351562, 493.1639709472656, 154.92779541015625, 468.84613037109375, 129.50405883789062, 168.75440979003906, 233.47784423828125, 512.3998413085938, 119.64061737060547, 556.7764892578125, 241.77093505859375, 182.42343139648438, 128.58103942871094, 578.37353515625, 581.6093139648438, 550.8936767578125, 161.80548095703125, 250.14013671875, 482.16851806640625, 560.2816162109375, 454.6529235839844, 503.877197265625, 496.91253662109375, 623.0774536132812, 527.398681640625, 602.5445556640625, 464.02838134765625, 293.161376953125, 503.79962158203125, 470.2943115234375, 510.8472900390625, 493.6968994140625, 158.47557067871094, 108.98702239990234, 485.9197998046875, 542.0147705078125, 173.41099548339844, 459.1908874511719, 502.79632568359375, 512.7339477539062, 527.74853515625, 269.4418640136719, 167.36280822753906, 93.86038970947266, 117.78247833251953, 207.34678649902344, 565.8731689453125, 528.3715209960938, 496.4306335449219, 196.72738647460938, 246.98147583007812, 428.489013671875, 524.1224365234375, 192.91322326660156, 312.53045654296875, 99.26538848876953, 228.7529754638672, 132.83810424804688, 459.8233642578125, 142.44046020507812, 558.8886108398438, 118.84561920166016, 165.15151977539062, 334.110107421875, 186.5319366455078, 259.73541259765625, 106.497802734375, 544.5601806640625, 107.24781036376953, 569.6817016601562, 196.16465759277344, 304.46832275390625, 667.6663818359375, 622.6756591796875, 448.46136474609375, 130.2456512451172, 538.0913696289062, 426.8254089355469, 528.4055786132812, 497.04705810546875, 260.3977966308594, 561.9994506835938, 546.2200317382812, 560.8434448242188, 481.52923583984375, 605.5374755859375, 580.3637084960938, 550.7440795898438, 90.91915893554688, 196.4369659423828, 143.36936950683594, 546.8565063476562, 469.3793029785156, 467.550048828125, 162.83753967285156, 588.1127319335938, 682.0172729492188, 661.7036743164062, 153.84201049804688, 492.1889953613281, 519.6193237304688, 841.0, 514.1614379882812, 475.1670837402344, 443.2174072265625, 453.40960693359375, 544.3858032226562, 157.62876892089844, 123.99152374267578, 521.6427001953125, 551.2432250976562, 169.8594970703125, 519.7033081054688, 554.6954956054688, 229.32408142089844, 229.2986297607422, 590.3251342773438, 304.42108154296875, 124.97328186035156, 222.88296508789062, 92.45539855957031, 209.26405334472656, 550.4700927734375, 225.89170837402344, 653.5790405273438, 525.9012451171875, 298.56805419921875, 293.015625, 87.7253646850586, 97.3862533569336, 226.0985107421875, 599.8952026367188, 443.54107666015625, 495.04571533203125, 128.44261169433594, 315.4637756347656, 164.54684448242188, 30.40509033203125, 583.1336059570312, 120.95883178710938, 567.1030883789062, 103.02774810791016, 127.09171295166016, 357.4935607910156, 187.14195251464844, 506.68060302734375, 593.2376708984375, 121.57257843017578, 548.6250610351562, 531.0394897460938, 251.9620361328125, 487.5865173339844, 281.25848388671875, 571.5602416992188, 174.24583435058594, 210.91688537597656, 264.34686279296875, 109.71942901611328, 520.9081420898438, 171.909912109375, 278.63470458984375, 253.55487060546875, 63.68696594238281, 480.7193603515625, 217.1285858154297, 209.22860717773438, 503.49322509765625, 464.54052734375, 533.5997924804688, 537.7225952148438, 595.9840698242188, 275.9112854003906, 246.1763458251953, 136.67666625976562, 464.07977294921875, 231.9710235595703, 189.29379272460938, 521.9619750976562, 127.62395477294922, 563.2215576171875, 348.2345275878906, 524.95068359375, 512.004638671875, 153.1316680908203, 143.7600555419922, 91.71541595458984, 109.7886962890625, 494.8033752441406, 510.0528564453125, 494.52252197265625, 218.4447479248047, 554.5457153320312, 519.4700317382812, 521.8424072265625, 549.2447509765625, 241.07533264160156, 616.0365600585938, 545.5519409179688, 606.8687744140625, 156.2489013671875, 625.2338256835938, 501.98419189453125, 260.77081298828125, 175.37527465820312, 592.85986328125, 395.3299865722656, 558.3994750976562, -1.9518896341323853, 517.5618286132812, -14.956634521484375, 495.78839111328125, 174.6983184814453, 88.71118927001953, 530.2198486328125, 160.96932983398438, 324.12725830078125, 491.59173583984375, 255.71571350097656, 624.7415771484375, 132.20755004882812, 242.38125610351562, 98.40320587158203, 135.12893676757812, 576.7591552734375, 184.93174743652344, 556.0768432617188, 295.2314758300781, 597.3260498046875, 507.02557373046875, 124.66890716552734, 131.8195343017578, 416.2467041015625, 418.2862548828125, 562.6569213867188, 536.3486938476562, 295.94110107421875, 139.62193298339844, 108.54291534423828, 169.2516326904297, 349.3611145019531, 218.31063842773438, 567.1618041992188, 461.8252258300781, 88.19776153564453, 540.2129516601562, 76.35028839111328, 605.7120361328125, 183.61251831054688, 488.41741943359375, 489.9322509765625, 256.338623046875, 499.2559814453125, 307.886474609375, 424.1193542480469, 554.1921997070312, 119.76282501220703, 243.5141143798828, 535.5484008789062, 507.3406982421875, 485.0561218261719, 550.995849609375, 257.7887268066406, 644.1945190429688, 441.26824951171875, 518.2648315429688, 148.2980194091797, 431.4607849121094, 537.059326171875, 567.3798217773438, 542.4351196289062, 196.44004821777344, 469.8528747558594, 227.11874389648438, 450.10107421875, 545.7483520507812, 529.253662109375, 482.0380554199219, 550.8927612304688, 496.78338623046875, 410.79095458984375, 116.25736236572266, 338.1784362792969, 587.5599975585938, 472.8959045410156, 161.9710693359375, 100.62113189697266, 256.8576965332031, 305.18603515625, 542.2900390625, 229.63197326660156, 268.8251953125, 6.002642631530762, 118.04569244384766, 81.32524108886719, 512.876953125, 464.77484130859375, 515.2639770507812, 141.50257873535156, 550.5608520507812, 505.8506774902344, 81.14328002929688, 568.3087158203125, 210.4840850830078, 204.8326416015625, 606.8457641601562, 274.1905822753906, 513.6383056640625, 596.4113159179688, 210.6641082763672, 602.0433959960938, 546.00732421875, 325.62786865234375, 509.3583984375, 417.3765563964844, 522.551513671875, 146.43125915527344, 208.1155548095703, 147.1536102294922, 211.7493133544922, 557.710693359375, 166.14691162109375, 498.9315185546875, 285.7588195800781, 455.68701171875, 76.90576171875, 215.4458465576172, 213.88392639160156, 515.1587524414062, 524.6768188476562, 475.408935546875, 528.9152221679688, 523.0507202148438, 540.126220703125], 'y_pred_int': [542, 522, 586, 100, 242, 288, 154, 562, 493, 155, 469, 130, 169, 233, 512, 120, 557, 242, 182, 129, 578, 582, 551, 162, 250, 482, 560, 455, 504, 497, 623, 527, 603, 464, 293, 504, 470, 511, 494, 158, 109, 486, 542, 173, 459, 503, 513, 528, 269, 167, 94, 118, 207, 566, 528, 496, 197, 247, 428, 524, 193, 313, 99, 229, 133, 460, 142, 559, 119, 165, 334, 187, 260, 106, 545, 107, 570, 196, 304, 668, 623, 448, 130, 538, 427, 528, 497, 260, 562, 546, 561, 482, 606, 580, 551, 91, 196, 143, 547, 469, 468, 163, 588, 682, 662, 154, 492, 520, 841, 514, 475, 443, 453, 544, 158, 124, 522, 551, 170, 520, 555, 229, 229, 590, 304, 125, 223, 92, 209, 550, 226, 654, 526, 299, 293, 88, 97, 226, 600, 444, 495, 128, 315, 165, 30, 583, 121, 567, 103, 127, 357, 187, 507, 593, 122, 549, 531, 252, 488, 281, 572, 174, 211, 264, 110, 521, 172, 279, 254, 64, 481, 217, 209, 503, 465, 534, 538, 596, 276, 246, 137, 464, 232, 189, 522, 128, 563, 348, 525, 512, 153, 144, 92, 110, 495, 510, 495, 218, 555, 519, 522, 549, 241, 616, 546, 607, 156, 625, 502, 261, 175, 593, 395, 558, 0, 518, 0, 496, 175, 89, 530, 161, 324, 492, 256, 625, 132, 242, 98, 135, 577, 185, 556, 295, 597, 507, 125, 132, 416, 418, 563, 536, 296, 140, 109, 169, 349, 218, 567, 462, 88, 540, 76, 606, 184, 488, 490, 256, 499, 308, 424, 554, 120, 244, 536, 507, 485, 551, 258, 644, 441, 518, 148, 431, 537, 567, 542, 196, 470, 227, 450, 546, 529, 482, 551, 497, 411, 116, 338, 588, 473, 162, 101, 257, 305, 542, 230, 269, 6, 118, 81, 513, 465, 515, 142, 551, 506, 81, 568, 210, 205, 607, 274, 514, 596, 211, 602, 546, 326, 509, 417, 523, 146, 208, 147, 212, 558, 166, 499, 286, 456, 77, 215, 214, 515, 525, 475, 529, 523, 540], 'y_true': [541, 343, 716, 100, 260, 319, 136, 738, 502, 134, 633, 113, 138, 161, 340, 103, 432, 208, 148, 164, 487, 583, 628, 159, 192, 700, 508, 502, 379, 421, 779, 590, 530, 288, 276, 550, 530, 446, 290, 215, 125, 363, 678, 227, 596, 370, 607, 591, 277, 168, 80, 101, 215, 614, 361, 564, 221, 212, 373, 372, 165, 254, 127, 213, 145, 381, 162, 425, 132, 161, 308, 196, 291, 74, 390, 108, 434, 241, 227, 441, 440, 370, 172, 344, 659, 375, 496, 215, 308, 449, 621, 629, 498, 790, 425, 139, 218, 154, 478, 413, 369, 143, 911, 863, 506, 129, 504, 687, 840, 433, 389, 333, 457, 621, 176, 102, 387, 429, 210, 448, 621, 243, 285, 655, 321, 135, 215, 89, 182, 474, 207, 718, 378, 274, 273, 100, 88, 202, 491, 667, 354, 95, 305, 147, 45, 901, 82, 921, 100, 93, 324, 223, 504, 431, 82, 831, 889, 256, 332, 286, 461, 252, 239, 307, 88, 481, 172, 256, 258, 78, 433, 236, 173, 352, 635, 799, 694, 1099, 321, 249, 128, 624, 184, 300, 590, 177, 657, 342, 336, 396, 144, 139, 89, 114, 832, 656, 523, 250, 410, 407, 586, 656, 242, 567, 904, 596, 133, 929, 661, 285, 179, 633, 333, 537, 31, 1042, 32, 332, 155, 118, 665, 202, 212, 379, 263, 457, 143, 301, 101, 158, 677, 174, 768, 248, 529, 579, 112, 168, 309, 360, 428, 537, 248, 181, 153, 164, 281, 183, 697, 475, 108, 448, 93, 588, 232, 376, 444, 317, 450, 256, 347, 610, 160, 280, 441, 408, 310, 539, 226, 715, 339, 491, 149, 356, 447, 797, 401, 205, 347, 169, 382, 516, 458, 666, 543, 382, 331, 143, 341, 802, 455, 144, 103, 228, 276, 409, 279, 235, 41, 135, 75, 412, 299, 563, 156, 500, 522, 82, 681, 243, 216, 477, 244, 660, 765, 232, 425, 378, 317, 552, 689, 292, 173, 200, 205, 233, 370, 168, 471, 248, 374, 109, 217, 222, 372, 312, 328, 999, 556, 521], 'train_acc': 1.0, 'test_acc': 0.0014878892733564014, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 26.0173, Test loss: 9.4217
In this epoch 100/5000, Training loss: 0.3361, Test loss: 0.3908
In this epoch 150/5000, Training loss: 0.0901, Test loss: 0.3380
In this epoch 200/5000, Training loss: 0.0543, Test loss: 0.3495
In this epoch 250/5000, Training loss: 0.0337, Test loss: 0.3611
In this epoch 300/5000, Training loss: 0.0213, Test loss: 0.3711
In this epoch 350/5000, Training loss: 0.0137, Test loss: 0.3796
In this epoch 400/5000, Training loss: 0.0089, Test loss: 0.3864
In this epoch 450/5000, Training loss: 0.0059, Test loss: 0.3919
In this epoch 500/5000, Training loss: 0.0040, Test loss: 0.3963
In this epoch 550/5000, Training loss: 0.0027, Test loss: 0.3997
In this epoch 600/5000, Training loss: 0.0019, Test loss: 0.4024
In this epoch 650/5000, Training loss: 0.0013, Test loss: 0.4045
In this epoch 700/5000, Training loss: 0.0010, Test loss: 0.4061
In this epoch 750/5000, Training loss: 0.0007, Test loss: 0.4073
In this epoch 800/5000, Training loss: 0.0005, Test loss: 0.4083
In this epoch 850/5000, Training loss: 0.0004, Test loss: 0.4091
In this epoch 900/5000, Training loss: 0.0004, Test loss: 0.4097
In this epoch 950/5000, Training loss: 0.0003, Test loss: 0.4101
In this epoch 1000/5000, Training loss: 0.0003, Test loss: 0.4105
In this epoch 1050/5000, Training loss: 0.0002, Test loss: 0.4109
In this epoch 1100/5000, Training loss: 0.0002, Test loss: 0.4111
In this epoch 1150/5000, Training loss: 0.0002, Test loss: 0.4114
In this epoch 1200/5000, Training loss: 0.0002, Test loss: 0.4116
In this epoch 1250/5000, Training loss: 0.0002, Test loss: 0.4118
In this epoch 1300/5000, Training loss: 0.0002, Test loss: 0.4119
In this epoch 1350/5000, Training loss: 0.0002, Test loss: 0.4121
In this epoch 1400/5000, Training loss: 0.0001, Test loss: 0.4123
In this epoch 1450/5000, Training loss: 0.0001, Test loss: 0.4124
In this epoch 1500/5000, Training loss: 0.0001, Test loss: 0.4126
In this epoch 1550/5000, Training loss: 0.0001, Test loss: 0.4127
In this epoch 1600/5000, Training loss: 0.0001, Test loss: 0.4129
In this epoch 1650/5000, Training loss: 0.0001, Test loss: 0.4130
In this epoch 1700/5000, Training loss: 0.0001, Test loss: 0.4132
In this epoch 1750/5000, Training loss: 0.0001, Test loss: 0.4134
In this epoch 1800/5000, Training loss: 0.0001, Test loss: 0.4135
In this epoch 1850/5000, Training loss: 0.0001, Test loss: 0.4137
In this epoch 1900/5000, Training loss: 0.0001, Test loss: 0.4138
In this epoch 1950/5000, Training loss: 0.0001, Test loss: 0.4140
In this epoch 2000/5000, Training loss: 6.4614, Test loss: 15.2364
In this epoch 2050/5000, Training loss: 0.1573, Test loss: 2.3590
In this epoch 2100/5000, Training loss: 0.0118, Test loss: 0.4278
In this epoch 2150/5000, Training loss: 0.0001, Test loss: 0.4146
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 0.4147
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 0.4148
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 0.4150
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 0.4151
In this epoch 2400/5000, Training loss: 0.0003, Test loss: 0.4157
In this epoch 2450/5000, Training loss: 12.4365, Test loss: 10.3191
In this epoch 2500/5000, Training loss: 0.0578, Test loss: 0.4872
In this epoch 2550/5000, Training loss: 0.0001, Test loss: 0.4160
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 0.4157
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 0.4159
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 0.4160
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 0.4161
In this epoch 2800/5000, Training loss: 0.0023, Test loss: 0.4211
In this epoch 2850/5000, Training loss: 3.7779, Test loss: 9.3861
In this epoch 2900/5000, Training loss: 0.0056, Test loss: 0.4225
In this epoch 2950/5000, Training loss: 0.0003, Test loss: 0.4167
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 0.4166
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 0.4167
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 0.4168
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 0.4169
In this epoch 3200/5000, Training loss: 0.0639, Test loss: 0.5523
In this epoch 3250/5000, Training loss: 4.7030, Test loss: 4.3837
In this epoch 3300/5000, Training loss: 0.0069, Test loss: 0.4180
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 0.4173
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 0.4173
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 0.4173
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 0.4174
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 0.4175
In this epoch 3600/5000, Training loss: 4.7916, Test loss: 10.8047
In this epoch 3650/5000, Training loss: 0.0168, Test loss: 1.7818
In this epoch 3700/5000, Training loss: 0.0090, Test loss: 0.4326
In this epoch 3750/5000, Training loss: 0.0001, Test loss: 0.4178
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 0.4178
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 0.4178
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 0.4179
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 0.4179
In this epoch 4000/5000, Training loss: 23.0868, Test loss: 49.9568
In this epoch 4050/5000, Training loss: 1.2629, Test loss: 0.4822
In this epoch 4100/5000, Training loss: 0.0005, Test loss: 0.4254
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 0.4182
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 0.4182
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 0.4182
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 0.4182
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 0.4183
In this epoch 4400/5000, Training loss: 0.4851, Test loss: 1.4586
In this epoch 4450/5000, Training loss: 1.6044, Test loss: 4.1735
In this epoch 4500/5000, Training loss: 0.0164, Test loss: 0.4215
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 0.4185
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 0.4185
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 0.4185
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 0.4185
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 0.4186
In this epoch 4800/5000, Training loss: 0.0136, Test loss: 0.4476
In this epoch 4850/5000, Training loss: 0.4242, Test loss: 2.0561
In this epoch 4900/5000, Training loss: 0.0375, Test loss: 0.4351
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 0.4188
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 0.4187
train_acc 0.3409969723183391
test_acc 0.41521626297577857
{'y_pred': [1.2737879753112793, 0.22432686388492584, 0.37909066677093506, 0.42833974957466125, 0.411354660987854, 0.166428804397583, 0.2625634968280792, 0.18803659081459045, -0.18096348643302917, 1.1897943019866943, 0.796059250831604, 0.2851482927799225, 0.6717991232872009, 0.010432270355522633, 0.4775468707084656, 0.1892283409833908, 0.6453448534011841, 0.9612783193588257, 1.7764195203781128, -0.01812421903014183, 1.7370964288711548, 2.1707098484039307, 0.4854334890842438, 3.291313886642456, 1.46964693069458, -0.4225102663040161, 1.0347862243652344, 0.6907482743263245, 0.1631859540939331, -0.1460663378238678, 0.18401029706001282, 1.4906245470046997, -0.23139795660972595, -0.6235018372535706, 1.1286031007766724, -0.21749788522720337, 1.6939140558242798, 0.2820097804069519, 1.3436930179595947, 0.03583306446671486, 1.5247634649276733, 0.43803438544273376, -0.2713075578212738, 0.8500312566757202, 0.31980574131011963, 0.3534869849681854, 0.7006972432136536, 0.23841364681720734, 0.6495094895362854, 0.26892733573913574, -0.3834807276725769, 2.6379029750823975, 0.17656640708446503, 2.791987419128418, 0.46408870816230774, 0.5836411714553833, 0.19631867110729218, 0.5206856727600098, 1.6368741989135742, 0.9977788329124451, -0.2480766922235489, 1.2383620738983154, 0.2509773373603821, 0.9342959523200989, 1.802409291267395, 0.3342477083206177, 1.9983865022659302, 1.098141074180603, 0.6647723913192749, 0.12481708824634552, 1.2942143678665161, 0.23622991144657135, 0.3182108402252197, 0.6407673358917236, 1.265488624572754, 1.506900668144226, 2.32977294921875, 0.07492520660161972, 0.24232128262519836, -0.03003017231822014, -0.01484843622893095, 2.6198060512542725, 1.178787112236023, 0.337903767824173, 3.2569048404693604, 0.3923218548297882, 1.9298046827316284, -0.4231970012187958, 0.8378292322158813, 0.9660212397575378, 0.1529720574617386, -0.24621708691120148, 0.07242453843355179, 0.504751443862915, 1.1318378448486328, 0.4921102225780487, 2.113208532333374, 0.8383486270904541, 1.576010823249817, -0.4351491928100586, 1.4136608839035034, 0.5505817532539368, -0.46545130014419556, 0.840986430644989, 1.4519518613815308, 1.2783864736557007, 1.1196433305740356, 3.288130283355713, -0.19184760749340057, 0.42621153593063354, -0.0122215049341321, 0.1886814534664154, 2.2373387813568115, -0.4609271287918091, 0.6871262192726135, -1.5486775636672974, 0.32541343569755554, 1.3366020917892456, 0.9947085976600647, 1.2046067714691162, 0.49962642788887024, 3.3727338314056396, 0.2285105139017105, -0.42744651436805725, 1.2157588005065918, 1.815087914466858, 1.0623289346694946, -0.5403168797492981, -0.5993349552154541, 1.9422913789749146, 0.08573892712593079, 0.29454848170280457, -0.6109731793403625, 0.733282208442688, 1.0993953943252563, 2.1262693405151367, 0.9063147306442261, 0.44172534346580505, 1.032265067100525, 1.5623157024383545, 0.12546302378177643, 0.002796356100589037, 0.21033068001270294, 0.07675644010305405, 1.3104512691497803, 0.8687132596969604, 1.6327847242355347, 0.201705664396286, 0.9980980753898621, 1.2614179849624634, 0.058650676161050797, 1.8474632501602173, -1.1202744245529175, 1.0980786085128784, -0.352902352809906, 0.43271297216415405, 1.306618571281433, 0.24228256940841675, 1.3032524585723877, -0.16895203292369843, 1.1724961996078491, 0.4981655478477478, 0.05364139750599861, 1.0164376497268677, -0.1708928495645523, 2.6275062561035156, 2.042116403579712, 3.153310537338257, -0.1855899840593338, 0.21818451583385468, 0.8642119765281677, -0.02020762488245964, 2.555300712585449, 1.8844727277755737, -0.25124382972717285, 1.5590296983718872, 0.009445209987461567, 0.5306494832038879, -0.16529907286167145, -0.16481105983257294, -0.14615465700626373, 0.4515221118927002, 0.42008158564567566, 0.1260058879852295, -0.28222376108169556, 1.262567400932312, 0.4282822608947754, -0.44351354241371155, 2.354013681411743, 1.289267897605896, 0.5837283134460449, 1.8321012258529663, -0.2189575582742691, 0.2380303293466568, 0.09219571202993393, 0.6315371990203857, 1.7630170583724976, 0.6257134079933167, -0.563249945640564, 1.3807543516159058, 0.5544688701629639, -0.2004663199186325, -0.5925487875938416, 0.3908156752586365, 2.185692548751831, 1.7262576818466187, 0.912727952003479, 1.4959512948989868, 0.1560622602701187, 1.1927039623260498, 2.323366165161133, 0.9138765335083008, 0.6771316528320312, 1.8592910766601562, 0.6960896253585815, 0.6758246421813965, -0.35340961813926697, 1.5957528352737427, 0.27526214718818665, 1.6819682121276855, -0.043273087590932846, -0.37481147050857544, 2.602067708969116, 0.20117829740047455, 1.2263225317001343, 0.9864646792411804, 0.32760173082351685, -0.5507956743240356, 0.546897292137146, 2.8204169273376465, 0.1936754733324051, 1.3839342594146729, 2.4645698070526123, 0.8776224851608276, 0.5471237301826477, 1.3364276885986328, 0.5411107540130615, -0.052494022995233536, 0.025387782603502274, 0.18172496557235718, 0.705041766166687, 0.7521365880966187, 1.9138001203536987, 0.47837284207344055, 3.565962314605713, 1.0538376569747925, -0.4499434530735016, 1.4522532224655151, 2.130108118057251, 0.5728548169136047, 0.6910536289215088, -0.3311966359615326, 0.31260237097740173, 2.0067403316497803, 1.618360996246338, 1.1897141933441162, 0.669028639793396, 0.555843710899353, -0.4444456100463867, 0.09393876045942307, 1.0358439683914185, 0.4168773293495178, 1.0948026180267334, 0.8019102215766907, 0.7598881721496582, 0.35596731305122375, 1.3098022937774658, 0.19123558700084686, 1.1238445043563843, 1.8522306680679321, 0.2563640773296356, 1.1449346542358398, 0.23707006871700287, 2.7179036140441895, 0.42404645681381226, 0.9294201135635376, 1.6093922853469849, 0.6636247038841248, -0.3273467719554901, 0.7286820411682129, -0.000374260765966028, -0.922681450843811, -0.638127863407135, 0.08093053102493286, -0.31852200627326965, 0.9515830278396606, 1.103498101234436, -0.6097678542137146, 2.7675883769989014, 1.9221514463424683, 4.470524787902832, 0.9918025135993958, 0.7347339987754822, -0.47366464138031006, 1.2771817445755005, -0.11873098462820053, -0.27673834562301636, -0.45747867226600647, 1.1563700437545776, 1.5956839323043823, -0.5544795989990234, -0.05639054998755455, 0.6997634172439575, 2.6711745262145996, 2.3883001804351807, 0.48001164197921753, -0.16606853902339935, -0.4212746322154999, 1.1381734609603882, 0.6034901142120361, -0.3567849099636078, 1.3718087673187256, 2.233001470565796, 2.609417676925659, 1.6083455085754395, -0.7331475019454956, 0.6788316965103149, 0.20979686081409454, -0.2558607757091522, 0.3844009339809418, 0.031232725828886032, 1.4443225860595703, -0.1764097809791565, 0.29484567046165466, 1.3459854125976562, 1.2260926961898804, 1.2095831632614136, 1.2046900987625122, 0.18389911949634552, 0.42166176438331604, 2.2461891174316406, -0.46949058771133423, 0.12090595811605453, -0.24636094272136688, 0.9045799970626831, 0.6254155039787292, -0.7729294300079346, 1.007533073425293, 1.0120586156845093, 0.6394217014312744], 'y_pred_int': [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 2, 0, 2, 2, 0, 3, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 3, 0, 3, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 2, 2, 0, 0, 0, 0, 3, 1, 0, 3, 0, 2, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 3, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 1, 0, 3, 0, 0, 1, 2, 1, 0, 0, 2, 0, 0, 0, 1, 1, 2, 1, 0, 1, 2, 0, 0, 0, 0, 1, 1, 2, 0, 1, 1, 0, 2, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 3, 2, 3, 0, 0, 1, 0, 3, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 2, 0, 0, 0, 1, 2, 1, 0, 1, 1, 0, 0, 0, 2, 2, 1, 1, 0, 1, 2, 1, 1, 2, 1, 1, 0, 2, 0, 2, 0, 0, 3, 0, 1, 1, 0, 0, 1, 3, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 1, 1, 2, 0, 4, 1, 0, 1, 2, 1, 1, 0, 0, 2, 2, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 2, 0, 1, 0, 3, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 3, 2, 4, 1, 1, 0, 1, 0, 0, 0, 1, 2, 0, 0, 1, 3, 2, 0, 0, 0, 1, 1, 0, 1, 2, 3, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 1, 1, 0, 1, 1, 1], 'y_true': [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 3, 0, 3, 1, 0, 5, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 2, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 0, 4, 0, 0, 0, 0, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 1, 2, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1, 1, 0, 0, 4, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 5, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 3, 1, 6, 0, 0, 0, 0, 3, 3, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 4, 2, 1, 1, 0, 0, 3, 2, 0, 2, 1, 0, 0, 1, 0, 1, 1, 0, 3, 0, 0, 1, 0, 0, 3, 3, 0, 1, 2, 0, 0, 1, 3, 0, 0, 0, 1, 0, 2, 1, 2, 2, 0, 1, 3, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 2, 0, 1, 0, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 5, 2, 4, 2, 0, 0, 2, 0, 1, 0, 2, 3, 0, 0, 1, 3, 4, 0, 0, 0, 1, 0, 0, 1, 4, 3, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0], 'train_acc': 0.3409969723183391, 'test_acc': 0.41521626297577857, 'X': 'X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 9.9110, Test loss: 14.6756
In this epoch 100/5000, Training loss: 1.2711, Test loss: 14.2705
In this epoch 150/5000, Training loss: 0.5100, Test loss: 13.9023
In this epoch 200/5000, Training loss: 0.2420, Test loss: 13.8879
In this epoch 250/5000, Training loss: 0.1222, Test loss: 13.8828
In this epoch 300/5000, Training loss: 0.0640, Test loss: 13.8786
In this epoch 350/5000, Training loss: 0.0343, Test loss: 13.8716
In this epoch 400/5000, Training loss: 0.0187, Test loss: 13.8617
In this epoch 450/5000, Training loss: 0.0104, Test loss: 13.8507
In this epoch 500/5000, Training loss: 0.0058, Test loss: 13.8400
In this epoch 550/5000, Training loss: 0.0033, Test loss: 13.8306
In this epoch 600/5000, Training loss: 0.0019, Test loss: 13.8227
In this epoch 650/5000, Training loss: 0.0011, Test loss: 13.8163
In this epoch 700/5000, Training loss: 0.0006, Test loss: 13.8112
In this epoch 750/5000, Training loss: 0.0004, Test loss: 13.8073
In this epoch 800/5000, Training loss: 0.0002, Test loss: 13.8044
In this epoch 850/5000, Training loss: 0.0001, Test loss: 13.8021
In this epoch 900/5000, Training loss: 0.0001, Test loss: 13.8004
In this epoch 950/5000, Training loss: 0.0000, Test loss: 13.7991
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 13.7981
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 13.7973
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 13.7968
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 13.7963
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 13.7959
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 13.7956
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 13.7954
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 13.7952
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 13.7950
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 13.7948
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 13.7946
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 13.7926
In this epoch 1600/5000, Training loss: 18.0783, Test loss: 36.4444
In this epoch 1650/5000, Training loss: 0.0124, Test loss: 13.7300
In this epoch 1700/5000, Training loss: 0.0005, Test loss: 13.8105
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 13.7952
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 13.7940
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 13.7938
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 13.7937
In this epoch 1950/5000, Training loss: 0.0849, Test loss: 13.7272
In this epoch 2000/5000, Training loss: 2.2747, Test loss: 13.7466
In this epoch 2050/5000, Training loss: 0.0144, Test loss: 13.7275
In this epoch 2100/5000, Training loss: 0.0001, Test loss: 13.7882
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 13.7939
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 13.7933
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 13.7933
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 13.7938
In this epoch 2350/5000, Training loss: 36.9639, Test loss: 52.6570
In this epoch 2400/5000, Training loss: 0.0612, Test loss: 13.7443
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 13.8050
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 13.7915
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 13.7929
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 13.7929
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 13.7929
In this epoch 2700/5000, Training loss: 0.0739, Test loss: 13.7191
In this epoch 2750/5000, Training loss: 2.8059, Test loss: 17.1473
In this epoch 2800/5000, Training loss: 0.0251, Test loss: 13.9011
In this epoch 2850/5000, Training loss: 0.0001, Test loss: 13.8002
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 13.7928
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 13.7927
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 13.7927
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 13.7940
In this epoch 3100/5000, Training loss: 31.2276, Test loss: 20.0936
In this epoch 3150/5000, Training loss: 0.0414, Test loss: 13.7488
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 13.7805
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 13.7928
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 13.7926
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 13.7926
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 13.7926
In this epoch 3450/5000, Training loss: 161.4468, Test loss: 250.9221
In this epoch 3500/5000, Training loss: 0.8573, Test loss: 15.7341
In this epoch 3550/5000, Training loss: 0.0049, Test loss: 13.7502
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 13.7897
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 13.7923
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 13.7926
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 13.7926
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 13.7926
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 13.7926
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4000/5000, Training loss: 4.2664, Test loss: 246.8479
In this epoch 4050/5000, Training loss: 0.1458, Test loss: 15.9572
In this epoch 4100/5000, Training loss: 0.0001, Test loss: 13.7517
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 13.7896
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 13.7923
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 13.7925
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 13.7925
train_acc 1.0
test_acc 0.2
{'y_pred': [15.497821807861328, 27.964561462402344, 23.46066665649414, 5.082797050476074, 17.108495712280273, 8.536524772644043, 6.75224494934082, 7.136754989624023, 15.625022888183594, 4.967563629150391, 18.16253089904785, 4.2675957679748535, 17.81679344177246, 9.471612930297852, 2.2976627349853516, 30.209442138671875, 10.348404884338379, 9.513786315917969, 25.21792221069336, 10.495705604553223, 10.664567947387695, 7.738918304443359, 10.830580711364746, 19.64787483215332, 30.15101432800293, 18.947017669677734, 32.00014114379883, 24.655534744262695, 26.11969566345215, 9.917773246765137, 3.348234176635742, 10.655088424682617, 4.580003261566162, 29.970409393310547, 15.295869827270508, 11.57625675201416, 24.969493865966797, 16.04564666748047, 14.765035629272461, 8.846875190734863, 13.924118995666504, 20.358867645263672, 32.19864273071289, 11.744857788085938, 12.577710151672363, 10.477707862854004, 6.944185256958008, 6.522333145141602, 12.963500022888184, 15.22269344329834, 4.1848273277282715, 2.2136683464050293, 9.129243850708008, 36.924285888671875, 7.0691237449646, 12.31357479095459, 19.458688735961914, 10.165254592895508, 7.18867301940918, 4.311477184295654, 5.312985420227051, 9.876204490661621, 11.911969184875488, 23.08675765991211, 5.490011215209961, 12.330422401428223, 13.04555892944336, 9.790009498596191, 24.276721954345703, 6.750085353851318, 15.303418159484863, 6.37905216217041, 14.328373908996582, 6.917235374450684, 16.436885833740234, 22.981985092163086, 5.5903000831604, 38.7518310546875, 12.576048851013184, 14.047210693359375, 15.744879722595215, 32.49421691894531, 26.034753799438477, 13.48437213897705, 2.50610613822937, 5.01769495010376, 9.537375450134277, 13.590731620788574, 5.744791507720947, 3.9003920555114746, 28.39204978942871, 41.359500885009766, 10.2572021484375, 16.597278594970703, 13.255956649780273, 19.003345489501953, 8.255210876464844, 7.420098304748535, 21.424808502197266, 13.831007957458496, 7.129021644592285, 31.216073989868164, 6.830584526062012, 7.207329750061035, 31.96136474609375, 24.51651954650879, 13.105717658996582, 21.74044418334961, 14.067998886108398, 15.42917537689209, 8.558629035949707, 12.602733612060547, 38.47040557861328, 27.39453887939453, 37.05732727050781, 8.577632904052734, 5.698548316955566, 57.468421936035156, 18.35088348388672, 38.22266387939453, 12.28983211517334, 17.710859298706055, 15.182319641113281, 6.351930141448975, 13.988201141357422, 13.822507858276367, 16.477264404296875, 7.209097862243652, 4.615309715270996, 11.6177396774292, 6.475683212280273, 37.56214904785156, 15.246085166931152, 3.6324946880340576, 6.560848236083984, 5.0480241775512695, 7.996090888977051, 16.55435562133789, 9.156081199645996, 16.884403228759766, 3.6139283180236816, 9.427580833435059, 11.548534393310547, 27.33035659790039, 11.539446830749512, 41.41347885131836, 18.958023071289062, 25.002267837524414, 8.625018119812012, 17.54334259033203, 10.488668441772461, 2.370856761932373, 2.9371578693389893, 29.25613784790039, 7.0494489669799805, 12.280377388000488, 30.002260208129883, 12.002023696899414, 5.089557647705078, 9.329886436462402, 13.025070190429688, 14.579495429992676, 8.031319618225098, 7.184471130371094, 8.077537536621094, 54.30142593383789, 14.88383674621582, 10.255683898925781, 4.41939115524292, 8.167877197265625, 6.42865514755249, 8.953463554382324, 9.148602485656738, 9.161301612854004, 18.59056854248047, 33.803619384765625, 19.72529411315918, 14.526007652282715, 8.95809555053711, 2.327815055847168, 11.718587875366211, 18.70714569091797, 8.915995597839355, 9.326886177062988, 35.90837860107422, 8.270334243774414, 2.6259450912475586, 10.83323860168457, 9.881264686584473, 1.416520357131958, 18.638442993164062, 4.135105609893799, 11.66112995147705, 18.574077606201172, 16.185035705566406, 15.993122100830078, 8.669857025146484, 6.065155029296875, 7.283086776733398, 7.553762435913086, 20.303579330444336, 13.693388938903809, 7.157323837280273, 35.73686981201172, 13.509010314941406, 9.879965782165527, 21.400604248046875, 7.602484226226807, 11.644707679748535, 29.84079933166504, 4.367774963378906, 6.465949535369873, 29.564571380615234, 7.72797966003418, 10.4110107421875, 6.853041172027588, 3.9419105052948, 8.660900115966797, 21.698558807373047, 4.7010087966918945, 22.116146087646484, 3.2588298320770264, 8.014782905578613, 5.987616539001465, 3.3695693016052246, 17.167888641357422, 6.2294511795043945, 15.981334686279297, 5.119352340698242, 5.318695068359375, 4.832376480102539, 8.416500091552734, 5.701366901397705, 8.003443717956543, 14.225342750549316, 17.76761245727539, 19.653369903564453, 32.04671096801758, 5.08267879486084, 7.363219261169434, 36.998779296875, 7.34002685546875, 5.560234069824219, 6.413871765136719, 9.021201133728027, 15.286343574523926, 5.476744651794434, 17.227828979492188, 24.452003479003906, 7.757974624633789, 3.517251968383789, 32.02107620239258, 10.131945610046387, 11.847923278808594, 3.4344921112060547, 12.858792304992676, 19.696401596069336, 32.14826202392578, 20.803760528564453, 25.902833938598633, 6.593595027923584, 14.107877731323242, 10.9797945022583, 15.92065143585205, 31.74062728881836, 3.9186906814575195, 9.514204978942871, 7.779457092285156, 14.90461540222168, 12.987664222717285, 1.3231940269470215, 32.006980895996094, 29.35057258605957, 13.35396671295166, 76.08219909667969, 18.803409576416016, 7.360579013824463, 17.369937896728516, 16.073997497558594, 16.133007049560547, 5.036332130432129, 14.089664459228516, 8.455195426940918, 16.209259033203125, 27.144739151000977, 6.200008392333984, 2.875763416290283, 3.4993839263916016, 39.429893493652344, 20.004291534423828, 8.12368392944336, 4.903487205505371, 17.177202224731445, 7.213183879852295, 17.61041259765625, 44.175052642822266, 3.2642674446105957, 8.404723167419434, 2.7317163944244385, 46.171470642089844, 37.77198791503906, 26.64531707763672, 6.163069725036621, 22.11687660217285, 16.348901748657227, 18.00357437133789, 18.348112106323242, 10.476948738098145, 25.63827133178711, 22.511512756347656, 26.307100296020508, 4.171879768371582, 34.91261291503906, 4.345940589904785, 17.964746475219727, 7.591622352600098, 6.391994476318359, 26.9877986907959, 13.894234657287598, 1.228023648262024, 34.54059982299805, 14.700592994689941, 13.474882125854492, 3.3317818641662598, 10.860834121704102, 13.166951179504395, 23.84772491455078, 10.117639541625977, 19.122575759887695, 13.098027229309082, 16.137619018554688, 17.258018493652344, 9.3463773727417, 6.329153060913086, 15.021051406860352, 5.906458854675293, 10.2603120803833, 7.916328430175781, 9.989014625549316, 4.660530090332031], 'y_pred_int': [15, 28, 23, 5, 17, 9, 7, 7, 16, 5, 18, 4, 18, 9, 2, 30, 10, 10, 25, 10, 11, 8, 11, 20, 30, 19, 32, 25, 26, 10, 3, 11, 5, 30, 15, 12, 25, 16, 15, 9, 14, 20, 32, 12, 13, 10, 7, 7, 13, 15, 4, 2, 9, 37, 7, 12, 19, 10, 7, 4, 5, 10, 12, 23, 5, 12, 13, 10, 24, 7, 15, 6, 14, 7, 16, 23, 6, 39, 13, 14, 16, 32, 26, 13, 3, 5, 10, 14, 6, 4, 28, 41, 10, 17, 13, 19, 8, 7, 21, 14, 7, 31, 7, 7, 32, 25, 13, 22, 14, 15, 9, 13, 38, 27, 37, 9, 6, 57, 18, 38, 12, 18, 15, 6, 14, 14, 16, 7, 5, 12, 6, 38, 15, 4, 7, 5, 8, 17, 9, 17, 4, 9, 12, 27, 12, 41, 19, 25, 9, 18, 10, 2, 3, 29, 7, 12, 30, 12, 5, 9, 13, 15, 8, 7, 8, 54, 15, 10, 4, 8, 6, 9, 9, 9, 19, 34, 20, 15, 9, 2, 12, 19, 9, 9, 36, 8, 3, 11, 10, 1, 19, 4, 12, 19, 16, 16, 9, 6, 7, 8, 20, 14, 7, 36, 14, 10, 21, 8, 12, 30, 4, 6, 30, 8, 10, 7, 4, 9, 22, 5, 22, 3, 8, 6, 3, 17, 6, 16, 5, 5, 5, 8, 6, 8, 14, 18, 20, 32, 5, 7, 37, 7, 6, 6, 9, 15, 5, 17, 24, 8, 4, 32, 10, 12, 3, 13, 20, 32, 21, 26, 7, 14, 11, 16, 32, 4, 10, 8, 15, 13, 1, 32, 29, 13, 76, 19, 7, 17, 16, 16, 5, 14, 8, 16, 27, 6, 3, 3, 39, 20, 8, 5, 17, 7, 18, 44, 3, 8, 3, 46, 38, 27, 6, 22, 16, 18, 18, 10, 26, 23, 26, 4, 35, 4, 18, 8, 6, 27, 14, 1, 35, 15, 13, 3, 11, 13, 24, 10, 19, 13, 16, 17, 9, 6, 15, 6, 10, 8, 10, 5], 'y_true': [15, 29, 26, 5, 22, 7, 4, 7, 14, 6, 26, 4, 23, 12, 2, 34, 9, 10, 18, 12, 10, 6, 13, 18, 32, 15, 39, 22, 18, 9, 4, 11, 5, 22, 15, 15, 23, 15, 11, 10, 14, 25, 31, 15, 11, 9, 6, 5, 13, 16, 4, 3, 11, 46, 11, 9, 21, 8, 4, 6, 4, 13, 9, 25, 6, 10, 10, 11, 24, 7, 14, 4, 12, 8, 16, 26, 3, 28, 16, 12, 14, 33, 35, 17, 3, 5, 11, 14, 4, 5, 28, 51, 12, 16, 12, 20, 8, 7, 22, 12, 5, 24, 8, 7, 33, 19, 10, 22, 9, 13, 7, 13, 30, 30, 38, 8, 5, 46, 23, 50, 16, 15, 18, 8, 12, 15, 16, 6, 4, 17, 6, 43, 12, 2, 6, 4, 6, 15, 10, 16, 3, 9, 15, 21, 15, 49, 16, 20, 7, 16, 10, 1, 2, 31, 10, 10, 29, 11, 5, 12, 14, 11, 11, 6, 10, 60, 16, 11, 6, 11, 5, 10, 8, 10, 19, 49, 13, 17, 9, 2, 9, 20, 8, 9, 40, 7, 4, 12, 8, 3, 16, 5, 14, 22, 24, 20, 8, 5, 9, 8, 26, 13, 6, 27, 15, 9, 26, 4, 11, 25, 4, 11, 29, 11, 13, 8, 2, 10, 26, 4, 18, 3, 6, 9, 4, 11, 8, 11, 5, 6, 4, 9, 5, 8, 14, 18, 19, 32, 4, 5, 39, 7, 6, 5, 12, 15, 5, 11, 31, 9, 3, 51, 7, 10, 2, 12, 22, 37, 18, 25, 6, 14, 9, 20, 36, 6, 9, 10, 13, 18, 1, 17, 24, 13, 100, 16, 5, 14, 17, 15, 3, 16, 8, 22, 30, 5, 3, 6, 42, 21, 10, 5, 12, 7, 17, 42, 3, 9, 4, 48, 34, 32, 9, 20, 11, 15, 17, 9, 17, 23, 27, 3, 31, 4, 27, 9, 6, 29, 11, 2, 28, 16, 11, 3, 10, 14, 28, 6, 14, 12, 12, 17, 8, 8, 16, 8, 9, 9, 8, 6], 'train_acc': 1.0, 'test_acc': 0.2, 'X': 'X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 790.6408, Test loss: 1314.4127
In this epoch 100/5000, Training loss: 145.9194, Test loss: 499.0395
In this epoch 150/5000, Training loss: 81.9128, Test loss: 451.1215
In this epoch 200/5000, Training loss: 54.3825, Test loss: 425.4700
In this epoch 250/5000, Training loss: 39.1956, Test loss: 412.1335
In this epoch 300/5000, Training loss: 29.3737, Test loss: 403.8310
In this epoch 350/5000, Training loss: 22.5019, Test loss: 398.1170
In this epoch 400/5000, Training loss: 17.4881, Test loss: 393.9203
In this epoch 450/5000, Training loss: 13.7380, Test loss: 390.7180
In this epoch 500/5000, Training loss: 10.8868, Test loss: 388.2162
In this epoch 550/5000, Training loss: 8.6922, Test loss: 386.2249
In this epoch 600/5000, Training loss: 6.9861, Test loss: 384.6133
In this epoch 650/5000, Training loss: 5.6477, Test loss: 383.2871
In this epoch 700/5000, Training loss: 4.5895, Test loss: 382.1796
In this epoch 750/5000, Training loss: 3.7465, Test loss: 381.2435
In this epoch 800/5000, Training loss: 3.0704, Test loss: 380.4446
In this epoch 850/5000, Training loss: 2.5249, Test loss: 379.7569
In this epoch 900/5000, Training loss: 2.0824, Test loss: 379.1622
In this epoch 950/5000, Training loss: 1.7217, Test loss: 378.6455
In this epoch 1000/5000, Training loss: 1.4265, Test loss: 378.1963
In this epoch 1050/5000, Training loss: 1.1840, Test loss: 377.8037
In this epoch 1100/5000, Training loss: 0.9842, Test loss: 377.4617
In this epoch 1150/5000, Training loss: 0.8190, Test loss: 377.1647
In this epoch 1200/5000, Training loss: 0.6823, Test loss: 376.9053
In this epoch 1250/5000, Training loss: 0.5688, Test loss: 376.6791
In this epoch 1300/5000, Training loss: 0.4744, Test loss: 376.4840
In this epoch 1350/5000, Training loss: 0.3959, Test loss: 376.3147
In this epoch 1400/5000, Training loss: 0.3304, Test loss: 376.1683
In this epoch 1450/5000, Training loss: 0.2757, Test loss: 376.0410
In this epoch 1500/5000, Training loss: 0.2301, Test loss: 375.9321
In this epoch 1550/5000, Training loss: 0.1920, Test loss: 375.8390
In this epoch 1600/5000, Training loss: 0.1601, Test loss: 375.7574
In this epoch 1650/5000, Training loss: 0.1335, Test loss: 375.6875
In this epoch 1700/5000, Training loss: 0.1113, Test loss: 375.6286
In this epoch 1750/5000, Training loss: 0.0927, Test loss: 375.5774
In this epoch 1800/5000, Training loss: 0.0771, Test loss: 375.5334
In this epoch 1850/5000, Training loss: 0.0642, Test loss: 375.4953
In this epoch 1900/5000, Training loss: 0.0534, Test loss: 375.4622
In this epoch 1950/5000, Training loss: 0.0443, Test loss: 375.4342
In this epoch 2000/5000, Training loss: 0.0368, Test loss: 375.4099
In this epoch 2050/5000, Training loss: 0.0306, Test loss: 375.3888
In this epoch 2100/5000, Training loss: 0.0254, Test loss: 375.3695
In this epoch 2150/5000, Training loss: 0.0210, Test loss: 375.3535
In this epoch 2200/5000, Training loss: 0.0174, Test loss: 375.3388
In this epoch 2250/5000, Training loss: 0.0145, Test loss: 375.3264
In this epoch 2300/5000, Training loss: 0.0120, Test loss: 375.3150
In this epoch 2350/5000, Training loss: 0.0099, Test loss: 375.3055
In this epoch 2400/5000, Training loss: 0.0083, Test loss: 375.2969
In this epoch 2450/5000, Training loss: 0.0069, Test loss: 375.2901
In this epoch 2500/5000, Training loss: 0.0057, Test loss: 375.2825
In this epoch 2550/5000, Training loss: 0.0047, Test loss: 375.2766
In this epoch 2600/5000, Training loss: 0.0040, Test loss: 375.2713
In this epoch 2650/5000, Training loss: 0.0033, Test loss: 375.2667
In this epoch 2700/5000, Training loss: 0.0028, Test loss: 375.2622
In this epoch 2750/5000, Training loss: 0.0023, Test loss: 375.2580
In this epoch 2800/5000, Training loss: 0.0020, Test loss: 375.2543
In this epoch 2850/5000, Training loss: 0.0017, Test loss: 375.2515
In this epoch 2900/5000, Training loss: 0.0014, Test loss: 375.2493
In this epoch 2950/5000, Training loss: 0.0012, Test loss: 375.2466
In this epoch 3000/5000, Training loss: 0.0011, Test loss: 375.2442
In this epoch 3050/5000, Training loss: 0.0009, Test loss: 375.2425
In this epoch 3100/5000, Training loss: 0.0008, Test loss: 375.2401
In this epoch 3150/5000, Training loss: 0.0007, Test loss: 375.2383
In this epoch 3200/5000, Training loss: 0.0006, Test loss: 375.2363
In this epoch 3250/5000, Training loss: 0.0006, Test loss: 375.2349
In this epoch 3300/5000, Training loss: 0.0005, Test loss: 375.2341
In this epoch 3350/5000, Training loss: 0.0005, Test loss: 375.2329
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 375.2311
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 375.2293
In this epoch 3500/5000, Training loss: 0.0004, Test loss: 375.2283
In this epoch 3550/5000, Training loss: 0.0004, Test loss: 375.2274
In this epoch 3600/5000, Training loss: 0.0003, Test loss: 375.2261
In this epoch 3650/5000, Training loss: 0.0003, Test loss: 375.2254
In this epoch 3700/5000, Training loss: 0.0003, Test loss: 375.2236
In this epoch 3750/5000, Training loss: 0.0003, Test loss: 375.2229
In this epoch 3800/5000, Training loss: 0.0003, Test loss: 375.2216
In this epoch 3850/5000, Training loss: 0.0003, Test loss: 375.2203
In this epoch 3900/5000, Training loss: 0.0002, Test loss: 375.2195
In this epoch 3950/5000, Training loss: 0.0002, Test loss: 375.2181
In this epoch 4000/5000, Training loss: 0.0002, Test loss: 375.2164
In this epoch 4050/5000, Training loss: 0.0002, Test loss: 375.2159
In this epoch 4100/5000, Training loss: 0.0002, Test loss: 375.2148
In this epoch 4150/5000, Training loss: 0.0002, Test loss: 375.2134
In this epoch 4200/5000, Training loss: 0.0002, Test loss: 375.2127
In this epoch 4250/5000, Training loss: 0.0002, Test loss: 375.2119
In this epoch 4300/5000, Training loss: 0.0002, Test loss: 375.2104
In this epoch 4350/5000, Training loss: 0.0002, Test loss: 375.2095
In this epoch 4400/5000, Training loss: 0.0002, Test loss: 375.2079
In this epoch 4450/5000, Training loss: 0.0001, Test loss: 375.2062
In this epoch 4500/5000, Training loss: 0.0001, Test loss: 375.2065
In this epoch 4550/5000, Training loss: 0.0001, Test loss: 375.2054
In this epoch 4600/5000, Training loss: 0.0001, Test loss: 375.2037
In this epoch 4650/5000, Training loss: 0.0001, Test loss: 375.2034
In this epoch 4700/5000, Training loss: 0.0001, Test loss: 375.2029
In this epoch 4750/5000, Training loss: 0.0001, Test loss: 375.2017
In this epoch 4800/5000, Training loss: 0.0001, Test loss: 375.2010
In this epoch 4850/5000, Training loss: 0.0001, Test loss: 375.2003
In this epoch 4900/5000, Training loss: 0.0001, Test loss: 375.1997
In this epoch 4950/5000, Training loss: 0.0001, Test loss: 375.1989
In this epoch 5000/5000, Training loss: 0.0001, Test loss: 375.1988
train_acc 1.0
test_acc 0.052941176470588235
{'y_pred': [717.2783813476562, 454.397216796875, 328.24285888671875, 644.5911865234375, 114.98268127441406, 1037.527099609375, 302.58770751953125, 630.3416137695312, 207.2316131591797, 102.59046173095703, 432.87506103515625, 152.70834350585938, 166.5331268310547, 490.3966064453125, 215.13140869140625, 445.63543701171875, 282.8772888183594, 368.4031982421875, 951.60595703125, 149.18948364257812, 169.10018920898438, 136.98648071289062, 447.60479736328125, 342.8298645019531, 453.4619140625, 208.6291046142578, 99.27867889404297, 783.8348999023438, 456.51348876953125, 533.4247436523438, 236.30088806152344, 467.5899353027344, 187.04949951171875, 502.70367431640625, 267.35003662109375, 74.80347442626953, 822.6103515625, 519.8380737304688, 683.5910034179688, 530.6809692382812, 74.50720977783203, 363.1611328125, 403.9557189941406, 449.9118957519531, 199.58509826660156, 342.76300048828125, 453.14599609375, 291.052734375, 1037.6878662109375, 294.72503662109375, 414.1414794921875, 134.64581298828125, 338.5782470703125, 345.4295654296875, 682.2774047851562, 529.9637451171875, 463.70025634765625, 894.59228515625, 198.38986206054688, 317.4112548828125, 55.51030349731445, 90.8202896118164, 106.87224578857422, 737.3351440429688, 208.04718017578125, 542.5497436523438, 94.85179901123047, 191.48240661621094, 130.56715393066406, 363.50494384765625, 407.06488037109375, 232.1144256591797, 403.7293701171875, 572.8222045898438, 177.13253784179688, 39.00074005126953, 513.8428955078125, 488.0125732421875, 468.2413330078125, 432.44189453125, 416.31475830078125, 816.035888671875, 559.2350463867188, 118.25121307373047, 532.8572387695312, 506.9493103027344, 115.54080963134766, 181.10667419433594, 425.6875, 375.1515197753906, 603.6871948242188, 164.65289306640625, 638.1502685546875, 585.918701171875, 264.7890930175781, 645.4664916992188, 345.5976257324219, 484.17041015625, 776.1151123046875, 845.6544799804688, 440.64691162109375, 764.9530029296875, 244.35870361328125, 136.53475952148438, 228.39234924316406, 157.02516174316406, 214.70069885253906, 520.110595703125, 392.4521179199219, 171.82752990722656, 355.5951232910156, 213.85438537597656, 98.83817291259766, 464.2314453125, 175.17156982421875, 373.8923645019531, 94.3363037109375, 275.01361083984375, 784.8851318359375, 195.1531219482422, 92.71898651123047, 357.16766357421875, 573.664794921875, 475.818115234375, 402.85284423828125, 778.0928955078125, 508.3822021484375, 484.3470153808594, 118.45021057128906, 245.55393981933594, 302.71826171875, 83.93949127197266, 557.1532592773438, 322.3186340332031, 361.8638916015625, 288.79052734375, 676.4007568359375, 265.25006103515625, 267.83154296875, 488.2770690917969, 334.5682067871094, 61.96671676635742, 147.5727081298828, 1314.213623046875, 320.4363708496094, 186.86402893066406, 806.3707275390625, 65.87406921386719, 115.7648696899414, 205.1608123779297, 208.40419006347656, 147.35389709472656, 481.14739990234375, 265.1758117675781, 464.8111572265625, 339.8030700683594, 439.4458312988281, 235.35287475585938, 332.1245422363281, 121.58899688720703, 429.41510009765625, 50.82685852050781, 113.84674835205078, 535.322265625, 169.86155700683594, 166.0414581298828, 337.4211730957031, 84.32402801513672, 150.5416717529297, 1391.870361328125, 530.854248046875, 659.8815307617188, 403.2153015136719, 136.2689971923828, 519.031005859375, 207.81947326660156, 288.4328308105469, 274.7765808105469, 167.05752563476562, 81.74506378173828, 86.96285247802734, 565.6826171875, 142.43011474609375, 166.6041259765625, 219.55517578125, 327.00677490234375, 568.6729736328125, 341.5955810546875, 721.5904541015625, 171.25437927246094, 223.69508361816406, 411.88525390625, 819.0799560546875, 188.48178100585938, 387.40447998046875, 439.488037109375, 690.5711669921875, 53.89374923706055, 358.1691589355469, 287.62371826171875, 326.0702819824219, 890.02099609375, 100.01942443847656, 217.72024536132812, 101.61807250976562, 218.86863708496094, 200.83255004882812, 591.7952270507812, 120.51155090332031, 130.67843627929688, 75.76288604736328, 159.05850219726562, 278.5955505371094, 240.540771484375, 547.8011474609375, 534.0658569335938, 279.97430419921875, 696.6508178710938, 321.0158996582031, 153.1842041015625, 643.751708984375, 395.5126953125, 233.2732391357422, 160.8829803466797, 448.9686279296875, 498.15875244140625, 191.57028198242188, 122.70196533203125, 241.6881561279297, 621.061279296875, 299.8431701660156, 786.1585693359375, 762.231689453125, 217.2602081298828, 198.99172973632812, 304.9009094238281, 260.4648132324219, 212.0837860107422, 722.742431640625, 147.72511291503906, 360.8214416503906, 249.57989501953125, 315.34857177734375, 133.67913818359375, 206.50405883789062, 285.1809997558594, 131.70448303222656, 280.663818359375, 530.426025390625, 372.1007385253906, 674.71044921875, 159.1250762939453, 159.4140625, 127.2403793334961, 305.07501220703125, 342.23870849609375, 309.787353515625, 55.504337310791016, 195.38092041015625, 399.2685546875, 551.6463623046875, 168.80487060546875, 339.022216796875, 363.154052734375, 90.61097717285156, 236.37188720703125, 618.5860595703125, 578.5807495117188, 176.833740234375, 206.798583984375, 159.1404571533203, 274.9649963378906, 234.3056640625, 416.8686218261719, 416.12799072265625, 327.1022644042969, 297.23040771484375, 322.8177795410156, 236.84567260742188, 92.1332778930664, 778.056640625, 197.69154357910156, 758.0675048828125, 97.51728057861328, 175.84226989746094, 270.22796630859375, 580.6475219726562, 502.2638854980469, 431.3113098144531, 166.91055297851562, 395.1468200683594, 149.03506469726562, 162.69912719726562, 772.6665649414062, 169.82470703125, 479.26763916015625, 560.6968383789062, 744.2900390625, 159.19252014160156, 141.3739471435547, 647.1015625, 324.3130187988281, 297.29974365234375, 278.5429992675781, 1024.996826171875, 709.7523803710938, 395.87359619140625, 182.55419921875, 302.5594177246094, 248.313232421875, 387.56988525390625, 153.23611450195312, 381.5111389160156, 527.7859497070312, 133.86923217773438, 224.89376831054688, 102.68444061279297, 62.7828254699707, 268.0607604980469, 847.2523193359375, 222.9829864501953, 381.2242736816406, 183.62315368652344, 232.75625610351562, 109.07797241210938, 229.48464965820312, 165.15292358398438, 404.4147644042969, 789.130859375, 476.6017150878906, 477.97552490234375, 496.09515380859375, 253.93331909179688, 537.8409423828125, 344.13360595703125, 578.365478515625, 414.2189025878906, 435.0601501464844, 749.4808959960938, 213.0249786376953], 'y_pred_int': [717, 454, 328, 645, 115, 1038, 303, 630, 207, 103, 433, 153, 167, 490, 215, 446, 283, 368, 952, 149, 169, 137, 448, 343, 453, 209, 99, 784, 457, 533, 236, 468, 187, 503, 267, 75, 823, 520, 684, 531, 75, 363, 404, 450, 200, 343, 453, 291, 1038, 295, 414, 135, 339, 345, 682, 530, 464, 895, 198, 317, 56, 91, 107, 737, 208, 543, 95, 191, 131, 364, 407, 232, 404, 573, 177, 39, 514, 488, 468, 432, 416, 816, 559, 118, 533, 507, 116, 181, 426, 375, 604, 165, 638, 586, 265, 645, 346, 484, 776, 846, 441, 765, 244, 137, 228, 157, 215, 520, 392, 172, 356, 214, 99, 464, 175, 374, 94, 275, 785, 195, 93, 357, 574, 476, 403, 778, 508, 484, 118, 246, 303, 84, 557, 322, 362, 289, 676, 265, 268, 488, 335, 62, 148, 1314, 320, 187, 806, 66, 116, 205, 208, 147, 481, 265, 465, 340, 439, 235, 332, 122, 429, 51, 114, 535, 170, 166, 337, 84, 151, 1392, 531, 660, 403, 136, 519, 208, 288, 275, 167, 82, 87, 566, 142, 167, 220, 327, 569, 342, 722, 171, 224, 412, 819, 188, 387, 439, 691, 54, 358, 288, 326, 890, 100, 218, 102, 219, 201, 592, 121, 131, 76, 159, 279, 241, 548, 534, 280, 697, 321, 153, 644, 396, 233, 161, 449, 498, 192, 123, 242, 621, 300, 786, 762, 217, 199, 305, 260, 212, 723, 148, 361, 250, 315, 134, 207, 285, 132, 281, 530, 372, 675, 159, 159, 127, 305, 342, 310, 56, 195, 399, 552, 169, 339, 363, 91, 236, 619, 579, 177, 207, 159, 275, 234, 417, 416, 327, 297, 323, 237, 92, 778, 198, 758, 98, 176, 270, 581, 502, 431, 167, 395, 149, 163, 773, 170, 479, 561, 744, 159, 141, 647, 324, 297, 279, 1025, 710, 396, 183, 303, 248, 388, 153, 382, 528, 134, 225, 103, 63, 268, 847, 223, 381, 184, 233, 109, 229, 165, 404, 789, 477, 478, 496, 254, 538, 344, 578, 414, 435, 749, 213], 'y_true': [724, 455, 329, 628, 121, 1027, 308, 629, 212, 102, 427, 150, 161, 490, 207, 446, 293, 376, 955, 153, 181, 131, 447, 332, 456, 202, 103, 797, 494, 545, 243, 464, 189, 507, 268, 81, 832, 506, 681, 521, 74, 361, 404, 450, 197, 350, 435, 292, 1060, 295, 403, 132, 340, 364, 683, 542, 466, 904, 191, 314, 54, 93, 108, 747, 206, 539, 97, 188, 136, 350, 406, 234, 405, 552, 173, 42, 513, 478, 506, 434, 408, 849, 559, 121, 548, 516, 119, 191, 428, 374, 582, 171, 621, 583, 264, 659, 354, 487, 753, 841, 446, 768, 241, 141, 228, 167, 216, 511, 397, 167, 367, 217, 105, 509, 170, 367, 88, 258, 778, 191, 93, 353, 566, 494, 407, 786, 550, 494, 110, 234, 300, 82, 544, 328, 376, 294, 669, 250, 260, 519, 340, 58, 153, 1297, 310, 178, 821, 65, 113, 210, 211, 149, 494, 255, 474, 343, 440, 240, 331, 132, 428, 45, 109, 527, 173, 166, 343, 87, 146, 1695, 523, 656, 404, 129, 529, 213, 307, 270, 175, 85, 89, 579, 135, 160, 210, 324, 572, 339, 719, 173, 231, 401, 805, 190, 396, 455, 680, 57, 353, 299, 312, 877, 104, 216, 101, 216, 200, 608, 117, 125, 76, 156, 270, 248, 556, 521, 278, 699, 308, 156, 667, 387, 234, 168, 458, 510, 199, 123, 243, 613, 291, 799, 779, 196, 198, 310, 279, 201, 698, 143, 375, 249, 331, 128, 210, 312, 141, 273, 528, 370, 680, 172, 165, 128, 312, 355, 326, 55, 187, 403, 554, 169, 323, 361, 89, 242, 621, 602, 181, 214, 161, 290, 243, 412, 409, 321, 301, 324, 241, 97, 790, 197, 778, 102, 176, 263, 583, 502, 430, 180, 384, 143, 158, 758, 173, 484, 565, 725, 149, 142, 649, 325, 302, 284, 1009, 708, 415, 176, 309, 259, 389, 161, 373, 521, 130, 245, 106, 68, 281, 840, 208, 372, 181, 242, 111, 226, 164, 392, 778, 459, 476, 506, 256, 537, 340, 588, 445, 412, 762, 215], 'train_acc': 1.0, 'test_acc': 0.052941176470588235, 'X': 'X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700', 'Y': 'Y_num_tokens_muc1700'}
