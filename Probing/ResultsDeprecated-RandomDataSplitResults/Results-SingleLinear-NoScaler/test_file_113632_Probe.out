Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 13.7884, Test loss: 8.8049
In this epoch 100/5000, Training loss: 0.0912, Test loss: 2.2196
In this epoch 150/5000, Training loss: 0.0017, Test loss: 2.2541
In this epoch 200/5000, Training loss: 0.0003, Test loss: 2.2572
In this epoch 250/5000, Training loss: 0.0002, Test loss: 2.2581
In this epoch 300/5000, Training loss: 0.0002, Test loss: 2.2585
In this epoch 350/5000, Training loss: 0.0001, Test loss: 2.2587
In this epoch 400/5000, Training loss: 0.0001, Test loss: 2.2589
In this epoch 450/5000, Training loss: 0.0001, Test loss: 2.2591
In this epoch 500/5000, Training loss: 0.0001, Test loss: 2.2593
In this epoch 550/5000, Training loss: 0.0001, Test loss: 2.2595
In this epoch 600/5000, Training loss: 0.0000, Test loss: 2.2597
In this epoch 650/5000, Training loss: 0.0000, Test loss: 2.2599
In this epoch 700/5000, Training loss: 0.0000, Test loss: 2.2601
In this epoch 750/5000, Training loss: 0.0000, Test loss: 2.2603
In this epoch 800/5000, Training loss: 0.0000, Test loss: 2.2604
In this epoch 850/5000, Training loss: 0.0000, Test loss: 2.2606
In this epoch 900/5000, Training loss: 0.0000, Test loss: 2.2607
In this epoch 950/5000, Training loss: 0.0000, Test loss: 2.2608
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 2.2609
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 2.2610
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 2.2610
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 2.2611
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 2.2611
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 2.2612
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 2.2612
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 2.2612
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 2.2612
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 2.2612
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 2.2613
In this epoch 2700/5000, Training loss: 205.7747, Test loss: 560.2949
In this epoch 2750/5000, Training loss: 0.8896, Test loss: 6.1300
In this epoch 2800/5000, Training loss: 0.0210, Test loss: 2.3137
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 2.2727
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 2.2710
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 2.2710
train_acc 0.34795090830449826
test_acc 0.403598615916955
{'y_pred': [0.5639209151268005, 0.6525222659111023, 0.8460344076156616, 0.8278419971466064, 1.3422242403030396, 0.668499767780304, 0.6549239754676819, 1.514754295349121, 1.4378572702407837, 0.8863791823387146, 1.1941797733306885, 1.410918116569519, 1.6740772724151611, 1.2857789993286133, 0.820122241973877, 0.5039610862731934, 2.102033853530884, 0.7147970795631409, 1.1345462799072266, 0.78891521692276, 0.814596951007843, 1.1326380968093872, 1.180403709411621, 1.4693043231964111, 1.1653430461883545, -0.07576301693916321, 1.2807233333587646, 0.583152711391449, 0.2185656726360321, 0.7591094374656677, 0.39229512214660645, 1.1599774360656738, 1.3019177913665771, 1.3341320753097534, 1.7209581136703491, 1.5975542068481445, 1.2196917533874512, 1.5990147590637207, 0.9716957807540894, 0.3908659815788269, 1.4471375942230225, 1.3053791522979736, 0.9214919209480286, 0.5894450545310974, 1.2203636169433594, 0.4831995368003845, 0.6435875296592712, 0.9686203002929688, 0.731583297252655, 0.5635185837745667, 0.5530228018760681, 0.6710290312767029, 1.2302567958831787, 1.2645779848098755, 1.6623988151550293, 1.097102165222168, 0.684357762336731, 1.0027271509170532, 0.5487873554229736, 1.234359860420227, 0.5784372687339783, 0.6975703835487366, 0.9427310824394226, 1.6992263793945312, 1.0069472789764404, 1.0779836177825928, 1.0324372053146362, 0.9249730706214905, 0.6618716716766357, 0.9676932692527771, 1.4468716382980347, 0.41436898708343506, 0.27947747707366943, 0.6596189141273499, 1.5224982500076294, 0.8272528052330017, 0.25086402893066406, 1.5887858867645264, 0.2524367570877075, 1.385001540184021, 1.008495807647705, 0.5052286982536316, 0.4084526300430298, 1.2260687351226807, 1.4325295686721802, 6.639821529388428, 1.3599774837493896, 0.6265656352043152, 1.2212615013122559, 0.9152147769927979, 1.3215832710266113, 1.3202435970306396, 0.7587195634841919, 1.1381045579910278, 0.9032610058784485, 1.4417273998260498, 1.4488856792449951, 0.21990959346294403, 1.3650091886520386, 1.0439164638519287, 1.0984792709350586, 0.6780497431755066, 0.9675862193107605, 0.9609318971633911, 1.0470175743103027, 0.12670966982841492, 1.1436940431594849, 0.9157764315605164, 1.6402201652526855, 0.9839860796928406, 0.2210889756679535, 0.754094660282135, 0.5789874196052551, 1.4070757627487183, 0.9412700533866882, 0.5212685465812683, 0.35000520944595337, 1.0894343852996826, 1.1068588495254517, 1.559097409248352, 0.4482673406600952, 1.0211418867111206, 0.43146297335624695, 0.2702578604221344, 1.4501430988311768, 1.0054168701171875, 0.8333637118339539, 2.1834824085235596, 1.4074807167053223, 1.2398115396499634, 0.6356792449951172, 1.6558856964111328, 0.5833263993263245, 1.4726181030273438, 0.21288901567459106, 0.7921832203865051, -0.5442590713500977, 0.5562491416931152, 1.2484394311904907, 1.659498929977417, 0.4303755760192871, -0.28664928674697876, 1.219448447227478, 1.7805333137512207, 1.3354908227920532, 1.4409241676330566, 0.6026946306228638, 0.3180631995201111, 0.788989245891571, 0.5438253283500671, 0.9080854058265686, 0.007264525629580021, 0.9573047757148743, 1.0441491603851318, 0.7957702875137329, 0.8508742451667786, 1.2614850997924805, 0.7028251886367798, 1.114025354385376, 0.5135637521743774, 0.6322594881057739, 1.0319277048110962, 1.7745529413223267, 0.8214847445487976, 1.472814679145813, 0.6626740097999573, 1.799736499786377, 1.2121543884277344, 1.2540404796600342, 0.6362327933311462, 0.05275633931159973, 1.046683430671692, 1.5688128471374512, 0.9932499527931213, 1.0763499736785889, 0.5970434546470642, 1.1921093463897705, 0.9037826657295227, 0.7588284015655518, 0.6219648718833923, 0.41479194164276123, 0.1842547208070755, 1.2492002248764038, 1.272881269454956, -0.30795779824256897, 1.4481600522994995, 0.9076980948448181, 1.0594146251678467, 0.6477814316749573, 0.9004450440406799, 0.5991710424423218, 1.0118377208709717, 1.4466543197631836, 0.5020108222961426, 0.8499086499214172, 1.3626691102981567, 0.8268272280693054, 1.0814439058303833, 1.0984406471252441, 0.8893412947654724, 1.1866188049316406, 0.45385628938674927, 0.8111719489097595, 0.38220369815826416, 0.7742890119552612, 1.2360174655914307, 0.5207831859588623, 0.3481525778770447, 0.9144125580787659, 0.8931759595870972, 0.6731180548667908, 0.702072262763977, 0.9734500050544739, 1.0111855268478394, 0.555766224861145, -0.2629031538963318, 0.19367928802967072, 0.4981813430786133, 1.180403709411621, 1.2643117904663086, 0.7560763359069824, 0.4994284212589264, 1.468034267425537, 1.4018182754516602, 0.3272075355052948, 0.251348614692688, 0.38350391387939453, 1.1800107955932617, 1.3120248317718506, 0.6559405326843262, 0.42864054441452026, 1.384399175643921, 1.3168363571166992, 0.7117874026298523, 1.2978787422180176, 1.298498511314392, 0.718336284160614, 1.446704626083374, 0.6137968897819519, 1.2261335849761963, 0.8207923173904419, 0.7646535038948059, 0.7272369861602783, 1.162967562675476, 0.9762594103813171, 1.5113394260406494, 1.0464204549789429, 1.2377221584320068, 1.2561705112457275, 0.6099250912666321, 1.0797474384307861, 0.7303702235221863, 0.6540653109550476, 0.9586123824119568, 0.9736078381538391, 0.8803759217262268, 0.9165423512458801, 0.3506978154182434, 1.2135009765625, 1.5034892559051514, 1.1822028160095215, 1.0107264518737793, 0.5365747809410095, 1.9172440767288208, 1.0183994770050049, 0.8165206909179688, 0.5728793740272522, 1.5322110652923584, 0.7447177767753601, 0.7452227473258972, 0.8621755242347717, 0.5822684168815613, 0.5843482613563538, 0.6834442615509033, 0.709013044834137, 1.2617857456207275, 1.2924818992614746, 1.1180752515792847, 0.5661676526069641, 0.57524573802948, 0.6473305821418762, 0.895542562007904, 1.096278190612793, 0.8562396168708801, 1.5439460277557373, 0.4030493497848511, 0.908631443977356, 1.9705168008804321, 0.06732910871505737, 0.7781952619552612, 0.709197461605072, 0.5683306455612183, 1.1237047910690308, 1.3425822257995605, 1.0880587100982666, 0.2529258131980896, 1.2120881080627441, 0.39395660161972046, 0.161470428109169, 1.0930140018463135, 0.8298978209495544, 0.9425258040428162, 1.239517331123352, 1.5297114849090576, 1.2678933143615723, 0.662993848323822, 1.1794164180755615, 1.3539103269577026, 1.7295928001403809, 0.7658171057701111, 0.6785117387771606, 1.0072121620178223, 0.4674873650074005, 0.31034600734710693, -0.23229077458381653, 0.6817964315414429, 1.3935647010803223, 0.5052496790885925, 0.8334630131721497, 0.012317345477640629, 0.8887962698936462, 1.0425152778625488, 2.5846824645996094, 1.0629382133483887, 1.0078201293945312, 1.0291259288787842, 0.8934083580970764, 0.29002809524536133, 1.3123027086257935, 0.4102860391139984, 0.14205831289291382, 1.5132908821105957, 1.1972897052764893, 0.6138249635696411, 0.016764076426625252, 0.7458974719047546, 1.1111350059509277, 1.1792322397232056, 0.5617788434028625, 1.098717451095581], 'y_pred_int': [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 2, 1, 2, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 2, 1, 0, 2, 0, 1, 1, 1, 0, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 1, 2, 0, 0, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 3, 1, 1, 1, 1, 0, 1, 0, 0, 2, 1, 1, 0, 1, 1, 1, 1, 1], 'y_true': [1, 1, 0, 1, 1, 0, 0, 0, 1, 3, 4, 0, 0, 4, 2, 2, 2, 2, 0, 0, 2, 3, 0, 1, 0, 0, 3, 1, 0, 0, 0, 1, 1, 1, 8, 0, 0, 0, 2, 1, 1, 2, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 1, 0, 5, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 2, 0, 1, 2, 7, 2, 0, 0, 0, 1, 3, 1, 1, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 2, 0, 2, 0, 1, 0, 0, 1, 5, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 2, 14, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 2, 0, 0, 1, 3, 0, 1, 0, 0, 3, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 9, 1, 4, 0, 1, 2, 0, 0, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 2, 2, 1, 1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 5, 1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 6, 1, 0, 0, 0, 2, 1, 2, 1, 2, 0, 0, 1, 7, 2, 1, 0, 0, 2, 0, 1, 1, 2, 1, 1, 0, 0, 1, 1, 0, 3, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 2, 1, 2, 0, 0, 3, 1, 3, 3, 0, 0, 2, 0, 3, 1, 1, 1, 1, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 1, 2, 2, 0, 1, 1, 0, 0, 0, 1, 2, 1, 0], 'train_acc': 0.34795090830449826, 'test_acc': 0.403598615916955, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 19.5376, Test loss: 129.4563
In this epoch 100/5000, Training loss: 0.7110, Test loss: 127.3597
In this epoch 150/5000, Training loss: 0.2905, Test loss: 128.0559
In this epoch 200/5000, Training loss: 0.2183, Test loss: 128.1899
In this epoch 250/5000, Training loss: 0.1809, Test loss: 128.2079
In this epoch 300/5000, Training loss: 0.1493, Test loss: 128.2114
In this epoch 350/5000, Training loss: 0.1215, Test loss: 128.2225
In this epoch 400/5000, Training loss: 0.0976, Test loss: 128.2444
In this epoch 450/5000, Training loss: 0.0777, Test loss: 128.2759
In this epoch 500/5000, Training loss: 0.0616, Test loss: 128.3149
In this epoch 550/5000, Training loss: 0.0488, Test loss: 128.3589
In this epoch 600/5000, Training loss: 0.0389, Test loss: 128.4055
In this epoch 650/5000, Training loss: 0.0314, Test loss: 128.4527
In this epoch 700/5000, Training loss: 0.0258, Test loss: 128.4987
In this epoch 750/5000, Training loss: 0.0218, Test loss: 128.5422
In this epoch 800/5000, Training loss: 0.0190, Test loss: 128.5822
In this epoch 850/5000, Training loss: 0.0170, Test loss: 128.6181
In this epoch 900/5000, Training loss: 0.0156, Test loss: 128.6497
In this epoch 950/5000, Training loss: 0.0148, Test loss: 128.6770
In this epoch 1000/5000, Training loss: 0.0142, Test loss: 128.7000
In this epoch 1050/5000, Training loss: 0.0138, Test loss: 128.7193
In this epoch 1100/5000, Training loss: 0.0136, Test loss: 128.7350
In this epoch 1150/5000, Training loss: 0.0134, Test loss: 128.7477
In this epoch 1200/5000, Training loss: 0.0133, Test loss: 128.7578
In this epoch 1250/5000, Training loss: 0.0133, Test loss: 128.7658
In this epoch 1300/5000, Training loss: 0.0133, Test loss: 128.7719
In this epoch 1350/5000, Training loss: 0.0133, Test loss: 128.7744
In this epoch 1400/5000, Training loss: 0.5213, Test loss: 130.3549
In this epoch 1450/5000, Training loss: 0.0171, Test loss: 129.0126
In this epoch 1500/5000, Training loss: 0.0132, Test loss: 128.7901
In this epoch 1550/5000, Training loss: 0.0159, Test loss: 128.5359
In this epoch 1600/5000, Training loss: 2.3261, Test loss: 134.7994
In this epoch 1650/5000, Training loss: 0.0533, Test loss: 129.2681
In this epoch 1700/5000, Training loss: 13.4644, Test loss: 130.0308
In this epoch 1750/5000, Training loss: 0.0168, Test loss: 128.8597
In this epoch 1800/5000, Training loss: 0.0154, Test loss: 128.7031
In this epoch 1850/5000, Training loss: 14.4603, Test loss: 135.2855
In this epoch 1900/5000, Training loss: 0.1012, Test loss: 129.2272
In this epoch 1950/5000, Training loss: 0.0142, Test loss: 128.7382
In this epoch 2000/5000, Training loss: 1.0116, Test loss: 132.4698
In this epoch 2050/5000, Training loss: 1.3939, Test loss: 130.5226
In this epoch 2100/5000, Training loss: 0.0138, Test loss: 128.8259
In this epoch 2150/5000, Training loss: 0.0132, Test loss: 128.7897
In this epoch 2200/5000, Training loss: 0.0133, Test loss: 128.7903
In this epoch 2250/5000, Training loss: 20.6383, Test loss: 133.4251
In this epoch 2300/5000, Training loss: 0.0225, Test loss: 128.3096
In this epoch 2350/5000, Training loss: 0.0134, Test loss: 128.7947
In this epoch 2400/5000, Training loss: 0.0132, Test loss: 128.7848
In this epoch 2450/5000, Training loss: 0.0132, Test loss: 128.7896
In this epoch 2500/5000, Training loss: 0.0132, Test loss: 128.7861
In this epoch 2550/5000, Training loss: 10.1303, Test loss: 130.6598
In this epoch 2600/5000, Training loss: 0.2077, Test loss: 129.6617
In this epoch 2650/5000, Training loss: 0.0135, Test loss: 128.7902
In this epoch 2700/5000, Training loss: 0.0132, Test loss: 128.7875
In this epoch 2750/5000, Training loss: 0.0132, Test loss: 128.7889
In this epoch 2800/5000, Training loss: 0.0132, Test loss: 128.7893
In this epoch 2850/5000, Training loss: 3.2527, Test loss: 138.6534
In this epoch 2900/5000, Training loss: 0.0289, Test loss: 128.1020
In this epoch 2950/5000, Training loss: 0.0148, Test loss: 128.6581
In this epoch 3000/5000, Training loss: 0.0133, Test loss: 128.7887
In this epoch 3050/5000, Training loss: 0.0132, Test loss: 128.7891
In this epoch 3100/5000, Training loss: 0.0132, Test loss: 128.7891
In this epoch 3150/5000, Training loss: 0.0132, Test loss: 128.7904
In this epoch 3200/5000, Training loss: 30.1171, Test loss: 194.5748
In this epoch 3250/5000, Training loss: 0.1441, Test loss: 128.2763
In this epoch 3300/5000, Training loss: 0.0141, Test loss: 128.7313
In this epoch 3350/5000, Training loss: 0.0132, Test loss: 128.7898
In this epoch 3400/5000, Training loss: 0.0132, Test loss: 128.7891
In this epoch 3450/5000, Training loss: 0.0132, Test loss: 128.7915
In this epoch 3500/5000, Training loss: 2.4859, Test loss: 130.9328
In this epoch 3550/5000, Training loss: 0.6611, Test loss: 131.4915
In this epoch 3600/5000, Training loss: 0.0392, Test loss: 128.5507
In this epoch 3650/5000, Training loss: 0.2947, Test loss: 128.9844
In this epoch 3700/5000, Training loss: 0.0503, Test loss: 128.9875
In this epoch 3750/5000, Training loss: 0.0177, Test loss: 128.6791
In this epoch 3800/5000, Training loss: 10.8732, Test loss: 136.0421
In this epoch 3850/5000, Training loss: 0.0139, Test loss: 129.0169
In this epoch 3900/5000, Training loss: 0.0142, Test loss: 128.8377
In this epoch 3950/5000, Training loss: 0.5600, Test loss: 128.1270
In this epoch 4000/5000, Training loss: 0.1369, Test loss: 128.3862
In this epoch 4050/5000, Training loss: 0.0166, Test loss: 128.7607
In this epoch 4100/5000, Training loss: 0.0134, Test loss: 128.8062
In this epoch 4150/5000, Training loss: 0.0271, Test loss: 128.5741
In this epoch 4200/5000, Training loss: 0.0160, Test loss: 131.7616
In this epoch 4250/5000, Training loss: 0.0133, Test loss: 128.9296
In this epoch 4300/5000, Training loss: 0.0134, Test loss: 128.8124
In this epoch 4350/5000, Training loss: 0.0132, Test loss: 128.7741
In this epoch 4400/5000, Training loss: 0.2945, Test loss: 131.6345
In this epoch 4450/5000, Training loss: 0.8520, Test loss: 132.9594
In this epoch 4500/5000, Training loss: 0.0138, Test loss: 128.8381
In this epoch 4550/5000, Training loss: 0.0133, Test loss: 128.7859
In this epoch 4600/5000, Training loss: 0.0132, Test loss: 128.7912
In this epoch 4650/5000, Training loss: 31.3178, Test loss: 192.2462
In this epoch 4700/5000, Training loss: 0.1994, Test loss: 128.5419
In this epoch 4750/5000, Training loss: 0.0192, Test loss: 128.7046
In this epoch 4800/5000, Training loss: 0.0133, Test loss: 128.7926
In this epoch 4850/5000, Training loss: 0.0132, Test loss: 128.7889
In this epoch 4900/5000, Training loss: 0.0132, Test loss: 128.7867
In this epoch 4950/5000, Training loss: 10.6732, Test loss: 128.1893
In this epoch 5000/5000, Training loss: 0.0629, Test loss: 128.6133
train_acc 0.9985294117647059
test_acc 0.029411764705882353
{'y_pred': [22.01347541809082, 14.493756294250488, 11.240132331848145, 10.869796752929688, 14.679590225219727, 18.167945861816406, 12.692124366760254, 14.229365348815918, 12.855045318603516, 12.844734191894531, 13.69117546081543, 12.264890670776367, 12.787425994873047, 9.556215286254883, 13.492227554321289, 12.871288299560547, 9.714532852172852, 14.159687042236328, 15.302983283996582, 14.843257904052734, 14.039007186889648, 15.347590446472168, 13.481837272644043, 15.212833404541016, 7.353862762451172, 7.500395774841309, 15.194780349731445, 16.47309112548828, 10.838375091552734, 13.851739883422852, 16.301788330078125, 19.771800994873047, 11.779766082763672, 14.402807235717773, 8.649343490600586, 14.31307601928711, 13.71699333190918, 13.592811584472656, 12.885846138000488, 17.48148536682129, 11.105963706970215, 13.168633460998535, 12.11157512664795, 9.355681419372559, 11.731097221374512, 19.463363647460938, 12.220138549804688, 8.897043228149414, 10.514291763305664, 18.7020263671875, 14.53903579711914, 16.9252872467041, 18.048431396484375, 12.486248970031738, 12.557489395141602, 11.367986679077148, 10.460822105407715, 11.830867767333984, 10.27952766418457, 12.732027053833008, 13.023407936096191, 17.18730926513672, 12.126415252685547, 14.774194717407227, 13.259896278381348, 11.881010055541992, 11.575301170349121, 17.894912719726562, 7.8451008796691895, 9.702116012573242, 13.258750915527344, 15.784743309020996, 13.55487060546875, 17.522722244262695, 23.18307113647461, 12.895883560180664, 17.976503372192383, 13.826131820678711, 16.854114532470703, 13.650118827819824, 12.966780662536621, 12.576559066772461, 16.838056564331055, 12.70655345916748, 15.162662506103516, 17.977027893066406, 15.338566780090332, 16.006359100341797, 14.595292091369629, 14.467560768127441, 18.966615676879883, 14.243805885314941, 16.995073318481445, 13.353617668151855, 13.111119270324707, 14.108240127563477, 15.531476974487305, 12.099321365356445, 11.632773399353027, 13.838906288146973, 9.326196670532227, 14.60596752166748, 11.972635269165039, 12.227614402770996, 7.4702229499816895, 13.88077163696289, 14.253915786743164, 15.250012397766113, 15.46432113647461, 15.902146339416504, 16.554311752319336, 11.992053031921387, 11.259071350097656, 19.483102798461914, 13.364625930786133, 15.60832405090332, 15.935300827026367, 14.731073379516602, 11.94448471069336, 16.00074005126953, 11.004837036132812, 18.595111846923828, 14.974006652832031, 16.390775680541992, 14.187811851501465, 17.479633331298828, 11.96853256225586, 6.27827262878418, 13.630647659301758, 14.5573148727417, 10.762796401977539, 14.738580703735352, 14.167211532592773, 15.298633575439453, 20.165969848632812, 14.517156600952148, 15.726985931396484, 7.87000846862793, 13.930830001831055, 19.040523529052734, 19.06035614013672, 15.834944725036621, 13.147387504577637, 15.855146408081055, 10.278165817260742, 14.463451385498047, 16.022829055786133, 6.957577705383301, 17.044021606445312, 12.779691696166992, 17.661115646362305, 15.786867141723633, 10.556952476501465, 14.841866493225098, 14.840984344482422, 15.724246978759766, 7.9244384765625, 12.244178771972656, 10.733670234680176, 14.581815719604492, 15.15639877319336, 14.914302825927734, 17.015291213989258, 17.754302978515625, 16.805255889892578, 17.574398040771484, 15.019026756286621, 11.554412841796875, 12.639058113098145, 17.005971908569336, 10.361589431762695, 10.168768882751465, 13.579225540161133, 20.236751556396484, 13.079051971435547, 11.433012008666992, 8.956075668334961, 14.141432762145996, 13.543069839477539, 12.603662490844727, 15.707867622375488, 11.495433807373047, 13.023319244384766, 11.282268524169922, 19.092193603515625, 11.339427947998047, 13.487010955810547, 16.615901947021484, 16.835172653198242, 13.3587007522583, 13.139666557312012, 15.690980911254883, 15.513628005981445, 15.626077651977539, 12.250840187072754, 6.61151647567749, 17.389450073242188, 11.80795669555664, 13.567940711975098, 13.952072143554688, 15.898405075073242, 17.535558700561523, 18.48312759399414, 11.595629692077637, 16.661792755126953, 4.895140647888184, 12.214584350585938, 15.022729873657227, 19.097789764404297, 12.216230392456055, 11.3983154296875, 8.433694839477539, 15.164566993713379, 14.557052612304688, 7.881181716918945, 16.756303787231445, 13.761908531188965, 15.948874473571777, 14.385387420654297, 13.569042205810547, 12.010581970214844, 16.50636100769043, 16.39415168762207, 16.594215393066406, 22.025129318237305, 16.980731964111328, 14.05735969543457, 10.76827621459961, 15.46957778930664, 11.95162582397461, 14.33324909210205, 11.327688217163086, 9.486772537231445, 12.861152648925781, 10.871781349182129, 18.30379867553711, 16.532211303710938, 20.334869384765625, 17.331308364868164, 11.580610275268555, 15.088619232177734, 15.890172004699707, 17.16299819946289, 11.878270149230957, 13.431194305419922, 16.66335105895996, 14.760647773742676, 17.49713134765625, 16.296733856201172, 12.88759994506836, 14.960493087768555, 17.29673194885254, 11.162156105041504, 17.003372192382812, 13.028003692626953, 12.378786087036133, 4.404280662536621, 12.498071670532227, 10.586706161499023, 12.051216125488281, 15.58366870880127, 10.865391731262207, 17.05246353149414, 16.919780731201172, 10.599384307861328, 19.333221435546875, 14.176233291625977, 17.06537437438965, 14.426189422607422, 19.14830780029297, 13.892496109008789, 16.270057678222656, 16.69134521484375, 20.876686096191406, 16.76156234741211, 17.542301177978516, 12.273557662963867, 6.771317958831787, 18.858497619628906, 15.230402946472168, 12.581827163696289, 12.992446899414062, 19.65337371826172, 15.433910369873047, 17.499370574951172, 8.985729217529297, 10.225144386291504, 15.807634353637695, 14.506135940551758, 9.56793212890625, 11.515989303588867, 13.292989730834961, 16.49278450012207, 17.843778610229492, 13.828900337219238, 14.926359176635742, 16.07343101501465, 15.12166976928711, 13.099699974060059, 13.197321891784668, 13.280202865600586, 12.302946090698242, 8.40401840209961, 16.367717742919922, 17.31277084350586, 15.900455474853516, 16.110851287841797, 9.692666053771973, 13.527619361877441, 14.862381935119629, 14.735630989074707, 12.152034759521484, 12.17563247680664, 20.70041275024414, 16.59354019165039, 17.262088775634766, 13.136346817016602, 15.63339614868164, 18.42034339904785, 11.585983276367188, 16.67131805419922, 13.244226455688477, 13.268881797790527, 10.176324844360352, 16.93866539001465, 15.72963809967041, 16.092676162719727, 44.77703857421875, 17.89998435974121, 11.531277656555176, 12.291950225830078, 16.48293685913086, 12.510984420776367, 11.917058944702148, 11.78278636932373, 16.019865036010742, 18.05370330810547, 9.545587539672852, 8.86260986328125, 13.60283374786377], 'y_pred_int': [22, 14, 11, 11, 15, 18, 13, 14, 13, 13, 14, 12, 13, 10, 13, 13, 10, 14, 15, 15, 14, 15, 13, 15, 7, 8, 15, 16, 11, 14, 16, 20, 12, 14, 9, 14, 14, 14, 13, 17, 11, 13, 12, 9, 12, 19, 12, 9, 11, 19, 15, 17, 18, 12, 13, 11, 10, 12, 10, 13, 13, 17, 12, 15, 13, 12, 12, 18, 8, 10, 13, 16, 14, 18, 23, 13, 18, 14, 17, 14, 13, 13, 17, 13, 15, 18, 15, 16, 15, 14, 19, 14, 17, 13, 13, 14, 16, 12, 12, 14, 9, 15, 12, 12, 7, 14, 14, 15, 15, 16, 17, 12, 11, 19, 13, 16, 16, 15, 12, 16, 11, 19, 15, 16, 14, 17, 12, 6, 14, 15, 11, 15, 14, 15, 20, 15, 16, 8, 14, 19, 19, 16, 13, 16, 10, 14, 16, 7, 17, 13, 18, 16, 11, 15, 15, 16, 8, 12, 11, 15, 15, 15, 17, 18, 17, 18, 15, 12, 13, 17, 10, 10, 14, 20, 13, 11, 9, 14, 14, 13, 16, 11, 13, 11, 19, 11, 13, 17, 17, 13, 13, 16, 16, 16, 12, 7, 17, 12, 14, 14, 16, 18, 18, 12, 17, 5, 12, 15, 19, 12, 11, 8, 15, 15, 8, 17, 14, 16, 14, 14, 12, 17, 16, 17, 22, 17, 14, 11, 15, 12, 14, 11, 9, 13, 11, 18, 17, 20, 17, 12, 15, 16, 17, 12, 13, 17, 15, 17, 16, 13, 15, 17, 11, 17, 13, 12, 4, 12, 11, 12, 16, 11, 17, 17, 11, 19, 14, 17, 14, 19, 14, 16, 17, 21, 17, 18, 12, 7, 19, 15, 13, 13, 20, 15, 17, 9, 10, 16, 15, 10, 12, 13, 16, 18, 14, 15, 16, 15, 13, 13, 13, 12, 8, 16, 17, 16, 16, 10, 14, 15, 15, 12, 12, 21, 17, 17, 13, 16, 18, 12, 17, 13, 13, 10, 17, 16, 16, 45, 18, 12, 12, 16, 13, 12, 12, 16, 18, 10, 9, 14], 'y_true': [4, 19, 9, 7, 7, 16, 22, 3, 10, 11, 25, 10, 4, 10, 53, 21, 16, 11, 16, 21, 47, 5, 33, 5, 18, 12, 13, 5, 4, 29, 14, 7, 17, 4, 5, 25, 34, 7, 14, 4, 37, 34, 21, 3, 11, 15, 22, 26, 8, 12, 8, 14, 9, 5, 38, 9, 4, 10, 34, 23, 7, 10, 14, 17, 13, 18, 12, 63, 11, 10, 7, 13, 32, 40, 3, 8, 4, 43, 10, 35, 11, 3, 7, 6, 26, 16, 25, 11, 5, 3, 5, 8, 17, 16, 21, 20, 2, 9, 18, 5, 10, 23, 6, 15, 17, 11, 3, 23, 19, 22, 8, 16, 18, 3, 10, 6, 7, 9, 2, 7, 10, 18, 6, 14, 10, 9, 8, 26, 6, 5, 9, 14, 21, 7, 9, 13, 15, 28, 16, 11, 35, 28, 7, 37, 10, 8, 13, 24, 7, 6, 4, 4, 6, 26, 2, 6, 27, 12, 5, 39, 10, 16, 6, 5, 5, 7, 5, 7, 9, 3, 16, 19, 35, 2, 10, 10, 11, 9, 2, 7, 34, 4, 22, 11, 4, 33, 9, 2, 20, 29, 2, 5, 17, 6, 15, 15, 19, 46, 8, 12, 9, 29, 14, 14, 9, 3, 11, 29, 3, 11, 7, 6, 5, 29, 4, 6, 10, 12, 6, 3, 6, 14, 20, 6, 20, 6, 3, 13, 7, 6, 4, 3, 14, 6, 12, 20, 3, 7, 24, 18, 15, 7, 10, 14, 7, 4, 7, 6, 30, 6, 8, 2, 2, 8, 4, 6, 10, 8, 22, 8, 13, 7, 10, 26, 12, 20, 12, 8, 11, 5, 7, 21, 36, 8, 5, 2, 9, 9, 11, 43, 39, 6, 15, 2, 4, 42, 34, 40, 20, 46, 19, 8, 6, 9, 21, 4, 10, 10, 13, 7, 10, 20, 6, 15, 9, 19, 9, 20, 4, 8, 12, 10, 13, 16, 21, 5, 15, 19, 4, 3, 14, 5, 60, 16, 39, 9, 11, 10, 13, 13, 14, 13, 13, 23, 19, 10, 11, 7, 16, 20], 'train_acc': 0.9985294117647059, 'test_acc': 0.029411764705882353, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 31627.4941, Test loss: 61081.0703
In this epoch 100/5000, Training loss: 17863.3848, Test loss: 60068.7070
In this epoch 150/5000, Training loss: 9767.2168, Test loss: 60662.0000
In this epoch 200/5000, Training loss: 5380.6206, Test loss: 61112.2266
In this epoch 250/5000, Training loss: 3060.5325, Test loss: 61435.4375
In this epoch 300/5000, Training loss: 1813.2838, Test loss: 61669.3945
In this epoch 350/5000, Training loss: 1121.5353, Test loss: 61850.2305
In this epoch 400/5000, Training loss: 725.1919, Test loss: 62000.2734
In this epoch 450/5000, Training loss: 491.3994, Test loss: 62130.4727
In this epoch 500/5000, Training loss: 350.0282, Test loss: 62245.4492
In this epoch 550/5000, Training loss: 262.7165, Test loss: 62347.1758
In this epoch 600/5000, Training loss: 207.7534, Test loss: 62436.6367
In this epoch 650/5000, Training loss: 172.4697, Test loss: 62514.7188
In this epoch 700/5000, Training loss: 149.2835, Test loss: 62582.4492
In this epoch 750/5000, Training loss: 133.5717, Test loss: 62640.9180
In this epoch 800/5000, Training loss: 122.4818, Test loss: 62691.3242
In this epoch 850/5000, Training loss: 114.2422, Test loss: 62734.8789
In this epoch 900/5000, Training loss: 107.7529, Test loss: 62772.7422
In this epoch 950/5000, Training loss: 102.3341, Test loss: 62806.0000
In this epoch 1000/5000, Training loss: 97.5696, Test loss: 62835.5781
In this epoch 1050/5000, Training loss: 93.2088, Test loss: 62862.3242
In this epoch 1100/5000, Training loss: 89.1040, Test loss: 62886.9414
In this epoch 1150/5000, Training loss: 85.1708, Test loss: 62910.0078
In this epoch 1200/5000, Training loss: 81.3633, Test loss: 62932.0078
In this epoch 1250/5000, Training loss: 77.6580, Test loss: 62953.3125
In this epoch 1300/5000, Training loss: 74.0450, Test loss: 62974.2266
In this epoch 1350/5000, Training loss: 70.5213, Test loss: 62994.9727
In this epoch 1400/5000, Training loss: 67.0880, Test loss: 63015.7188
In this epoch 1450/5000, Training loss: 63.7481, Test loss: 63036.5664
In this epoch 1500/5000, Training loss: 60.5054, Test loss: 63057.6367
In this epoch 1550/5000, Training loss: 57.3635, Test loss: 63078.9297
In this epoch 1600/5000, Training loss: 54.3260, Test loss: 63100.5117
In this epoch 1650/5000, Training loss: 51.3962, Test loss: 63122.3789
In this epoch 1700/5000, Training loss: 48.5766, Test loss: 63144.5195
In this epoch 1750/5000, Training loss: 45.8694, Test loss: 63166.9180
In this epoch 1800/5000, Training loss: 43.2762, Test loss: 63189.5547
In this epoch 1850/5000, Training loss: 40.7981, Test loss: 63212.4062
In this epoch 1900/5000, Training loss: 38.4355, Test loss: 63235.4141
In this epoch 1950/5000, Training loss: 36.1885, Test loss: 63258.5664
In this epoch 2000/5000, Training loss: 34.0567, Test loss: 63281.8125
In this epoch 2050/5000, Training loss: 32.0392, Test loss: 63305.0938
In this epoch 2100/5000, Training loss: 30.1345, Test loss: 63328.4023
In this epoch 2150/5000, Training loss: 28.3410, Test loss: 63351.6641
In this epoch 2200/5000, Training loss: 26.6565, Test loss: 63374.8555
In this epoch 2250/5000, Training loss: 25.0787, Test loss: 63397.9180
In this epoch 2300/5000, Training loss: 23.6046, Test loss: 63420.8125
In this epoch 2350/5000, Training loss: 22.2312, Test loss: 63443.5078
In this epoch 2400/5000, Training loss: 20.9552, Test loss: 63465.9531
In this epoch 2450/5000, Training loss: 19.7730, Test loss: 63488.0898
In this epoch 2500/5000, Training loss: 18.6810, Test loss: 63509.9062
In this epoch 2550/5000, Training loss: 17.6752, Test loss: 63531.3438
In this epoch 2600/5000, Training loss: 16.7515, Test loss: 63552.3828
In this epoch 2650/5000, Training loss: 15.9060, Test loss: 63572.9766
In this epoch 2700/5000, Training loss: 15.1344, Test loss: 63593.1016
In this epoch 2750/5000, Training loss: 14.4325, Test loss: 63612.7188
In this epoch 2800/5000, Training loss: 13.7962, Test loss: 63631.8008
In this epoch 2850/5000, Training loss: 13.2212, Test loss: 63650.3320
In this epoch 2900/5000, Training loss: 12.7035, Test loss: 63668.2734
In this epoch 2950/5000, Training loss: 12.2389, Test loss: 63685.6133
In this epoch 3000/5000, Training loss: 11.8236, Test loss: 63702.3320
In this epoch 3050/5000, Training loss: 11.4537, Test loss: 63718.4023
In this epoch 3100/5000, Training loss: 11.1254, Test loss: 63733.8477
In this epoch 3150/5000, Training loss: 10.8353, Test loss: 63748.6367
In this epoch 3200/5000, Training loss: 10.5798, Test loss: 63762.7500
In this epoch 3250/5000, Training loss: 10.3559, Test loss: 63776.2031
In this epoch 3300/5000, Training loss: 10.1603, Test loss: 63788.9844
In this epoch 3350/5000, Training loss: 9.9903, Test loss: 63801.1055
In this epoch 3400/5000, Training loss: 9.8432, Test loss: 63812.5781
In this epoch 3450/5000, Training loss: 9.7163, Test loss: 63823.3906
In this epoch 3500/5000, Training loss: 9.6076, Test loss: 63833.5547
In this epoch 3550/5000, Training loss: 9.5147, Test loss: 63843.1016
In this epoch 3600/5000, Training loss: 9.4359, Test loss: 63852.0234
In this epoch 3650/5000, Training loss: 9.3692, Test loss: 63860.3555
In this epoch 3700/5000, Training loss: 9.3131, Test loss: 63868.0938
In this epoch 3750/5000, Training loss: 9.2662, Test loss: 63875.2734
In this epoch 3800/5000, Training loss: 9.2272, Test loss: 63881.9258
In this epoch 3850/5000, Training loss: 9.1949, Test loss: 63888.0352
In this epoch 3900/5000, Training loss: 9.1683, Test loss: 63893.6719
In this epoch 3950/5000, Training loss: 9.1465, Test loss: 63898.8242
In this epoch 4000/5000, Training loss: 9.1288, Test loss: 63903.5352
In this epoch 4050/5000, Training loss: 9.1145, Test loss: 63907.8203
In this epoch 4100/5000, Training loss: 9.1030, Test loss: 63911.7031
In this epoch 4150/5000, Training loss: 9.0938, Test loss: 63915.2266
In this epoch 4200/5000, Training loss: 9.0866, Test loss: 63918.3906
In this epoch 4250/5000, Training loss: 9.0808, Test loss: 63921.2305
In this epoch 4300/5000, Training loss: 9.0763, Test loss: 63923.7734
In this epoch 4350/5000, Training loss: 9.0728, Test loss: 63926.0352
In this epoch 4400/5000, Training loss: 9.0702, Test loss: 63928.0469
In this epoch 4450/5000, Training loss: 9.0681, Test loss: 63929.8203
In this epoch 4500/5000, Training loss: 9.0665, Test loss: 63931.3906
In this epoch 4550/5000, Training loss: 9.0654, Test loss: 63932.7539
In this epoch 4600/5000, Training loss: 9.0645, Test loss: 63933.9531
In this epoch 4650/5000, Training loss: 9.0638, Test loss: 63934.9883
In this epoch 4700/5000, Training loss: 9.0633, Test loss: 63935.8945
In this epoch 4750/5000, Training loss: 9.0630, Test loss: 63936.6719
In this epoch 4800/5000, Training loss: 9.0627, Test loss: 63937.3477
In this epoch 4850/5000, Training loss: 9.2565, Test loss: 63964.5781
In this epoch 4900/5000, Training loss: 11.2195, Test loss: 64003.8477
In this epoch 4950/5000, Training loss: 9.0750, Test loss: 63945.0352
In this epoch 5000/5000, Training loss: 9.0624, Test loss: 63939.6133
train_acc 0.9985294117647059
test_acc 0.0
{'y_pred': [359.6488037109375, 455.9679870605469, 410.1031494140625, 478.823974609375, 234.18008422851562, 342.255859375, 269.653076171875, 337.6940002441406, 457.6103515625, 439.6119384765625, 334.89825439453125, 381.6558837890625, 386.6212463378906, 373.2193298339844, 310.6356506347656, 350.00653076171875, 369.97821044921875, 352.5295104980469, 329.92279052734375, 342.64801025390625, 341.07666015625, 395.5389709472656, 346.112060546875, 323.7326965332031, 402.2143249511719, 462.6634521484375, 373.3232421875, 304.3634033203125, 347.77001953125, 425.439453125, 348.9974670410156, 348.6448974609375, 353.0905456542969, 322.17340087890625, 304.234130859375, 313.28997802734375, 376.1739807128906, 356.9923095703125, 345.1955261230469, 265.25054931640625, 241.06771850585938, 281.93157958984375, 378.02032470703125, 299.11627197265625, 361.6935729980469, 408.10009765625, 276.99517822265625, 404.1488342285156, 309.48388671875, 353.77813720703125, 361.35638427734375, 319.6495361328125, 349.3118896484375, 363.759521484375, 317.91070556640625, 419.8562316894531, 438.54998779296875, 286.0037841796875, 277.838623046875, 405.8228759765625, 308.178466796875, 320.531005859375, 358.98126220703125, 283.83721923828125, 306.83013916015625, 366.8463134765625, 365.0580139160156, 257.90240478515625, 430.772705078125, 349.5557861328125, 292.0513000488281, 303.2973937988281, 388.37841796875, 296.267822265625, 352.5606384277344, 377.59234619140625, 284.4117126464844, 294.2156066894531, 396.7979736328125, 408.8040466308594, 315.6392822265625, 349.4935302734375, 327.2536926269531, 571.4098510742188, 391.8446350097656, 315.18157958984375, 194.99169921875, 318.8489074707031, 433.9825439453125, 331.177001953125, 288.29205322265625, 271.1287536621094, 445.43524169921875, 364.4173583984375, 411.7206726074219, 345.44683837890625, 334.0492858886719, 383.43951416015625, 380.50262451171875, 377.2071228027344, 407.6493225097656, 363.23809814453125, 393.13214111328125, 469.0357971191406, 422.2431640625, 330.4791564941406, 351.20648193359375, 311.9378662109375, 283.5473327636719, 366.3624267578125, 422.39398193359375, 389.0413818359375, 280.54388427734375, 398.1705322265625, 365.8646240234375, 331.3214111328125, 330.1247253417969, 349.9480285644531, 366.90313720703125, 395.7832946777344, 388.0342102050781, 368.2307434082031, 276.5490417480469, 415.23309326171875, 306.27984619140625, 412.54364013671875, 326.01800537109375, 312.215087890625, 289.157958984375, 304.30078125, 329.64276123046875, 432.5314636230469, 192.60191345214844, 377.6189270019531, 323.869873046875, 338.3951416015625, 315.3329162597656, 362.47930908203125, 329.60186767578125, 300.2447509765625, 343.71575927734375, 400.2667236328125, 388.690185546875, 321.22119140625, 344.7358703613281, 339.6516418457031, 341.9398498535156, 250.2041015625, 331.5960998535156, 290.6866455078125, 402.5684814453125, 379.5877380371094, 277.5014953613281, 335.6713562011719, 351.28753662109375, 386.4493408203125, 359.325439453125, 352.489013671875, 316.690673828125, 426.0033264160156, 401.082763671875, 220.88055419921875, 383.46728515625, 339.08099365234375, 274.8936767578125, 344.4581604003906, 389.319091796875, 365.899169921875, 336.02197265625, 380.1122741699219, 413.8828125, 434.61334228515625, 380.3349914550781, 236.74058532714844, 365.95709228515625, 359.10791015625, 230.6337890625, 400.53057861328125, 349.87799072265625, 393.07037353515625, 371.552978515625, 376.2777404785156, 464.527099609375, 370.0975646972656, 324.53973388671875, 359.2038269042969, 368.3941650390625, 341.5222473144531, 371.7818908691406, 357.8793640136719, 340.1704406738281, 307.4504699707031, 405.56658935546875, 453.5553283691406, 432.7930603027344, 298.297119140625, 396.903564453125, 358.5441589355469, 263.9832763671875, 466.0858154296875, 342.11614990234375, 420.6839294433594, 429.3902893066406, 329.9616394042969, 319.8193054199219, 332.4345397949219, 226.680419921875, 235.14532470703125, 389.43438720703125, 433.5536193847656, 274.0310363769531, 347.5286865234375, 424.83575439453125, 402.9976806640625, 404.23138427734375, 385.2708435058594, 297.8786315917969, 387.26739501953125, 224.7427215576172, 292.22467041015625, 320.2423400878906, 304.9664611816406, 427.7896728515625, 315.41522216796875, 270.20477294921875, 366.6287841796875, 354.0135498046875, 291.1842346191406, 420.28558349609375, 334.22802734375, 356.6361999511719, 358.0899353027344, 358.3748779296875, 383.32281494140625, 351.3475341796875, 364.95654296875, 381.5060119628906, 273.0327453613281, 430.6588134765625, 298.4008483886719, 205.30001831054688, 391.54180908203125, 391.8234558105469, 392.7339172363281, 370.6130676269531, 326.76654052734375, 355.888671875, 334.8225402832031, 422.87384033203125, 316.54205322265625, 300.408447265625, 345.4573669433594, 347.5099182128906, 440.1322021484375, 495.4768371582031, 410.217529296875, 319.11456298828125, 412.26458740234375, 441.48785400390625, 326.2689514160156, 271.50823974609375, 385.7041015625, 307.3948974609375, 326.0614929199219, 400.0123291015625, 388.4461975097656, 335.2940979003906, 414.57220458984375, 360.069091796875, 362.2803649902344, 370.80047607421875, 348.64678955078125, 327.7996826171875, 371.86016845703125, 339.8260192871094, 359.40814208984375, 396.0553283691406, 339.96563720703125, 371.97332763671875, 411.33843994140625, 312.7868957519531, 364.557861328125, 411.51153564453125, 385.4699401855469, 358.4283447265625, 482.1242370605469, 370.8439636230469, 377.7321472167969, 385.697998046875, 380.7003479003906, 243.35308837890625, 373.7777099609375, 383.81207275390625, 273.117431640625, 478.518798828125, 384.9420471191406, 421.98114013671875, 385.5489501953125, 363.1228942871094, 432.0849914550781, 341.06695556640625, 368.6307678222656, 400.95977783203125, 394.9906005859375, 464.4324951171875, 357.57861328125, 294.6293029785156, 348.3546142578125, 308.41851806640625, 381.36749267578125, 292.1366271972656, 395.21478271484375, 388.00238037109375, 395.4378356933594, 359.17987060546875, 315.8603515625, 334.82037353515625, 409.9053955078125, 321.5992431640625, 390.8981628417969, 376.97705078125, 428.8549499511719, 286.6806945800781, 399.3184814453125, 304.0074462890625, 472.1285400390625, 367.3875427246094, 284.2594299316406, 281.1131591796875, 411.835205078125, 414.55010986328125, 881.4135131835938, 368.9546203613281, 437.64227294921875, 404.84100341796875, 442.7835388183594, 308.50054931640625, 390.9669189453125, 370.1910400390625, 387.0846862792969], 'y_pred_int': [360, 456, 410, 479, 234, 342, 270, 338, 458, 440, 335, 382, 387, 373, 311, 350, 370, 353, 330, 343, 341, 396, 346, 324, 402, 463, 373, 304, 348, 425, 349, 349, 353, 322, 304, 313, 376, 357, 345, 265, 241, 282, 378, 299, 362, 408, 277, 404, 309, 354, 361, 320, 349, 364, 318, 420, 439, 286, 278, 406, 308, 321, 359, 284, 307, 367, 365, 258, 431, 350, 292, 303, 388, 296, 353, 378, 284, 294, 397, 409, 316, 349, 327, 571, 392, 315, 195, 319, 434, 331, 288, 271, 445, 364, 412, 345, 334, 383, 381, 377, 408, 363, 393, 469, 422, 330, 351, 312, 284, 366, 422, 389, 281, 398, 366, 331, 330, 350, 367, 396, 388, 368, 277, 415, 306, 413, 326, 312, 289, 304, 330, 433, 193, 378, 324, 338, 315, 362, 330, 300, 344, 400, 389, 321, 345, 340, 342, 250, 332, 291, 403, 380, 278, 336, 351, 386, 359, 352, 317, 426, 401, 221, 383, 339, 275, 344, 389, 366, 336, 380, 414, 435, 380, 237, 366, 359, 231, 401, 350, 393, 372, 376, 465, 370, 325, 359, 368, 342, 372, 358, 340, 307, 406, 454, 433, 298, 397, 359, 264, 466, 342, 421, 429, 330, 320, 332, 227, 235, 389, 434, 274, 348, 425, 403, 404, 385, 298, 387, 225, 292, 320, 305, 428, 315, 270, 367, 354, 291, 420, 334, 357, 358, 358, 383, 351, 365, 382, 273, 431, 298, 205, 392, 392, 393, 371, 327, 356, 335, 423, 317, 300, 345, 348, 440, 495, 410, 319, 412, 441, 326, 272, 386, 307, 326, 400, 388, 335, 415, 360, 362, 371, 349, 328, 372, 340, 359, 396, 340, 372, 411, 313, 365, 412, 385, 358, 482, 371, 378, 386, 381, 243, 374, 384, 273, 479, 385, 422, 386, 363, 432, 341, 369, 401, 395, 464, 358, 295, 348, 308, 381, 292, 395, 388, 395, 359, 316, 335, 410, 322, 391, 377, 429, 287, 399, 304, 472, 367, 284, 281, 412, 415, 881, 369, 438, 405, 443, 309, 391, 370, 387], 'y_true': [607, 425, 144, 74, 433, 69, 546, 164, 313, 558, 213, 367, 545, 511, 370, 175, 391, 173, 81, 215, 382, 172, 663, 680, 195, 192, 473, 299, 180, 665, 401, 115, 450, 181, 350, 168, 276, 676, 200, 76, 726, 753, 239, 446, 212, 535, 39, 405, 350, 436, 350, 726, 102, 364, 149, 207, 100, 163, 534, 457, 211, 521, 209, 515, 207, 266, 184, 256, 165, 436, 619, 801, 694, 324, 211, 500, 220, 169, 274, 765, 434, 166, 507, 76, 285, 232, 85, 643, 550, 105, 526, 403, 373, 242, 192, 373, 493, 45, 466, 243, 32, 312, 244, 607, 166, 87, 380, 729, 761, 527, 660, 142, 148, 392, 439, 511, 200, 174, 222, 857, 323, 147, 76, 752, 747, 491, 203, 155, 799, 197, 621, 122, 273, 155, 421, 260, 358, 901, 624, 899, 121, 340, 423, 86, 352, 209, 449, 453, 155, 122, 312, 682, 590, 164, 511, 73, 449, 340, 469, 125, 471, 392, 634, 125, 165, 514, 224, 141, 60, 281, 814, 901, 614, 80, 191, 283, 286, 487, 881, 147, 266, 296, 572, 240, 149, 425, 519, 456, 643, 203, 544, 583, 443, 1695, 849, 221, 139, 681, 661, 480, 631, 442, 167, 375, 428, 738, 791, 1026, 290, 521, 121, 165, 145, 285, 833, 312, 431, 783, 898, 226, 189, 645, 141, 227, 341, 447, 89, 387, 235, 590, 347, 510, 771, 156, 457, 355, 1016, 164, 403, 350, 1044, 387, 779, 476, 180, 35, 154, 397, 407, 122, 1027, 506, 962, 307, 664, 135, 407, 516, 1064, 89, 84, 523, 559, 934, 239, 234, 167, 308, 208, 743, 407, 232, 452, 148, 583, 837, 201, 226, 513, 185, 131, 752, 656, 54, 498, 89, 258, 495, 257, 436, 880, 284, 351, 552, 616, 243, 80, 373, 214, 425, 493, 544, 113, 612, 204, 532, 116, 265, 440, 234, 197, 322, 106, 251, 707, 452, 220, 474, 109, 249, 733, 935, 279, 502, 351, 173, 633, 168, 177, 366, 543, 256, 296, 516, 200, 318, 209, 471, 440, 326], 'train_acc': 0.9985294117647059, 'test_acc': 0.0, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 53.8812, Test loss: 20.8144
In this epoch 100/5000, Training loss: 0.8080, Test loss: 1.5376
In this epoch 150/5000, Training loss: 0.2241, Test loss: 1.3133
In this epoch 200/5000, Training loss: 0.1099, Test loss: 1.3524
In this epoch 250/5000, Training loss: 0.0542, Test loss: 1.3801
In this epoch 300/5000, Training loss: 0.0271, Test loss: 1.4012
In this epoch 350/5000, Training loss: 0.0138, Test loss: 1.4172
In this epoch 400/5000, Training loss: 0.0072, Test loss: 1.4290
In this epoch 450/5000, Training loss: 0.0039, Test loss: 1.4376
In this epoch 500/5000, Training loss: 0.0022, Test loss: 1.4437
In this epoch 550/5000, Training loss: 0.0013, Test loss: 1.4480
In this epoch 600/5000, Training loss: 0.0008, Test loss: 1.4510
In this epoch 650/5000, Training loss: 0.0006, Test loss: 1.4531
In this epoch 700/5000, Training loss: 0.0005, Test loss: 1.4545
In this epoch 750/5000, Training loss: 0.0004, Test loss: 1.4554
In this epoch 800/5000, Training loss: 0.0003, Test loss: 1.4560
In this epoch 850/5000, Training loss: 0.0003, Test loss: 1.4564
In this epoch 900/5000, Training loss: 0.0003, Test loss: 1.4567
In this epoch 950/5000, Training loss: 0.0003, Test loss: 1.4568
In this epoch 1000/5000, Training loss: 0.0003, Test loss: 1.4569
In this epoch 1050/5000, Training loss: 0.0003, Test loss: 1.4569
In this epoch 1100/5000, Training loss: 0.0003, Test loss: 1.4569
In this epoch 1150/5000, Training loss: 0.0003, Test loss: 1.4569
In this epoch 1200/5000, Training loss: 0.0003, Test loss: 1.4568
In this epoch 1250/5000, Training loss: 0.0003, Test loss: 1.4567
In this epoch 1300/5000, Training loss: 0.0003, Test loss: 1.4567
In this epoch 1350/5000, Training loss: 0.0002, Test loss: 1.4566
In this epoch 1400/5000, Training loss: 0.0002, Test loss: 1.4565
In this epoch 1450/5000, Training loss: 0.0002, Test loss: 1.4564
In this epoch 1500/5000, Training loss: 0.0002, Test loss: 1.4563
In this epoch 1550/5000, Training loss: 0.0002, Test loss: 1.4562
In this epoch 1600/5000, Training loss: 0.0002, Test loss: 1.4561
In this epoch 1650/5000, Training loss: 0.0002, Test loss: 1.4560
In this epoch 1700/5000, Training loss: 0.0002, Test loss: 1.4559
In this epoch 1750/5000, Training loss: 0.0002, Test loss: 1.4558
In this epoch 1800/5000, Training loss: 0.0002, Test loss: 1.4557
In this epoch 1850/5000, Training loss: 0.0002, Test loss: 1.4556
In this epoch 1900/5000, Training loss: 0.0002, Test loss: 1.4555
In this epoch 1950/5000, Training loss: 0.0002, Test loss: 1.4554
In this epoch 2000/5000, Training loss: 0.0005, Test loss: 1.4603
In this epoch 2050/5000, Training loss: 10.7366, Test loss: 22.1932
In this epoch 2100/5000, Training loss: 0.0073, Test loss: 1.5902
In this epoch 2150/5000, Training loss: 0.0003, Test loss: 1.4560
In this epoch 2200/5000, Training loss: 0.0002, Test loss: 1.4552
In this epoch 2250/5000, Training loss: 0.0001, Test loss: 1.4548
In this epoch 2300/5000, Training loss: 0.0001, Test loss: 1.4547
In this epoch 2350/5000, Training loss: 0.0001, Test loss: 1.4546
In this epoch 2400/5000, Training loss: 0.0001, Test loss: 1.4546
In this epoch 2450/5000, Training loss: 0.0093, Test loss: 1.4955
In this epoch 2500/5000, Training loss: 0.9509, Test loss: 12.1646
In this epoch 2550/5000, Training loss: 0.0878, Test loss: 1.5251
In this epoch 2600/5000, Training loss: 0.0001, Test loss: 1.4526
In this epoch 2650/5000, Training loss: 0.0001, Test loss: 1.4544
In this epoch 2700/5000, Training loss: 0.0001, Test loss: 1.4541
In this epoch 2750/5000, Training loss: 0.0001, Test loss: 1.4540
In this epoch 2800/5000, Training loss: 0.0001, Test loss: 1.4540
In this epoch 2850/5000, Training loss: 0.0001, Test loss: 1.4539
In this epoch 2900/5000, Training loss: 0.0001, Test loss: 1.4538
In this epoch 2950/5000, Training loss: 380.1527, Test loss: 115.3555
In this epoch 3000/5000, Training loss: 1.5535, Test loss: 1.5367
In this epoch 3050/5000, Training loss: 0.0101, Test loss: 1.4468
In this epoch 3100/5000, Training loss: 0.0001, Test loss: 1.4542
In this epoch 3150/5000, Training loss: 0.0001, Test loss: 1.4534
In this epoch 3200/5000, Training loss: 0.0001, Test loss: 1.4535
In this epoch 3250/5000, Training loss: 0.0001, Test loss: 1.4534
In this epoch 3300/5000, Training loss: 0.0001, Test loss: 1.4534
In this epoch 3350/5000, Training loss: 0.0014, Test loss: 1.4475
In this epoch 3400/5000, Training loss: 0.2625, Test loss: 9.0025
In this epoch 3450/5000, Training loss: 0.0033, Test loss: 1.5110
In this epoch 3500/5000, Training loss: 0.0002, Test loss: 1.4522
In this epoch 3550/5000, Training loss: 0.0001, Test loss: 1.4533
In this epoch 3600/5000, Training loss: 0.0001, Test loss: 1.4531
In this epoch 3650/5000, Training loss: 0.0001, Test loss: 1.4530
In this epoch 3700/5000, Training loss: 0.0001, Test loss: 1.4530
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 1.4530
In this epoch 3800/5000, Training loss: 257.2978, Test loss: 93.4793
In this epoch 3850/5000, Training loss: 0.0144, Test loss: 2.0270
In this epoch 3900/5000, Training loss: 0.0028, Test loss: 1.4538
In this epoch 3950/5000, Training loss: 0.0001, Test loss: 1.4536
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 1.4528
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 1.4527
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 1.4527
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 1.4527
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 1.4525
In this epoch 4250/5000, Training loss: 90.2123, Test loss: 30.6779
In this epoch 4300/5000, Training loss: 0.3069, Test loss: 1.4791
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 1.4586
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 1.4531
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 1.4525
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 1.4525
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 1.4525
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 1.4525
In this epoch 4650/5000, Training loss: 0.0179, Test loss: 1.4555
In this epoch 4700/5000, Training loss: 14.2665, Test loss: 8.1796
In this epoch 4750/5000, Training loss: 0.0028, Test loss: 1.4970
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 1.4497
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 1.4524
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 1.4524
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 1.4524
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 1.4524
train_acc 0.3426070501730104
test_acc 0.41730103806228375
{'y_pred': [1.1485257148742676, 0.7777494192123413, 0.7772960662841797, 0.5692963004112244, 1.2625120878219604, 0.4297359585762024, 0.7987884879112244, 0.40789204835891724, 1.3565077781677246, 1.2330996990203857, 0.3666357100009918, 0.6992026567459106, 0.5194252729415894, 0.8874306082725525, 0.8069910407066345, 0.7144981026649475, 0.223591610789299, 0.6225565075874329, 0.8396506905555725, 1.6238292455673218, 0.5406699776649475, 0.7672116160392761, 1.3198109865188599, 1.2223377227783203, 1.7339977025985718, 1.0600576400756836, 0.6183443069458008, 0.5493776202201843, 0.6374187469482422, -0.019718550145626068, 1.0301412343978882, 1.3911292552947998, 0.32935789227485657, 1.524021863937378, 0.03185265511274338, 0.6068301796913147, 0.9800539612770081, 1.0856363773345947, 0.8762280344963074, 0.5883225202560425, 1.4563671350479126, 1.0486979484558105, 0.5338755249977112, 1.0585875511169434, 0.7073936462402344, -0.3301977217197418, 1.1488324403762817, 0.5273590087890625, 0.9828453660011292, 1.2761338949203491, 1.703750491142273, 0.2475498914718628, 0.6010966897010803, 0.9671320915222168, 0.4252096712589264, 0.5360598564147949, -0.5229175686836243, 0.8866351246833801, 0.8263888359069824, 0.22655989229679108, 0.6143327355384827, 0.5138811469078064, 1.3346874713897705, 1.9652196168899536, 1.0753276348114014, 0.7964551448822021, 0.7762916088104248, 1.7325115203857422, 1.7453681230545044, 1.289827585220337, 1.1663373708724976, 1.0217756032943726, 0.9485285878181458, 0.9244655966758728, 0.933591902256012, 1.7558362483978271, 1.068809151649475, 1.510079264640808, 1.0278619527816772, 1.2317034006118774, 1.120859980583191, 0.7005769610404968, 1.2007867097854614, -0.3267711102962494, 1.4745525121688843, 0.6690288782119751, 0.7912670373916626, 0.5944371819496155, 0.9911682605743408, 1.264420509338379, 1.0082460641860962, 2.0259742736816406, 1.2311383485794067, 1.471811056137085, 0.12403542548418045, 0.8990269303321838, 1.044015645980835, 1.2745548486709595, 1.6653425693511963, 0.5454872250556946, 0.5843697190284729, 0.6992033123970032, 1.9208296537399292, 1.2281559705734253, 1.2447398900985718, 1.0637925863265991, 0.20902986824512482, 0.48462072014808655, 0.8713139295578003, 0.8705556988716125, 0.7305470108985901, 0.6985503435134888, 2.4573044776916504, -0.12233827263116837, 0.6062868237495422, 0.8696299195289612, 0.8707531690597534, 1.071457862854004, 0.7860100865364075, -0.3364579677581787, 1.2166372537612915, 1.6988955736160278, 1.9469059705734253, 0.944953441619873, 1.6803065538406372, 0.8563135862350464, -0.27643635869026184, 2.1897995471954346, 0.9626410603523254, -0.5067614912986755, 1.0299384593963623, 0.25631189346313477, 1.1908342838287354, 1.6033042669296265, 1.2912324666976929, 1.3350307941436768, 1.6703723669052124, 1.1278218030929565, 0.5799388289451599, 0.8773149251937866, 1.2473349571228027, 0.09253182262182236, 0.522822916507721, 0.3152393400669098, 0.9656622409820557, 0.38611745834350586, 7.737521082162857e-05, -0.0707264170050621, 0.6093606352806091, 1.4682294130325317, 1.1394020318984985, 2.1357686519622803, 1.410187840461731, 0.4408588409423828, 0.7207587957382202, 0.6468130350112915, 0.7165235280990601, 1.1156455278396606, 0.3193056583404541, 1.3354252576828003, 0.15115322172641754, 0.8415130972862244, 0.005404726602137089, 1.7713172435760498, 0.28850093483924866, 0.05299554020166397, 0.1764562875032425, 1.1846611499786377, 1.2150213718414307, 0.33238643407821655, 1.1135886907577515, 1.1688824892044067, 1.5382323265075684, 1.013128638267517, 0.8725101351737976, 1.417396903038025, 0.4640752077102661, 1.3881858587265015, 1.1793712377548218, 0.7081966996192932, 0.14058008790016174, 0.9127331376075745, 1.7556756734848022, 0.4979117512702942, 0.2557362914085388, 1.10015070438385, 1.7482306957244873, -1.0980275869369507, 1.9973920583724976, 0.6046802401542664, 1.0704047679901123, 0.4923427999019623, 0.5155505537986755, 0.34526315331459045, 0.8670434355735779, 0.9783089756965637, 1.2118648290634155, 1.9023933410644531, 0.35034698247909546, 0.4830290675163269, 1.268796682357788, 1.2851811647415161, 0.9110434055328369, 2.9344747066497803, 0.17900685966014862, -0.05954303592443466, -0.052108656615018845, 1.0294665098190308, 0.9180790185928345, 0.3384144902229309, 1.4786752462387085, 1.5291028022766113, 1.0776677131652832, 1.6550456285476685, 1.2311581373214722, -0.33561772108078003, 0.2879870533943176, 0.760482132434845, 0.5720938444137573, 2.009159564971924, 0.5870388746261597, 0.7778142094612122, 0.32140398025512695, 0.6169185638427734, 0.9065159559249878, 0.8352424502372742, 0.36991220712661743, 0.7021389603614807, 1.8850936889648438, 0.05174458771944046, 0.9104844331741333, 1.0195306539535522, 1.040796160697937, 1.1859568357467651, 1.0489203929901123, 0.5552712678909302, 1.2026258707046509, 0.6577906012535095, 0.7853415608406067, 0.8302060961723328, 1.6386600732803345, 1.357185959815979, 1.3235454559326172, 1.1049360036849976, 1.4317243099212646, 0.2230420857667923, 0.6252442002296448, 0.15437766909599304, 0.6168122291564941, 0.6788527369499207, 0.5903109908103943, -0.39429911971092224, 1.845380425453186, 0.653701901435852, 1.2149608135223389, 1.5759843587875366, 1.117488145828247, 0.07879633456468582, 0.33807891607284546, 0.03383009880781174, 0.6405600309371948, 0.5530628561973572, 0.8628129959106445, 0.12431792169809341, 0.9629573225975037, 0.11192942410707474, 0.12617464363574982, 1.4034260511398315, 1.032597541809082, -0.18802961707115173, 1.105594277381897, 1.3149036169052124, 0.8669642210006714, 0.32766616344451904, 1.2884798049926758, 0.8304070830345154, 0.8766266107559204, 2.1982061862945557, 1.1070736646652222, 0.3678993284702301, 1.7089085578918457, 0.3286340832710266, 1.1961650848388672, 0.596296489238739, 1.6090797185897827, 1.8619521856307983, 1.0627650022506714, 0.08016938716173172, 1.313504695892334, 0.9388464093208313, 1.2180917263031006, 0.9765967726707458, 0.8880531191825867, 1.1516070365905762, 1.2575162649154663, 0.7422577738761902, 1.1424061059951782, 0.9145511388778687, 1.1201839447021484, 1.449965476989746, 1.3141063451766968, 1.5022883415222168, 0.18617849051952362, 0.6917931437492371, 2.1471872329711914, 1.108803391456604, -0.2353685349225998, 1.1684833765029907, 1.293383002281189, 1.2186633348464966, 0.1275419145822525, 0.994259238243103, 1.3249884843826294, 0.786629855632782, 0.867561399936676, 1.0162273645401, 0.6113543510437012, 1.3716355562210083, -0.12091615796089172, 0.701413094997406, 0.40490517020225525, 0.29896339774131775, 1.8076928853988647, 2.1752917766571045, -0.308834046125412, -0.4167788028717041, 0.9625870585441589, 1.094614863395691, 2.2630176544189453, -0.4063739776611328, 0.9858331084251404, 0.741628110408783, 0.7206555008888245, 0.7697929739952087, 1.6070324182510376, 0.19319818913936615, -0.08840235322713852, 0.3585869073867798, 1.3691768646240234, 0.7108806371688843], 'y_pred_int': [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1, 2, 1, 0, 2, 1, 0, 1, 0, 1, 2, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 2, 0, 0, 0, 1, 1, 0, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 0, 0, 1, 2, 0, 2, 1, 1, 0, 1, 0, 1, 1, 1, 2, 0, 0, 1, 1, 1, 3, 0, 0, 0, 1, 1, 0, 1, 2, 1, 2, 1, 0, 0, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 2, 1, 1, 2, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 2, 0, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 2, 2, 0, 0, 1, 1, 2, 0, 1, 1, 1, 1, 2, 0, 0, 0, 1, 1], 'y_true': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 3, 0, 0, 1, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 2, 1, 0, 4, 0, 2, 3, 0, 1, 8, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 1, 0, 6, 1, 1, 1, 1, 0, 0, 1, 1, 3, 1, 0, 0, 1, 1, 1, 1, 4, 0, 0, 1, 2, 3, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 0, 0, 1, 1, 2, 0, 1, 2, 3, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 4, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 2, 0, 1, 2, 1, 1, 0, 0, 1, 0, 2, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 3, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 4, 0, 1, 2, 0, 0, 4, 2, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 0, 1, 2, 0, 0, 3, 2, 0, 1, 1, 1, 0, 2, 2, 0, 0, 1, 1, 0, 1, 1, 0, 4, 0, 0, 2, 1, 3, 2, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 2, 1, 1, 6, 0, 1, 3, 1, 3, 1, 0, 1, 1, 3, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 2], 'train_acc': 0.3426070501730104, 'test_acc': 0.41730103806228375, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 87.0704, Test loss: 138.6094
In this epoch 100/5000, Training loss: 32.6759, Test loss: 105.4926
In this epoch 150/5000, Training loss: 15.4692, Test loss: 109.1552
In this epoch 200/5000, Training loss: 7.1741, Test loss: 112.5150
In this epoch 250/5000, Training loss: 3.3495, Test loss: 114.9765
In this epoch 300/5000, Training loss: 1.6183, Test loss: 116.6448
In this epoch 350/5000, Training loss: 0.8399, Test loss: 117.7349
In this epoch 400/5000, Training loss: 0.4887, Test loss: 118.4385
In this epoch 450/5000, Training loss: 0.3275, Test loss: 118.8928
In this epoch 500/5000, Training loss: 0.2502, Test loss: 119.1892
In this epoch 550/5000, Training loss: 0.2099, Test loss: 119.3866
In this epoch 600/5000, Training loss: 0.1860, Test loss: 119.5229
In this epoch 650/5000, Training loss: 0.1695, Test loss: 119.6221
In this epoch 700/5000, Training loss: 0.1562, Test loss: 119.6993
In this epoch 750/5000, Training loss: 0.1446, Test loss: 119.7638
In this epoch 800/5000, Training loss: 0.1339, Test loss: 119.8215
In this epoch 850/5000, Training loss: 0.1238, Test loss: 119.8758
In this epoch 900/5000, Training loss: 0.1141, Test loss: 119.9289
In this epoch 950/5000, Training loss: 0.1049, Test loss: 119.9820
In this epoch 1000/5000, Training loss: 0.0960, Test loss: 120.0358
In this epoch 1050/5000, Training loss: 0.0877, Test loss: 120.0907
In this epoch 1100/5000, Training loss: 0.0797, Test loss: 120.1468
In this epoch 1150/5000, Training loss: 0.0723, Test loss: 120.2043
In this epoch 1200/5000, Training loss: 0.0653, Test loss: 120.2629
In this epoch 1250/5000, Training loss: 0.0587, Test loss: 120.3228
In this epoch 1300/5000, Training loss: 0.0526, Test loss: 120.3834
In this epoch 1350/5000, Training loss: 0.0470, Test loss: 120.4449
In this epoch 1400/5000, Training loss: 0.0418, Test loss: 120.5067
In this epoch 1450/5000, Training loss: 0.0370, Test loss: 120.5689
In this epoch 1500/5000, Training loss: 0.0327, Test loss: 120.6311
In this epoch 1550/5000, Training loss: 0.0287, Test loss: 120.6930
In this epoch 1600/5000, Training loss: 0.0251, Test loss: 120.7544
In this epoch 1650/5000, Training loss: 0.0219, Test loss: 120.8151
In this epoch 1700/5000, Training loss: 0.0190, Test loss: 120.8748
In this epoch 1750/5000, Training loss: 0.0298, Test loss: 121.0189
In this epoch 1800/5000, Training loss: 9.2661, Test loss: 121.5503
In this epoch 1850/5000, Training loss: 0.0712, Test loss: 120.9939
In this epoch 1900/5000, Training loss: 0.0112, Test loss: 121.0729
In this epoch 1950/5000, Training loss: 0.0097, Test loss: 121.1182
In this epoch 2000/5000, Training loss: 0.0084, Test loss: 121.1614
In this epoch 2050/5000, Training loss: 0.0073, Test loss: 121.2031
In this epoch 2100/5000, Training loss: 0.0063, Test loss: 121.2436
In this epoch 2150/5000, Training loss: 0.0054, Test loss: 121.2828
In this epoch 2200/5000, Training loss: 104.8903, Test loss: 186.0853
In this epoch 2250/5000, Training loss: 0.2603, Test loss: 122.1344
In this epoch 2300/5000, Training loss: 0.0042, Test loss: 121.3994
In this epoch 2350/5000, Training loss: 0.0031, Test loss: 121.4055
In this epoch 2400/5000, Training loss: 0.0027, Test loss: 121.4331
In this epoch 2450/5000, Training loss: 0.0023, Test loss: 121.4591
In this epoch 2500/5000, Training loss: 0.0020, Test loss: 121.4843
In this epoch 2550/5000, Training loss: 0.0017, Test loss: 121.5084
In this epoch 2600/5000, Training loss: 0.0057, Test loss: 121.5736
In this epoch 2650/5000, Training loss: 0.8845, Test loss: 126.7461
In this epoch 2700/5000, Training loss: 0.0890, Test loss: 121.7376
In this epoch 2750/5000, Training loss: 0.0011, Test loss: 121.5867
In this epoch 2800/5000, Training loss: 0.0008, Test loss: 121.6021
In this epoch 2850/5000, Training loss: 0.0007, Test loss: 121.6171
In this epoch 2900/5000, Training loss: 0.0006, Test loss: 121.6319
In this epoch 2950/5000, Training loss: 0.0005, Test loss: 121.6460
In this epoch 3000/5000, Training loss: 0.0004, Test loss: 121.6594
In this epoch 3050/5000, Training loss: 5.5670, Test loss: 130.8547
In this epoch 3100/5000, Training loss: 4.3529, Test loss: 121.7698
In this epoch 3150/5000, Training loss: 0.0013, Test loss: 121.7350
In this epoch 3200/5000, Training loss: 0.0003, Test loss: 121.6967
In this epoch 3250/5000, Training loss: 0.0002, Test loss: 121.7096
In this epoch 3300/5000, Training loss: 0.0002, Test loss: 121.7182
In this epoch 3350/5000, Training loss: 0.0002, Test loss: 121.7261
In this epoch 3400/5000, Training loss: 0.0001, Test loss: 121.7336
In this epoch 3450/5000, Training loss: 0.0001, Test loss: 121.7407
In this epoch 3500/5000, Training loss: 45.1573, Test loss: 208.7068
In this epoch 3550/5000, Training loss: 3.7148, Test loss: 124.0219
In this epoch 3600/5000, Training loss: 0.0229, Test loss: 121.7315
In this epoch 3650/5000, Training loss: 0.0002, Test loss: 121.7638
In this epoch 3700/5000, Training loss: 0.0001, Test loss: 121.7672
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 121.7713
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 121.7754
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 121.7792
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 121.7829
In this epoch 3950/5000, Training loss: 47.0137, Test loss: 290.5414
In this epoch 4000/5000, Training loss: 1.0987, Test loss: 122.0922
In this epoch 4050/5000, Training loss: 0.0074, Test loss: 121.7746
In this epoch 4100/5000, Training loss: 0.0001, Test loss: 121.7914
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 121.7961
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 121.7984
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 121.8004
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 121.8022
In this epoch 4350/5000, Training loss: 13.0478, Test loss: 215.2252
In this epoch 4400/5000, Training loss: 0.2544, Test loss: 121.7689
In this epoch 4450/5000, Training loss: 0.0020, Test loss: 121.8062
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 121.8067
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 121.8094
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 121.8105
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 121.8116
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 121.8125
In this epoch 4750/5000, Training loss: 226.6809, Test loss: 219.6908
In this epoch 4800/5000, Training loss: 0.5720, Test loss: 123.2073
In this epoch 4850/5000, Training loss: 0.0041, Test loss: 121.7902
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 121.8136
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 121.8161
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 121.8168
train_acc 1.0
test_acc 0.03823529411764706
{'y_pred': [7.4516472816467285, 11.352959632873535, 12.571114540100098, 14.748729705810547, 13.375523567199707, 19.162797927856445, 11.888148307800293, 16.26133155822754, 14.242642402648926, 11.75790786743164, 13.512078285217285, 20.326732635498047, 11.64801025390625, 10.594765663146973, 17.760034561157227, 17.179658889770508, 10.912957191467285, 8.234808921813965, 18.55592918395996, 17.797243118286133, 14.399758338928223, 7.4353508949279785, 16.1984920501709, 7.120031833648682, 19.709171295166016, 14.161478996276855, 11.102019309997559, 12.794583320617676, 18.272090911865234, 10.739171981811523, 6.17800760269165, 15.031620025634766, 19.838680267333984, 4.979311466217041, 23.40330696105957, 10.753705978393555, 14.896912574768066, 7.2192912101745605, 8.51719856262207, 18.65717887878418, 14.061589241027832, 20.368762969970703, 15.386421203613281, 22.66344451904297, 14.444539070129395, 9.872932434082031, 21.899375915527344, 14.352193832397461, 8.152960777282715, 18.321882247924805, 4.181853294372559, 14.395344734191895, 8.963879585266113, 19.19365119934082, 5.425451278686523, 10.904629707336426, 13.82653522491455, 5.965059757232666, 19.482629776000977, 10.14242935180664, 20.48903465270996, 15.003317832946777, 7.697633266448975, 19.791683197021484, 21.200178146362305, 20.677730560302734, 13.92819595336914, 10.32090950012207, 11.17541217803955, 14.25313663482666, 19.78219223022461, 12.122945785522461, 29.651277542114258, 16.336471557617188, 7.114509105682373, 15.095940589904785, 22.03061866760254, 16.911466598510742, 12.605271339416504, 14.325758934020996, 16.864255905151367, 12.015907287597656, 11.290692329406738, 8.24016284942627, 14.534133911132812, 9.6421537399292, 10.566134452819824, 6.780807971954346, 12.674872398376465, 8.71467113494873, 11.875523567199707, 20.103879928588867, 14.850920677185059, 15.85622501373291, 6.408440589904785, 16.585817337036133, 16.49179458618164, 15.805299758911133, 16.912649154663086, 12.965113639831543, 4.112143039703369, 12.78792667388916, 26.973957061767578, 10.632118225097656, 12.035551071166992, 13.439957618713379, 21.568979263305664, 22.16330909729004, 18.570383071899414, 6.823718070983887, 10.633641242980957, 23.307098388671875, 17.253833770751953, 7.335642337799072, 15.329839706420898, 5.5969953536987305, 10.603121757507324, 7.9152960777282715, 16.873132705688477, 10.055987358093262, 15.381338119506836, 22.142534255981445, 13.040728569030762, 20.073955535888672, 16.6436710357666, 17.586671829223633, 14.651168823242188, 12.486302375793457, 17.74500846862793, 28.1326847076416, 19.494619369506836, 8.519003868103027, 6.676768779754639, 18.944726943969727, 19.182315826416016, 17.823400497436523, 17.590593338012695, 18.49387550354004, 14.333434104919434, 18.16780662536621, 7.602667331695557, 21.976303100585938, 11.182662010192871, 14.2013521194458, 10.829106330871582, 12.973827362060547, 14.598998069763184, 16.698959350585938, 8.274861335754395, 12.351637840270996, 12.90304946899414, 25.991586685180664, 14.729557037353516, 9.842350959777832, 12.222569465637207, 16.78835105895996, 12.302210807800293, 14.929966926574707, 8.995383262634277, 11.612543106079102, 17.351709365844727, 2.552845001220703, 21.222354888916016, 10.44543170928955, 8.690705299377441, 12.297993659973145, 13.668370246887207, 13.658767700195312, 16.242420196533203, 5.207061290740967, 13.13302230834961, 13.64989948272705, 14.815526962280273, 22.080982208251953, 15.94461727142334, 12.014776229858398, 11.200778007507324, 14.27479362487793, 16.760046005249023, 17.873159408569336, 13.70332145690918, 10.871891975402832, 15.871150970458984, 20.79750633239746, 21.962087631225586, 19.96019744873047, 19.709444046020508, 9.523260116577148, 13.343344688415527, 14.485480308532715, 18.890932083129883, 15.551568031311035, 21.52134132385254, 12.467024803161621, 11.030674934387207, 14.56350040435791, 18.981287002563477, 13.922308921813965, 6.0604376792907715, 20.929841995239258, 5.9999518394470215, 11.232254981994629, 11.837376594543457, 20.460351943969727, 11.518856048583984, 9.14195442199707, 21.77313232421875, 12.343701362609863, 9.687607765197754, 14.296934127807617, 10.911199569702148, 8.842852592468262, 19.575725555419922, 20.441259384155273, 14.32180118560791, 22.507862091064453, 12.279803276062012, 6.761876583099365, 14.16589641571045, 13.879186630249023, 13.943170547485352, 5.063368797302246, 10.181984901428223, 13.028082847595215, 16.2874698638916, 23.243064880371094, 15.298542022705078, 14.001832962036133, 22.524293899536133, 7.448204517364502, 7.73340368270874, 18.098142623901367, 18.89568519592285, 27.846172332763672, 11.006449699401855, 8.068777084350586, 14.354636192321777, 12.287610054016113, 20.261693954467773, 19.53076171875, 16.986156463623047, 4.999977111816406, 10.50096607208252, 13.264610290527344, 16.0665283203125, 17.820466995239258, 15.10062313079834, 20.123167037963867, 14.690237045288086, 8.7810640335083, 21.917417526245117, 11.904226303100586, 18.372304916381836, 15.284427642822266, 16.244144439697266, 24.19287872314453, 19.374961853027344, 7.17583703994751, 17.343442916870117, 9.017024040222168, 17.44862174987793, 16.784833908081055, 6.495364189147949, 22.825700759887695, 17.249666213989258, 11.70109748840332, 13.034843444824219, 16.268583297729492, 9.942841529846191, 5.681437015533447, 12.033114433288574, 12.21878719329834, 12.397762298583984, 15.075672149658203, 16.314659118652344, 12.231255531311035, 9.637085914611816, 15.857115745544434, 17.402997970581055, 17.807958602905273, 19.927236557006836, 4.69178581237793, 21.625232696533203, 17.761144638061523, 18.3536434173584, 18.855539321899414, 1.0187678337097168, 21.414169311523438, 13.65365219116211, 11.563246726989746, 20.048006057739258, 4.693452835083008, 16.201051712036133, 1.757060170173645, 19.913421630859375, 17.037982940673828, 13.34991455078125, 14.481627464294434, 12.238286018371582, 16.66377067565918, 17.850940704345703, 13.44141674041748, 5.0517144203186035, 10.699246406555176, 13.168418884277344, 8.078597068786621, 7.804625988006592, 11.128193855285645, 16.22280502319336, 21.532175064086914, 19.02236557006836, 13.344561576843262, 8.158828735351562, 8.560117721557617, 6.483386993408203, 11.69939136505127, 7.820413112640381, 18.684663772583008, 17.508697509765625, 14.638209342956543, 13.53514575958252, 10.98261833190918, 11.61136531829834, 17.147737503051758, 9.296539306640625, 17.634294509887695, 15.720677375793457, 16.571056365966797, 21.05584716796875, 12.001771926879883, 14.934734344482422, 6.375765323638916, 19.72654914855957, 15.18985652923584, 11.627760887145996, 22.631994247436523, 10.968849182128906, 18.33023452758789, 13.183011054992676, 11.154850959777832], 'y_pred_int': [7, 11, 13, 15, 13, 19, 12, 16, 14, 12, 14, 20, 12, 11, 18, 17, 11, 8, 19, 18, 14, 7, 16, 7, 20, 14, 11, 13, 18, 11, 6, 15, 20, 5, 23, 11, 15, 7, 9, 19, 14, 20, 15, 23, 14, 10, 22, 14, 8, 18, 4, 14, 9, 19, 5, 11, 14, 6, 19, 10, 20, 15, 8, 20, 21, 21, 14, 10, 11, 14, 20, 12, 30, 16, 7, 15, 22, 17, 13, 14, 17, 12, 11, 8, 15, 10, 11, 7, 13, 9, 12, 20, 15, 16, 6, 17, 16, 16, 17, 13, 4, 13, 27, 11, 12, 13, 22, 22, 19, 7, 11, 23, 17, 7, 15, 6, 11, 8, 17, 10, 15, 22, 13, 20, 17, 18, 15, 12, 18, 28, 19, 9, 7, 19, 19, 18, 18, 18, 14, 18, 8, 22, 11, 14, 11, 13, 15, 17, 8, 12, 13, 26, 15, 10, 12, 17, 12, 15, 9, 12, 17, 3, 21, 10, 9, 12, 14, 14, 16, 5, 13, 14, 15, 22, 16, 12, 11, 14, 17, 18, 14, 11, 16, 21, 22, 20, 20, 10, 13, 14, 19, 16, 22, 12, 11, 15, 19, 14, 6, 21, 6, 11, 12, 20, 12, 9, 22, 12, 10, 14, 11, 9, 20, 20, 14, 23, 12, 7, 14, 14, 14, 5, 10, 13, 16, 23, 15, 14, 23, 7, 8, 18, 19, 28, 11, 8, 14, 12, 20, 20, 17, 5, 11, 13, 16, 18, 15, 20, 15, 9, 22, 12, 18, 15, 16, 24, 19, 7, 17, 9, 17, 17, 6, 23, 17, 12, 13, 16, 10, 6, 12, 12, 12, 15, 16, 12, 10, 16, 17, 18, 20, 5, 22, 18, 18, 19, 1, 21, 14, 12, 20, 5, 16, 2, 20, 17, 13, 14, 12, 17, 18, 13, 5, 11, 13, 8, 8, 11, 16, 22, 19, 13, 8, 9, 6, 12, 8, 19, 18, 15, 14, 11, 12, 17, 9, 18, 16, 17, 21, 12, 15, 6, 20, 15, 12, 23, 11, 18, 13, 11], 'y_true': [24, 35, 29, 6, 6, 11, 6, 5, 5, 37, 10, 9, 8, 23, 7, 76, 8, 29, 49, 7, 8, 15, 11, 4, 8, 8, 21, 21, 29, 9, 7, 11, 18, 18, 15, 18, 15, 9, 2, 11, 16, 7, 9, 6, 7, 5, 13, 8, 12, 12, 8, 6, 9, 10, 11, 4, 14, 12, 8, 8, 2, 12, 11, 21, 5, 7, 4, 4, 19, 7, 5, 14, 38, 4, 11, 16, 5, 15, 4, 7, 10, 32, 6, 11, 9, 9, 22, 14, 60, 2, 41, 11, 22, 39, 22, 14, 19, 8, 6, 10, 8, 18, 13, 7, 23, 6, 21, 13, 7, 21, 26, 10, 10, 29, 14, 8, 10, 9, 14, 2, 9, 13, 19, 35, 3, 13, 13, 8, 5, 9, 29, 7, 10, 30, 26, 5, 22, 6, 12, 54, 3, 4, 14, 11, 4, 28, 13, 5, 16, 13, 21, 17, 14, 7, 6, 9, 14, 3, 9, 6, 18, 4, 8, 17, 11, 11, 49, 14, 6, 7, 39, 9, 6, 10, 9, 7, 30, 16, 8, 17, 22, 29, 12, 7, 4, 15, 17, 5, 14, 10, 13, 6, 23, 5, 14, 9, 15, 11, 17, 35, 12, 2, 9, 14, 9, 25, 7, 23, 22, 11, 33, 9, 14, 3, 5, 20, 12, 23, 23, 10, 12, 21, 41, 17, 14, 13, 12, 4, 10, 25, 19, 13, 15, 5, 11, 2, 17, 18, 24, 6, 17, 3, 22, 4, 12, 13, 9, 42, 6, 20, 9, 7, 14, 19, 38, 16, 6, 6, 25, 4, 8, 6, 6, 6, 35, 3, 34, 13, 10, 12, 12, 3, 12, 16, 11, 15, 7, 3, 34, 25, 5, 6, 9, 29, 14, 5, 9, 8, 12, 6, 9, 25, 9, 18, 8, 3, 15, 12, 32, 4, 15, 21, 11, 7, 13, 12, 32, 6, 20, 6, 10, 16, 13, 20, 13, 11, 14, 14, 10, 28, 18, 6, 9, 8, 5, 11, 23, 5, 16, 5, 7, 8, 11, 20, 8, 24, 10, 17, 11, 8], 'train_acc': 1.0, 'test_acc': 0.03823529411764706, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 45109.5273, Test loss: 45164.1484
In this epoch 100/5000, Training loss: 38287.9883, Test loss: 44835.7422
In this epoch 150/5000, Training loss: 32175.5820, Test loss: 45362.6875
In this epoch 200/5000, Training loss: 26731.2070, Test loss: 45985.4961
In this epoch 250/5000, Training loss: 22021.4766, Test loss: 46620.8086
In this epoch 300/5000, Training loss: 18015.0996, Test loss: 47243.0234
In this epoch 350/5000, Training loss: 14647.6846, Test loss: 47840.5508
In this epoch 400/5000, Training loss: 11845.1465, Test loss: 48406.7969
In this epoch 450/5000, Training loss: 9532.6240, Test loss: 48937.6289
In this epoch 500/5000, Training loss: 7638.7148, Test loss: 49430.8555
In this epoch 550/5000, Training loss: 6097.8491, Test loss: 49885.8594
In this epoch 600/5000, Training loss: 4851.5332, Test loss: 50303.2188
In this epoch 650/5000, Training loss: 3848.7617, Test loss: 50684.2734
In this epoch 700/5000, Training loss: 3045.8210, Test loss: 51030.9297
In this epoch 750/5000, Training loss: 2405.7559, Test loss: 51345.3828
In this epoch 800/5000, Training loss: 1897.6719, Test loss: 51629.9844
In this epoch 850/5000, Training loss: 1495.9631, Test loss: 51887.1016
In this epoch 900/5000, Training loss: 1179.5676, Test loss: 52119.0469
In this epoch 950/5000, Training loss: 931.2721, Test loss: 52328.0547
In this epoch 1000/5000, Training loss: 737.0961, Test loss: 52516.1836
In this epoch 1050/5000, Training loss: 585.7421, Test loss: 52685.3711
In this epoch 1100/5000, Training loss: 468.1289, Test loss: 52837.4062
In this epoch 1150/5000, Training loss: 376.9924, Test loss: 52973.9297
In this epoch 1200/5000, Training loss: 306.5475, Test loss: 53096.4258
In this epoch 1250/5000, Training loss: 252.2097, Test loss: 53206.2500
In this epoch 1300/5000, Training loss: 210.3601, Test loss: 53304.6836
In this epoch 1350/5000, Training loss: 178.1549, Test loss: 53392.8320
In this epoch 1400/5000, Training loss: 153.3693, Test loss: 53471.7539
In this epoch 1450/5000, Training loss: 134.2701, Test loss: 53542.4141
In this epoch 1500/5000, Training loss: 119.5121, Test loss: 53605.6602
In this epoch 1550/5000, Training loss: 108.0563, Test loss: 53662.3008
In this epoch 1600/5000, Training loss: 99.1029, Test loss: 53713.0547
In this epoch 1650/5000, Training loss: 92.0391, Test loss: 53758.6016
In this epoch 1700/5000, Training loss: 86.3971, Test loss: 53799.5312
In this epoch 1750/5000, Training loss: 81.8212, Test loss: 53836.3945
In this epoch 1800/5000, Training loss: 78.0422, Test loss: 53869.6953
In this epoch 1850/5000, Training loss: 74.8567, Test loss: 53899.8906
In this epoch 1900/5000, Training loss: 72.1120, Test loss: 53927.3672
In this epoch 1950/5000, Training loss: 69.6941, Test loss: 53952.4961
In this epoch 2000/5000, Training loss: 67.5178, Test loss: 53975.6250
In this epoch 2050/5000, Training loss: 65.5200, Test loss: 53997.0195
In this epoch 2100/5000, Training loss: 63.6542, Test loss: 54016.9531
In this epoch 2150/5000, Training loss: 61.8862, Test loss: 54035.6523
In this epoch 2200/5000, Training loss: 60.1912, Test loss: 54053.3125
In this epoch 2250/5000, Training loss: 58.5514, Test loss: 54070.1289
In this epoch 2300/5000, Training loss: 56.9540, Test loss: 54086.2305
In this epoch 2350/5000, Training loss: 55.3901, Test loss: 54101.7852
In this epoch 2400/5000, Training loss: 53.8534, Test loss: 54116.8711
In this epoch 2450/5000, Training loss: 52.3401, Test loss: 54131.6289
In this epoch 2500/5000, Training loss: 50.8475, Test loss: 54146.1289
In this epoch 2550/5000, Training loss: 49.3741, Test loss: 54160.4297
In this epoch 2600/5000, Training loss: 47.9194, Test loss: 54174.6250
In this epoch 2650/5000, Training loss: 46.4832, Test loss: 54188.7305
In this epoch 2700/5000, Training loss: 45.0659, Test loss: 54202.8242
In this epoch 2750/5000, Training loss: 43.6681, Test loss: 54216.9258
In this epoch 2800/5000, Training loss: 42.2906, Test loss: 54231.0664
In this epoch 2850/5000, Training loss: 40.9342, Test loss: 54245.2656
In this epoch 2900/5000, Training loss: 39.6000, Test loss: 54259.5430
In this epoch 2950/5000, Training loss: 38.2890, Test loss: 54273.9297
In this epoch 3000/5000, Training loss: 37.0023, Test loss: 54288.4062
In this epoch 3050/5000, Training loss: 35.7409, Test loss: 54303.0000
In this epoch 3100/5000, Training loss: 34.5057, Test loss: 54317.7188
In this epoch 3150/5000, Training loss: 33.2978, Test loss: 54332.5469
In this epoch 3200/5000, Training loss: 32.1180, Test loss: 54347.4883
In this epoch 3250/5000, Training loss: 30.9672, Test loss: 54362.5430
In this epoch 3300/5000, Training loss: 29.8463, Test loss: 54377.6953
In this epoch 3350/5000, Training loss: 28.7558, Test loss: 54392.9531
In this epoch 3400/5000, Training loss: 27.6966, Test loss: 54408.2969
In this epoch 3450/5000, Training loss: 26.6692, Test loss: 54423.7305
In this epoch 3500/5000, Training loss: 25.6740, Test loss: 54439.2383
In this epoch 3550/5000, Training loss: 24.7117, Test loss: 54454.7969
In this epoch 3600/5000, Training loss: 23.7824, Test loss: 54470.4062
In this epoch 3650/5000, Training loss: 22.8865, Test loss: 54486.0469
In this epoch 3700/5000, Training loss: 22.0242, Test loss: 54501.7188
In this epoch 3750/5000, Training loss: 21.1955, Test loss: 54517.3906
In this epoch 3800/5000, Training loss: 20.4006, Test loss: 54533.0547
In this epoch 3850/5000, Training loss: 19.6394, Test loss: 54548.6992
In this epoch 3900/5000, Training loss: 18.9117, Test loss: 54564.3086
In this epoch 3950/5000, Training loss: 18.2173, Test loss: 54579.8555
In this epoch 4000/5000, Training loss: 17.5559, Test loss: 54595.3438
In this epoch 4050/5000, Training loss: 16.9272, Test loss: 54610.7305
In this epoch 4100/5000, Training loss: 16.3307, Test loss: 54626.0469
In this epoch 4150/5000, Training loss: 72.2549, Test loss: 54861.4531
In this epoch 4200/5000, Training loss: 15.6831, Test loss: 54657.4023
In this epoch 4250/5000, Training loss: 14.8402, Test loss: 54668.1719
In this epoch 4300/5000, Training loss: 14.4071, Test loss: 54680.8008
In this epoch 4350/5000, Training loss: 13.9971, Test loss: 54693.8320
In this epoch 4400/5000, Training loss: 13.6092, Test loss: 54706.7656
In this epoch 4450/5000, Training loss: 13.2438, Test loss: 54719.0820
In this epoch 4500/5000, Training loss: 20.0622, Test loss: 54724.3672
In this epoch 4550/5000, Training loss: 12.6824, Test loss: 54742.2734
In this epoch 4600/5000, Training loss: 12.3568, Test loss: 54753.0352
In this epoch 4650/5000, Training loss: 12.0982, Test loss: 54763.9648
In this epoch 4700/5000, Training loss: 11.8547, Test loss: 54774.5430
In this epoch 4750/5000, Training loss: 11.6251, Test loss: 54784.9844
In this epoch 4800/5000, Training loss: 11.4094, Test loss: 54795.3320
In this epoch 4850/5000, Training loss: 11.2407, Test loss: 54765.9297
In this epoch 4900/5000, Training loss: 11.4387, Test loss: 54809.0312
In this epoch 4950/5000, Training loss: 10.8833, Test loss: 54822.2422
In this epoch 5000/5000, Training loss: 10.7329, Test loss: 54831.1992
train_acc 0.9963235294117647
test_acc 0.0
{'y_pred': [347.3547668457031, 262.489501953125, 66.16770935058594, 304.915283203125, 386.4817810058594, 322.365478515625, 255.8258819580078, 433.11187744140625, 198.80908203125, 459.32958984375, 287.9609069824219, 210.13568115234375, 362.4549255371094, 433.8485412597656, 376.9598693847656, 224.48582458496094, 367.71209716796875, 376.8804931640625, 595.537353515625, 278.7036437988281, 276.9313659667969, 338.06207275390625, 348.8226623535156, 284.9112548828125, 418.36968994140625, 173.9043426513672, 241.3256378173828, 373.64434814453125, 259.2484436035156, 558.4826049804688, 429.8872985839844, 447.36065673828125, 371.9104919433594, 344.7929382324219, 299.2724304199219, 300.3468017578125, 378.960205078125, 309.46905517578125, 309.0001220703125, 357.99639892578125, 465.8611145019531, 412.0138854980469, 300.7138977050781, 305.1180725097656, 273.0257568359375, 351.5614013671875, 326.31787109375, 457.9581298828125, 466.1906433105469, 448.2874755859375, 325.6054992675781, 412.0848693847656, 258.8680419921875, 294.95269775390625, 278.2410888671875, 316.6368103027344, 416.5508117675781, 359.72479248046875, 435.29058837890625, 443.98529052734375, 316.7771911621094, 308.62384033203125, 331.86566162109375, 652.3400268554688, 391.6542663574219, 367.61566162109375, 328.7173156738281, 553.3572998046875, 423.96527099609375, 334.2692565917969, 457.39141845703125, 335.6573791503906, 421.56854248046875, 385.6108703613281, 528.6691284179688, 257.949462890625, 353.093505859375, 277.5704345703125, 355.68316650390625, 487.27154541015625, 310.5452575683594, 287.9259338378906, 342.392333984375, 385.72100830078125, 303.22491455078125, 399.2725830078125, 342.85626220703125, 272.1855773925781, 484.57696533203125, 375.20391845703125, 327.48272705078125, 330.9273376464844, 329.55169677734375, 402.9669494628906, 425.8590393066406, 140.150146484375, 363.455810546875, 296.1905212402344, 294.20953369140625, 242.2586212158203, 416.3092346191406, 293.45880126953125, 287.1037292480469, 302.9286193847656, 345.7281188964844, 295.7776184082031, 345.95587158203125, 399.0812683105469, 356.5985107421875, 403.5519104003906, 312.1878662109375, 361.6676940917969, 215.8665771484375, 171.10049438476562, 373.0083923339844, 360.3426818847656, 495.7350158691406, 393.2691650390625, 435.8916015625, 294.8083190917969, 254.1542205810547, 377.32763671875, 429.0330810546875, 378.1011657714844, 116.56095886230469, 385.102783203125, 488.77783203125, 403.4840393066406, 331.8983459472656, 272.7830505371094, 343.1832275390625, 255.76597595214844, 451.1422424316406, 304.13592529296875, 385.3597412109375, 322.58197021484375, 332.57757568359375, 260.862548828125, 551.1620483398438, 279.43048095703125, 247.58367919921875, 292.5418701171875, 326.6358642578125, 579.6981811523438, 301.4712219238281, 358.7514953613281, 414.4741516113281, 391.447509765625, 445.5800476074219, 412.56085205078125, 420.57550048828125, 185.073974609375, 253.53427124023438, 272.26666259765625, 312.6227722167969, 296.0968017578125, 502.37744140625, 380.47064208984375, 315.54949951171875, 567.3150024414062, 177.85569763183594, 293.98736572265625, 497.26025390625, 409.935302734375, 344.4086608886719, 256.2065124511719, 355.1471862792969, 408.8975524902344, 419.10943603515625, 468.68487548828125, 454.41741943359375, 382.1541442871094, 279.5517578125, 553.383056640625, 303.2177429199219, 528.2442016601562, 415.2791748046875, 309.5241394042969, 267.8310241699219, 537.9999389648438, 362.1641540527344, 309.1480407714844, 427.59417724609375, 324.929443359375, 239.46458435058594, 258.7437744140625, 374.01251220703125, 357.7390441894531, 561.0398559570312, 451.2618408203125, 194.94325256347656, 360.38751220703125, 406.5600280761719, 282.67889404296875, 351.98724365234375, 231.24139404296875, 329.2351379394531, 363.38641357421875, 413.05377197265625, 413.1280822753906, 364.8734436035156, 440.8582763671875, 320.6929626464844, 377.2745361328125, 323.02264404296875, 457.863037109375, 485.48309326171875, 357.4785461425781, 148.46240234375, 349.0723571777344, 295.8409118652344, 485.1419677734375, 352.90081787109375, 384.3011169433594, 423.3568115234375, 397.26507568359375, 472.2728271484375, 250.57711791992188, 310.48193359375, 149.18870544433594, 307.40594482421875, 239.61563110351562, 516.9589233398438, 289.19207763671875, 316.622802734375, 409.928955078125, 501.1208801269531, 406.155517578125, 390.0216064453125, 493.0859069824219, 327.58648681640625, 375.47772216796875, 350.11578369140625, 302.3700866699219, 464.3154296875, 387.7576599121094, 194.51377868652344, 450.84820556640625, 215.0379180908203, 181.82247924804688, 321.2266845703125, 595.431396484375, 482.0592346191406, 313.79718017578125, 402.01397705078125, 257.8742980957031, 405.11920166015625, 447.179443359375, 540.2975463867188, 328.3671875, 457.9314880371094, 313.89886474609375, 348.19989013671875, 365.041259765625, 115.44429016113281, 444.26141357421875, 173.19268798828125, 288.618408203125, 223.99313354492188, 471.5640869140625, 485.96282958984375, 297.20159912109375, 325.12371826171875, 274.75885009765625, 431.9081115722656, 482.2831726074219, 349.62860107421875, 353.2516174316406, 492.025390625, 398.1988525390625, 423.79498291015625, 335.5814208984375, 349.8258972167969, 364.71051025390625, 409.5849304199219, 456.849365234375, 256.1027526855469, 354.9030456542969, 352.27728271484375, 191.9552764892578, 416.565185546875, 370.98077392578125, 217.19622802734375, 378.66790771484375, 562.8698120117188, 299.9521179199219, 327.37896728515625, 273.05755615234375, 613.6588134765625, 579.770263671875, 305.9461669921875, 296.8736877441406, 517.6998901367188, 241.2026824951172, 401.23565673828125, 349.7449951171875, 402.91644287109375, 534.9193725585938, 458.1771240234375, 308.97222900390625, 475.82037353515625, 229.6951446533203, 470.21826171875, 365.23291015625, 340.6886901855469, 212.04165649414062, 282.0001525878906, 210.25267028808594, 334.1244812011719, 374.6235656738281, 334.7979431152344, 361.74871826171875, 310.98468017578125, 232.36415100097656, 618.26611328125, 308.03497314453125, 340.6732177734375, 449.252197265625, 380.0966796875, 281.7220458984375, 367.7438049316406, 634.5859985351562, 313.3019714355469, 437.64752197265625, 326.46771240234375, 386.88189697265625, 290.38372802734375, 229.30747985839844, 232.16873168945312, 407.24468994140625, 289.7521057128906, 439.0893859863281, 357.12451171875, 519.2389526367188, 312.5263977050781, 458.5232849121094, 365.70684814453125, 524.4390869140625, 318.6517333984375, 464.3939514160156], 'y_pred_int': [347, 262, 66, 305, 386, 322, 256, 433, 199, 459, 288, 210, 362, 434, 377, 224, 368, 377, 596, 279, 277, 338, 349, 285, 418, 174, 241, 374, 259, 558, 430, 447, 372, 345, 299, 300, 379, 309, 309, 358, 466, 412, 301, 305, 273, 352, 326, 458, 466, 448, 326, 412, 259, 295, 278, 317, 417, 360, 435, 444, 317, 309, 332, 652, 392, 368, 329, 553, 424, 334, 457, 336, 422, 386, 529, 258, 353, 278, 356, 487, 311, 288, 342, 386, 303, 399, 343, 272, 485, 375, 327, 331, 330, 403, 426, 140, 363, 296, 294, 242, 416, 293, 287, 303, 346, 296, 346, 399, 357, 404, 312, 362, 216, 171, 373, 360, 496, 393, 436, 295, 254, 377, 429, 378, 117, 385, 489, 403, 332, 273, 343, 256, 451, 304, 385, 323, 333, 261, 551, 279, 248, 293, 327, 580, 301, 359, 414, 391, 446, 413, 421, 185, 254, 272, 313, 296, 502, 380, 316, 567, 178, 294, 497, 410, 344, 256, 355, 409, 419, 469, 454, 382, 280, 553, 303, 528, 415, 310, 268, 538, 362, 309, 428, 325, 239, 259, 374, 358, 561, 451, 195, 360, 407, 283, 352, 231, 329, 363, 413, 413, 365, 441, 321, 377, 323, 458, 485, 357, 148, 349, 296, 485, 353, 384, 423, 397, 472, 251, 310, 149, 307, 240, 517, 289, 317, 410, 501, 406, 390, 493, 328, 375, 350, 302, 464, 388, 195, 451, 215, 182, 321, 595, 482, 314, 402, 258, 405, 447, 540, 328, 458, 314, 348, 365, 115, 444, 173, 289, 224, 472, 486, 297, 325, 275, 432, 482, 350, 353, 492, 398, 424, 336, 350, 365, 410, 457, 256, 355, 352, 192, 417, 371, 217, 379, 563, 300, 327, 273, 614, 580, 306, 297, 518, 241, 401, 350, 403, 535, 458, 309, 476, 230, 470, 365, 341, 212, 282, 210, 334, 375, 335, 362, 311, 232, 618, 308, 341, 449, 380, 282, 368, 635, 313, 438, 326, 387, 290, 229, 232, 407, 290, 439, 357, 519, 313, 459, 366, 524, 319, 464], 'y_true': [279, 384, 244, 322, 86, 220, 690, 491, 279, 260, 350, 450, 414, 299, 112, 277, 466, 236, 216, 494, 495, 284, 530, 337, 579, 467, 528, 643, 255, 535, 163, 387, 929, 334, 425, 180, 349, 446, 148, 309, 469, 461, 87, 62, 823, 857, 346, 212, 216, 209, 164, 475, 577, 203, 61, 121, 312, 481, 103, 207, 136, 787, 403, 141, 88, 148, 477, 449, 542, 312, 434, 324, 209, 629, 201, 372, 606, 235, 276, 342, 532, 435, 200, 109, 200, 539, 401, 144, 827, 548, 212, 88, 76, 89, 346, 256, 233, 367, 291, 93, 355, 110, 608, 610, 239, 709, 392, 58, 80, 380, 163, 680, 203, 283, 200, 543, 572, 511, 463, 124, 412, 233, 161, 266, 261, 955, 814, 332, 154, 92, 312, 666, 446, 167, 172, 210, 150, 373, 464, 244, 297, 407, 500, 200, 215, 80, 428, 96, 45, 74, 212, 455, 57, 321, 161, 191, 621, 387, 158, 149, 150, 596, 406, 194, 129, 364, 296, 765, 322, 237, 659, 165, 392, 222, 484, 487, 133, 313, 41, 139, 367, 79, 235, 425, 232, 186, 498, 332, 173, 407, 85, 295, 765, 474, 206, 718, 507, 224, 197, 672, 331, 259, 741, 241, 191, 254, 176, 310, 837, 415, 833, 645, 389, 561, 372, 552, 241, 363, 372, 180, 243, 62, 401, 155, 352, 344, 373, 156, 424, 783, 753, 265, 330, 521, 424, 694, 796, 164, 42, 412, 599, 907, 248, 298, 430, 679, 406, 234, 502, 444, 247, 375, 162, 114, 401, 292, 980, 326, 237, 194, 156, 676, 1026, 184, 103, 559, 370, 550, 285, 339, 629, 605, 77, 583, 76, 101, 440, 867, 452, 300, 327, 379, 552, 220, 659, 385, 166, 706, 81, 122, 370, 590, 272, 408, 259, 418, 681, 655, 300, 153, 376, 723, 211, 557, 353, 354, 139, 84, 160, 167, 157, 82, 199, 407, 234, 226, 598, 57, 256, 247, 332, 470, 440, 228, 356, 714, 108, 753, 617, 558, 898, 134, 69, 237, 201, 635, 670, 619, 507, 143], 'train_acc': 0.9963235294117647, 'test_acc': 0.0, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 1433.4969, Test loss: 708.7750
In this epoch 100/5000, Training loss: 2.0969, Test loss: 86.8033
In this epoch 150/5000, Training loss: 0.0280, Test loss: 80.1589
In this epoch 200/5000, Training loss: 0.0008, Test loss: 80.1761
In this epoch 250/5000, Training loss: 0.0001, Test loss: 80.1926
In this epoch 300/5000, Training loss: 0.0001, Test loss: 80.1883
In this epoch 350/5000, Training loss: 0.0000, Test loss: 80.1871
In this epoch 400/5000, Training loss: 0.0000, Test loss: 80.1865
In this epoch 450/5000, Training loss: 0.0000, Test loss: 80.1862
In this epoch 500/5000, Training loss: 0.0000, Test loss: 80.1859
In this epoch 550/5000, Training loss: 0.0000, Test loss: 80.1858
In this epoch 600/5000, Training loss: 0.0000, Test loss: 80.1857
In this epoch 650/5000, Training loss: 0.0000, Test loss: 80.1857
In this epoch 700/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 750/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 800/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 850/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 900/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 950/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 80.1856
In this epoch 2350/5000, Training loss: 340029.3750, Test loss: 70762.2812
In this epoch 2400/5000, Training loss: 384.6115, Test loss: 971.2260
In this epoch 2450/5000, Training loss: 7.0020, Test loss: 84.8762
In this epoch 2500/5000, Training loss: 0.0174, Test loss: 81.7959
In this epoch 2550/5000, Training loss: 0.0002, Test loss: 81.7846
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 81.7875
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 81.7889
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 81.7890
In this epoch 4650/5000, Training loss: 26743.3750, Test loss: 52292.3828
In this epoch 4700/5000, Training loss: 448.0635, Test loss: 108.2575
In this epoch 4750/5000, Training loss: 1.0105, Test loss: 92.8315
In this epoch 4800/5000, Training loss: 0.0093, Test loss: 92.6191
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 92.6535
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 92.6442
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 92.6442
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 92.6443
train_acc 0.3420004325259516
test_acc 0.2576470588235294
{'y_pred': [10.736759185791016, 3.1937272548675537, -16.440717697143555, 6.595248699188232, 12.132904052734375, -13.443214416503906, -8.663445472717285, 4.749892711639404, 25.059579849243164, 1.6328710317611694, 9.587740898132324, 3.392934799194336, 3.52463436126709, 15.127949714660645, -1.4347186088562012, 7.967250347137451, -1.4039641618728638, -26.6290283203125, 3.435185432434082, 4.061323642730713, -11.51782512664795, 8.196052551269531, 2.183562755584717, 5.285387992858887, -0.22111424803733826, 22.531038284301758, 8.254831314086914, 0.9608315229415894, 8.83810043334961, 2.035191059112549, 1.0474892854690552, 22.903154373168945, -3.9263973236083984, -10.255229949951172, 8.740884780883789, -2.9642961025238037, 7.288954257965088, 6.220371246337891, -4.000508785247803, 9.340991973876953, 0.760413408279419, 5.186212062835693, 0.5120786428451538, 25.441545486450195, 3.6480369567871094, 1.4775465726852417, 11.204099655151367, 11.50903606414795, -8.299336433410645, -0.5322767496109009, -3.691547393798828, -19.315584182739258, -0.31802162528038025, -3.847811698913574, -4.673375606536865, 14.2725248336792, -5.2774200439453125, 11.503582954406738, 2.9113996028900146, 13.771958351135254, 11.674220085144043, 12.442301750183105, 2.3573591709136963, -5.674620151519775, -4.6722025871276855, -1.9968382120132446, 8.968317985534668, -0.7802692651748657, 11.815140724182129, 5.0673322677612305, 5.801809787750244, -16.885910034179688, 5.3508758544921875, 13.010802268981934, -14.301481246948242, 10.712845802307129, -2.6939406394958496, 0.01313200406730175, -4.667141437530518, -13.394947052001953, -0.6950250864028931, 20.35613441467285, -3.8355278968811035, 1.7315322160720825, -15.513497352600098, 5.131380081176758, 16.81230354309082, 14.702261924743652, -10.90732192993164, 23.34360694885254, 5.190540790557861, 1.122968077659607, -0.8473203182220459, 18.527502059936523, -0.5002964735031128, -3.918222665786743, -13.236287117004395, -1.3752785921096802, 19.285945892333984, -10.958319664001465, -0.8114134073257446, 11.380447387695312, -3.289246082305908, 6.143981456756592, -4.174494743347168, 6.450960636138916, -0.2407263219356537, -4.767073154449463, -0.9513221979141235, 5.091318607330322, -3.4485068321228027, 9.300965309143066, -8.83974838256836, 0.930946946144104, -4.748511791229248, 10.669737815856934, 6.846330642700195, 1.5648889541625977, -0.832695722579956, 2.034470558166504, 18.330698013305664, 5.188755512237549, -3.2733702659606934, 1.8380776643753052, -10.649046897888184, 13.144120216369629, 2.836155891418457, -5.821977138519287, -0.6186591386795044, 7.778486251831055, 13.20022964477539, 5.882765769958496, 2.840550184249878, 2.9281396865844727, 7.804575443267822, 7.576703071594238, -9.547907829284668, 2.6019845008850098, 0.9479762315750122, 1.8398855924606323, 0.5054486989974976, 13.464892387390137, 14.454834938049316, 6.586286544799805, -1.180620551109314, -6.769110202789307, 2.374232769012451, 0.15900173783302307, -2.350139617919922, -5.189648628234863, 11.92836856842041, -13.564085960388184, -4.758410930633545, 1.4651998281478882, -8.69545841217041, -1.3424261808395386, -6.674219131469727, -4.856805324554443, -0.5873438119888306, 2.217663049697876, 2.2562060356140137, 9.835082054138184, 0.7219299077987671, -33.09773254394531, 6.693873882293701, -5.425691604614258, 17.968347549438477, 2.3420357704162598, -0.7195286750793457, 13.822867393493652, 3.2785935401916504, 12.648788452148438, -5.202670574188232, 11.321148872375488, 2.7642998695373535, -4.4813055992126465, 19.14272689819336, 7.6599321365356445, 1.303972601890564, -16.263046264648438, -22.797914505004883, -6.725878715515137, 5.019097328186035, 13.457857131958008, 19.177919387817383, -0.3868747651576996, -7.680113315582275, 22.669788360595703, 5.748675903305411e-05, -3.22499418258667, -1.8733783960342407, -14.672049522399902, -11.152453422546387, 7.6545867919921875, 9.37340259552002, 0.3357314169406891, 7.7847580909729, 1.4679020643234253, 14.996903419494629, 24.66098976135254, -10.631057739257812, 4.252966403961182, -9.715984344482422, -9.562052726745605, -2.93674373626709, -7.011031627655029, -2.336125373840332, -6.9366068840026855, 5.967057228088379, 2.671980857849121, 0.18535581231117249, -7.162819862365723, 5.034172058105469, -3.717538833618164, 3.1472010612487793, -11.166616439819336, 5.0015974044799805, 10.404231071472168, 3.723881244659424, 8.868058204650879, -4.514500617980957, 7.619046688079834, -9.709046363830566, -4.88794469833374, 19.78562355041504, 12.292561531066895, 25.956998825073242, -2.2695627212524414, 5.176333427429199, -15.046154975891113, 19.40043067932129, -2.0523226261138916, 0.6328455209732056, -3.9753007888793945, -3.2707138061523438, -8.30429458618164, 12.500604629516602, 12.686371803283691, -5.105954647064209, -9.63536548614502, -7.274722576141357, 9.606228828430176, 17.88692283630371, -7.1863484382629395, 4.771850109100342, -1.9819365739822388, 5.458982944488525, 13.236836433410645, -1.5863841772079468, -3.0595602989196777, 3.1029257774353027, 0.9823049306869507, -6.565542697906494, 9.491179466247559, -11.714005470275879, 0.40732279419898987, -12.195545196533203, -7.661139011383057, 2.1372509002685547, 1.759752631187439, 6.566361427307129, -0.15992626547813416, 1.2737120389938354, -2.726722002029419, -9.302696228027344, -1.5625890493392944, -13.804390907287598, -3.0222864151000977, -8.049642562866211, 9.69252872467041, 2.971334934234619, 0.48321542143821716, 6.68491268157959, -5.789881229400635, 6.153757572174072, 0.31640854477882385, 5.73666524887085, -2.5441665649414062, -3.6444544792175293, 25.16591453552246, -3.666656017303467, -5.946798801422119, 9.46928596496582, 2.9416344165802, 11.118151664733887, 10.977744102478027, -12.045830726623535, 4.613174915313721, -5.576586723327637, -0.20970210433006287, -2.115345001220703, -6.918564319610596, 16.640352249145508, 5.6193718910217285, 2.1220476627349854, -3.427371025085449, 24.247394561767578, -2.056102991104126, 5.026676654815674, 10.78060245513916, -12.096969604492188, 12.8994140625, 1.2046574354171753, -2.1371307373046875, -4.77537202835083, -14.261698722839355, 0.04149952158331871, -5.103113651275635, -8.175623893737793, -1.6931407451629639, 16.42300796508789, -2.784670352935791, -13.264310836791992, 0.43992605805397034, 15.695805549621582, 1.8476101160049438, -6.251923084259033, -18.114139556884766, 14.219854354858398, 11.660225868225098, -21.39348030090332, -6.573159694671631, 1.2863396406173706, 14.947386741638184, -12.030123710632324, 4.574174404144287, 4.747002124786377, 13.372708320617676, 6.149789810180664, -14.320348739624023, 11.010542869567871, 10.954123497009277, -6.8791608810424805, -3.437971353530884, 6.849828243255615, -5.220640659332275, 13.224678993225098, -1.547385811805725, 2.47157621383667, 14.850641250610352], 'y_pred_int': [11, 3, 0, 7, 12, 0, 0, 5, 25, 2, 10, 3, 4, 15, 0, 8, 0, 0, 3, 4, 0, 8, 2, 5, 0, 23, 8, 1, 9, 2, 1, 23, 0, 0, 9, 0, 7, 6, 0, 9, 1, 5, 1, 25, 4, 1, 11, 12, 0, 0, 0, 0, 0, 0, 0, 14, 0, 12, 3, 14, 12, 12, 2, 0, 0, 0, 9, 0, 12, 5, 6, 0, 5, 13, 0, 11, 0, 0, 0, 0, 0, 20, 0, 2, 0, 5, 17, 15, 0, 23, 5, 1, 0, 19, 0, 0, 0, 0, 19, 0, 0, 11, 0, 6, 0, 6, 0, 0, 0, 5, 0, 9, 0, 1, 0, 11, 7, 2, 0, 2, 18, 5, 0, 2, 0, 13, 3, 0, 0, 8, 13, 6, 3, 3, 8, 8, 0, 3, 1, 2, 1, 13, 14, 7, 0, 0, 2, 0, 0, 0, 12, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 10, 1, 0, 7, 0, 18, 2, 0, 14, 3, 13, 0, 11, 3, 0, 19, 8, 1, 0, 0, 0, 5, 13, 19, 0, 0, 23, 0, 0, 0, 0, 0, 8, 9, 0, 8, 1, 15, 25, 0, 4, 0, 0, 0, 0, 0, 0, 6, 3, 0, 0, 5, 0, 3, 0, 5, 10, 4, 9, 0, 8, 0, 0, 20, 12, 26, 0, 5, 0, 19, 0, 1, 0, 0, 0, 13, 13, 0, 0, 0, 10, 18, 0, 5, 0, 5, 13, 0, 0, 3, 1, 0, 9, 0, 0, 0, 0, 2, 2, 7, 0, 1, 0, 0, 0, 0, 0, 0, 10, 3, 0, 7, 0, 6, 0, 6, 0, 0, 25, 0, 0, 9, 3, 11, 11, 0, 5, 0, 0, 0, 0, 17, 6, 2, 0, 24, 0, 5, 11, 0, 13, 1, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 16, 2, 0, 0, 14, 12, 0, 0, 1, 15, 0, 5, 5, 13, 6, 0, 11, 11, 0, 0, 7, 0, 13, 0, 2, 15], 'y_true': [1, 0, 0, 2, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 6, 0, 0, 0, 1, 2, 1, 1, 0, 1, 0, 0, 2, 0, 0, 1, 1, 0, 1, 3, 1, 1, 7, 0, 0, 1, 0, 0, 1, 1, 0, 3, 0, 1, 3, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 2, 1, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 0, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 2, 1, 2, 0, 1, 1, 0, 1, 0, 1, 0, 0, 3, 0, 0, 1, 2, 0, 1, 1, 1, 2, 1, 1, 2, 0, 0, 4, 1, 0, 0, 2, 1, 0, 1, 0, 0, 0, 1, 0, 1, 8, 1, 1, 0, 0, 0, 1, 2, 2, 1, 0, 2, 0, 1, 5, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 7, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 3, 1, 5, 2, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 0, 0, 1, 1, 0, 1, 0, 1, 1, 2, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 0, 0, 2, 3, 1, 1, 2, 0, 0, 3, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 1, 1, 1, 2, 0, 2, 1, 3, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 2, 2, 0, 0, 1, 3, 0, 0, 2, 0, 0, 0, 3, 0, 1, 0, 1, 1, 0, 2, 2, 0, 0, 3, 1, 0, 1, 1, 0, 1, 0, 0], 'train_acc': 0.3420004325259516, 'test_acc': 0.2576470588235294, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 1704.8724, Test loss: 185.4128
In this epoch 100/5000, Training loss: 7.0963, Test loss: 106.8356
In this epoch 150/5000, Training loss: 0.0862, Test loss: 101.7458
In this epoch 200/5000, Training loss: 0.0002, Test loss: 101.8944
In this epoch 250/5000, Training loss: 0.0000, Test loss: 101.8596
In this epoch 300/5000, Training loss: 0.0000, Test loss: 101.8633
In this epoch 350/5000, Training loss: 0.0000, Test loss: 101.8636
In this epoch 400/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 450/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 500/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 550/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 600/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 650/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 700/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 750/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 800/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 850/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 900/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 950/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 101.8635
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 101.8634
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 101.8745
In this epoch 2800/5000, Training loss: 435.2530, Test loss: 9659.8262
In this epoch 2850/5000, Training loss: 11.7869, Test loss: 147.4559
In this epoch 2900/5000, Training loss: 0.3070, Test loss: 101.2545
In this epoch 2950/5000, Training loss: 0.0009, Test loss: 101.3278
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 101.2457
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 101.2499
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 101.2498
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 101.2498
train_acc 1.0
test_acc 0.029429065743944636
{'y_pred': [-0.31564465165138245, 3.6264185905456543, 10.383500099182129, -4.851302146911621, 25.27850341796875, 39.78744125366211, 37.61946105957031, 15.880247116088867, 3.2579853534698486, 29.351390838623047, 6.960970401763916, 7.526702880859375, 21.351932525634766, 10.561671257019043, 27.302968978881836, 20.371763229370117, 21.17101287841797, 8.775875091552734, 10.278377532958984, 20.176321029663086, 7.396548748016357, 14.591778755187988, 13.635048866271973, 14.889067649841309, 9.544354438781738, 11.945545196533203, 30.652814865112305, 6.242478370666504, 13.85000991821289, 17.214454650878906, -0.8981658816337585, 26.112680435180664, 21.059288024902344, 1.1014480590820312, 28.565439224243164, 1.4193228483200073, 9.281365394592285, 9.525132179260254, 4.6867194175720215, 28.766996383666992, 16.65298843383789, 7.352301120758057, 15.396611213684082, 25.9233341217041, 8.195975303649902, 26.65767478942871, 41.17379379272461, 13.772315979003906, 4.450042724609375, 15.6106595993042, 0.06964834779500961, 7.835240840911865, 7.81793737411499, 6.297092437744141, 4.463434219360352, 34.03909683227539, 13.962964057922363, 10.595301628112793, 28.66731834411621, 17.323362350463867, 41.477806091308594, 20.926607131958008, 19.07098960876465, 0.41473039984703064, 12.621991157531738, 22.78302764892578, -3.9747159481048584, 13.097641944885254, 32.43302917480469, 28.36277961730957, 16.551883697509766, 30.300092697143555, 31.979677200317383, 10.098506927490234, 28.899633407592773, 13.886263847351074, 10.957867622375488, 11.86968994140625, 2.9723060131073, 26.00278663635254, 4.154658794403076, 6.8822407722473145, 25.272947311401367, 7.9829325675964355, 19.893598556518555, 18.484872817993164, 14.97547435760498, 13.51964282989502, 19.432254791259766, 8.687808990478516, 23.97437858581543, 16.340986251831055, 12.948287963867188, 1.5732163190841675, 21.81397819519043, 3.770150899887085, 1.453356385231018, -1.2659116983413696, 2.1840155124664307, 17.250244140625, 23.44989585876465, 19.23670196533203, 2.3434689044952393, 19.171789169311523, 18.402563095092773, 41.390438079833984, 1.6751347780227661, 8.007081031799316, 14.066787719726562, 0.6865676045417786, 15.244377136230469, 3.2369492053985596, 13.807015419006348, 20.970422744750977, 9.945003509521484, 11.455348014831543, 18.624807357788086, 3.8173134326934814, 15.281108856201172, 6.42108154296875, 26.136184692382812, 32.15842056274414, 8.613957405090332, 18.056535720825195, 2.091718912124634, 30.944562911987305, 9.962233543395996, 14.52961254119873, 10.981410026550293, 17.063133239746094, 5.120251655578613, 11.577471733093262, 7.638565540313721, -0.13727393746376038, 5.768378734588623, 22.72791290283203, 31.282655715942383, 19.4283504486084, 22.411893844604492, 19.921730041503906, 19.82301902770996, 20.85818099975586, 14.354243278503418, -1.4431148767471313, 30.93021011352539, 3.1203222274780273, 31.93575096130371, 8.59752368927002, 1.8518880605697632, 29.76026725769043, 8.265464782714844, 16.446725845336914, 9.6127347946167, 10.931319236755371, 4.600423336029053, 2.6253585815429688, 15.914216041564941, 6.110006809234619, 20.59514045715332, 25.06657600402832, 25.50997543334961, 2.028646945953369, 19.377527236938477, 19.360939025878906, 9.39868450164795, 13.780776023864746, -2.362416982650757, 28.112449645996094, 32.096351623535156, 6.683997631072998, 8.571551322937012, -5.4771342277526855, 26.032596588134766, 18.9397029876709, 0.3820147216320038, 23.869569778442383, 1.4002443552017212, 23.26650047302246, 10.252076148986816, 9.890294075012207, 16.382375717163086, 7.578477382659912, 9.109991073608398, 35.9968147277832, -5.065494060516357, 15.131150245666504, 8.696148872375488, 12.66606330871582, -2.1387112140655518, 28.65498161315918, 2.150312662124634, 1.7651127576828003, 8.188614845275879, 7.570982456207275, 16.88399887084961, 12.453457832336426, 28.11351776123047, 24.656089782714844, 24.474069595336914, 2.2487714290618896, 10.64476490020752, 27.031496047973633, 7.165461540222168, 22.740007400512695, 13.799375534057617, 43.94561004638672, 21.11615753173828, 14.83248233795166, 39.7180290222168, 22.22532844543457, 19.970699310302734, 10.033556938171387, 24.325672149658203, 21.87848472595215, 17.26786231994629, 23.132604598999023, 18.227632522583008, 16.608455657958984, 29.518735885620117, 12.192161560058594, -0.5732037425041199, -1.9896489381790161, 6.482114315032959, 8.775348663330078, 18.064809799194336, 7.60625696182251, -1.2408620119094849, 24.963491439819336, 24.11969566345215, 22.352123260498047, 11.764632225036621, 22.528362274169922, 22.055538177490234, 21.720983505249023, 29.24782943725586, 15.20528507232666, 25.403976440429688, 10.600577354431152, 1.8754013776779175, 26.915616989135742, 14.570016860961914, 24.636980056762695, 4.053513526916504, 4.085658550262451, 7.364956378936768, 7.986690998077393, 20.18425178527832, 10.806721687316895, 31.952241897583008, 17.462228775024414, 9.923946380615234, 3.648548126220703, 35.99044418334961, 11.479012489318848, 11.009321212768555, -3.978348731994629, 5.475315570831299, 14.202391624450684, 11.145110130310059, 11.280049324035645, 20.665935516357422, -3.509533166885376, -1.7510849237442017, 20.809324264526367, 21.079174041748047, 10.318161964416504, 13.854523658752441, 10.399006843566895, 15.817766189575195, 28.979284286499023, 5.074243068695068, 2.0023374557495117, 13.52632999420166, 21.838069915771484, 31.617834091186523, 24.111167907714844, -2.2745440006256104, -3.8343350887298584, 16.934072494506836, 3.882932424545288, 7.080660343170166, 9.462376594543457, 23.11484146118164, 0.76814204454422, 24.42552375793457, 13.045554161071777, 26.691102981567383, 25.868038177490234, 27.09724998474121, 10.03769588470459, -0.7648398280143738, 15.01172161102295, 22.851093292236328, 8.2760591506958, 21.558202743530273, 14.759411811828613, 37.78693389892578, 7.29835319519043, -6.349904537200928, 16.142105102539062, 31.958728790283203, 22.522933959960938, 28.774948120117188, 24.794204711914062, -4.638612747192383, 12.412732124328613, 11.944657325744629, 17.198719024658203, 17.915184020996094, 10.302857398986816, 11.412968635559082, 3.3713791370391846, 23.195524215698242, 16.95184898376465, 17.6771240234375, 22.00476837158203, 16.943750381469727, 20.321500778198242, 8.234777450561523, 14.311895370483398, -3.3390891551971436, 26.224519729614258, 22.8823184967041, 8.916240692138672, -0.0281316377222538, -0.6462656855583191, 13.345108985900879, 10.907588005065918, 14.33505916595459, 26.699918746948242, 12.110474586486816, 9.8652925491333, 25.110088348388672, 0.4205192029476166, 29.685148239135742, 25.17701530456543, 2.4638540744781494, 13.770970344543457, 7.833768844604492, 33.350608825683594], 'y_pred_int': [0, 4, 10, 0, 25, 40, 38, 16, 3, 29, 7, 8, 21, 11, 27, 20, 21, 9, 10, 20, 7, 15, 14, 15, 10, 12, 31, 6, 14, 17, 0, 26, 21, 1, 29, 1, 9, 10, 5, 29, 17, 7, 15, 26, 8, 27, 41, 14, 4, 16, 0, 8, 8, 6, 4, 34, 14, 11, 29, 17, 41, 21, 19, 0, 13, 23, 0, 13, 32, 28, 17, 30, 32, 10, 29, 14, 11, 12, 3, 26, 4, 7, 25, 8, 20, 18, 15, 14, 19, 9, 24, 16, 13, 2, 22, 4, 1, 0, 2, 17, 23, 19, 2, 19, 18, 41, 2, 8, 14, 1, 15, 3, 14, 21, 10, 11, 19, 4, 15, 6, 26, 32, 9, 18, 2, 31, 10, 15, 11, 17, 5, 12, 8, 0, 6, 23, 31, 19, 22, 20, 20, 21, 14, 0, 31, 3, 32, 9, 2, 30, 8, 16, 10, 11, 5, 3, 16, 6, 21, 25, 26, 2, 19, 19, 9, 14, 0, 28, 32, 7, 9, 0, 26, 19, 0, 24, 1, 23, 10, 10, 16, 8, 9, 36, 0, 15, 9, 13, 0, 29, 2, 2, 8, 8, 17, 12, 28, 25, 24, 2, 11, 27, 7, 23, 14, 44, 21, 15, 40, 22, 20, 10, 24, 22, 17, 23, 18, 17, 30, 12, 0, 0, 6, 9, 18, 8, 0, 25, 24, 22, 12, 23, 22, 22, 29, 15, 25, 11, 2, 27, 15, 25, 4, 4, 7, 8, 20, 11, 32, 17, 10, 4, 36, 11, 11, 0, 5, 14, 11, 11, 21, 0, 0, 21, 21, 10, 14, 10, 16, 29, 5, 2, 14, 22, 32, 24, 0, 0, 17, 4, 7, 9, 23, 1, 24, 13, 27, 26, 27, 10, 0, 15, 23, 8, 22, 15, 38, 7, 0, 16, 32, 23, 29, 25, 0, 12, 12, 17, 18, 10, 11, 3, 23, 17, 18, 22, 17, 20, 8, 14, 0, 26, 23, 9, 0, 0, 13, 11, 14, 27, 12, 10, 25, 0, 30, 25, 2, 14, 8, 33], 'y_true': [6, 4, 16, 8, 10, 23, 20, 36, 8, 36, 2, 6, 12, 2, 21, 18, 9, 10, 11, 9, 16, 10, 7, 18, 10, 5, 13, 10, 10, 14, 6, 9, 27, 4, 20, 11, 6, 6, 4, 13, 8, 8, 12, 11, 12, 11, 16, 10, 6, 38, 2, 18, 5, 7, 7, 60, 14, 33, 11, 26, 25, 31, 4, 6, 6, 12, 5, 6, 19, 19, 10, 17, 25, 8, 24, 22, 8, 13, 5, 59, 5, 9, 12, 6, 35, 23, 6, 10, 15, 8, 27, 10, 4, 3, 30, 8, 8, 5, 7, 14, 18, 10, 3, 9, 24, 18, 4, 2, 16, 7, 14, 28, 11, 23, 7, 10, 12, 8, 33, 4, 34, 22, 9, 9, 5, 11, 6, 36, 10, 13, 7, 12, 12, 9, 4, 30, 29, 11, 25, 8, 24, 17, 11, 1, 15, 2, 11, 2, 6, 35, 8, 6, 10, 6, 4, 1, 18, 3, 39, 25, 9, 4, 15, 50, 3, 23, 4, 13, 11, 29, 7, 4, 21, 14, 28, 10, 5, 34, 9, 16, 11, 9, 6, 14, 5, 17, 3, 18, 16, 14, 4, 3, 10, 6, 15, 22, 26, 15, 18, 8, 14, 30, 10, 10, 7, 39, 11, 10, 27, 18, 13, 10, 50, 28, 12, 16, 20, 9, 49, 8, 10, 10, 39, 9, 21, 7, 8, 15, 18, 14, 18, 53, 32, 12, 15, 17, 25, 6, 7, 10, 7, 27, 8, 3, 4, 6, 15, 4, 20, 15, 6, 4, 22, 23, 8, 8, 22, 14, 8, 13, 23, 6, 10, 14, 10, 3, 15, 5, 10, 18, 13, 9, 29, 10, 24, 9, 6, 6, 9, 7, 6, 8, 16, 6, 31, 29, 16, 20, 13, 3, 3, 6, 14, 12, 13, 3, 15, 10, 5, 6, 14, 31, 14, 8, 7, 10, 14, 6, 22, 32, 15, 7, 15, 14, 8, 17, 10, 9, 11, 35, 8, 16, 17, 5, 5, 5, 23, 7, 4, 14, 2, 6, 31, 10, 13, 10, 10, 5, 6, 38], 'train_acc': 1.0, 'test_acc': 0.029429065743944636, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 784.9559, Test loss: 12705.1064
In this epoch 100/5000, Training loss: 3.0363, Test loss: 12718.8740
In this epoch 150/5000, Training loss: 0.0445, Test loss: 12728.2090
In this epoch 200/5000, Training loss: 0.0002, Test loss: 12729.5342
In this epoch 250/5000, Training loss: 0.0000, Test loss: 12729.5918
In this epoch 300/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 350/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 400/5000, Training loss: 0.0000, Test loss: 12729.6074
In this epoch 450/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 500/5000, Training loss: 0.0000, Test loss: 12729.6074
In this epoch 550/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 600/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 650/5000, Training loss: 0.0000, Test loss: 12729.6064
In this epoch 700/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 750/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 800/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 850/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 900/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 950/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 12729.6074
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 12729.6074
In this epoch 1250/5000, Training loss: 13905.4268, Test loss: 16248.9824
In this epoch 1300/5000, Training loss: 18.4821, Test loss: 12742.8652
In this epoch 1350/5000, Training loss: 0.2709, Test loss: 12731.9375
In this epoch 1400/5000, Training loss: 0.0021, Test loss: 12729.4648
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 1600/5000, Training loss: 0.0005, Test loss: 12729.6973
In this epoch 1650/5000, Training loss: 2835.8855, Test loss: 15171.3887
In this epoch 1700/5000, Training loss: 6.6732, Test loss: 12754.9932
In this epoch 1750/5000, Training loss: 0.0318, Test loss: 12729.5576
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 12729.6250
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 12729.6064
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 12729.6123
In this epoch 1950/5000, Training loss: 0.0005, Test loss: 12729.6914
In this epoch 2000/5000, Training loss: 37.2850, Test loss: 14441.9219
In this epoch 2050/5000, Training loss: 19.4135, Test loss: 12732.7080
In this epoch 2100/5000, Training loss: 0.0731, Test loss: 12729.3682
In this epoch 2150/5000, Training loss: 0.0007, Test loss: 12729.6680
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 12729.6035
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 12729.6123
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 12729.5869
In this epoch 2350/5000, Training loss: 2210.9717, Test loss: 17885.2598
In this epoch 2400/5000, Training loss: 4.4901, Test loss: 12764.1963
In this epoch 2450/5000, Training loss: 0.0063, Test loss: 12728.9355
In this epoch 2500/5000, Training loss: 0.0008, Test loss: 12729.6387
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 12729.6064
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 2700/5000, Training loss: 0.0094, Test loss: 12729.9326
In this epoch 2750/5000, Training loss: 0.0995, Test loss: 13594.9561
In this epoch 2800/5000, Training loss: 0.0073, Test loss: 12738.3164
In this epoch 2850/5000, Training loss: 0.0042, Test loss: 12729.3916
In this epoch 2900/5000, Training loss: 0.0004, Test loss: 12729.6357
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 12729.6064
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 12729.6064
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 12729.6074
In this epoch 3150/5000, Training loss: 5526.4062, Test loss: 23245.9805
In this epoch 3200/5000, Training loss: 2.0968, Test loss: 12899.9590
In this epoch 3250/5000, Training loss: 0.7028, Test loss: 12730.1152
In this epoch 3300/5000, Training loss: 0.0074, Test loss: 12729.6680
In this epoch 3350/5000, Training loss: 0.0001, Test loss: 12729.6152
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 12729.6064
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 12729.6074
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 12729.6074
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 12729.6064
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 12729.6074
In this epoch 3650/5000, Training loss: 0.0004, Test loss: 12729.6680
In this epoch 3700/5000, Training loss: 3753.7573, Test loss: 15088.9941
In this epoch 3750/5000, Training loss: 20.1099, Test loss: 12753.0342
In this epoch 3800/5000, Training loss: 0.1169, Test loss: 12728.9678
In this epoch 3850/5000, Training loss: 0.0003, Test loss: 12729.5439
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 12729.6035
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 12729.6094
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 12729.6045
In this epoch 4050/5000, Training loss: 0.0003, Test loss: 12729.5518
In this epoch 4100/5000, Training loss: 1834.0541, Test loss: 16805.9609
In this epoch 4150/5000, Training loss: 5.0991, Test loss: 12737.5293
In this epoch 4200/5000, Training loss: 0.0034, Test loss: 12729.0635
In this epoch 4250/5000, Training loss: 0.0009, Test loss: 12729.5518
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 12729.6006
In this epoch 4350/5000, Training loss: 0.2754, Test loss: 12714.9131
In this epoch 4400/5000, Training loss: 36.2817, Test loss: 12674.4492
In this epoch 4450/5000, Training loss: 171.4467, Test loss: 13044.8828
In this epoch 4500/5000, Training loss: 0.0627, Test loss: 12732.1621
In this epoch 4550/5000, Training loss: 0.0071, Test loss: 12729.7529
In this epoch 4600/5000, Training loss: 14624.9707, Test loss: 21370.7246
In this epoch 4650/5000, Training loss: 49.5365, Test loss: 12773.2021
In this epoch 4700/5000, Training loss: 0.1274, Test loss: 12728.6738
In this epoch 4750/5000, Training loss: 0.0015, Test loss: 12729.4932
In this epoch 4800/5000, Training loss: 0.0001, Test loss: 12729.6328
In this epoch 4850/5000, Training loss: 448.8788, Test loss: 13507.5029
In this epoch 4900/5000, Training loss: 22.3788, Test loss: 12873.3018
In this epoch 4950/5000, Training loss: 0.0053, Test loss: 12728.9990
In this epoch 5000/5000, Training loss: 0.0084, Test loss: 12729.7969
train_acc 1.0
test_acc 0.008823529411764706
{'y_pred': [494.63751220703125, 206.36170959472656, 154.88987731933594, 438.4615783691406, 192.83900451660156, 212.37393188476562, 132.89010620117188, 198.22747802734375, 144.53775024414062, 79.17891693115234, 414.18426513671875, 481.575927734375, 525.5043334960938, 89.14452362060547, 504.6236877441406, 186.86949157714844, 529.2824096679688, 10.097128868103027, 456.10223388671875, 359.0870361328125, 528.4208984375, 561.6604614257812, 109.2464370727539, 305.9378356933594, 499.3526611328125, 504.7029113769531, 549.4244995117188, 426.23309326171875, 256.93927001953125, 522.7506713867188, 537.2348022460938, 492.02783203125, 456.8128662109375, 552.9349365234375, 444.87152099609375, 460.21453857421875, 157.22421264648438, 331.4688720703125, 241.0485382080078, 252.0266571044922, 515.083984375, 98.78523254394531, 437.16278076171875, 197.49957275390625, 475.4000244140625, 132.64552307128906, 449.2657470703125, 119.46280670166016, 477.1288757324219, 467.9136047363281, 457.92620849609375, 311.4864807128906, 559.9501342773438, 290.365234375, 276.2331237792969, 504.9779052734375, 474.87689208984375, 179.9565887451172, 574.4069213867188, 108.74658966064453, 574.5891723632812, 541.6187133789062, 244.41822814941406, 565.0288696289062, 468.34942626953125, 87.6266098022461, 494.53729248046875, 473.31787109375, 509.7284851074219, 389.88592529296875, 385.1107177734375, 131.70050048828125, 102.24517059326172, 226.41075134277344, 492.264892578125, 485.78955078125, 470.9193420410156, 494.8925476074219, 183.76361083984375, 312.7754821777344, 407.220947265625, 481.7723083496094, 462.53216552734375, 318.45233154296875, 518.0364379882812, 401.68231201171875, 528.2308349609375, 197.3072509765625, 531.6681518554688, 332.18780517578125, 389.2981262207031, 134.34097290039062, 544.8816528320312, 532.4451904296875, 317.11395263671875, 531.46240234375, 497.041015625, 455.39666748046875, 444.6025695800781, 215.0655517578125, 510.83917236328125, 461.7895202636719, 551.3272705078125, 247.16827392578125, 394.9580078125, 134.62066650390625, 341.2451171875, 507.8839416503906, 173.23655700683594, 271.90789794921875, 542.087158203125, 468.3001708984375, 210.73208618164062, 146.6060791015625, 143.17489624023438, 266.73468017578125, 511.0898742675781, 229.7495574951172, 205.44973754882812, 526.87109375, 104.39168548583984, 541.9524536132812, 275.48822021484375, 495.52447509765625, 23.775169372558594, 528.759521484375, 171.3416748046875, 145.56741333007812, 501.51434326171875, 88.57176971435547, 296.47119140625, 154.72361755371094, 372.0291442871094, 140.19960021972656, 62.27638626098633, 497.7708740234375, 443.0317077636719, 446.6688537597656, 514.9651489257812, 497.8877868652344, 495.56268310546875, 511.8216857910156, 212.37265014648438, 524.2369995117188, 159.84109497070312, 486.44073486328125, 466.4527282714844, 477.8922119140625, 533.5266723632812, 208.5015411376953, 134.89877319335938, 504.8659362792969, 306.44305419921875, 228.189208984375, 136.7867889404297, 372.92816162109375, 473.24041748046875, 136.71710205078125, 188.02029418945312, 383.3415832519531, 455.8900451660156, 203.5083465576172, 249.60302734375, 492.5894775390625, 497.51519775390625, 513.1497192382812, 274.9707336425781, 164.2417755126953, 507.46649169921875, 472.8446350097656, 519.9590454101562, 493.1941833496094, 134.53919982910156, 545.2105102539062, 456.3717346191406, 472.8692321777344, 174.11859130859375, 193.72776794433594, 474.67254638671875, 28.020673751831055, 516.9295654296875, 437.24224853515625, 339.16845703125, 121.37995910644531, 532.8995361328125, 440.23455810546875, 426.1075134277344, 473.95416259765625, 419.25390625, 500.7193908691406, 479.87347412109375, 498.4396667480469, 170.4534149169922, 99.6898193359375, 226.23387145996094, 432.5535583496094, 132.79812622070312, 481.9729919433594, 260.7403564453125, 557.1928100585938, 106.62348175048828, 304.0342712402344, 411.83294677734375, 503.753173828125, 402.0498046875, 218.78709411621094, 52.16239547729492, 480.721923828125, 314.1437072753906, 143.12515258789062, 126.96566009521484, 524.9583129882812, 500.2709045410156, 386.8516845703125, 131.33282470703125, 488.0122375488281, 476.9691162109375, 416.1378173828125, 242.7269287109375, 499.1017150878906, 193.68939208984375, 504.03173828125, 83.78588104248047, 111.61398315429688, 77.3966293334961, 453.4689636230469, 180.92514038085938, 526.5282592773438, 513.8839721679688, 497.90203857421875, 497.19781494140625, 491.0468444824219, 448.3898010253906, 351.96405029296875, 391.998779296875, 450.74542236328125, 118.18879699707031, 470.9138488769531, 178.87188720703125, 550.2938232421875, 111.35943603515625, 461.6280822753906, 427.18218994140625, 252.27639770507812, 180.0762939453125, 345.40240478515625, 544.796142578125, 179.22161865234375, 127.29680633544922, 202.75830078125, 384.59466552734375, 185.50735473632812, 457.0516357421875, 281.09881591796875, 330.2409362792969, 268.1345520019531, 551.297119140625, 427.3179626464844, 415.6892395019531, 545.024169921875, 441.24481201171875, 201.4887237548828, 216.2290802001953, 210.4847412109375, 119.90579986572266, 392.02239990234375, 523.1585083007812, 318.47052001953125, 169.26156616210938, 239.21876525878906, 538.9595336914062, 586.6989135742188, 452.9271240234375, 444.4543762207031, 147.76556396484375, 438.78717041015625, 515.90234375, 526.8654174804688, 329.9752197265625, 154.748779296875, 541.4909057617188, 475.9371337890625, 538.6710205078125, 442.9700012207031, 461.89617919921875, 112.8561782836914, 510.20758056640625, 69.32610321044922, 126.73182678222656, 191.0110626220703, 168.70787048339844, 272.1222229003906, 190.82464599609375, 248.8106231689453, 181.87835693359375, 274.8270263671875, 510.6431884765625, 492.84820556640625, 409.5855712890625, 576.3921508789062, 506.9107666015625, 500.6755676269531, 481.555908203125, 237.00674438476562, 455.13568115234375, 544.5725708007812, 518.3399047851562, 370.3433837890625, 144.69781494140625, 129.31593322753906, 523.4790649414062, 182.48681640625, 532.3465576171875, 470.6669006347656, 169.85186767578125, 239.21435546875, 494.7165222167969, 116.28845977783203, 401.597412109375, 232.22225952148438, 296.8017578125, 243.80245971679688, 103.73883819580078, 840.9249877929688, 294.5542297363281, 395.428466796875, 502.355712890625, 182.6644287109375, 214.4788818359375, 455.0419921875, 324.557373046875, 215.0955810546875, 454.0379333496094, 254.31643676757812, 601.6709594726562, 267.974853515625, 422.1029968261719, 570.842041015625, 496.3420715332031, 189.49501037597656], 'y_pred_int': [495, 206, 155, 438, 193, 212, 133, 198, 145, 79, 414, 482, 526, 89, 505, 187, 529, 10, 456, 359, 528, 562, 109, 306, 499, 505, 549, 426, 257, 523, 537, 492, 457, 553, 445, 460, 157, 331, 241, 252, 515, 99, 437, 197, 475, 133, 449, 119, 477, 468, 458, 311, 560, 290, 276, 505, 475, 180, 574, 109, 575, 542, 244, 565, 468, 88, 495, 473, 510, 390, 385, 132, 102, 226, 492, 486, 471, 495, 184, 313, 407, 482, 463, 318, 518, 402, 528, 197, 532, 332, 389, 134, 545, 532, 317, 531, 497, 455, 445, 215, 511, 462, 551, 247, 395, 135, 341, 508, 173, 272, 542, 468, 211, 147, 143, 267, 511, 230, 205, 527, 104, 542, 275, 496, 24, 529, 171, 146, 502, 89, 296, 155, 372, 140, 62, 498, 443, 447, 515, 498, 496, 512, 212, 524, 160, 486, 466, 478, 534, 209, 135, 505, 306, 228, 137, 373, 473, 137, 188, 383, 456, 204, 250, 493, 498, 513, 275, 164, 507, 473, 520, 493, 135, 545, 456, 473, 174, 194, 475, 28, 517, 437, 339, 121, 533, 440, 426, 474, 419, 501, 480, 498, 170, 100, 226, 433, 133, 482, 261, 557, 107, 304, 412, 504, 402, 219, 52, 481, 314, 143, 127, 525, 500, 387, 131, 488, 477, 416, 243, 499, 194, 504, 84, 112, 77, 453, 181, 527, 514, 498, 497, 491, 448, 352, 392, 451, 118, 471, 179, 550, 111, 462, 427, 252, 180, 345, 545, 179, 127, 203, 385, 186, 457, 281, 330, 268, 551, 427, 416, 545, 441, 201, 216, 210, 120, 392, 523, 318, 169, 239, 539, 587, 453, 444, 148, 439, 516, 527, 330, 155, 541, 476, 539, 443, 462, 113, 510, 69, 127, 191, 169, 272, 191, 249, 182, 275, 511, 493, 410, 576, 507, 501, 482, 237, 455, 545, 518, 370, 145, 129, 523, 182, 532, 471, 170, 239, 495, 116, 402, 232, 297, 244, 104, 841, 295, 395, 502, 183, 214, 455, 325, 215, 454, 254, 602, 268, 422, 571, 496, 189], 'y_true': [448, 221, 155, 350, 180, 182, 135, 183, 155, 82, 329, 412, 446, 76, 455, 203, 790, 37, 523, 297, 537, 440, 92, 280, 458, 437, 718, 405, 228, 388, 881, 849, 402, 543, 346, 341, 172, 257, 241, 193, 487, 80, 521, 204, 372, 160, 448, 110, 690, 350, 546, 273, 378, 274, 267, 535, 355, 148, 912, 96, 721, 418, 280, 770, 837, 83, 889, 328, 498, 327, 316, 144, 93, 215, 497, 391, 437, 523, 172, 260, 284, 376, 600, 268, 492, 296, 327, 190, 366, 322, 312, 125, 443, 723, 290, 397, 330, 384, 624, 233, 478, 610, 705, 224, 351, 146, 301, 613, 168, 237, 828, 505, 212, 187, 147, 300, 433, 207, 199, 431, 139, 607, 239, 370, 32, 657, 149, 115, 391, 110, 276, 160, 295, 147, 87, 527, 339, 302, 507, 813, 792, 486, 217, 814, 130, 562, 474, 372, 670, 189, 117, 708, 233, 211, 156, 373, 333, 109, 166, 321, 587, 216, 196, 367, 370, 350, 262, 161, 527, 516, 441, 562, 155, 686, 374, 431, 187, 217, 731, 35, 552, 445, 327, 102, 429, 372, 574, 380, 456, 308, 384, 358, 185, 100, 214, 857, 103, 356, 205, 1060, 89, 249, 328, 552, 300, 241, 57, 599, 284, 143, 111, 507, 799, 331, 122, 365, 557, 297, 255, 539, 200, 380, 84, 141, 65, 463, 167, 530, 388, 778, 694, 649, 324, 285, 358, 565, 125, 663, 166, 748, 103, 367, 376, 244, 176, 254, 401, 201, 155, 165, 324, 209, 370, 280, 279, 242, 466, 362, 350, 761, 373, 221, 207, 209, 110, 351, 471, 272, 172, 258, 778, 574, 455, 624, 101, 364, 445, 679, 260, 185, 308, 407, 422, 440, 436, 88, 697, 58, 103, 200, 201, 238, 187, 235, 146, 301, 724, 659, 340, 550, 605, 392, 832, 233, 334, 579, 514, 270, 157, 122, 656, 163, 445, 572, 126, 236, 387, 121, 294, 210, 266, 237, 97, 840, 238, 254, 373, 202, 237, 756, 270, 195, 741, 257, 823, 292, 339, 484, 831, 222], 'train_acc': 1.0, 'test_acc': 0.008823529411764706, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_1_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 19094.6445, Test loss: 24042.0840
In this epoch 100/5000, Training loss: 87.2137, Test loss: 17307.3672
In this epoch 150/5000, Training loss: 0.4219, Test loss: 17279.8691
In this epoch 200/5000, Training loss: 0.0024, Test loss: 17274.2070
In this epoch 250/5000, Training loss: 0.0006, Test loss: 17275.4160
In this epoch 300/5000, Training loss: 0.0006, Test loss: 17275.3730
In this epoch 350/5000, Training loss: 0.0005, Test loss: 17275.4102
In this epoch 400/5000, Training loss: 0.0005, Test loss: 17275.4492
In this epoch 450/5000, Training loss: 0.0005, Test loss: 17275.4863
In this epoch 500/5000, Training loss: 0.0004, Test loss: 17275.5234
In this epoch 550/5000, Training loss: 0.0004, Test loss: 17275.5527
In this epoch 600/5000, Training loss: 0.0004, Test loss: 17275.5879
In this epoch 650/5000, Training loss: 0.0004, Test loss: 17275.6152
In this epoch 700/5000, Training loss: 0.0004, Test loss: 17275.6348
In this epoch 750/5000, Training loss: 0.0004, Test loss: 17275.6562
In this epoch 800/5000, Training loss: 0.0004, Test loss: 17275.6719
In this epoch 850/5000, Training loss: 0.0004, Test loss: 17275.6855
In this epoch 900/5000, Training loss: 0.0004, Test loss: 17275.7012
In this epoch 950/5000, Training loss: 0.0004, Test loss: 17275.7109
In this epoch 1000/5000, Training loss: 0.0004, Test loss: 17275.7188
In this epoch 1050/5000, Training loss: 0.0004, Test loss: 17275.7246
In this epoch 1100/5000, Training loss: 0.0004, Test loss: 17275.7305
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 17275.7324
In this epoch 1200/5000, Training loss: 0.0004, Test loss: 17275.7363
In this epoch 1250/5000, Training loss: 0.0004, Test loss: 17275.7383
In this epoch 1300/5000, Training loss: 0.0004, Test loss: 17275.7383
In this epoch 1350/5000, Training loss: 0.0004, Test loss: 17275.7402
In this epoch 1400/5000, Training loss: 0.0004, Test loss: 17275.7422
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 17275.7422
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 17275.7422
In this epoch 1550/5000, Training loss: 0.0004, Test loss: 17275.7422
In this epoch 1600/5000, Training loss: 0.0004, Test loss: 17275.7422
In this epoch 1650/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 1700/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 17275.7422
In this epoch 1900/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 1950/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2000/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2050/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2250/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2300/5000, Training loss: 0.0004, Test loss: 17275.7422
In this epoch 2350/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2400/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2450/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2600/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2650/5000, Training loss: 0.0004, Test loss: 17275.7441
In this epoch 2700/5000, Training loss: 924664.0000, Test loss: 179112.4375
In this epoch 2750/5000, Training loss: 8536.8262, Test loss: 17962.4570
In this epoch 2800/5000, Training loss: 47.0884, Test loss: 17268.6406
In this epoch 2850/5000, Training loss: 0.0869, Test loss: 17290.2461
In this epoch 2900/5000, Training loss: 0.0012, Test loss: 17295.2852
In this epoch 2950/5000, Training loss: 0.0004, Test loss: 17294.7090
In this epoch 3000/5000, Training loss: 0.0004, Test loss: 17294.6621
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 17294.6582
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 17294.6602
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 17294.6621
In this epoch 3200/5000, Training loss: 0.0004, Test loss: 17294.6621
In this epoch 3250/5000, Training loss: 0.0004, Test loss: 17294.6621
In this epoch 3300/5000, Training loss: 0.0004, Test loss: 17294.6621
In this epoch 3350/5000, Training loss: 0.0004, Test loss: 17294.6641
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 17294.6621
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 3500/5000, Training loss: 0.0004, Test loss: 17294.6621
In this epoch 3550/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 3600/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 3650/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 3700/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 3750/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 3800/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 3850/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 3900/5000, Training loss: 0.0004, Test loss: 17294.6641
In this epoch 3950/5000, Training loss: 0.0004, Test loss: 17294.6641
In this epoch 4000/5000, Training loss: 0.0004, Test loss: 17294.6621
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 17294.6641
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4200/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4350/5000, Training loss: 0.0004, Test loss: 17294.6641
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4450/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4500/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4600/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4650/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 17294.6660
In this epoch 5000/5000, Training loss: 0.0004, Test loss: 17294.6660
train_acc 0.9992647058823529
test_acc 0.014705882352941176
{'y_pred': [191.27127075195312, 556.5685424804688, 138.6293487548828, 221.74853515625, 538.517333984375, 500.2837219238281, 482.2730712890625, 225.70294189453125, 132.41555786132812, 369.65594482421875, 402.5477600097656, 292.6971740722656, 217.95748901367188, 470.4922180175781, 525.980224609375, 368.5194091796875, 300.8751525878906, 326.0095520019531, 80.92848205566406, 449.19134521484375, 135.85549926757812, 184.8134307861328, 516.8905639648438, 456.0888977050781, 198.07901000976562, 167.40902709960938, 229.7486114501953, 563.993408203125, 508.86029052734375, 459.2339782714844, 485.6394348144531, 490.927734375, 521.7055053710938, 245.51785278320312, 174.39715576171875, 396.74853515625, 490.22760009765625, 501.19598388671875, 504.8125915527344, 142.47982788085938, 515.7755737304688, 563.7811889648438, 182.572021484375, 81.82843017578125, 133.8167724609375, 52.629310607910156, 545.2147216796875, 281.9175720214844, 128.7078857421875, 538.4019165039062, 343.2891845703125, 436.7435302734375, 257.3740234375, 103.467041015625, 182.39239501953125, 320.337158203125, 168.14466857910156, 499.88677978515625, 568.9822387695312, 508.2294006347656, 499.4372253417969, 331.13079833984375, 359.56829833984375, 498.1769104003906, 123.989501953125, 349.73492431640625, 128.3602752685547, 531.9810791015625, 565.606689453125, 467.28558349609375, 124.48040771484375, 493.8520202636719, 506.67645263671875, 516.2048950195312, 273.93798828125, 456.77203369140625, 436.50439453125, 177.35043334960938, 120.21073913574219, 477.2825622558594, 502.4043884277344, 93.32756805419922, 544.3512573242188, 492.9852294921875, 538.7644653320312, 153.0260009765625, 300.3251037597656, 310.7880859375, 62.226806640625, 84.76255798339844, 545.4603271484375, 198.57391357421875, 314.54547119140625, 238.1895751953125, 506.3499755859375, 159.80194091796875, 233.51651000976562, 528.8051147460938, 110.29536437988281, 504.9628601074219, 111.73237609863281, 264.0184326171875, 504.91015625, 27.853883743286133, 505.236572265625, 242.05963134765625, 479.22021484375, 234.81982421875, 493.3177185058594, 523.7017211914062, 440.50982666015625, 389.04632568359375, 50.353118896484375, 391.62042236328125, 496.4429931640625, 102.07386779785156, 543.9898071289062, 493.0469970703125, 82.58575439453125, 125.263671875, 555.2101440429688, 493.6324768066406, 436.32586669921875, 406.25885009765625, 557.6036376953125, 227.59246826171875, 394.53814697265625, 448.05316162109375, 448.6087646484375, 498.5453186035156, 106.15802001953125, 416.34918212890625, 496.59869384765625, 429.78704833984375, 548.014404296875, 508.1602783203125, 577.0176391601562, 562.3383178710938, 510.9068603515625, 513.4782104492188, 239.73956298828125, 169.9269561767578, 551.5846557617188, 456.59576416015625, 541.0569458007812, 118.9005355834961, 499.36334228515625, 525.4115600585938, 256.37945556640625, 332.1559143066406, 536.3640747070312, 515.2538452148438, 123.75666809082031, 114.70902252197266, 268.2372741699219, 516.8744506835938, 154.26113891601562, 271.72735595703125, 474.75128173828125, 67.66378784179688, 464.746826171875, 356.5195007324219, 273.573486328125, 56.810203552246094, 401.90362548828125, 68.76805114746094, 432.0440368652344, 125.64470672607422, 169.636474609375, 488.5655517578125, 76.70724487304688, 193.42666625976562, 271.9190979003906, 351.75982666015625, 481.2845458984375, 518.9619140625, 503.3377380371094, 446.4905090332031, 474.4100036621094, 226.92181396484375, 384.3851013183594, 462.9432373046875, 78.83748626708984, 491.8349609375, 320.8185729980469, 202.45211791992188, 220.98480224609375, 127.46769714355469, 215.6416778564453, 95.92991638183594, 517.2816772460938, 118.01980590820312, 66.56232452392578, 373.0, 53.374916076660156, 203.73451232910156, 525.6431274414062, 222.21466064453125, 86.24981689453125, 208.9938507080078, 524.1725463867188, 55.676082611083984, 106.19355773925781, 185.0362091064453, 475.26611328125, 375.73760986328125, 496.7679138183594, 452.8089904785156, 130.71066284179688, 474.0940246582031, 110.05543518066406, 553.5126342773438, 178.64015197753906, 302.733642578125, 55.95733642578125, 133.6729736328125, 376.10382080078125, 512.3841552734375, 520.4620971679688, 550.6905517578125, 551.7117919921875, 490.59173583984375, 294.2579040527344, 337.53460693359375, 168.5419921875, 264.9568176269531, 471.68603515625, 516.2506713867188, 202.7293701171875, 113.71956634521484, 530.5148315429688, 524.7666625976562, 200.29754638671875, 269.3433837890625, 225.93374633789062, 263.70294189453125, 515.6925659179688, 467.66925048828125, 55.96403503417969, 515.0889892578125, 448.1711730957031, 498.4333801269531, 480.526611328125, 198.01406860351562, 460.38201904296875, 100.9787368774414, 217.53062438964844, 590.0777587890625, 465.94073486328125, 251.15895080566406, 294.8493347167969, 492.6114196777344, 506.8441162109375, 450.34588623046875, 537.2847290039062, 434.4315490722656, 550.8986206054688, 190.76504516601562, 205.79335021972656, 143.80572509765625, 499.2679748535156, 203.6138916015625, 207.4418182373047, 411.9520568847656, 464.21630859375, 513.6697387695312, 141.23733520507812, 514.877685546875, 510.3386535644531, 468.46771240234375, 252.357421875, 132.2113037109375, 455.1697082519531, 435.72833251953125, 202.69973754882812, 536.9847412109375, 441.30706787109375, 400.40960693359375, 53.28045654296875, 482.0748291015625, 397.55010986328125, 480.27716064453125, 262.17626953125, 520.8510131835938, 445.0024719238281, 369.07977294921875, 479.20428466796875, 364.5016174316406, 49.124847412109375, 266.28533935546875, 155.303466796875, 305.583740234375, 486.430908203125, 423.21685791015625, 251.81130981445312, 518.8890380859375, 478.281982421875, 536.8734741210938, 204.46473693847656, 549.1412963867188, 294.2767639160156, 524.8024291992188, 146.36508178710938, 498.76544189453125, 486.1163024902344, 174.02926635742188, 321.09014892578125, 500.081298828125, 177.7976531982422, 154.99996948242188, 91.90296936035156, 496.81036376953125, 98.40447998046875, 438.1129150390625, 504.39105224609375, 123.13522338867188, 95.68531036376953, 424.3387451171875, 327.900146484375, 492.16461181640625, 560.7680053710938, 206.32437133789062, 193.5, 442.5779724121094, 495.9609069824219, 471.5697021484375, 231.5007781982422, 243.237060546875, 152.2870635986328, 120.18836975097656, 507.3612365722656, 456.49566650390625, 459.31939697265625, 111.90723419189453, 121.15438079833984, 239.93849182128906, 144.86154174804688, 63.394874572753906, 531.3689575195312, 294.92962646484375], 'y_pred_int': [191, 557, 139, 222, 539, 500, 482, 226, 132, 370, 403, 293, 218, 470, 526, 369, 301, 326, 81, 449, 136, 185, 517, 456, 198, 167, 230, 564, 509, 459, 486, 491, 522, 246, 174, 397, 490, 501, 505, 142, 516, 564, 183, 82, 134, 53, 545, 282, 129, 538, 343, 437, 257, 103, 182, 320, 168, 500, 569, 508, 499, 331, 360, 498, 124, 350, 128, 532, 566, 467, 124, 494, 507, 516, 274, 457, 437, 177, 120, 477, 502, 93, 544, 493, 539, 153, 300, 311, 62, 85, 545, 199, 315, 238, 506, 160, 234, 529, 110, 505, 112, 264, 505, 28, 505, 242, 479, 235, 493, 524, 441, 389, 50, 392, 496, 102, 544, 493, 83, 125, 555, 494, 436, 406, 558, 228, 395, 448, 449, 499, 106, 416, 497, 430, 548, 508, 577, 562, 511, 513, 240, 170, 552, 457, 541, 119, 499, 525, 256, 332, 536, 515, 124, 115, 268, 517, 154, 272, 475, 68, 465, 357, 274, 57, 402, 69, 432, 126, 170, 489, 77, 193, 272, 352, 481, 519, 503, 446, 474, 227, 384, 463, 79, 492, 321, 202, 221, 127, 216, 96, 517, 118, 67, 373, 53, 204, 526, 222, 86, 209, 524, 56, 106, 185, 475, 376, 497, 453, 131, 474, 110, 554, 179, 303, 56, 134, 376, 512, 520, 551, 552, 491, 294, 338, 169, 265, 472, 516, 203, 114, 531, 525, 200, 269, 226, 264, 516, 468, 56, 515, 448, 498, 481, 198, 460, 101, 218, 590, 466, 251, 295, 493, 507, 450, 537, 434, 551, 191, 206, 144, 499, 204, 207, 412, 464, 514, 141, 515, 510, 468, 252, 132, 455, 436, 203, 537, 441, 400, 53, 482, 398, 480, 262, 521, 445, 369, 479, 365, 49, 266, 155, 306, 486, 423, 252, 519, 478, 537, 204, 549, 294, 525, 146, 499, 486, 174, 321, 500, 178, 155, 92, 497, 98, 438, 504, 123, 96, 424, 328, 492, 561, 206, 194, 443, 496, 472, 232, 243, 152, 120, 507, 456, 459, 112, 121, 240, 145, 63, 531, 295], 'y_true': [200, 955, 147, 188, 912, 694, 708, 205, 139, 264, 350, 280, 163, 458, 523, 305, 301, 280, 88, 436, 116, 166, 1027, 310, 159, 124, 211, 823, 428, 495, 510, 367, 361, 165, 161, 376, 445, 726, 433, 147, 907, 901, 155, 94, 203, 113, 380, 207, 181, 658, 340, 369, 241, 108, 178, 256, 146, 771, 504, 361, 705, 296, 284, 756, 144, 305, 156, 743, 634, 405, 137, 387, 516, 355, 266, 608, 312, 187, 169, 389, 425, 82, 718, 288, 512, 164, 277, 275, 88, 109, 685, 192, 243, 209, 934, 155, 190, 397, 103, 572, 153, 218, 308, 35, 783, 207, 446, 237, 689, 511, 962, 340, 61, 360, 438, 136, 679, 478, 100, 134, 503, 350, 324, 300, 768, 209, 351, 347, 375, 778, 100, 438, 761, 323, 787, 598, 541, 484, 878, 354, 201, 160, 911, 607, 779, 118, 403, 494, 222, 288, 802, 1064, 135, 154, 214, 514, 167, 249, 516, 108, 629, 270, 200, 58, 350, 76, 569, 109, 163, 762, 108, 165, 244, 235, 384, 431, 569, 443, 445, 226, 321, 379, 84, 395, 285, 171, 189, 122, 223, 112, 935, 130, 39, 373, 83, 182, 794, 202, 100, 193, 617, 80, 128, 146, 506, 322, 346, 404, 162, 487, 136, 484, 199, 292, 55, 127, 326, 392, 617, 752, 495, 436, 260, 280, 176, 241, 339, 444, 203, 118, 507, 446, 185, 279, 214, 243, 515, 1060, 62, 506, 353, 545, 579, 196, 372, 80, 220, 833, 467, 244, 230, 507, 603, 792, 790, 362, 805, 239, 197, 164, 445, 159, 201, 401, 659, 611, 121, 485, 548, 643, 214, 160, 434, 518, 168, 328, 342, 283, 63, 762, 334, 351, 191, 396, 370, 335, 382, 295, 57, 215, 167, 244, 570, 390, 247, 397, 453, 461, 200, 753, 211, 407, 170, 533, 373, 153, 334, 1238, 173, 155, 123, 612, 106, 417, 461, 132, 108, 348, 337, 403, 701, 203, 180, 318, 403, 589, 249, 232, 142, 100, 334, 522, 484, 89, 105, 212, 162, 87, 367, 289], 'train_acc': 0.9992647058823529, 'test_acc': 0.014705882352941176, 'X': 'X_TANL_layer_1_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_2_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 26881.7012, Test loss: 44027.0508
In this epoch 100/5000, Training loss: 62.2605, Test loss: 16184.3311
In this epoch 150/5000, Training loss: 0.2853, Test loss: 16263.3135
In this epoch 200/5000, Training loss: 0.0008, Test loss: 16276.6914
In this epoch 250/5000, Training loss: 0.0005, Test loss: 16274.8066
In this epoch 300/5000, Training loss: 0.0004, Test loss: 16274.6475
In this epoch 350/5000, Training loss: 0.0003, Test loss: 16274.6211
In this epoch 400/5000, Training loss: 0.0003, Test loss: 16274.5859
In this epoch 450/5000, Training loss: 0.0002, Test loss: 16274.5566
In this epoch 500/5000, Training loss: 0.0001, Test loss: 16274.5293
In this epoch 550/5000, Training loss: 0.0001, Test loss: 16274.5029
In this epoch 600/5000, Training loss: 0.0001, Test loss: 16274.4795
In this epoch 650/5000, Training loss: 0.0001, Test loss: 16274.4590
In this epoch 700/5000, Training loss: 0.0000, Test loss: 16274.4414
In this epoch 750/5000, Training loss: 0.0000, Test loss: 16274.4258
In this epoch 800/5000, Training loss: 0.0000, Test loss: 16274.4092
In this epoch 850/5000, Training loss: 0.0000, Test loss: 16274.4004
In this epoch 900/5000, Training loss: 0.0000, Test loss: 16274.3867
In this epoch 950/5000, Training loss: 0.0000, Test loss: 16274.3809
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 16274.3770
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 16274.3711
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 16274.3623
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 16274.3604
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 16274.3594
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 16274.3564
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 16274.3564
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 16274.3564
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 16274.3535
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 16274.3535
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 16274.3516
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 16274.3486
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 16274.3486
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 16274.3477
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 16274.3486
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 16274.3486
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 16274.3486
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 16274.3477
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 16274.3486
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 16274.3477
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 16274.3477
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 16274.3486
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 16274.3477
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 16274.3486
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 16274.3506
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 16274.3486
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 16274.3486
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 16274.3486
In this epoch 3050/5000, Training loss: 1654738.5000, Test loss: 1707442.5000
In this epoch 3100/5000, Training loss: 6575.4243, Test loss: 23726.5449
In this epoch 3150/5000, Training loss: 53.2126, Test loss: 16477.2715
In this epoch 3200/5000, Training loss: 0.2240, Test loss: 16291.0889
In this epoch 3250/5000, Training loss: 0.0012, Test loss: 16288.5928
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 16289.3740
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 16289.4355
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 16289.4385
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 16289.4385
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 16289.4375
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 16289.4355
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 16289.4355
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 16289.4355
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 16289.4346
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 16289.4355
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 16289.4355
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 16289.4346
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 16289.4346
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 16289.4346
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 16289.4346
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 16289.4346
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 16289.4346
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 16289.4326
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 16289.4326
train_acc 1.0
test_acc 0.014705882352941176
{'y_pred': [484.9565124511719, 553.1864013671875, 150.07968139648438, 190.44020080566406, 179.29879760742188, 208.82608032226562, 517.3564453125, 424.929443359375, 564.1921997070312, 539.1614990234375, 516.2562866210938, 601.8157348632812, 488.09808349609375, 529.4835815429688, 88.09027862548828, 458.6770324707031, 490.1789855957031, 560.1124877929688, 63.57879638671875, 117.50619506835938, 108.52609252929688, 346.0204772949219, 487.4814147949219, 484.2315368652344, 479.53717041015625, 184.841552734375, 311.6096496582031, 446.3041687011719, 447.89068603515625, 214.677001953125, 108.19149780273438, 91.26617431640625, 455.3953552246094, 99.11465454101562, 544.8047485351562, 468.54827880859375, 277.055419921875, 382.4096984863281, 474.955078125, 198.93536376953125, 76.606689453125, 251.92828369140625, 467.66546630859375, 145.24261474609375, 518.2890625, 430.5669860839844, 164.56539916992188, 135.640869140625, 260.4404602050781, 520.0872192382812, 214.5393829345703, 197.35736083984375, 521.7977905273438, 159.61729431152344, 124.17919158935547, 249.71627807617188, 381.8365173339844, 510.6582336425781, 117.58985137939453, 193.05067443847656, 405.0540466308594, 48.944252014160156, 249.58770751953125, 129.42416381835938, 550.4667358398438, 497.5207214355469, 450.3916320800781, 519.4862670898438, 217.31468200683594, 285.8968200683594, 494.1241149902344, 446.0785827636719, 186.26931762695312, 93.2382583618164, 402.7960205078125, 131.33810424804688, 113.36465454101562, 534.009765625, 176.36148071289062, 449.9057922363281, 553.4099731445312, 501.5921325683594, 129.18276977539062, 431.8597717285156, 503.96124267578125, 464.38787841796875, 171.73988342285156, 288.6255798339844, 173.19937133789062, 531.0015258789062, 368.6542663574219, 186.0612030029297, 240.92999267578125, 166.55108642578125, 82.57763671875, 563.3759765625, 165.6551971435547, 517.90087890625, 228.47650146484375, 351.0829772949219, 155.12208557128906, 485.9533996582031, 487.51031494140625, 493.6315612792969, 182.9074249267578, 482.3511657714844, 143.34869384765625, 546.3811645507812, 546.6956176757812, 198.13702392578125, 493.4183349609375, 217.7830047607422, 495.0553283691406, 531.1166381835938, 329.79931640625, 143.0880126953125, 327.70306396484375, 468.7602844238281, 289.07647705078125, 518.027587890625, 515.9049072265625, 582.5291748046875, 524.0878295898438, 255.26913452148438, 515.6528930664062, 307.9463806152344, 465.7506408691406, 474.4461975097656, 156.31004333496094, 547.2560424804688, 450.6938171386719, 409.5137939453125, 503.5597839355469, 161.76011657714844, 545.4334106445312, 158.64630126953125, 553.8678588867188, 386.8026428222656, 50.22398376464844, 439.8616943359375, 576.1616821289062, 529.23828125, 519.3912353515625, 188.0177001953125, 107.99359893798828, 239.17137145996094, 470.0723571777344, 225.7186279296875, 550.2705688476562, 497.1021728515625, 501.718017578125, 232.4042510986328, 542.25634765625, 97.01860809326172, 219.81690979003906, 462.7124938964844, 175.93585205078125, 504.1880187988281, 377.1551818847656, 398.0445251464844, 130.28570556640625, 505.2583923339844, 166.4967803955078, 451.58001708984375, 213.64796447753906, 420.7096862792969, 214.364013671875, 494.5897521972656, 166.8189239501953, 110.02546691894531, 163.22055053710938, 237.62672424316406, 559.6673583984375, 272.08953857421875, 545.3237915039062, 219.1220703125, 552.9992065429688, 314.37353515625, 347.86669921875, 515.5692138671875, 251.6862030029297, 471.5075988769531, 129.2120361328125, 455.3331604003906, 429.2889404296875, 417.4340515136719, 553.2361450195312, 114.393798828125, 535.064208984375, 489.3861389160156, 488.3315734863281, 557.3854370117188, 526.1644287109375, 310.68585205078125, 518.9970703125, 510.8776550292969, 444.6281433105469, 92.93869018554688, 77.17910766601562, 502.7059631347656, 540.1124877929688, 283.6546936035156, 45.48890686035156, 483.3619079589844, 123.40310668945312, 839.9999389648438, 409.7704162597656, 525.9893188476562, 61.83863067626953, 504.0246887207031, 154.17247009277344, 553.9423217773438, 496.2241516113281, 361.2317810058594, 532.1836547851562, 264.2423400878906, 399.5999450683594, 512.801513671875, 504.6128845214844, 175.57510375976562, 502.5618591308594, 154.34027099609375, 423.7730407714844, 577.8317260742188, 211.85816955566406, 225.6690673828125, 606.7587890625, 432.1254577636719, 138.38662719726562, 507.5437316894531, 511.7273864746094, 517.8405151367188, 444.4253234863281, 477.2909240722656, 118.2174072265625, 129.4340362548828, 199.74234008789062, 137.9151611328125, 519.6444702148438, 524.453125, 558.5027465820312, 521.6727294921875, 454.5658874511719, 537.6661987304688, 101.37222290039062, 499.856689453125, 148.22900390625, 534.302001953125, 70.76688385009766, 155.13108825683594, 252.49380493164062, 469.372802734375, 489.4424133300781, 507.3314514160156, 474.3742370605469, 147.99061584472656, 485.5051574707031, 164.98284912109375, 490.24041748046875, 170.09036254882812, 314.86053466796875, 177.1832275390625, 244.45281982421875, 383.44879150390625, 579.6782836914062, 256.15618896484375, 452.76959228515625, 466.07476806640625, 117.77269744873047, 428.9985656738281, 139.43722534179688, 112.44863891601562, 211.1817626953125, 90.41189575195312, 481.6811218261719, 648.6249389648438, 509.7656555175781, 224.63949584960938, 422.30316162109375, 539.7920532226562, 421.8720397949219, 534.7965698242188, 507.9251403808594, 193.43934631347656, 483.0252990722656, 232.5736541748047, 500.2932434082031, 539.5469360351562, 495.73675537109375, 99.35247802734375, 338.1939392089844, 76.3131103515625, 431.5166015625, 93.4998779296875, 90.04520416259766, 193.30699157714844, 487.7607421875, 479.9174499511719, 560.4983520507812, 227.23394775390625, 245.15740966796875, 304.27239990234375, 573.3178100585938, 87.51521301269531, 499.5378112792969, 524.8685913085938, 501.3407287597656, 112.93955993652344, 167.97784423828125, 487.2463073730469, 166.22061157226562, 466.3065490722656, 397.7934265136719, 468.75823974609375, 178.87939453125, 222.9782257080078, 164.76641845703125, 482.47723388671875, 339.02313232421875, 573.7376098632812, 413.3251647949219, 508.684814453125, 511.760986328125, 476.4230651855469, 83.51087951660156, 427.01519775390625, 558.6627197265625, 510.9930725097656, 263.7716979980469, 137.46673583984375, 523.7352294921875, 477.90814208984375, 302.4920654296875, 503.9743957519531, 523.7968139648438, 502.1669006347656, 558.2261352539062, 296.25799560546875, 564.7601318359375, 471.2642517089844], 'y_pred_int': [485, 553, 150, 190, 179, 209, 517, 425, 564, 539, 516, 602, 488, 529, 88, 459, 490, 560, 64, 118, 109, 346, 487, 484, 480, 185, 312, 446, 448, 215, 108, 91, 455, 99, 545, 469, 277, 382, 475, 199, 77, 252, 468, 145, 518, 431, 165, 136, 260, 520, 215, 197, 522, 160, 124, 250, 382, 511, 118, 193, 405, 49, 250, 129, 550, 498, 450, 519, 217, 286, 494, 446, 186, 93, 403, 131, 113, 534, 176, 450, 553, 502, 129, 432, 504, 464, 172, 289, 173, 531, 369, 186, 241, 167, 83, 563, 166, 518, 228, 351, 155, 486, 488, 494, 183, 482, 143, 546, 547, 198, 493, 218, 495, 531, 330, 143, 328, 469, 289, 518, 516, 583, 524, 255, 516, 308, 466, 474, 156, 547, 451, 410, 504, 162, 545, 159, 554, 387, 50, 440, 576, 529, 519, 188, 108, 239, 470, 226, 550, 497, 502, 232, 542, 97, 220, 463, 176, 504, 377, 398, 130, 505, 166, 452, 214, 421, 214, 495, 167, 110, 163, 238, 560, 272, 545, 219, 553, 314, 348, 516, 252, 472, 129, 455, 429, 417, 553, 114, 535, 489, 488, 557, 526, 311, 519, 511, 445, 93, 77, 503, 540, 284, 45, 483, 123, 840, 410, 526, 62, 504, 154, 554, 496, 361, 532, 264, 400, 513, 505, 176, 503, 154, 424, 578, 212, 226, 607, 432, 138, 508, 512, 518, 444, 477, 118, 129, 200, 138, 520, 524, 559, 522, 455, 538, 101, 500, 148, 534, 71, 155, 252, 469, 489, 507, 474, 148, 486, 165, 490, 170, 315, 177, 244, 383, 580, 256, 453, 466, 118, 429, 139, 112, 211, 90, 482, 649, 510, 225, 422, 540, 422, 535, 508, 193, 483, 233, 500, 540, 496, 99, 338, 76, 432, 93, 90, 193, 488, 480, 560, 227, 245, 304, 573, 88, 500, 525, 501, 113, 168, 487, 166, 466, 398, 469, 179, 223, 165, 482, 339, 574, 413, 509, 512, 476, 84, 427, 559, 511, 264, 137, 524, 478, 302, 504, 524, 502, 558, 296, 565, 471], 'y_true': [351, 445, 162, 178, 200, 211, 403, 302, 505, 414, 397, 778, 374, 507, 74, 396, 370, 644, 84, 161, 78, 293, 827, 407, 373, 184, 285, 424, 355, 203, 109, 94, 669, 80, 418, 350, 224, 299, 492, 193, 60, 214, 417, 116, 544, 463, 194, 117, 258, 511, 198, 207, 441, 203, 158, 228, 311, 487, 150, 231, 663, 55, 234, 118, 758, 765, 309, 570, 191, 279, 552, 446, 138, 100, 351, 101, 103, 607, 161, 344, 743, 342, 139, 491, 504, 432, 181, 292, 155, 605, 318, 187, 241, 157, 86, 685, 192, 709, 243, 284, 127, 831, 494, 645, 201, 351, 143, 387, 783, 213, 448, 191, 350, 422, 280, 160, 259, 666, 249, 610, 778, 613, 452, 260, 392, 279, 434, 415, 181, 537, 391, 313, 1342, 140, 680, 159, 466, 347, 61, 348, 714, 537, 435, 135, 93, 212, 509, 187, 596, 714, 456, 212, 572, 88, 244, 317, 196, 493, 244, 310, 128, 391, 136, 363, 235, 911, 169, 532, 221, 105, 200, 243, 447, 266, 680, 207, 899, 277, 297, 466, 251, 455, 199, 386, 296, 339, 481, 138, 491, 445, 762, 643, 771, 312, 456, 662, 428, 105, 81, 542, 392, 268, 65, 372, 149, 841, 686, 1027, 132, 716, 180, 588, 487, 302, 542, 243, 351, 577, 898, 155, 446, 186, 390, 768, 256, 184, 1099, 327, 162, 786, 391, 408, 352, 475, 130, 143, 215, 158, 461, 541, 611, 431, 387, 370, 100, 412, 142, 478, 88, 171, 252, 833, 649, 367, 756, 113, 548, 167, 522, 225, 291, 174, 237, 321, 1238, 270, 719, 336, 114, 497, 156, 169, 207, 108, 562, 430, 502, 242, 488, 656, 490, 795, 516, 146, 343, 234, 761, 617, 775, 122, 280, 61, 367, 113, 89, 212, 446, 565, 901, 260, 231, 334, 711, 95, 458, 545, 392, 130, 180, 385, 117, 361, 401, 493, 156, 237, 162, 369, 317, 670, 360, 679, 477, 889, 56, 575, 641, 633, 228, 163, 540, 572, 264, 786, 350, 489, 389, 262, 490, 452], 'train_acc': 1.0, 'test_acc': 0.014705882352941176, 'X': 'X_TANL_layer_2_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_3_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 25849.6914, Test loss: 62865.1055
In this epoch 100/5000, Training loss: 13.6703, Test loss: 13835.6299
In this epoch 150/5000, Training loss: 2.1999, Test loss: 13825.4385
In this epoch 200/5000, Training loss: 0.0119, Test loss: 13814.1475
In this epoch 250/5000, Training loss: 0.0001, Test loss: 13814.9854
In this epoch 300/5000, Training loss: 0.0000, Test loss: 13815.0264
In this epoch 350/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 400/5000, Training loss: 0.0000, Test loss: 13815.0283
In this epoch 450/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 500/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 550/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 600/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 650/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 700/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 750/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 800/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 850/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 900/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 950/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 13815.0283
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 13815.0283
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 13815.0283
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 13815.0312
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 13815.0293
In this epoch 3200/5000, Training loss: 154419.6250, Test loss: 647394.9375
In this epoch 3250/5000, Training loss: 1633.9437, Test loss: 16659.0508
In this epoch 3300/5000, Training loss: 20.1358, Test loss: 13858.7812
In this epoch 3350/5000, Training loss: 0.0018, Test loss: 13873.6416
In this epoch 3400/5000, Training loss: 0.0002, Test loss: 13870.9209
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 13871.0049
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 13870.9971
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 13870.9990
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 13871.0000
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 13871.0000
train_acc 1.0
test_acc 0.0029411764705882353
{'y_pred': [354.50091552734375, 542.4675903320312, 542.403076171875, 475.2610168457031, 269.5025634765625, 563.3154296875, 520.0986938476562, 136.70860290527344, 345.10430908203125, 472.57373046875, 394.8790588378906, 594.989501953125, 545.9652099609375, 465.51031494140625, 78.78663635253906, 220.4925079345703, 524.575927734375, 506.40692138671875, 138.20619201660156, 528.0238037109375, 514.3568115234375, 550.35791015625, 91.53911590576172, 96.416259765625, 225.39515686035156, 457.17169189453125, 484.8785705566406, 358.7648620605469, 524.867919921875, 377.931884765625, 242.0801544189453, 201.77716064453125, 154.65560913085938, 104.391845703125, 76.8112564086914, 534.8695068359375, 543.346923828125, 487.0647277832031, 510.5418701171875, 527.5718994140625, 413.40264892578125, 522.6156005859375, 496.88848876953125, 173.9407196044922, 502.0286560058594, 452.071533203125, 406.60986328125, 468.9600830078125, 192.20960998535156, 107.98297882080078, 236.4664764404297, 559.927978515625, 291.22845458984375, 217.80615234375, 547.957763671875, 110.94375610351562, 212.8427276611328, 375.9049987792969, 295.7648010253906, 376.63580322265625, 546.32861328125, 530.3477172851562, 472.8529052734375, 512.1502075195312, 471.30560302734375, 489.607666015625, 120.50037384033203, 150.5148468017578, 468.9094543457031, 372.3709411621094, 446.45037841796875, 488.4544372558594, 428.61456298828125, 472.48541259765625, 562.7545166015625, 497.7281799316406, 465.1824951171875, 439.90399169921875, 508.02703857421875, 125.00617218017578, 509.49591064453125, 173.81069946289062, 122.77058410644531, 56.840824127197266, 528.2250366210938, 480.194580078125, 163.96023559570312, 468.121337890625, 140.39480590820312, 534.673583984375, 50.70131301879883, 243.4337921142578, 48.77930450439453, 180.12330627441406, 196.56893920898438, 525.2372436523438, 284.4251403808594, 168.2277374267578, 114.42359161376953, 609.8563842773438, 475.70440673828125, 185.4300537109375, 493.675537109375, 469.3121337890625, 271.37847900390625, 472.9905090332031, 495.586181640625, 511.8683166503906, 254.3374786376953, 596.6981201171875, 555.6070556640625, 525.4459228515625, 478.9620056152344, 345.561279296875, 77.71624755859375, 597.2589111328125, 101.06318664550781, 275.22955322265625, 420.67413330078125, 464.5458679199219, 500.1014404296875, 183.444091796875, 506.8470458984375, 201.55015563964844, 350.03643798828125, 556.6049194335938, 260.8681640625, 535.4677124023438, 242.20257568359375, 179.61831665039062, 607.0289916992188, 116.36260986328125, 353.85211181640625, 472.64788818359375, 343.0526428222656, 122.51590728759766, 535.5462036132812, 334.7003479003906, 520.499755859375, 477.72607421875, 611.15966796875, 534.8441162109375, 558.9944458007812, 840.0, 502.75048828125, 270.6385498046875, 502.74652099609375, 512.0595092773438, 84.82455444335938, 226.4264373779297, 566.989990234375, 360.92071533203125, 193.1217803955078, 331.1072998046875, 147.04470825195312, 433.0088806152344, 437.8927001953125, 287.0621337890625, 538.6929931640625, 314.3232421875, 466.9933166503906, 444.2197265625, 324.73211669921875, 433.3950500488281, 123.96484375, 140.2641143798828, 227.6374053955078, 326.88763427734375, 513.0545654296875, 530.272705078125, 183.76025390625, 518.2778930664062, 337.05419921875, 299.7252197265625, 181.76283264160156, 504.1466979980469, 228.52159118652344, 134.385498046875, 449.697021484375, 474.6134338378906, 538.5809326171875, 210.7114715576172, 99.087890625, 485.9980163574219, 100.8387451171875, 513.2691040039062, 421.92254638671875, 2.5895495414733887, 491.16986083984375, 514.208740234375, 540.6510009765625, 478.7728576660156, 530.666748046875, 523.1768798828125, 531.2955932617188, 139.47679138183594, 464.5184631347656, 236.6418914794922, 437.771728515625, 56.28262710571289, 144.21839904785156, 97.68274688720703, 515.2954711914062, 531.067138671875, 533.1138916015625, 426.6578369140625, 218.6106414794922, 517.201904296875, 166.13673400878906, 512.4081420898438, 549.1063232421875, 81.26203155517578, 326.5316162109375, 573.0200805664062, 120.64342498779297, 529.1932373046875, 82.6822509765625, 563.0947265625, 184.44686889648438, 208.03660583496094, 498.83917236328125, 507.4075927734375, 275.5033874511719, 223.6161346435547, 448.50537109375, 493.92034912109375, 470.6357421875, 119.50849914550781, 542.6212768554688, 583.1212158203125, 517.2159423828125, 202.1014404296875, 83.35503387451172, 588.9456787109375, 441.1772766113281, 181.03500366210938, 489.1104736328125, 161.99942016601562, 406.3030700683594, 382.74981689453125, 473.6767578125, 248.75196838378906, 493.4949035644531, 296.59307861328125, 584.32470703125, 280.6617126464844, 448.9590759277344, 570.24365234375, 242.6870880126953, 613.443115234375, 496.870361328125, 173.0440216064453, 522.21826171875, 381.9120178222656, 528.3680419921875, 125.49859619140625, 514.8214111328125, 466.2467346191406, 94.13666534423828, 536.8101806640625, 129.7323760986328, 498.3478698730469, 135.58055114746094, 228.7977752685547, 125.34226989746094, 376.31121826171875, 21.648439407348633, 493.0485534667969, 507.64752197265625, 579.5762939453125, 532.4714965820312, 123.84791564941406, 208.80726623535156, 596.1760864257812, 636.0279541015625, 265.4808349609375, 531.6949462890625, 178.01406860351562, 381.41314697265625, 483.2251281738281, 143.5980224609375, 465.26763916015625, 143.07083129882812, 53.31900405883789, 316.20867919921875, 191.8122100830078, 147.6774444580078, 480.2372131347656, 524.5655517578125, 577.6871948242188, 551.171875, 477.8502197265625, 281.6715393066406, 325.963134765625, 581.7982788085938, 589.5449829101562, 431.1219482421875, 178.6048126220703, 558.3489990234375, 163.23117065429688, 160.5137176513672, 222.21511840820312, 297.66827392578125, 427.3514099121094, 203.2696533203125, 438.3455505371094, 41.55870056152344, 342.56231689453125, 104.43109130859375, 334.2574768066406, 452.008544921875, 575.3343505859375, 516.3822021484375, 223.24208068847656, 423.0382080078125, 151.06137084960938, 91.11134338378906, 503.65081787109375, 501.7291259765625, 451.76959228515625, 498.3879699707031, 563.2901000976562, 219.53115844726562, 222.22943115234375, 209.41165161132812, 482.24139404296875, 408.03057861328125, 444.3631591796875, 586.9183349609375, 477.53961181640625, 215.6331024169922, 494.59320068359375, 132.34600830078125, 270.3670654296875, 184.3464813232422, 310.3108215332031, 530.082275390625, 620.3103637695312, 150.63832092285156, 267.7145080566406], 'y_pred_int': [355, 542, 542, 475, 270, 563, 520, 137, 345, 473, 395, 595, 546, 466, 79, 220, 525, 506, 138, 528, 514, 550, 92, 96, 225, 457, 485, 359, 525, 378, 242, 202, 155, 104, 77, 535, 543, 487, 511, 528, 413, 523, 497, 174, 502, 452, 407, 469, 192, 108, 236, 560, 291, 218, 548, 111, 213, 376, 296, 377, 546, 530, 473, 512, 471, 490, 121, 151, 469, 372, 446, 488, 429, 472, 563, 498, 465, 440, 508, 125, 509, 174, 123, 57, 528, 480, 164, 468, 140, 535, 51, 243, 49, 180, 197, 525, 284, 168, 114, 610, 476, 185, 494, 469, 271, 473, 496, 512, 254, 597, 556, 525, 479, 346, 78, 597, 101, 275, 421, 465, 500, 183, 507, 202, 350, 557, 261, 535, 242, 180, 607, 116, 354, 473, 343, 123, 536, 335, 520, 478, 611, 535, 559, 840, 503, 271, 503, 512, 85, 226, 567, 361, 193, 331, 147, 433, 438, 287, 539, 314, 467, 444, 325, 433, 124, 140, 228, 327, 513, 530, 184, 518, 337, 300, 182, 504, 229, 134, 450, 475, 539, 211, 99, 486, 101, 513, 422, 3, 491, 514, 541, 479, 531, 523, 531, 139, 465, 237, 438, 56, 144, 98, 515, 531, 533, 427, 219, 517, 166, 512, 549, 81, 327, 573, 121, 529, 83, 563, 184, 208, 499, 507, 276, 224, 449, 494, 471, 120, 543, 583, 517, 202, 83, 589, 441, 181, 489, 162, 406, 383, 474, 249, 493, 297, 584, 281, 449, 570, 243, 613, 497, 173, 522, 382, 528, 125, 515, 466, 94, 537, 130, 498, 136, 229, 125, 376, 22, 493, 508, 580, 532, 124, 209, 596, 636, 265, 532, 178, 381, 483, 144, 465, 143, 53, 316, 192, 148, 480, 525, 578, 551, 478, 282, 326, 582, 590, 431, 179, 558, 163, 161, 222, 298, 427, 203, 438, 42, 343, 104, 334, 452, 575, 516, 223, 423, 151, 91, 504, 502, 452, 498, 563, 220, 222, 209, 482, 408, 444, 587, 478, 216, 495, 132, 270, 184, 310, 530, 620, 151, 268], 'y_true': [298, 715, 643, 286, 260, 659, 404, 158, 314, 659, 296, 585, 447, 352, 90, 194, 898, 366, 105, 435, 807, 446, 112, 109, 197, 687, 392, 283, 621, 284, 237, 158, 147, 109, 80, 367, 487, 683, 425, 835, 337, 406, 385, 178, 518, 372, 376, 529, 213, 79, 187, 701, 260, 182, 901, 121, 241, 324, 240, 299, 556, 561, 629, 350, 428, 444, 82, 164, 570, 349, 347, 468, 331, 527, 738, 356, 507, 450, 577, 134, 521, 148, 113, 76, 599, 401, 146, 328, 163, 758, 77, 198, 88, 160, 167, 624, 280, 200, 104, 634, 384, 149, 635, 480, 269, 516, 369, 373, 256, 506, 901, 445, 367, 295, 89, 392, 142, 249, 373, 417, 380, 156, 396, 173, 284, 880, 252, 392, 212, 163, 863, 129, 313, 583, 272, 110, 796, 280, 440, 427, 732, 1060, 528, 841, 466, 251, 332, 497, 100, 217, 426, 266, 182, 279, 131, 471, 558, 258, 449, 260, 471, 333, 288, 420, 122, 143, 218, 274, 814, 601, 171, 453, 294, 237, 182, 660, 207, 138, 353, 547, 556, 164, 203, 429, 122, 432, 608, 42, 612, 390, 881, 338, 370, 837, 693, 151, 400, 239, 962, 108, 205, 94, 656, 392, 564, 339, 168, 548, 168, 786, 431, 74, 280, 688, 102, 649, 90, 464, 184, 143, 543, 661, 270, 188, 602, 827, 389, 95, 423, 500, 599, 187, 81, 511, 376, 216, 656, 159, 308, 308, 407, 165, 645, 256, 445, 258, 434, 523, 239, 747, 389, 226, 407, 290, 388, 116, 394, 292, 45, 431, 123, 491, 116, 210, 156, 295, 53, 664, 482, 447, 521, 132, 189, 641, 1062, 205, 685, 209, 331, 815, 153, 391, 156, 126, 256, 170, 165, 436, 425, 801, 468, 833, 259, 275, 805, 773, 351, 187, 828, 166, 139, 204, 294, 428, 193, 312, 54, 305, 101, 285, 384, 903, 439, 189, 296, 175, 89, 343, 471, 563, 792, 507, 193, 191, 195, 548, 326, 686, 459, 545, 199, 469, 129, 279, 167, 281, 459, 624, 178, 259], 'train_acc': 1.0, 'test_acc': 0.0029411764705882353, 'X': 'X_TANL_layer_3_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_4_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 13639.9609, Test loss: 68781.7109
In this epoch 100/5000, Training loss: 354.5798, Test loss: 17238.4824
In this epoch 150/5000, Training loss: 0.2335, Test loss: 17212.5098
In this epoch 200/5000, Training loss: 0.0051, Test loss: 17149.3125
In this epoch 250/5000, Training loss: 0.0004, Test loss: 17151.5938
In this epoch 300/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 350/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 400/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 450/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 500/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 550/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 600/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 650/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 700/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 750/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 800/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 850/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 900/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 950/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 1000/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 1050/5000, Training loss: 0.0004, Test loss: 17151.7441
In this epoch 1100/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 17151.7441
In this epoch 1200/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 1250/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 1300/5000, Training loss: 0.0004, Test loss: 17151.7441
In this epoch 1350/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 1400/5000, Training loss: 0.0004, Test loss: 17151.7441
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 1550/5000, Training loss: 0.0004, Test loss: 17151.7441
In this epoch 1600/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 1650/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 1700/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 1900/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 1950/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2000/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 2050/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2250/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2300/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2350/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2400/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2450/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 17151.7461
In this epoch 2600/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2650/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2700/5000, Training loss: 0.0004, Test loss: 17151.7480
In this epoch 2750/5000, Training loss: 3393958.7500, Test loss: 3539696.2500
In this epoch 2800/5000, Training loss: 6338.8848, Test loss: 21498.9219
In this epoch 2850/5000, Training loss: 139.7707, Test loss: 17349.3184
In this epoch 2900/5000, Training loss: 0.7289, Test loss: 17181.8242
In this epoch 2950/5000, Training loss: 0.0031, Test loss: 17164.6387
In this epoch 3000/5000, Training loss: 0.0004, Test loss: 17164.7832
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 17164.9590
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 17164.9531
In this epoch 3200/5000, Training loss: 0.0004, Test loss: 17164.9531
In this epoch 3250/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 3300/5000, Training loss: 0.0004, Test loss: 17164.9531
In this epoch 3350/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 17164.9531
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 17164.9531
In this epoch 3500/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 3550/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 3600/5000, Training loss: 0.0004, Test loss: 17164.9531
In this epoch 3650/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 3700/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 3750/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 3800/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 3850/5000, Training loss: 0.0004, Test loss: 17164.9531
In this epoch 3900/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 3950/5000, Training loss: 0.0004, Test loss: 17164.9531
In this epoch 4000/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4200/5000, Training loss: 0.0004, Test loss: 17164.9531
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4350/5000, Training loss: 0.0004, Test loss: 17164.9531
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4450/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4500/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4600/5000, Training loss: 0.0004, Test loss: 17164.9531
In this epoch 4650/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 17164.9512
In this epoch 5000/5000, Training loss: 0.0004, Test loss: 17164.9512
train_acc 0.9992647058823529
test_acc 0.008823529411764706
{'y_pred': [77.87861633300781, 594.608642578125, 479.4542236328125, 108.52823638916016, 544.8236694335938, 513.2239990234375, 367.38262939453125, 186.8188934326172, 357.4887390136719, 257.1347351074219, 506.4927978515625, 461.008056640625, 134.2511749267578, 568.2488403320312, 565.6033325195312, 556.8986206054688, 568.0769653320312, 441.58648681640625, 586.2677612304688, 474.6268310546875, 513.5260620117188, 557.3665161132812, 297.6041564941406, 397.42578125, 246.98008728027344, 98.78225708007812, 116.68883514404297, 558.0484008789062, 577.9334106445312, 568.6062622070312, 552.9829711914062, 293.9656982421875, 462.6056823730469, 136.3154296875, 549.3875122070312, 65.74330139160156, 515.16796875, 398.8785095214844, 490.83251953125, 490.60223388671875, 137.11111450195312, 256.8692932128906, 565.7202758789062, 572.3919677734375, 108.4560317993164, 72.58838653564453, 297.8941650390625, 152.27493286132812, 259.58892822265625, 290.9488830566406, 531.0797729492188, 187.6250457763672, 505.2059326171875, 271.35223388671875, 381.5519104003906, 128.04798889160156, 107.8494644165039, 186.0028839111328, 155.80856323242188, 547.7870483398438, 247.28878784179688, 93.4451675415039, 491.2913818359375, 486.03338623046875, 67.26838684082031, 473.76239013671875, 594.500244140625, 545.3988037109375, 169.13528442382812, 185.4091339111328, 479.3265075683594, 542.9578247070312, 415.5050048828125, 567.1031494140625, 578.6602172851562, 507.87689208984375, 470.78228759765625, 117.78245544433594, 484.4800109863281, 255.71823120117188, 505.94140625, 86.87567901611328, 59.2970085144043, 413.7491455078125, 307.5920715332031, 491.88214111328125, 250.9064178466797, 310.24298095703125, 265.7715759277344, 298.7158203125, 303.51971435546875, 385.74212646484375, 485.8749694824219, 479.97705078125, 200.0125274658203, 202.36891174316406, 544.101806640625, 583.3016967773438, 238.25265502929688, 196.11216735839844, 278.3861083984375, 531.7672119140625, 480.0686950683594, 71.78455352783203, 520.4558715820312, 189.20407104492188, 540.0284423828125, 613.6982421875, 189.64434814453125, 245.1804656982422, 122.06563568115234, 435.58331298828125, 217.71751403808594, 334.5301208496094, 206.4937286376953, 298.1005859375, 480.2088928222656, 559.6792602539062, 206.5530242919922, 60.60063552856445, 140.464599609375, 61.30763626098633, 535.89111328125, 417.02032470703125, 487.4897766113281, 453.2894287109375, 143.18919372558594, 507.7931823730469, 218.79640197753906, 498.6946716308594, 80.21430206298828, 583.0748901367188, 237.56163024902344, 498.2925109863281, 525.7650146484375, 87.66534423828125, 75.00617218017578, 601.7655639648438, 408.54010009765625, 541.7880249023438, 193.8682098388672, 188.96453857421875, 490.15704345703125, 529.869873046875, 537.2328491210938, 147.56129455566406, 538.4905395507812, 214.4237518310547, 310.83062744140625, 480.88909912109375, 160.83738708496094, 401.7354431152344, 250.1898956298828, 74.49222564697266, 602.2596435546875, 576.9048461914062, 496.06024169921875, 534.8626098632812, 245.83065795898438, 551.5292358398438, 462.0003662109375, 489.66827392578125, 296.552734375, 262.7686767578125, 521.4535522460938, 485.1070556640625, 193.37449645996094, 502.2509765625, 241.35325622558594, 355.59466552734375, 154.6400909423828, 271.0603332519531, 497.2281188964844, 585.88720703125, 515.13037109375, 146.3937225341797, 289.62591552734375, 542.01806640625, 175.650390625, 123.64513397216797, 74.3624496459961, 530.3053588867188, 463.7853698730469, 425.5457763671875, 472.122802734375, 133.09959411621094, 77.42277526855469, 307.9351806640625, 532.3280639648438, 481.0641784667969, 79.9511489868164, 429.8039245605469, 54.768943786621094, 464.5094299316406, 656.4178466796875, 401.3279724121094, 402.707763671875, 57.54048538208008, 242.06597900390625, 594.6417846679688, 441.69573974609375, 242.8953094482422, 254.04872131347656, 214.7804718017578, 568.3095703125, 565.39990234375, 550.9169311523438, 521.5508422851562, 180.60867309570312, 591.145263671875, 86.43108367919922, 37.76231002807617, 85.81262969970703, 417.7383728027344, 507.016845703125, 57.049983978271484, 458.5966796875, 559.0848999023438, 148.84768676757812, 190.7225799560547, 180.81578063964844, 108.2414779663086, 230.3174285888672, 120.2266616821289, 432.41424560546875, 547.9739379882812, 498.21514892578125, 172.16249084472656, 132.2207489013672, 477.1708679199219, 179.4761199951172, 247.4331512451172, 137.80819702148438, 522.0068969726562, 623.7260131835938, 214.17440795898438, 462.7880554199219, 185.11708068847656, 482.8385314941406, 448.13543701171875, 357.625, 202.46148681640625, 586.84033203125, 27.788183212280273, 369.4251403808594, 556.5961303710938, 187.9337158203125, 529.4453735351562, 409.6472473144531, 493.609375, 461.072509765625, 439.4023132324219, 101.75492095947266, 564.8587646484375, 125.76753997802734, 489.3189697265625, 124.30504608154297, 59.618919372558594, 121.44164276123047, 156.3241424560547, 489.7880859375, 452.93902587890625, 524.3699340820312, 123.2762680053711, 550.8198852539062, 532.0128784179688, 136.28262329101562, 578.9974365234375, 90.66297912597656, 125.72005462646484, 247.5138702392578, 534.8411254882812, 483.95819091796875, 181.7183074951172, 426.60968017578125, 211.1194305419922, 589.8492431640625, 117.36890411376953, 530.2705688476562, 560.1414184570312, 557.9620971679688, 528.2815551757812, 487.8512878417969, 92.60308074951172, 255.56149291992188, 501.13018798828125, 433.70703125, 224.5441131591797, 312.8567199707031, 192.2227325439453, 115.49784088134766, 155.00001525878906, 520.3261108398438, 211.93763732910156, 130.1548309326172, 369.1397399902344, 613.2246704101562, 470.82855224609375, 235.85984802246094, 178.31845092773438, 490.7783203125, 529.4268188476562, 502.30120849609375, 214.5501708984375, 97.46309661865234, 85.75983428955078, 97.5374526977539, 491.1260070800781, 514.6817016601562, 508.1227111816406, 433.5501403808594, 202.1093292236328, 449.343994140625, 496.08978271484375, 519.1959838867188, 506.0901184082031, 544.9931640625, 238.89283752441406, 429.16888427734375, 127.01703643798828, 100.0566177368164, 595.8557739257812, 536.6802978515625, 339.5403137207031, 52.57502365112305, 273.6736755371094, 514.7291870117188, 288.301513671875, 562.790771484375, 225.26377868652344, 564.4469604492188, 534.83349609375, 538.739013671875, 327.4521484375, 139.7453155517578, 245.58985900878906, 536.7811889648438, 561.7703857421875, 475.0946044921875, 313.1546325683594], 'y_pred_int': [78, 595, 479, 109, 545, 513, 367, 187, 357, 257, 506, 461, 134, 568, 566, 557, 568, 442, 586, 475, 514, 557, 298, 397, 247, 99, 117, 558, 578, 569, 553, 294, 463, 136, 549, 66, 515, 399, 491, 491, 137, 257, 566, 572, 108, 73, 298, 152, 260, 291, 531, 188, 505, 271, 382, 128, 108, 186, 156, 548, 247, 93, 491, 486, 67, 474, 595, 545, 169, 185, 479, 543, 416, 567, 579, 508, 471, 118, 484, 256, 506, 87, 59, 414, 308, 492, 251, 310, 266, 299, 304, 386, 486, 480, 200, 202, 544, 583, 238, 196, 278, 532, 480, 72, 520, 189, 540, 614, 190, 245, 122, 436, 218, 335, 206, 298, 480, 560, 207, 61, 140, 61, 536, 417, 487, 453, 143, 508, 219, 499, 80, 583, 238, 498, 526, 88, 75, 602, 409, 542, 194, 189, 490, 530, 537, 148, 538, 214, 311, 481, 161, 402, 250, 74, 602, 577, 496, 535, 246, 552, 462, 490, 297, 263, 521, 485, 193, 502, 241, 356, 155, 271, 497, 586, 515, 146, 290, 542, 176, 124, 74, 530, 464, 426, 472, 133, 77, 308, 532, 481, 80, 430, 55, 465, 656, 401, 403, 58, 242, 595, 442, 243, 254, 215, 568, 565, 551, 522, 181, 591, 86, 38, 86, 418, 507, 57, 459, 559, 149, 191, 181, 108, 230, 120, 432, 548, 498, 172, 132, 477, 179, 247, 138, 522, 624, 214, 463, 185, 483, 448, 358, 202, 587, 28, 369, 557, 188, 529, 410, 494, 461, 439, 102, 565, 126, 489, 124, 60, 121, 156, 490, 453, 524, 123, 551, 532, 136, 579, 91, 126, 248, 535, 484, 182, 427, 211, 590, 117, 530, 560, 558, 528, 488, 93, 256, 501, 434, 225, 313, 192, 115, 155, 520, 212, 130, 369, 613, 471, 236, 178, 491, 529, 502, 215, 97, 86, 98, 491, 515, 508, 434, 202, 449, 496, 519, 506, 545, 239, 429, 127, 100, 596, 537, 340, 53, 274, 515, 288, 563, 225, 564, 535, 539, 327, 140, 246, 537, 562, 475, 313], 'y_true': [105, 457, 690, 127, 738, 758, 331, 209, 312, 224, 605, 434, 160, 355, 513, 921, 1064, 911, 805, 420, 708, 583, 291, 326, 238, 100, 135, 317, 903, 447, 670, 242, 488, 161, 901, 98, 934, 340, 438, 907, 154, 238, 544, 430, 151, 113, 291, 153, 205, 284, 437, 237, 431, 217, 335, 143, 125, 195, 208, 387, 212, 181, 367, 577, 113, 332, 459, 1238, 178, 226, 510, 638, 334, 580, 621, 485, 384, 103, 371, 210, 656, 77, 55, 370, 280, 466, 230, 257, 228, 244, 292, 283, 439, 434, 207, 163, 898, 790, 273, 200, 260, 733, 379, 83, 446, 161, 613, 765, 182, 191, 141, 633, 212, 342, 235, 294, 999, 723, 301, 97, 158, 64, 911, 391, 476, 323, 186, 388, 243, 299, 86, 403, 218, 379, 378, 115, 93, 628, 391, 543, 203, 212, 382, 714, 473, 174, 824, 226, 285, 624, 204, 312, 200, 123, 659, 849, 593, 655, 242, 732, 450, 401, 264, 255, 429, 617, 153, 495, 208, 331, 164, 258, 445, 787, 904, 119, 279, 364, 162, 126, 84, 370, 420, 339, 491, 177, 90, 291, 395, 390, 78, 358, 62, 518, 634, 362, 302, 75, 244, 657, 344, 199, 212, 215, 540, 778, 370, 656, 185, 685, 126, 37, 126, 350, 687, 60, 464, 504, 159, 202, 181, 96, 249, 106, 588, 507, 574, 163, 136, 350, 215, 256, 132, 504, 624, 199, 428, 176, 792, 395, 274, 170, 878, 35, 289, 752, 204, 487, 292, 452, 375, 460, 62, 572, 117, 537, 132, 108, 181, 187, 560, 292, 598, 111, 426, 641, 154, 486, 142, 130, 240, 464, 332, 209, 333, 211, 493, 148, 585, 478, 585, 431, 601, 80, 286, 328, 727, 187, 286, 191, 166, 155, 432, 155, 110, 295, 455, 550, 254, 168, 832, 785, 382, 206, 135, 138, 132, 699, 422, 575, 290, 221, 440, 608, 656, 881, 752, 200, 351, 127, 117, 552, 1016, 284, 76, 257, 508, 300, 361, 266, 1006, 794, 765, 298, 129, 215, 452, 646, 675, 248], 'train_acc': 0.9992647058823529, 'test_acc': 0.008823529411764706, 'X': 'X_TANL_layer_4_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_5_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 4462.1992, Test loss: 53206.1836
In this epoch 100/5000, Training loss: 686.5753, Test loss: 18195.3730
In this epoch 150/5000, Training loss: 4.4789, Test loss: 17036.6250
In this epoch 200/5000, Training loss: 0.0034, Test loss: 17056.0820
In this epoch 250/5000, Training loss: 0.0005, Test loss: 17053.3848
In this epoch 300/5000, Training loss: 0.0005, Test loss: 17053.3027
In this epoch 350/5000, Training loss: 0.0004, Test loss: 17053.2578
In this epoch 400/5000, Training loss: 0.0004, Test loss: 17053.2129
In this epoch 450/5000, Training loss: 0.0004, Test loss: 17053.1738
In this epoch 500/5000, Training loss: 0.0004, Test loss: 17053.1348
In this epoch 550/5000, Training loss: 0.0004, Test loss: 17053.1035
In this epoch 600/5000, Training loss: 0.0004, Test loss: 17053.0684
In this epoch 650/5000, Training loss: 0.0004, Test loss: 17053.0410
In this epoch 700/5000, Training loss: 0.0004, Test loss: 17053.0156
In this epoch 750/5000, Training loss: 0.0004, Test loss: 17052.9883
In this epoch 800/5000, Training loss: 0.0004, Test loss: 17052.9668
In this epoch 850/5000, Training loss: 0.0004, Test loss: 17052.9453
In this epoch 900/5000, Training loss: 0.0004, Test loss: 17052.9336
In this epoch 950/5000, Training loss: 0.0004, Test loss: 17052.9160
In this epoch 1000/5000, Training loss: 0.0004, Test loss: 17052.9023
In this epoch 1050/5000, Training loss: 0.0004, Test loss: 17052.8965
In this epoch 1100/5000, Training loss: 0.0004, Test loss: 17052.8848
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 17052.8789
In this epoch 1200/5000, Training loss: 0.0004, Test loss: 17052.8730
In this epoch 1250/5000, Training loss: 0.0004, Test loss: 17052.8672
In this epoch 1300/5000, Training loss: 0.0004, Test loss: 17052.8652
In this epoch 1350/5000, Training loss: 0.0004, Test loss: 17052.8633
In this epoch 1400/5000, Training loss: 0.0004, Test loss: 17052.8613
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 17052.8613
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 17052.8613
In this epoch 1550/5000, Training loss: 0.0004, Test loss: 17052.8574
In this epoch 1600/5000, Training loss: 0.0004, Test loss: 17052.8574
In this epoch 1650/5000, Training loss: 0.0004, Test loss: 17052.8574
In this epoch 1700/5000, Training loss: 0.0004, Test loss: 17052.8574
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 1900/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 1950/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2000/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2050/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2250/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2300/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2350/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2400/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2450/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2600/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2650/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2700/5000, Training loss: 0.0004, Test loss: 17052.8535
In this epoch 2750/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2800/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2850/5000, Training loss: 0.0004, Test loss: 17052.8535
In this epoch 2900/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 2950/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 3000/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 3200/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 3250/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 3300/5000, Training loss: 0.0004, Test loss: 17052.8555
In this epoch 3350/5000, Training loss: 0.0004, Test loss: 17052.8535
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 17052.8535
In this epoch 3450/5000, Training loss: 620006.0625, Test loss: 1712221.3750
In this epoch 3500/5000, Training loss: 440.6183, Test loss: 26733.2324
In this epoch 3550/5000, Training loss: 9.0131, Test loss: 16933.8086
In this epoch 3600/5000, Training loss: 0.2568, Test loss: 17060.4746
In this epoch 3650/5000, Training loss: 0.0005, Test loss: 17052.1523
In this epoch 3700/5000, Training loss: 0.0004, Test loss: 17051.5332
In this epoch 3750/5000, Training loss: 0.0004, Test loss: 17051.5410
In this epoch 3800/5000, Training loss: 0.0004, Test loss: 17051.5273
In this epoch 3850/5000, Training loss: 0.0004, Test loss: 17051.5156
In this epoch 3900/5000, Training loss: 0.0004, Test loss: 17051.5098
In this epoch 3950/5000, Training loss: 0.0004, Test loss: 17051.5000
In this epoch 4000/5000, Training loss: 0.0004, Test loss: 17051.4922
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 17051.4863
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 17051.4824
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 17051.4766
In this epoch 4200/5000, Training loss: 0.0004, Test loss: 17051.4727
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 17051.4707
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 17051.4648
In this epoch 4350/5000, Training loss: 0.0004, Test loss: 17051.4629
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 17051.4570
In this epoch 4450/5000, Training loss: 0.0004, Test loss: 17051.4570
In this epoch 4500/5000, Training loss: 0.0004, Test loss: 17051.4570
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 17051.4570
In this epoch 4600/5000, Training loss: 0.0004, Test loss: 17051.4531
In this epoch 4650/5000, Training loss: 0.0004, Test loss: 17051.4531
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 17051.4512
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 17051.4492
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 17051.4512
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 17051.4512
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 17051.4473
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 17051.4473
In this epoch 5000/5000, Training loss: 0.0004, Test loss: 17051.4473
train_acc 0.9992647058823529
test_acc 0.0058823529411764705
{'y_pred': [153.21844482421875, 209.2996826171875, 103.77564239501953, 506.8737487792969, 195.6324920654297, 202.1835174560547, 526.3255615234375, 384.0840148925781, 237.09860229492188, 238.41220092773438, 211.59130859375, 115.89225006103516, 138.04391479492188, 587.8606567382812, 16.814620971679688, 588.43212890625, 481.5334777832031, 543.5576782226562, 458.5246887207031, 448.6255798339844, 234.77328491210938, 535.622802734375, 480.7876281738281, 229.13572692871094, 535.6278076171875, 236.7535400390625, 202.478515625, 262.5090637207031, 434.505126953125, 213.08152770996094, 605.445556640625, 123.05692291259766, 140.2746124267578, 581.4866943359375, 67.34407806396484, 598.2886962890625, 201.7674560546875, 598.81640625, 487.2149658203125, 472.5923156738281, 150.01620483398438, 492.3510437011719, 535.541259765625, 221.55091857910156, 572.8572387695312, 104.45711517333984, 474.6922912597656, 588.2396240234375, 132.8417205810547, 501.1591491699219, 226.7820587158203, 363.5096130371094, 477.2414855957031, 477.1825256347656, 165.69189453125, 79.63443756103516, 254.04638671875, 146.83096313476562, 266.0436096191406, 258.48773193359375, 536.1405639648438, 157.62884521484375, 514.0638427734375, 306.1053161621094, 485.5888366699219, 524.5272216796875, 549.8314208984375, 485.983642578125, 172.46401977539062, 534.2636108398438, 447.04058837890625, 516.2562866210938, 270.7389221191406, 223.39804077148438, 219.73480224609375, 197.1630859375, 277.77728271484375, 555.6361694335938, 107.44560241699219, 483.3378601074219, 155.8746337890625, 238.1717071533203, 219.2778778076172, 169.31234741210938, 473.34906005859375, 595.861572265625, 550.2203369140625, 489.5439453125, 509.2803649902344, 635.43896484375, 75.40037536621094, 497.75372314453125, 150.19981384277344, 111.8648910522461, 210.0637664794922, 340.3884582519531, 445.9331970214844, 197.53150939941406, 456.7398376464844, 157.4189453125, 510.7749328613281, 255.71688842773438, 268.9183044433594, 99.54691314697266, 138.52392578125, 544.9668579101562, 521.0050048828125, 66.50174713134766, 590.5700073242188, 143.3064727783203, 407.2880859375, 141.37884521484375, 272.8966979980469, 646.9160766601562, 547.6615600585938, 367.4264831542969, 582.90576171875, 235.88327026367188, 212.9760284423828, 519.8919677734375, 465.8401794433594, 56.38660430908203, 484.7682189941406, 484.885986328125, 235.4830322265625, 502.4300842285156, 607.6107177734375, 178.5517120361328, 542.9291381835938, 156.35171508789062, 516.5910034179688, 166.58230590820312, 472.5848693847656, 556.3385620117188, 532.8406982421875, 313.94805908203125, 445.23980712890625, 527.207763671875, 479.38671875, 534.3450927734375, 614.8526611328125, 274.9090881347656, 191.67031860351562, 417.8920593261719, 99.14410400390625, 574.9251098632812, 225.9220733642578, 485.19915771484375, 80.43405151367188, 480.2956848144531, 409.8771057128906, 634.162841796875, 534.9218139648438, 431.4861145019531, 528.3032836914062, 488.6994323730469, 470.0877990722656, 503.11285400390625, 563.9921875, 553.9493408203125, 194.51234436035156, 139.27676391601562, 326.3368835449219, 127.27323150634766, 99.51231384277344, 213.3276824951172, 199.3540496826172, 601.0738525390625, 554.537353515625, 437.38555908203125, 233.60501098632812, 538.8416748046875, 544.9628295898438, 207.50587463378906, 152.06500244140625, 66.48526763916016, 202.27252197265625, 79.36742401123047, 451.4514465332031, 252.5965118408203, 404.2051696777344, 98.37808990478516, 507.1633605957031, 444.5408630371094, 360.1977233886719, 407.86712646484375, 585.0430297851562, 590.322509765625, 113.55139923095703, 485.74072265625, 494.11041259765625, 85.9679183959961, 92.88968658447266, 379.4151611328125, 452.7934265136719, 217.43807983398438, 200.36746215820312, 168.634033203125, 567.8779296875, 579.3616943359375, 269.56201171875, 257.5208740234375, 525.4786376953125, 293.222900390625, 478.0602722167969, 341.9180603027344, 192.8885498046875, 251.2858428955078, 523.0340576171875, 273.934814453125, 539.9650268554688, 54.37059020996094, 566.5772705078125, 545.6815185546875, 482.0725402832031, 140.167236328125, 143.5352325439453, 489.0952453613281, 628.4364624023438, 424.4654541015625, 179.4188995361328, 96.7931900024414, 508.5585632324219, 458.7398986816406, 486.6981506347656, 71.14131927490234, 91.52802276611328, 543.5728759765625, 145.01683044433594, 499.3311462402344, 219.66152954101562, 330.4983215332031, 267.649169921875, 452.1307067871094, 291.7589416503906, 489.7900695800781, 280.39605712890625, 240.13052368164062, 162.75527954101562, 232.92739868164062, 498.2276306152344, 385.2085266113281, 244.85369873046875, 455.6198425292969, 544.8853149414062, 513.6896362304688, 423.4378967285156, 459.5119323730469, 221.6942901611328, 112.5642318725586, 536.800048828125, 456.8188171386719, 350.87225341796875, 618.3629760742188, 109.77307891845703, 557.2423095703125, 249.54852294921875, 458.613525390625, 294.0660705566406, 575.2725219726562, 165.6019287109375, 594.0550537109375, 289.818603515625, 89.75820922851562, 21.244712829589844, 473.16790771484375, 303.134521484375, 471.1895751953125, 142.3516387939453, 146.90542602539062, 239.78948974609375, 101.3642578125, 149.91810607910156, 93.02788543701172, 109.7314453125, 519.5103759765625, 606.8035278320312, 427.1106872558594, 278.7245178222656, 182.03646850585938, 394.0768127441406, 137.5069580078125, 522.7900390625, 93.35454559326172, 127.34191131591797, 537.5271606445312, 273.04437255859375, 53.18830490112305, 504.5832214355469, 356.52618408203125, 273.32415771484375, 422.4623718261719, 277.8659973144531, 386.531494140625, 520.535400390625, 486.1009826660156, 530.4710083007812, 319.1388244628906, 471.9649963378906, 77.56053924560547, 168.5599365234375, 244.35516357421875, 520.9982299804688, 560.8822631835938, 566.8511352539062, 488.5574645996094, 585.9793701171875, 159.0661163330078, 138.01422119140625, 525.8355712890625, 551.89208984375, 583.9560546875, 574.0110473632812, 170.83677673339844, 316.966064453125, 574.0579833984375, 546.6077880859375, 543.6099853515625, 192.63308715820312, 194.63363647460938, 36.90995788574219, 110.46595764160156, 464.6290588378906, 465.4110412597656, 219.65843200683594, 589.8698120117188, 525.7639770507812, 255.28671264648438, 96.88812255859375, 404.7198181152344, 144.2791290283203, 580.169677734375, 490.5364074707031, 435.5729675292969, 104.37580108642578, 85.09297943115234, 484.7115478515625, 417.8639221191406, 393.5154113769531, 488.7016906738281], 'y_pred_int': [153, 209, 104, 507, 196, 202, 526, 384, 237, 238, 212, 116, 138, 588, 17, 588, 482, 544, 459, 449, 235, 536, 481, 229, 536, 237, 202, 263, 435, 213, 605, 123, 140, 581, 67, 598, 202, 599, 487, 473, 150, 492, 536, 222, 573, 104, 475, 588, 133, 501, 227, 364, 477, 477, 166, 80, 254, 147, 266, 258, 536, 158, 514, 306, 486, 525, 550, 486, 172, 534, 447, 516, 271, 223, 220, 197, 278, 556, 107, 483, 156, 238, 219, 169, 473, 596, 550, 490, 509, 635, 75, 498, 150, 112, 210, 340, 446, 198, 457, 157, 511, 256, 269, 100, 139, 545, 521, 67, 591, 143, 407, 141, 273, 647, 548, 367, 583, 236, 213, 520, 466, 56, 485, 485, 235, 502, 608, 179, 543, 156, 517, 167, 473, 556, 533, 314, 445, 527, 479, 534, 615, 275, 192, 418, 99, 575, 226, 485, 80, 480, 410, 634, 535, 431, 528, 489, 470, 503, 564, 554, 195, 139, 326, 127, 100, 213, 199, 601, 555, 437, 234, 539, 545, 208, 152, 66, 202, 79, 451, 253, 404, 98, 507, 445, 360, 408, 585, 590, 114, 486, 494, 86, 93, 379, 453, 217, 200, 169, 568, 579, 270, 258, 525, 293, 478, 342, 193, 251, 523, 274, 540, 54, 567, 546, 482, 140, 144, 489, 628, 424, 179, 97, 509, 459, 487, 71, 92, 544, 145, 499, 220, 330, 268, 452, 292, 490, 280, 240, 163, 233, 498, 385, 245, 456, 545, 514, 423, 460, 222, 113, 537, 457, 351, 618, 110, 557, 250, 459, 294, 575, 166, 594, 290, 90, 21, 473, 303, 471, 142, 147, 240, 101, 150, 93, 110, 520, 607, 427, 279, 182, 394, 138, 523, 93, 127, 538, 273, 53, 505, 357, 273, 422, 278, 387, 521, 486, 530, 319, 472, 78, 169, 244, 521, 561, 567, 489, 586, 159, 138, 526, 552, 584, 574, 171, 317, 574, 547, 544, 193, 195, 37, 110, 465, 465, 220, 590, 526, 255, 97, 405, 144, 580, 491, 436, 104, 85, 485, 418, 394, 489], 'y_true': [162, 276, 98, 542, 182, 168, 417, 308, 241, 263, 251, 98, 176, 877, 35, 534, 507, 1342, 391, 342, 187, 380, 1027, 139, 336, 209, 210, 277, 349, 175, 433, 81, 154, 801, 93, 912, 197, 752, 831, 867, 159, 511, 387, 187, 1006, 89, 591, 662, 96, 379, 206, 301, 607, 404, 138, 123, 200, 191, 197, 207, 434, 133, 507, 264, 450, 688, 828, 351, 134, 466, 726, 1015, 294, 210, 208, 227, 275, 367, 111, 340, 180, 259, 179, 143, 608, 546, 779, 396, 612, 537, 102, 418, 155, 92, 194, 266, 635, 234, 403, 147, 835, 235, 237, 101, 101, 585, 344, 139, 898, 178, 663, 168, 261, 791, 543, 331, 718, 182, 224, 493, 373, 87, 495, 384, 269, 455, 441, 187, 613, 161, 431, 171, 700, 1064, 693, 257, 832, 733, 493, 714, 785, 217, 228, 360, 118, 686, 162, 527, 39, 539, 606, 1044, 453, 339, 705, 675, 506, 387, 386, 515, 230, 176, 296, 131, 84, 170, 156, 529, 707, 475, 210, 361, 660, 173, 169, 108, 213, 88, 328, 247, 285, 108, 373, 382, 313, 286, 493, 590, 129, 474, 452, 92, 148, 333, 338, 204, 201, 141, 358, 542, 233, 273, 552, 324, 315, 296, 204, 226, 404, 259, 380, 84, 657, 425, 376, 124, 169, 480, 773, 701, 157, 154, 444, 375, 557, 56, 87, 901, 167, 588, 205, 299, 240, 452, 262, 436, 254, 231, 178, 226, 880, 321, 258, 351, 878, 455, 533, 370, 191, 119, 617, 378, 307, 426, 159, 448, 250, 633, 258, 874, 164, 399, 296, 88, 76, 364, 294, 449, 180, 185, 237, 118, 109, 105, 105, 445, 752, 666, 249, 201, 334, 103, 566, 117, 165, 440, 198, 100, 593, 340, 215, 318, 249, 305, 805, 389, 407, 227, 397, 83, 164, 260, 621, 572, 504, 428, 424, 175, 153, 364, 796, 459, 463, 153, 280, 406, 686, 385, 236, 236, 31, 111, 579, 339, 159, 706, 403, 230, 78, 309, 180, 783, 629, 407, 112, 103, 539, 299, 302, 639], 'train_acc': 0.9992647058823529, 'test_acc': 0.0058823529411764705, 'X': 'X_TANL_layer_5_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_6_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 3699.4504, Test loss: 52230.0195
In this epoch 100/5000, Training loss: 887.2153, Test loss: 10380.0557
In this epoch 150/5000, Training loss: 6.0911, Test loss: 9639.7764
In this epoch 200/5000, Training loss: 0.0012, Test loss: 9634.3574
In this epoch 250/5000, Training loss: 0.0005, Test loss: 9634.8174
In this epoch 300/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 350/5000, Training loss: 0.0004, Test loss: 9634.8320
In this epoch 400/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 450/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 500/5000, Training loss: 0.0004, Test loss: 9634.8320
In this epoch 550/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 600/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 650/5000, Training loss: 0.0004, Test loss: 9634.8320
In this epoch 700/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 750/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 800/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 850/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 900/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 950/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 1000/5000, Training loss: 0.0004, Test loss: 9634.8320
In this epoch 1050/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 1100/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 1200/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 1250/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 1300/5000, Training loss: 0.0004, Test loss: 9634.8320
In this epoch 1350/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 1400/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 1550/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 1600/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 1650/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 1700/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 9634.8320
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 1900/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 1950/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 2000/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2050/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 9634.8301
In this epoch 2250/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2300/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2350/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2400/5000, Training loss: 0.0004, Test loss: 9634.8320
In this epoch 2450/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 9634.8320
In this epoch 2600/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2650/5000, Training loss: 0.0004, Test loss: 9634.8320
In this epoch 2700/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2750/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2800/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2850/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2900/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 2950/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 3000/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 9634.8330
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 9634.8311
In this epoch 3200/5000, Training loss: 9959733.0000, Test loss: 24065130.0000
In this epoch 3250/5000, Training loss: 139827.5156, Test loss: 31717.7422
In this epoch 3300/5000, Training loss: 355.0889, Test loss: 9719.5732
In this epoch 3350/5000, Training loss: 1.8222, Test loss: 9632.7500
In this epoch 3400/5000, Training loss: 0.0005, Test loss: 9633.6924
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 9633.9053
In this epoch 3500/5000, Training loss: 0.0004, Test loss: 9633.9277
In this epoch 3550/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 3600/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 3650/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 3700/5000, Training loss: 0.0004, Test loss: 9633.9316
In this epoch 3750/5000, Training loss: 0.0004, Test loss: 9633.9316
In this epoch 3800/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 3850/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 3900/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 3950/5000, Training loss: 0.0004, Test loss: 9633.9307
In this epoch 4000/5000, Training loss: 0.0004, Test loss: 9633.9316
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 4200/5000, Training loss: 0.0004, Test loss: 9633.9307
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 9633.9316
In this epoch 4350/5000, Training loss: 0.0004, Test loss: 9633.9307
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 4450/5000, Training loss: 0.0004, Test loss: 9633.9316
In this epoch 4500/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 4600/5000, Training loss: 0.0004, Test loss: 9633.9316
In this epoch 4650/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 9633.9316
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 9633.9316
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 9633.9316
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 9633.9326
In this epoch 5000/5000, Training loss: 0.0004, Test loss: 9633.9326
train_acc 0.9992647058823529
test_acc 0.0016868512110726644
{'y_pred': [212.9213104248047, 241.32371520996094, 238.33642578125, 52.36450958251953, 518.8220825195312, 469.3699645996094, 569.5775146484375, 592.0443115234375, 549.29833984375, 579.9946899414062, 587.3372802734375, 336.2015380859375, 196.92633056640625, 521.6715698242188, 546.8470458984375, 562.4192504882812, 517.614501953125, 35.243499755859375, 117.0446548461914, 553.619384765625, 237.0977020263672, 569.1509399414062, 643.3695068359375, -0.4763050675392151, 54.04169464111328, 542.017333984375, 243.6316375732422, 215.3331298828125, 108.20093536376953, 580.208251953125, 280.3651123046875, 556.9907836914062, 69.10762786865234, 536.889892578125, 275.96728515625, 161.37318420410156, 326.5186462402344, 54.673179626464844, 232.6885986328125, 605.9503173828125, 503.48797607421875, 267.7253723144531, 26.88445281982422, 304.7585144042969, 132.7279510498047, 407.1181640625, 505.5047607421875, 489.3208923339844, 159.25265502929688, 136.86997985839844, 514.2193603515625, 475.21051025390625, 77.78530883789062, 282.2454833984375, 99.62715148925781, 525.3739013671875, 557.7720947265625, 112.00939178466797, 314.0557861328125, 106.65857696533203, 155.0, 532.3378295898438, 181.0497283935547, 468.7911376953125, 99.75101470947266, 489.62030029296875, 167.21324157714844, 168.7281036376953, 341.0079345703125, 134.61595153808594, 143.14183044433594, 440.31927490234375, 154.4058380126953, 399.978759765625, 120.8951187133789, 543.4586181640625, 640.4341430664062, 267.8544006347656, 528.4717407226562, 293.41094970703125, 484.3137512207031, 593.1116333007812, 126.7074203491211, 499.42742919921875, 170.9130859375, 487.20074462890625, 514.018310546875, 138.02732849121094, 38.93897247314453, 97.03865814208984, 307.4040222167969, 122.3370132446289, 116.40352630615234, 347.12506103515625, 375.09295654296875, 264.5877380371094, 232.04441833496094, 497.154052734375, 360.6514587402344, 325.3367919921875, 181.0099334716797, 552.59912109375, 562.0537719726562, 240.04257202148438, 467.61981201171875, 95.13304901123047, 303.3980407714844, 124.24987030029297, 272.8163146972656, 515.9024658203125, 236.6119842529297, 294.0364074707031, 128.0485382080078, 320.704345703125, 472.74627685546875, 254.79222106933594, 172.54444885253906, 566.3212890625, 170.8839569091797, 519.7846069335938, 534.5084228515625, 521.1573486328125, 253.58396911621094, 499.51995849609375, 77.56169891357422, 476.3677978515625, 544.9169921875, 480.3081970214844, 263.470947265625, 546.0684814453125, 546.68994140625, 121.36721801757812, 509.033203125, 438.0489501953125, 61.1641845703125, 223.9611053466797, 183.3156280517578, 530.5338134765625, 546.71240234375, 585.863037109375, 501.71844482421875, 158.84278869628906, 209.79977416992188, 208.5257110595703, 254.50462341308594, 421.0788879394531, 636.4637451171875, 240.21177673339844, 467.87646484375, 732.6140747070312, 318.82135009765625, 191.17825317382812, 319.1348876953125, 133.80352783203125, 197.83267211914062, 513.7955932617188, 171.27084350585938, 445.268310546875, 208.8624725341797, 144.13621520996094, 261.49969482421875, 370.36309814453125, 147.2970428466797, 584.6666259765625, 584.02001953125, 531.5167236328125, 315.376220703125, 230.6655731201172, 534.9473876953125, 136.6084442138672, 446.8058776855469, 488.7691345214844, 578.6041259765625, 583.8089599609375, 52.243553161621094, 556.81787109375, 395.7374267578125, 317.2619934082031, 565.88330078125, 526.5201416015625, 152.14877319335938, 574.249755859375, 542.345703125, 121.85226440429688, 497.5622863769531, 232.23268127441406, 611.0977172851562, 514.821044921875, 564.9166870117188, 118.1827163696289, 321.70782470703125, 489.29425048828125, 239.5304718017578, 223.1986846923828, 460.7828063964844, 596.0297241210938, 104.33065795898438, 476.50274658203125, 399.1031799316406, 499.1956787109375, 496.0363464355469, 616.8939208984375, 442.47406005859375, 145.08578491210938, 604.32568359375, 512.1085815429688, 200.75001525878906, 590.3623046875, 267.81097412109375, 244.79188537597656, 519.5966796875, 570.794921875, 593.3583984375, 150.7405548095703, 218.39865112304688, 325.13970947265625, 567.6898193359375, 89.82278442382812, 503.88250732421875, 166.2703399658203, 625.859130859375, 231.87095642089844, 229.31723022460938, 129.4785614013672, 585.9822998046875, 473.4164123535156, 543.5127563476562, 146.22019958496094, 230.70455932617188, 518.7426147460938, 300.4471740722656, 627.8493041992188, 583.9530029296875, 567.6224365234375, 603.8995971679688, 439.5741271972656, 122.42998504638672, 112.83434295654297, 471.65496826171875, 511.043701171875, 366.7087707519531, 572.335205078125, 549.3870239257812, 170.66590881347656, 193.26988220214844, 155.94227600097656, 87.83824157714844, 517.1812133789062, 139.53392028808594, 168.6904754638672, 261.0945739746094, 67.6915512084961, 533.7589111328125, 535.543701171875, 191.81431579589844, 108.52462768554688, 117.71849060058594, 589.5057373046875, 196.2111053466797, 600.4161376953125, 301.73797607421875, 514.8590698242188, 519.5328369140625, 611.4141845703125, 88.4903335571289, 128.3842315673828, 198.72659301757812, 514.1395263671875, 602.3351440429688, 105.92280578613281, 78.51397705078125, 200.68731689453125, 124.6215591430664, 511.8326416015625, 572.3880615234375, 135.50918579101562, 507.6915588378906, 570.9189453125, 448.1367492675781, 657.612060546875, 509.57464599609375, 431.4626770019531, 218.35862731933594, 215.27896118164062, 273.41546630859375, 506.9432678222656, 555.56689453125, 678.6484375, 235.35177612304688, 95.64339447021484, 539.5951538085938, 504.99005126953125, 254.21324157714844, 208.63812255859375, 269.9344482421875, 256.6022033691406, 214.7489013671875, 60.65934753417969, 524.5603637695312, 368.9556884765625, 92.84181213378906, 480.42803955078125, 191.23875427246094, 125.60936737060547, 458.4227600097656, 528.8089599609375, 89.94868469238281, 522.0202026367188, 195.84921264648438, 527.7064208984375, 153.23338317871094, 523.489013671875, 373.0000305175781, 522.3629150390625, 243.25198364257812, 543.8338012695312, 285.9687805175781, 210.9071502685547, 123.74647521972656, 341.30804443359375, 467.8978576660156, 542.75341796875, 126.11223602294922, 112.90033721923828, 227.34144592285156, 439.67645263671875, 502.85736083984375, 181.83184814453125, 265.23040771484375, 191.32135009765625, 566.4873046875, 498.8686828613281, 471.1701965332031, 200.9589080810547, 139.96530151367188, 327.6352233886719, 533.3427124023438, 11.56315803527832, 317.0886535644531, 445.327392578125], 'y_pred_int': [213, 241, 238, 52, 519, 469, 570, 592, 549, 580, 587, 336, 197, 522, 547, 562, 518, 35, 117, 554, 237, 569, 643, 0, 54, 542, 244, 215, 108, 580, 280, 557, 69, 537, 276, 161, 327, 55, 233, 606, 503, 268, 27, 305, 133, 407, 506, 489, 159, 137, 514, 475, 78, 282, 100, 525, 558, 112, 314, 107, 155, 532, 181, 469, 100, 490, 167, 169, 341, 135, 143, 440, 154, 400, 121, 543, 640, 268, 528, 293, 484, 593, 127, 499, 171, 487, 514, 138, 39, 97, 307, 122, 116, 347, 375, 265, 232, 497, 361, 325, 181, 553, 562, 240, 468, 95, 303, 124, 273, 516, 237, 294, 128, 321, 473, 255, 173, 566, 171, 520, 535, 521, 254, 500, 78, 476, 545, 480, 263, 546, 547, 121, 509, 438, 61, 224, 183, 531, 547, 586, 502, 159, 210, 209, 255, 421, 636, 240, 468, 733, 319, 191, 319, 134, 198, 514, 171, 445, 209, 144, 261, 370, 147, 585, 584, 532, 315, 231, 535, 137, 447, 489, 579, 584, 52, 557, 396, 317, 566, 527, 152, 574, 542, 122, 498, 232, 611, 515, 565, 118, 322, 489, 240, 223, 461, 596, 104, 477, 399, 499, 496, 617, 442, 145, 604, 512, 201, 590, 268, 245, 520, 571, 593, 151, 218, 325, 568, 90, 504, 166, 626, 232, 229, 129, 586, 473, 544, 146, 231, 519, 300, 628, 584, 568, 604, 440, 122, 113, 472, 511, 367, 572, 549, 171, 193, 156, 88, 517, 140, 169, 261, 68, 534, 536, 192, 109, 118, 590, 196, 600, 302, 515, 520, 611, 88, 128, 199, 514, 602, 106, 79, 201, 125, 512, 572, 136, 508, 571, 448, 658, 510, 431, 218, 215, 273, 507, 556, 679, 235, 96, 540, 505, 254, 209, 270, 257, 215, 61, 525, 369, 93, 480, 191, 126, 458, 529, 90, 522, 196, 528, 153, 523, 373, 522, 243, 544, 286, 211, 124, 341, 468, 543, 126, 113, 227, 440, 503, 182, 265, 191, 566, 499, 471, 201, 140, 328, 533, 12, 317, 445], 'y_true': [254, 215, 203, 59, 679, 457, 521, 681, 835, 431, 762, 232, 193, 677, 446, 435, 395, 62, 112, 537, 280, 468, 905, 57, 77, 449, 244, 194, 125, 778, 189, 878, 164, 397, 244, 170, 297, 74, 207, 477, 448, 257, 78, 296, 95, 292, 556, 312, 198, 157, 629, 522, 31, 254, 99, 444, 572, 143, 208, 89, 155, 318, 191, 666, 135, 373, 131, 187, 279, 164, 145, 327, 162, 294, 130, 577, 601, 197, 669, 297, 378, 537, 151, 471, 209, 586, 404, 136, 55, 153, 321, 78, 80, 323, 262, 263, 210, 441, 300, 296, 127, 665, 590, 242, 392, 98, 342, 126, 206, 676, 226, 256, 132, 273, 663, 233, 124, 390, 155, 461, 778, 1016, 292, 358, 118, 786, 577, 370, 247, 538, 409, 99, 456, 521, 110, 279, 225, 407, 562, 486, 586, 160, 209, 148, 242, 425, 863, 258, 403, 768, 240, 259, 258, 142, 167, 431, 180, 340, 196, 133, 189, 273, 149, 459, 771, 725, 238, 239, 485, 128, 550, 310, 770, 440, 63, 399, 350, 286, 423, 418, 150, 391, 530, 89, 363, 187, 680, 596, 752, 172, 200, 339, 222, 167, 395, 899, 108, 403, 321, 361, 364, 837, 334, 127, 511, 761, 202, 407, 273, 294, 323, 681, 889, 148, 284, 311, 611, 138, 450, 154, 389, 146, 180, 109, 614, 587, 505, 163, 214, 496, 270, 624, 512, 552, 631, 326, 148, 166, 437, 516, 331, 796, 605, 158, 221, 279, 37, 440, 130, 104, 175, 143, 601, 628, 226, 167, 129, 507, 173, 797, 227, 527, 569, 613, 128, 156, 226, 388, 559, 126, 39, 237, 128, 447, 738, 239, 354, 585, 599, 544, 471, 539, 169, 158, 290, 521, 403, 457, 206, 111, 585, 528, 195, 217, 211, 232, 143, 86, 591, 269, 141, 598, 158, 132, 579, 529, 180, 832, 164, 474, 135, 513, 373, 502, 220, 697, 251, 156, 89, 248, 343, 516, 174, 101, 238, 391, 504, 209, 237, 165, 547, 733, 502, 172, 190, 350, 593, 98, 309, 507], 'train_acc': 0.9992647058823529, 'test_acc': 0.0016868512110726644, 'X': 'X_TANL_layer_6_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_7_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 27058.9062, Test loss: 39883.2773
In this epoch 100/5000, Training loss: 508.2307, Test loss: 14448.6787
In this epoch 150/5000, Training loss: 4.0252, Test loss: 13788.4297
In this epoch 200/5000, Training loss: 0.0443, Test loss: 13734.5166
In this epoch 250/5000, Training loss: 0.0008, Test loss: 13733.7676
In this epoch 300/5000, Training loss: 0.0007, Test loss: 13733.4736
In this epoch 350/5000, Training loss: 0.0007, Test loss: 13733.4346
In this epoch 400/5000, Training loss: 0.0006, Test loss: 13733.4004
In this epoch 450/5000, Training loss: 0.0006, Test loss: 13733.3662
In this epoch 500/5000, Training loss: 0.0005, Test loss: 13733.3330
In this epoch 550/5000, Training loss: 0.0005, Test loss: 13733.3018
In this epoch 600/5000, Training loss: 0.0005, Test loss: 13733.2705
In this epoch 650/5000, Training loss: 0.0004, Test loss: 13733.2432
In this epoch 700/5000, Training loss: 0.0004, Test loss: 13733.2227
In this epoch 750/5000, Training loss: 0.0004, Test loss: 13733.2031
In this epoch 800/5000, Training loss: 0.0004, Test loss: 13733.1807
In this epoch 850/5000, Training loss: 0.0004, Test loss: 13733.1660
In this epoch 900/5000, Training loss: 0.0004, Test loss: 13733.1504
In this epoch 950/5000, Training loss: 0.0004, Test loss: 13733.1396
In this epoch 1000/5000, Training loss: 0.0004, Test loss: 13733.1279
In this epoch 1050/5000, Training loss: 0.0004, Test loss: 13733.1211
In this epoch 1100/5000, Training loss: 0.0004, Test loss: 13733.1133
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 13733.1064
In this epoch 1200/5000, Training loss: 0.0004, Test loss: 13733.1035
In this epoch 1250/5000, Training loss: 0.0004, Test loss: 13733.1016
In this epoch 1300/5000, Training loss: 0.0004, Test loss: 13733.0977
In this epoch 1350/5000, Training loss: 0.0004, Test loss: 13733.0947
In this epoch 1400/5000, Training loss: 0.0004, Test loss: 13733.0947
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 13733.0928
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 13733.0947
In this epoch 1550/5000, Training loss: 0.0004, Test loss: 13733.0918
In this epoch 1600/5000, Training loss: 0.0004, Test loss: 13733.0898
In this epoch 1650/5000, Training loss: 0.0004, Test loss: 13733.0898
In this epoch 1700/5000, Training loss: 0.0004, Test loss: 13733.0898
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 13733.0898
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 13733.0889
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 13733.0889
In this epoch 1900/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 1950/5000, Training loss: 0.0004, Test loss: 13733.0889
In this epoch 2000/5000, Training loss: 0.0004, Test loss: 13733.0889
In this epoch 2050/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 13733.0889
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 2250/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 2300/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 2350/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 2400/5000, Training loss: 0.0004, Test loss: 13733.0889
In this epoch 2450/5000, Training loss: 0.0004, Test loss: 13733.0889
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 13733.0889
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 2600/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 2650/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 2700/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 2750/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 2800/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 2850/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 2900/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 2950/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 3000/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 3200/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 3250/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 3300/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 3350/5000, Training loss: 0.0004, Test loss: 13733.0859
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 13733.0869
In this epoch 3500/5000, Training loss: 262513.3125, Test loss: 4478956.5000
In this epoch 3550/5000, Training loss: 9844.9014, Test loss: 22870.9805
In this epoch 3600/5000, Training loss: 12.9343, Test loss: 13654.8164
In this epoch 3650/5000, Training loss: 0.5859, Test loss: 13700.9443
In this epoch 3700/5000, Training loss: 0.0033, Test loss: 13717.6904
In this epoch 3750/5000, Training loss: 0.0004, Test loss: 13717.3057
In this epoch 3800/5000, Training loss: 0.0004, Test loss: 13717.2646
In this epoch 3850/5000, Training loss: 0.0004, Test loss: 13717.2676
In this epoch 3900/5000, Training loss: 0.0004, Test loss: 13717.2676
In this epoch 3950/5000, Training loss: 0.0004, Test loss: 13717.2676
In this epoch 4000/5000, Training loss: 0.0004, Test loss: 13717.2666
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 13717.2646
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 13717.2666
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 13717.2646
In this epoch 4200/5000, Training loss: 0.0004, Test loss: 13717.2666
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 13717.2646
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 13717.2646
In this epoch 4350/5000, Training loss: 0.0004, Test loss: 13717.2637
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 13717.2646
In this epoch 4450/5000, Training loss: 0.0004, Test loss: 13717.2617
In this epoch 4500/5000, Training loss: 0.0004, Test loss: 13717.2646
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 13717.2646
In this epoch 4600/5000, Training loss: 0.0004, Test loss: 13717.2637
In this epoch 4650/5000, Training loss: 0.0004, Test loss: 13717.2617
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 13717.2637
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 13717.2637
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 13717.2617
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 13717.2637
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 13717.2637
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 13717.2617
In this epoch 5000/5000, Training loss: 0.0004, Test loss: 13717.2617
train_acc 0.9992647058823529
test_acc 0.0012975778546712802
{'y_pred': [161.15878295898438, 347.084228515625, 365.7756652832031, 263.9162902832031, 598.2406005859375, 419.2159118652344, 493.0708312988281, 116.88304901123047, 493.1192321777344, 248.51513671875, 175.96615600585938, 113.3321304321289, 363.70904541015625, 486.1262512207031, 330.5735778808594, 251.7018585205078, 589.12109375, 188.70265197753906, 244.51051330566406, 156.53236389160156, 433.7043151855469, 497.4863586425781, 516.4345703125, 522.8655395507812, 218.51934814453125, 417.0364685058594, 464.43292236328125, 308.56878662109375, 388.5633544921875, 450.4322814941406, 89.80036163330078, 439.9367370605469, 404.0359802246094, 144.64767456054688, 117.58580780029297, 390.0649719238281, 578.6910400390625, 546.3265991210938, 600.4069213867188, 488.3612365722656, 318.6388244628906, 277.2815246582031, 138.568603515625, 302.42218017578125, 167.69046020507812, 563.004638671875, 426.7762145996094, 179.85498046875, 239.4198760986328, 308.3305358886719, 307.4894714355469, 123.31150817871094, 520.3275146484375, 235.43807983398438, 637.1275634765625, 290.18499755859375, 514.4086303710938, 519.3320922851562, 522.556396484375, 477.8913879394531, 510.8738708496094, 197.9307861328125, 247.62132263183594, 419.8365783691406, 476.25274658203125, 114.52920532226562, 219.84585571289062, 197.69129943847656, 310.9407653808594, 193.76498413085938, 76.10265350341797, 467.6882019042969, 447.8082275390625, 304.2922668457031, 203.58828735351562, 622.421875, 137.5797882080078, 606.8702392578125, 312.53228759765625, 566.5913696289062, 312.7079162597656, 142.74435424804688, 416.597412109375, 71.37025451660156, 479.3222961425781, 560.98291015625, 254.8419647216797, 239.14122009277344, 254.6554718017578, 607.5323486328125, 110.78341674804688, 241.4580078125, 354.9560546875, 570.5321655273438, 387.21722412109375, 546.7470092773438, 412.8211364746094, 654.4111328125, 515.4513549804688, 143.09259033203125, 536.600830078125, 168.91427612304688, 493.19451904296875, 564.8357543945312, 114.68724822998047, 494.9152526855469, 637.4000244140625, 489.9299621582031, 530.9996948242188, 147.5655059814453, 266.7029113769531, 604.7194213867188, 63.66755676269531, 499.7247009277344, 566.3661499023438, 106.4341812133789, 279.3026428222656, 475.7397155761719, 56.145057678222656, 224.57049560546875, 208.46951293945312, 529.9454345703125, 240.26858520507812, 197.192626953125, 283.2340393066406, 307.93353271484375, 455.5831604003906, 247.95370483398438, 191.92987060546875, 218.6966094970703, 220.4573516845703, 455.372802734375, 596.1199340820312, 190.389404296875, 634.958740234375, 115.1330795288086, 110.94297790527344, 39.894378662109375, 466.8375549316406, 568.787109375, 104.69650268554688, 436.79193115234375, 536.515869140625, 545.6278076171875, 59.00222396850586, 397.2289733886719, 95.13517761230469, 638.6610717773438, 489.7931213378906, 417.83172607421875, 230.6807861328125, 351.5160827636719, 594.7469482421875, 545.9509887695312, 534.3522338867188, 243.68148803710938, 104.2359848022461, 507.1441955566406, 539.83447265625, 538.4478149414062, 498.9463195800781, 219.27572631835938, 725.3590087890625, 536.7167358398438, 407.8808898925781, 604.8067016601562, 603.6217651367188, 524.424560546875, 512.6815185546875, 506.0162048339844, 505.63690185546875, 210.246337890625, 236.84695434570312, 501.7140197753906, 615.05908203125, 562.00390625, 347.96917724609375, 371.8213806152344, 84.82052612304688, 581.4239501953125, 560.6922607421875, 565.03857421875, 416.8920593261719, 216.50515747070312, 597.84716796875, 654.1788330078125, 285.2056884765625, 181.40786743164062, 275.43701171875, 209.25868225097656, 218.69717407226562, 473.4216003417969, 442.650634765625, 112.75270080566406, 191.14132690429688, 635.4522094726562, 212.0105743408203, 189.20091247558594, 738.9822998046875, 440.68023681640625, 590.987060546875, 538.6812133789062, 287.4956359863281, 183.07577514648438, 505.7147521972656, 92.2405776977539, 610.066162109375, 512.9334716796875, 84.15091705322266, 170.093017578125, 162.50173950195312, 344.15374755859375, 184.56085205078125, 212.0787353515625, 59.10017013549805, 493.34759521484375, 547.6361083984375, 602.142333984375, 186.48863220214844, 501.8342590332031, 302.391845703125, 452.245849609375, 514.4244384765625, 546.5867919921875, 166.30284118652344, 654.5341186523438, 454.82122802734375, 498.6413269042969, 243.8708038330078, 512.063232421875, 111.4172592163086, 151.05130004882812, 317.33197021484375, 72.93190002441406, 77.63348388671875, 197.11029052734375, 190.46517944335938, 544.3541259765625, 556.7449340820312, 92.99253845214844, 495.073486328125, 448.2113952636719, 555.4790649414062, 463.2004089355469, 575.8767700195312, 641.0352783203125, 94.27436828613281, 606.2619018554688, 503.3212890625, 340.80401611328125, 485.7245788574219, 486.2023620605469, 126.96575927734375, 465.69091796875, 224.46778869628906, 615.9814453125, 118.59498596191406, 478.9006042480469, 553.8998413085938, 352.82171630859375, 473.9765930175781, 96.8747787475586, 230.7794189453125, 277.8440246582031, 622.3894653320312, 50.213600158691406, 110.5445785522461, 422.4782409667969, 67.1572494506836, 65.09488677978516, 474.1868896484375, 572.8684692382812, 187.54107666015625, 355.5269470214844, 447.3165283203125, 240.3447265625, 610.1700439453125, 581.8380126953125, 99.25245666503906, 160.90814208984375, 496.62396240234375, 544.4313354492188, 327.63006591796875, 284.1097412109375, 514.3546142578125, 387.6343994140625, 532.1746826171875, 149.60157775878906, 170.0419921875, 424.6156005859375, 139.91595458984375, 119.3325424194336, 216.51126098632812, 523.7919921875, 64.06352233886719, 189.48570251464844, 333.5148620605469, 539.500244140625, 568.3638916015625, 464.2898864746094, 393.944091796875, 540.993896484375, 605.6304321289062, 190.173095703125, 405.5459289550781, 469.2797546386719, 284.6851806640625, 480.5475158691406, 198.3289794921875, 219.84925842285156, 459.12017822265625, 161.26210021972656, 471.6805114746094, 487.89642333984375, 187.053466796875, 434.6905822753906, 617.13720703125, 592.449462890625, 232.45431518554688, 622.3189697265625, 243.68212890625, 485.0682067871094, 125.15133666992188, 375.3651428222656, 582.0524291992188, 390.1686096191406, 589.5384521484375, 501.7731628417969, 177.18038940429688, 151.01792907714844, 503.6700134277344, -15.994278907775879, 252.35504150390625, 609.37890625, 83.0067138671875, 380.1758117675781, 149.6024932861328, 505.73382568359375, 302.43536376953125, 651.0145263671875], 'y_pred_int': [161, 347, 366, 264, 598, 419, 493, 117, 493, 249, 176, 113, 364, 486, 331, 252, 589, 189, 245, 157, 434, 497, 516, 523, 219, 417, 464, 309, 389, 450, 90, 440, 404, 145, 118, 390, 579, 546, 600, 488, 319, 277, 139, 302, 168, 563, 427, 180, 239, 308, 307, 123, 520, 235, 637, 290, 514, 519, 523, 478, 511, 198, 248, 420, 476, 115, 220, 198, 311, 194, 76, 468, 448, 304, 204, 622, 138, 607, 313, 567, 313, 143, 417, 71, 479, 561, 255, 239, 255, 608, 111, 241, 355, 571, 387, 547, 413, 654, 515, 143, 537, 169, 493, 565, 115, 495, 637, 490, 531, 148, 267, 605, 64, 500, 566, 106, 279, 476, 56, 225, 208, 530, 240, 197, 283, 308, 456, 248, 192, 219, 220, 455, 596, 190, 635, 115, 111, 40, 467, 569, 105, 437, 537, 546, 59, 397, 95, 639, 490, 418, 231, 352, 595, 546, 534, 244, 104, 507, 540, 538, 499, 219, 725, 537, 408, 605, 604, 524, 513, 506, 506, 210, 237, 502, 615, 562, 348, 372, 85, 581, 561, 565, 417, 217, 598, 654, 285, 181, 275, 209, 219, 473, 443, 113, 191, 635, 212, 189, 739, 441, 591, 539, 287, 183, 506, 92, 610, 513, 84, 170, 163, 344, 185, 212, 59, 493, 548, 602, 186, 502, 302, 452, 514, 547, 166, 655, 455, 499, 244, 512, 111, 151, 317, 73, 78, 197, 190, 544, 557, 93, 495, 448, 555, 463, 576, 641, 94, 606, 503, 341, 486, 486, 127, 466, 224, 616, 119, 479, 554, 353, 474, 97, 231, 278, 622, 50, 111, 422, 67, 65, 474, 573, 188, 356, 447, 240, 610, 582, 99, 161, 497, 544, 328, 284, 514, 388, 532, 150, 170, 425, 140, 119, 217, 524, 64, 189, 334, 540, 568, 464, 394, 541, 606, 190, 406, 469, 285, 481, 198, 220, 459, 161, 472, 488, 187, 435, 617, 592, 232, 622, 244, 485, 125, 375, 582, 390, 590, 502, 177, 151, 504, 0, 252, 609, 83, 380, 150, 506, 302, 651], 'y_true': [125, 300, 299, 155, 614, 292, 396, 117, 331, 245, 180, 129, 337, 1015, 317, 203, 724, 165, 169, 105, 596, 352, 601, 889, 234, 380, 349, 250, 446, 911, 111, 424, 244, 179, 89, 321, 425, 572, 464, 415, 211, 276, 172, 296, 206, 361, 395, 172, 128, 292, 312, 164, 443, 237, 613, 291, 409, 358, 361, 329, 284, 181, 165, 370, 572, 211, 222, 173, 262, 207, 102, 775, 354, 307, 170, 431, 191, 726, 232, 532, 220, 167, 331, 97, 554, 700, 240, 251, 227, 676, 119, 265, 284, 583, 353, 429, 296, 430, 643, 124, 332, 133, 719, 607, 87, 406, 495, 513, 370, 191, 230, 681, 107, 507, 428, 104, 237, 378, 108, 203, 156, 547, 189, 232, 191, 296, 634, 244, 186, 205, 141, 367, 849, 187, 878, 156, 145, 89, 444, 577, 60, 635, 488, 317, 103, 326, 110, 521, 535, 280, 167, 334, 437, 366, 616, 242, 86, 542, 448, 935, 591, 198, 747, 441, 321, 491, 459, 955, 448, 569, 520, 198, 183, 380, 706, 1006, 274, 243, 106, 741, 401, 898, 590, 258, 753, 1044, 260, 200, 270, 148, 202, 373, 297, 209, 214, 631, 154, 169, 768, 312, 399, 371, 254, 158, 558, 155, 643, 420, 136, 164, 165, 324, 207, 183, 141, 546, 392, 342, 155, 676, 280, 497, 614, 590, 169, 506, 550, 813, 233, 390, 84, 199, 291, 118, 122, 185, 140, 355, 478, 79, 434, 678, 583, 379, 503, 912, 54, 898, 543, 232, 373, 299, 73, 452, 181, 621, 195, 373, 363, 257, 356, 177, 200, 243, 513, 88, 109, 389, 105, 57, 621, 771, 209, 244, 342, 209, 562, 334, 76, 194, 511, 484, 292, 253, 579, 318, 438, 137, 156, 450, 116, 170, 230, 487, 100, 216, 285, 332, 765, 342, 322, 799, 408, 173, 510, 471, 286, 378, 161, 267, 328, 108, 367, 447, 198, 423, 662, 768, 259, 545, 204, 548, 203, 295, 560, 316, 544, 677, 185, 155, 523, 32, 168, 468, 113, 521, 239, 338, 233, 423], 'train_acc': 0.9992647058823529, 'test_acc': 0.0012975778546712802, 'X': 'X_TANL_layer_7_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_8_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 67571.6250, Test loss: 24963.3418
In this epoch 100/5000, Training loss: 200.3349, Test loss: 15908.6504
In this epoch 150/5000, Training loss: 0.7147, Test loss: 14666.7256
In this epoch 200/5000, Training loss: 0.0641, Test loss: 14643.2441
In this epoch 250/5000, Training loss: 0.0013, Test loss: 14642.1016
In this epoch 300/5000, Training loss: 0.0011, Test loss: 14642.0107
In this epoch 350/5000, Training loss: 0.0009, Test loss: 14641.9561
In this epoch 400/5000, Training loss: 0.0008, Test loss: 14641.9092
In this epoch 450/5000, Training loss: 0.0007, Test loss: 14641.8623
In this epoch 500/5000, Training loss: 0.0007, Test loss: 14641.8223
In this epoch 550/5000, Training loss: 0.0006, Test loss: 14641.7822
In this epoch 600/5000, Training loss: 0.0005, Test loss: 14641.7383
In this epoch 650/5000, Training loss: 0.0005, Test loss: 14641.7090
In this epoch 700/5000, Training loss: 0.0005, Test loss: 14641.6797
In this epoch 750/5000, Training loss: 0.0004, Test loss: 14641.6504
In this epoch 800/5000, Training loss: 0.0004, Test loss: 14641.6211
In this epoch 850/5000, Training loss: 0.0004, Test loss: 14641.5977
In this epoch 900/5000, Training loss: 0.0004, Test loss: 14641.5771
In this epoch 950/5000, Training loss: 0.0004, Test loss: 14641.5635
In this epoch 1000/5000, Training loss: 0.0004, Test loss: 14641.5488
In this epoch 1050/5000, Training loss: 0.0004, Test loss: 14641.5410
In this epoch 1100/5000, Training loss: 0.0004, Test loss: 14641.5234
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 14641.5146
In this epoch 1200/5000, Training loss: 0.0004, Test loss: 14641.5117
In this epoch 1250/5000, Training loss: 0.0004, Test loss: 14641.5059
In this epoch 1300/5000, Training loss: 0.0004, Test loss: 14641.5029
In this epoch 1350/5000, Training loss: 0.0004, Test loss: 14641.4971
In this epoch 1400/5000, Training loss: 0.0004, Test loss: 14641.4941
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 14641.4902
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 14641.4883
In this epoch 1550/5000, Training loss: 0.0004, Test loss: 14641.4854
In this epoch 1600/5000, Training loss: 0.0004, Test loss: 14641.4854
In this epoch 1650/5000, Training loss: 0.0004, Test loss: 14641.4854
In this epoch 1700/5000, Training loss: 0.0004, Test loss: 14641.4844
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 14641.4854
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 14641.4844
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 1900/5000, Training loss: 0.0004, Test loss: 14641.4844
In this epoch 1950/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 2000/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 2050/5000, Training loss: 0.0004, Test loss: 14641.4844
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 14641.4844
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 14641.4814
In this epoch 2250/5000, Training loss: 0.0004, Test loss: 14641.4795
In this epoch 2300/5000, Training loss: 0.0004, Test loss: 14641.4814
In this epoch 2350/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 2400/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 2450/5000, Training loss: 0.0004, Test loss: 14641.4795
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 14641.4814
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 14641.4814
In this epoch 2600/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 2650/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 2700/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 2750/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 2800/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 2850/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 2900/5000, Training loss: 0.0004, Test loss: 14641.4844
In this epoch 2950/5000, Training loss: 0.0004, Test loss: 14641.4795
In this epoch 3000/5000, Training loss: 0.0004, Test loss: 14641.4844
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 3200/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 3250/5000, Training loss: 0.0004, Test loss: 14641.4814
In this epoch 3300/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 3350/5000, Training loss: 0.0004, Test loss: 14641.4795
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 14641.4844
In this epoch 3500/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 3550/5000, Training loss: 0.0004, Test loss: 14641.4824
In this epoch 3600/5000, Training loss: 60233416.0000, Test loss: 77016520.0000
In this epoch 3650/5000, Training loss: 366192.4688, Test loss: 17967.5820
In this epoch 3700/5000, Training loss: 116.1864, Test loss: 15563.1416
In this epoch 3750/5000, Training loss: 12.3677, Test loss: 14697.3105
In this epoch 3800/5000, Training loss: 0.0077, Test loss: 14677.2324
In this epoch 3850/5000, Training loss: 0.0004, Test loss: 14675.9473
In this epoch 3900/5000, Training loss: 0.0004, Test loss: 14675.8740
In this epoch 3950/5000, Training loss: 0.0004, Test loss: 14675.8623
In this epoch 4000/5000, Training loss: 0.0004, Test loss: 14675.8633
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 14675.8623
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 14675.8564
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 14675.8564
In this epoch 4200/5000, Training loss: 0.0004, Test loss: 14675.8564
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 14675.8535
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 14675.8506
In this epoch 4350/5000, Training loss: 0.0004, Test loss: 14675.8486
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 14675.8486
In this epoch 4450/5000, Training loss: 0.0004, Test loss: 14675.8516
In this epoch 4500/5000, Training loss: 0.0004, Test loss: 14675.8477
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 14675.8506
In this epoch 4600/5000, Training loss: 0.0004, Test loss: 14675.8486
In this epoch 4650/5000, Training loss: 0.0004, Test loss: 14675.8477
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 14675.8457
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 14675.8457
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 14675.8477
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 14675.8447
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 14675.8447
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 14675.8457
In this epoch 5000/5000, Training loss: 0.0004, Test loss: 14675.8477
train_acc 0.9992647058823529
test_acc 0.00157439446366782
{'y_pred': [158.9088134765625, 350.40118408203125, 37.89373016357422, 390.4364318847656, 490.5332946777344, 451.6224365234375, 293.8005676269531, 115.87762451171875, 236.87429809570312, 632.0305786132812, 480.7172546386719, 217.70352172851562, 570.3643798828125, 533.9603881835938, 217.9436798095703, 688.5001220703125, 464.2128601074219, 497.4236755371094, 244.12530517578125, 299.5313415527344, 528.6292114257812, 555.2330932617188, 449.1800842285156, 322.6666564941406, 234.6337127685547, 575.7373046875, 65.96575164794922, 493.1046142578125, 124.55339050292969, 593.0023803710938, 87.65689849853516, 469.1342468261719, 256.0515441894531, 262.7285461425781, 162.70733642578125, 471.18017578125, 493.71087646484375, 436.826416015625, 577.0401611328125, 485.8454284667969, 626.1376953125, 530.7999877929688, 488.4206237792969, 174.92807006835938, 574.9240112304688, 218.94192504882812, 353.0928649902344, 268.2331848144531, 498.6920166015625, 541.7139282226562, 233.46791076660156, 461.8296813964844, 578.2206420898438, 446.8470153808594, 599.4381103515625, 46.957393646240234, 191.4210968017578, 556.5587768554688, 85.42515563964844, 88.21045684814453, 535.7568359375, 517.3815307617188, 439.6775207519531, 399.9132080078125, 140.1605682373047, 127.85552978515625, 376.4538879394531, 239.03968811035156, 558.2228393554688, 205.48452758789062, 525.0971069335938, 118.25468444824219, 473.1979064941406, 125.51577758789062, 134.53607177734375, 72.80027770996094, 472.3265075683594, 161.97647094726562, 468.45526123046875, 143.97152709960938, 132.85098266601562, 114.02690887451172, 418.29864501953125, 636.8984375, 484.2526550292969, 261.5333251953125, 239.62875366210938, 62.83339309692383, 97.41539001464844, 367.5361633300781, 439.8571472167969, 18.030073165893555, 387.3666076660156, 599.1294555664062, 286.8627014160156, 595.2688598632812, 372.9999694824219, 484.6018981933594, 527.5745239257812, 632.6550903320312, 556.8648681640625, 537.4327392578125, 377.9341125488281, 359.935302734375, 30.256540298461914, 105.40048217773438, 425.7737121582031, 558.474609375, 475.9815673828125, 447.2340393066406, 395.0507507324219, 649.1094970703125, 413.2718505859375, 202.2643280029297, 257.5682373046875, 204.5557098388672, 217.93650817871094, 454.8116455078125, 161.797119140625, 514.53466796875, 153.39642333984375, 188.08074951171875, 335.814453125, 445.3771057128906, 534.8461303710938, 579.929443359375, 88.40130615234375, 492.349853515625, 108.42280578613281, 655.091064453125, 670.15576171875, 134.4786376953125, 661.5313720703125, 556.2777099609375, 252.375, 524.7800903320312, 164.9552001953125, 350.38134765625, 565.5676879882812, -2.9509403705596924, 372.6141357421875, 192.92599487304688, 501.0537414550781, 251.01010131835938, 578.4963989257812, 120.88540649414062, 450.1040344238281, 67.66218566894531, 267.0015563964844, 443.1710510253906, 83.09039306640625, 162.32952880859375, 529.9592895507812, 11.553654670715332, 435.14202880859375, 485.7189636230469, 151.07891845703125, 543.0831298828125, 651.469482421875, 98.90666198730469, 494.6497497558594, 180.15953063964844, 649.73681640625, 125.50910949707031, 507.9375305175781, 454.81536865234375, 152.3750762939453, 47.09922790527344, 163.6640167236328, 589.6085815429688, 443.3963317871094, 63.42658615112305, 472.90673828125, 449.1405944824219, 543.1338500976562, 284.0564270019531, 363.1371154785156, 485.6728820800781, 503.5065612792969, 91.53620910644531, 71.94959259033203, 197.7482147216797, 334.3027038574219, 449.1639709472656, 267.7685852050781, 521.3331909179688, 592.9049682617188, 155.24171447753906, 535.1136474609375, 102.695556640625, 446.8580017089844, 536.0298461914062, 444.45257568359375, 522.0894165039062, 465.9011535644531, 521.1852416992188, 201.7559356689453, 141.2045440673828, 165.94805908203125, 32.802207946777344, 111.57136535644531, 581.9181518554688, 181.60958862304688, 263.61444091796875, 474.7637939453125, 163.22669982910156, 506.7862243652344, 107.553955078125, 106.68025207519531, 542.240966796875, 341.3327941894531, 156.9197235107422, 135.57847595214844, 607.2808837890625, 330.3451843261719, 25.457895278930664, 244.23980712890625, 453.3147888183594, 155.00001525878906, 250.59982299804688, 534.7788696289062, 227.39236450195312, 425.28271484375, 493.1258239746094, 146.48861694335938, 147.34925842285156, 599.1792602539062, 110.51237487792969, 232.6650848388672, 485.9612121582031, 215.130126953125, 402.51580810546875, 375.79345703125, 532.5322265625, 242.57749938964844, 364.16839599609375, 425.58001708984375, 174.02796936035156, 535.6871948242188, 84.2974624633789, -24.13985252380371, 191.70236206054688, 400.5276794433594, 483.3255310058594, 40.90879821777344, 186.17486572265625, 287.5857849121094, 488.1743469238281, 577.9021606445312, 663.599365234375, 170.92037963867188, 475.1706848144531, 155.91465759277344, 475.5080261230469, 139.83238220214844, 158.51504516601562, 90.17221069335938, 463.3256530761719, 99.88890075683594, 120.8072509765625, 168.5729217529297, 517.9503784179688, 564.128173828125, 421.9918518066406, 469.0487060546875, 484.8381652832031, 548.8782348632812, 187.9849853515625, 164.23008728027344, 135.4445343017578, 467.0701599121094, 150.83370971679688, 358.2164611816406, 574.4915771484375, 527.1712646484375, 88.33268737792969, 493.0469055175781, 589.9725341796875, 523.8582153320312, 364.9585876464844, 224.9236602783203, 619.1613159179688, 460.0372009277344, 298.5010070800781, 20.322296142578125, 564.27099609375, 528.1094970703125, 634.6375122070312, 29.396743774414062, 258.3175964355469, 303.8646240234375, 527.5216674804688, 492.9411926269531, 528.7559814453125, 144.18258666992188, 612.4481811523438, 249.5108642578125, 138.70835876464844, 106.20977020263672, 306.8415222167969, 553.8309936523438, 582.0531616210938, 106.66964721679688, 733.1603393554688, 132.44683837890625, 99.85977935791016, 80.62799072265625, 168.59661865234375, 40.562313079833984, 509.9651184082031, 172.97222900390625, 148.9901123046875, 231.10081481933594, 513.734130859375, 392.4886779785156, 144.24623107910156, 192.5915985107422, 548.9457397460938, 290.5865478515625, 377.31005859375, 483.4268493652344, 441.6466369628906, 166.99923706054688, 56.62230682373047, 676.6078491210938, 534.6206665039062, 376.6899719238281, 510.5459899902344, 623.2277221679688, 296.0019226074219, 126.6604232788086, 498.9455261230469, 249.59902954101562, 547.2356567382812, 452.5824890136719, 498.3908996582031, 532.4523315429688, 553.7736206054688, 363.5879211425781, 540.4214477539062], 'y_pred_int': [159, 350, 38, 390, 491, 452, 294, 116, 237, 632, 481, 218, 570, 534, 218, 689, 464, 497, 244, 300, 529, 555, 449, 323, 235, 576, 66, 493, 125, 593, 88, 469, 256, 263, 163, 471, 494, 437, 577, 486, 626, 531, 488, 175, 575, 219, 353, 268, 499, 542, 233, 462, 578, 447, 599, 47, 191, 557, 85, 88, 536, 517, 440, 400, 140, 128, 376, 239, 558, 205, 525, 118, 473, 126, 135, 73, 472, 162, 468, 144, 133, 114, 418, 637, 484, 262, 240, 63, 97, 368, 440, 18, 387, 599, 287, 595, 373, 485, 528, 633, 557, 537, 378, 360, 30, 105, 426, 558, 476, 447, 395, 649, 413, 202, 258, 205, 218, 455, 162, 515, 153, 188, 336, 445, 535, 580, 88, 492, 108, 655, 670, 134, 662, 556, 252, 525, 165, 350, 566, 0, 373, 193, 501, 251, 578, 121, 450, 68, 267, 443, 83, 162, 530, 12, 435, 486, 151, 543, 651, 99, 495, 180, 650, 126, 508, 455, 152, 47, 164, 590, 443, 63, 473, 449, 543, 284, 363, 486, 504, 92, 72, 198, 334, 449, 268, 521, 593, 155, 535, 103, 447, 536, 444, 522, 466, 521, 202, 141, 166, 33, 112, 582, 182, 264, 475, 163, 507, 108, 107, 542, 341, 157, 136, 607, 330, 25, 244, 453, 155, 251, 535, 227, 425, 493, 146, 147, 599, 111, 233, 486, 215, 403, 376, 533, 243, 364, 426, 174, 536, 84, 0, 192, 401, 483, 41, 186, 288, 488, 578, 664, 171, 475, 156, 476, 140, 159, 90, 463, 100, 121, 169, 518, 564, 422, 469, 485, 549, 188, 164, 135, 467, 151, 358, 574, 527, 88, 493, 590, 524, 365, 225, 619, 460, 299, 20, 564, 528, 635, 29, 258, 304, 528, 493, 529, 144, 612, 250, 139, 106, 307, 554, 582, 107, 733, 132, 100, 81, 169, 41, 510, 173, 149, 231, 514, 392, 144, 193, 549, 291, 377, 483, 442, 167, 57, 677, 535, 377, 511, 623, 296, 127, 499, 250, 547, 453, 498, 532, 554, 364, 540], 'y_true': [237, 342, 154, 550, 407, 453, 276, 107, 261, 681, 337, 189, 657, 889, 132, 513, 1042, 446, 174, 228, 504, 386, 509, 244, 163, 511, 58, 486, 144, 457, 141, 670, 216, 279, 208, 832, 375, 329, 431, 697, 487, 334, 634, 155, 466, 216, 356, 238, 510, 727, 258, 289, 552, 452, 526, 108, 172, 465, 177, 90, 731, 464, 312, 321, 141, 144, 263, 193, 406, 247, 370, 82, 354, 75, 182, 103, 629, 128, 389, 187, 248, 31, 349, 477, 462, 218, 195, 156, 78, 283, 641, 80, 285, 466, 341, 373, 373, 556, 596, 628, 721, 828, 269, 305, 64, 136, 480, 384, 572, 350, 359, 560, 404, 149, 262, 150, 214, 292, 100, 602, 155, 174, 274, 443, 299, 459, 167, 328, 94, 831, 512, 202, 1099, 445, 304, 395, 235, 326, 805, 41, 296, 235, 669, 206, 355, 172, 401, 112, 299, 506, 164, 227, 394, 81, 726, 286, 121, 372, 506, 162, 433, 236, 605, 102, 294, 343, 201, 88, 158, 655, 522, 55, 438, 409, 321, 243, 344, 533, 513, 121, 78, 249, 291, 403, 285, 590, 380, 143, 433, 162, 356, 448, 402, 534, 337, 452, 199, 222, 221, 69, 175, 484, 191, 244, 296, 211, 635, 103, 185, 701, 237, 184, 73, 498, 273, 62, 227, 447, 155, 169, 431, 210, 339, 396, 148, 186, 537, 166, 237, 404, 244, 351, 253, 350, 173, 279, 436, 123, 867, 119, 60, 233, 419, 471, 125, 242, 212, 385, 421, 901, 176, 833, 143, 403, 138, 89, 139, 587, 96, 99, 131, 350, 543, 376, 616, 355, 643, 201, 197, 155, 449, 185, 232, 374, 435, 112, 448, 446, 367, 309, 155, 765, 385, 161, 108, 505, 554, 389, 125, 220, 251, 677, 544, 361, 146, 877, 121, 84, 123, 259, 761, 899, 156, 358, 193, 151, 199, 144, 87, 492, 160, 122, 206, 387, 606, 139, 199, 753, 240, 350, 347, 476, 256, 87, 758, 545, 312, 797, 790, 244, 176, 878, 216, 1044, 464, 807, 617, 461, 228, 406], 'train_acc': 0.9992647058823529, 'test_acc': 0.00157439446366782, 'X': 'X_TANL_layer_8_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_9_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 293314.5938, Test loss: 32347.9414
In this epoch 100/5000, Training loss: 805.4839, Test loss: 17964.1152
In this epoch 150/5000, Training loss: 15.8207, Test loss: 18019.7988
In this epoch 200/5000, Training loss: 0.0471, Test loss: 18049.8652
In this epoch 250/5000, Training loss: 0.0004, Test loss: 18047.2480
In this epoch 300/5000, Training loss: 0.0004, Test loss: 18047.7578
In this epoch 350/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 400/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 450/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 500/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 550/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 600/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 650/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 700/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 750/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 800/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 850/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 900/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 950/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 1000/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 1050/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 1100/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 1200/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 1250/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 1300/5000, Training loss: 0.0004, Test loss: 18047.7988
In this epoch 1350/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 1400/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 18047.7969
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 1550/5000, Training loss: 0.0004, Test loss: 18047.7988
In this epoch 1600/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 1650/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 1700/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 1900/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 1950/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 2000/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 2050/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2250/5000, Training loss: 0.0004, Test loss: 18047.7988
In this epoch 2300/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2350/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2400/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2450/5000, Training loss: 0.0004, Test loss: 18047.7988
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2600/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2650/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2700/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2750/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2800/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2850/5000, Training loss: 0.0004, Test loss: 18047.7988
In this epoch 2900/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 2950/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 3000/5000, Training loss: 0.0004, Test loss: 18047.7988
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 18047.7988
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 3200/5000, Training loss: 0.0004, Test loss: 18047.7969
In this epoch 3250/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 3300/5000, Training loss: 0.0004, Test loss: 18047.7988
In this epoch 3350/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 3500/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 3550/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 3600/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 3650/5000, Training loss: 0.0004, Test loss: 18047.8027
In this epoch 3700/5000, Training loss: 0.0004, Test loss: 18047.8008
In this epoch 3750/5000, Training loss: 17158860.0000, Test loss: 2853872.0000
In this epoch 3800/5000, Training loss: 53842.2070, Test loss: 17926.3164
In this epoch 3850/5000, Training loss: 20.4171, Test loss: 18836.3809
In this epoch 3900/5000, Training loss: 2.1592, Test loss: 17967.0273
In this epoch 3950/5000, Training loss: 0.0123, Test loss: 17972.2266
In this epoch 4000/5000, Training loss: 0.0004, Test loss: 17974.2441
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 17974.3789
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 17974.4102
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 17974.3457
In this epoch 4200/5000, Training loss: 0.0004, Test loss: 17974.3828
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 17974.4082
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 17974.3398
In this epoch 4350/5000, Training loss: 0.0004, Test loss: 17974.3770
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 17974.3770
In this epoch 4450/5000, Training loss: 0.0004, Test loss: 17974.3770
In this epoch 4500/5000, Training loss: 0.0004, Test loss: 17974.3750
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 17974.3770
In this epoch 4600/5000, Training loss: 0.0004, Test loss: 17974.3789
In this epoch 4650/5000, Training loss: 0.0004, Test loss: 17974.3789
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 17974.3770
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 17974.3789
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 17974.3770
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 17974.3789
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 17974.3789
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 17974.3789
In this epoch 5000/5000, Training loss: 0.0004, Test loss: 17974.3789
train_acc 0.9992647058823529
test_acc 0.0012889273356401383
{'y_pred': [323.7316589355469, 629.5655517578125, 103.74864959716797, 182.15963745117188, 630.32373046875, 503.661865234375, 636.9251708984375, 41.09539031982422, 109.73180389404297, 674.0317993164062, 585.61279296875, 364.49993896484375, 332.4184875488281, 192.33078002929688, 35.06748962402344, 214.5380401611328, 331.2732238769531, 470.3000183105469, 221.6946258544922, 248.02919006347656, 467.87774658203125, 734.8175659179688, 529.5820922851562, 128.37289428710938, 539.2181396484375, 600.765625, 328.8191223144531, 311.96514892578125, 570.4389038085938, 507.60711669921875, 467.5847473144531, 621.0831909179688, 496.25665283203125, 81.89434051513672, 684.8721313476562, 265.5404052734375, 239.567138671875, 659.486083984375, 292.56170654296875, 111.10153198242188, 135.8861083984375, 599.853759765625, 87.29875183105469, 200.5988006591797, 218.93606567382812, 559.140380859375, 106.18033599853516, 532.1697387695312, 202.42105102539062, 280.7346496582031, 234.84228515625, 126.55267333984375, 378.2696228027344, 359.1368713378906, 446.1700744628906, 464.4138488769531, 283.96893310546875, 127.58893585205078, 716.2716674804688, 269.840576171875, 254.43504333496094, 61.550071716308594, 156.60641479492188, 398.9486389160156, 640.0792846679688, 120.39947509765625, 330.440185546875, 480.0184326171875, 557.5591430664062, 357.8202819824219, 493.6864013671875, 546.1400756835938, 443.5259704589844, 466.7897033691406, 237.618408203125, 397.1971740722656, 116.93494415283203, 719.13623046875, 368.0515441894531, 90.02396392822266, 132.87318420410156, 735.0897827148438, 533.9185180664062, 94.4769515991211, 78.80115509033203, 526.6620483398438, 482.51568603515625, 341.01275634765625, 559.33056640625, 449.2472229003906, 639.4300537109375, 165.33056640625, 154.83450317382812, 508.0686950683594, 567.5435791015625, 650.9779663085938, 602.41796875, 142.43328857421875, 644.0786743164062, 447.4656677246094, 687.2784423828125, 48.77922058105469, 354.1848449707031, 589.5842895507812, 326.0745849609375, 364.4836120605469, 223.63629150390625, 701.373291015625, 137.76760864257812, 190.66867065429688, 266.02947998046875, 81.16560363769531, 140.65753173828125, 179.06246948242188, 582.797119140625, 364.7005310058594, 74.80792236328125, 199.73959350585938, 529.4755249023438, 72.03191375732422, 474.3835754394531, 497.8565368652344, 471.25726318359375, 38.20209503173828, 256.9278564453125, 388.2244873046875, 526.5068969726562, 228.95896911621094, 34.96967315673828, 171.9029083251953, 147.60418701171875, 62.74830627441406, 432.3494873046875, 233.3269805908203, 30.982263565063477, 319.6496276855469, 541.4982299804688, 392.0926818847656, 588.4814453125, 471.6703186035156, 269.8790283203125, 157.50596618652344, 424.17218017578125, 530.4202880859375, 417.0363464355469, 175.84140014648438, 367.1666259765625, 482.3193359375, 234.582275390625, 223.78236389160156, 591.6544189453125, 222.57241821289062, 185.05001831054688, 535.5057983398438, 252.22706604003906, 272.5519104003906, 662.3121948242188, 519.71435546875, 412.4992370605469, 481.265625, 500.4289855957031, 663.8993530273438, 441.2401123046875, 672.02685546875, 389.1589050292969, 60.214393615722656, 123.1687240600586, 283.4488525390625, 398.3800048828125, 645.8259887695312, 305.1039123535156, 620.3906860351562, 32.95684051513672, 398.5708923339844, 663.4617919921875, 324.05078125, 201.9364013671875, 368.6450500488281, 532.8218383789062, 96.59795379638672, 578.542724609375, 589.9491577148438, 528.700439453125, 449.7937316894531, 492.3690185546875, 128.65353393554688, 97.44828033447266, 304.0986328125, 488.0203552246094, 472.6403503417969, 378.17742919921875, 106.37235260009766, 286.3022766113281, 303.7519836425781, 828.0741577148438, 485.7910461425781, 641.0610961914062, 181.7340087890625, 536.688720703125, 158.56924438476562, 122.89751434326172, 39.32874298095703, 504.5269775390625, 338.3392639160156, 533.574462890625, 131.255615234375, 530.5738525390625, 165.40211486816406, -6.787359714508057, 82.20979309082031, 61.135643005371094, 285.9186706542969, 581.5618286132812, 48.216644287109375, 422.66485595703125, 574.1951293945312, 67.97753143310547, 26.927507400512695, 126.6959457397461, 410.4312744140625, 575.45556640625, 632.394775390625, 213.3820343017578, 511.4998779296875, 563.2470092773438, 399.0802307128906, 239.64886474609375, 125.76232147216797, 250.32814025878906, 623.1392822265625, 531.158203125, 521.3463745117188, 494.6154479980469, 457.40069580078125, 273.48321533203125, 431.4178161621094, 51.16962432861328, 277.2157287597656, 701.5799560546875, 507.8456726074219, 285.5588073730469, 152.0147705078125, 565.1134643554688, 609.2929077148438, 541.3712158203125, 292.7529602050781, 563.802490234375, 325.9574279785156, 185.4516143798828, 572.2271118164062, 381.5135498046875, 484.4181823730469, 458.1553649902344, 58.901432037353516, 359.2849426269531, 175.40817260742188, 414.4963684082031, 366.6470642089844, 247.14340209960938, 496.1539611816406, 266.61004638671875, 619.0760498046875, 542.1781616210938, 536.0723876953125, 512.234375, 450.0922546386719, 1.7000670433044434, 475.7130126953125, 485.2165832519531, 372.2071533203125, 343.4364929199219, 713.8753051757812, 332.98699951171875, 90.68401336669922, 347.2711181640625, 513.2022705078125, 151.03106689453125, 432.5117492675781, 260.503173828125, 555.6338500976562, 760.3770141601562, 151.63034057617188, 556.703125, 198.64422607421875, 374.8567199707031, -35.608001708984375, 275.3483581542969, 357.0873718261719, 95.29077911376953, 300.8507080078125, 156.565673828125, 139.6994171142578, 442.9701843261719, 422.1800537109375, 293.10009765625, 416.02972412109375, 637.9447631835938, 119.73503875732422, 118.8877182006836, 350.0587463378906, 506.9042053222656, 601.2439575195312, 259.6704406738281, 567.6709594726562, 286.6213073730469, 619.101806640625, 169.53277587890625, 511.37786865234375, 326.4488830566406, 53.32189178466797, 323.2891540527344, 669.66650390625, 379.4394226074219, 478.0163879394531, 370.3049621582031, 613.3369140625, 181.85716247558594, 308.68988037109375, 237.25816345214844, 774.5523071289062, 226.38528442382812, 514.4291381835938, 551.41455078125, 176.8638458251953, 75.5037612915039, 557.3980102539062, 195.50717163085938, 269.70703125, 70.06267547607422, 31.639612197875977, 551.902587890625, 275.75811767578125, 120.97132110595703, 166.99844360351562, 148.11692810058594, 527.7484130859375, 445.1612243652344, 139.14724731445312, 679.02490234375, 246.2982177734375], 'y_pred_int': [324, 630, 104, 182, 630, 504, 637, 41, 110, 674, 586, 364, 332, 192, 35, 215, 331, 470, 222, 248, 468, 735, 530, 128, 539, 601, 329, 312, 570, 508, 468, 621, 496, 82, 685, 266, 240, 659, 293, 111, 136, 600, 87, 201, 219, 559, 106, 532, 202, 281, 235, 127, 378, 359, 446, 464, 284, 128, 716, 270, 254, 62, 157, 399, 640, 120, 330, 480, 558, 358, 494, 546, 444, 467, 238, 397, 117, 719, 368, 90, 133, 735, 534, 94, 79, 527, 483, 341, 559, 449, 639, 165, 155, 508, 568, 651, 602, 142, 644, 447, 687, 49, 354, 590, 326, 364, 224, 701, 138, 191, 266, 81, 141, 179, 583, 365, 75, 200, 529, 72, 474, 498, 471, 38, 257, 388, 527, 229, 35, 172, 148, 63, 432, 233, 31, 320, 541, 392, 588, 472, 270, 158, 424, 530, 417, 176, 367, 482, 235, 224, 592, 223, 185, 536, 252, 273, 662, 520, 412, 481, 500, 664, 441, 672, 389, 60, 123, 283, 398, 646, 305, 620, 33, 399, 663, 324, 202, 369, 533, 97, 579, 590, 529, 450, 492, 129, 97, 304, 488, 473, 378, 106, 286, 304, 828, 486, 641, 182, 537, 159, 123, 39, 505, 338, 534, 131, 531, 165, 0, 82, 61, 286, 582, 48, 423, 574, 68, 27, 127, 410, 575, 632, 213, 511, 563, 399, 240, 126, 250, 623, 531, 521, 495, 457, 273, 431, 51, 277, 702, 508, 286, 152, 565, 609, 541, 293, 564, 326, 185, 572, 382, 484, 458, 59, 359, 175, 414, 367, 247, 496, 267, 619, 542, 536, 512, 450, 2, 476, 485, 372, 343, 714, 333, 91, 347, 513, 151, 433, 261, 556, 760, 152, 557, 199, 375, 0, 275, 357, 95, 301, 157, 140, 443, 422, 293, 416, 638, 120, 119, 350, 507, 601, 260, 568, 287, 619, 170, 511, 326, 53, 323, 670, 379, 478, 370, 613, 182, 309, 237, 775, 226, 514, 551, 177, 76, 557, 196, 270, 70, 32, 552, 276, 121, 167, 148, 528, 445, 139, 679, 246], 'y_true': [307, 670, 147, 164, 587, 445, 631, 115, 124, 877, 638, 341, 226, 196, 108, 186, 187, 547, 155, 221, 1297, 494, 611, 169, 385, 823, 309, 297, 412, 350, 498, 624, 355, 92, 466, 353, 233, 503, 188, 103, 215, 324, 73, 181, 172, 403, 143, 583, 292, 210, 155, 95, 340, 256, 341, 310, 163, 206, 519, 157, 314, 162, 127, 470, 445, 175, 351, 401, 460, 292, 392, 344, 494, 598, 211, 635, 241, 814, 380, 186, 178, 658, 645, 198, 100, 539, 321, 314, 542, 433, 373, 122, 156, 390, 680, 477, 364, 213, 445, 536, 380, 180, 244, 392, 212, 444, 204, 496, 171, 160, 227, 76, 105, 139, 334, 331, 175, 163, 670, 173, 296, 600, 412, 175, 181, 554, 443, 202, 113, 118, 153, 172, 371, 329, 191, 318, 562, 365, 422, 607, 209, 297, 461, 687, 331, 209, 328, 319, 256, 131, 436, 192, 235, 606, 207, 158, 613, 431, 367, 285, 962, 473, 359, 773, 484, 110, 131, 185, 587, 732, 224, 338, 92, 543, 558, 305, 200, 438, 577, 109, 372, 771, 415, 476, 586, 142, 187, 243, 646, 461, 231, 54, 193, 171, 466, 407, 601, 162, 443, 112, 144, 62, 401, 350, 407, 165, 352, 168, 109, 170, 176, 192, 805, 177, 778, 410, 216, 102, 200, 660, 445, 762, 240, 588, 1026, 401, 101, 89, 266, 527, 437, 342, 343, 340, 188, 468, 78, 233, 726, 413, 298, 104, 428, 752, 574, 205, 846, 299, 149, 904, 528, 786, 417, 119, 428, 89, 502, 572, 238, 448, 212, 346, 523, 360, 635, 375, 80, 901, 1016, 189, 193, 338, 273, 154, 337, 579, 164, 325, 214, 455, 621, 127, 420, 256, 348, 125, 233, 279, 99, 116, 166, 134, 376, 340, 291, 312, 393, 180, 76, 297, 346, 529, 184, 363, 196, 693, 89, 452, 304, 158, 249, 434, 527, 583, 326, 791, 185, 212, 288, 662, 243, 373, 392, 98, 149, 570, 215, 286, 85, 83, 369, 209, 266, 194, 161, 406, 326, 88, 619, 262], 'train_acc': 0.9992647058823529, 'test_acc': 0.0012889273356401383, 'X': 'X_TANL_layer_9_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_10_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 462649.1250, Test loss: 59037.5938
In this epoch 100/5000, Training loss: 1282.4626, Test loss: 26208.0566
In this epoch 150/5000, Training loss: 24.8748, Test loss: 25549.1777
In this epoch 200/5000, Training loss: 0.1421, Test loss: 25516.6387
In this epoch 250/5000, Training loss: 0.0578, Test loss: 25517.9062
In this epoch 300/5000, Training loss: 0.0469, Test loss: 25516.4062
In this epoch 350/5000, Training loss: 0.0372, Test loss: 25515.3652
In this epoch 400/5000, Training loss: 0.0289, Test loss: 25514.4043
In this epoch 450/5000, Training loss: 0.0220, Test loss: 25513.5000
In this epoch 500/5000, Training loss: 0.0165, Test loss: 25512.6660
In this epoch 550/5000, Training loss: 0.0121, Test loss: 25511.9062
In this epoch 600/5000, Training loss: 0.0088, Test loss: 25511.2207
In this epoch 650/5000, Training loss: 0.0062, Test loss: 25510.6172
In this epoch 700/5000, Training loss: 0.0044, Test loss: 25510.0879
In this epoch 750/5000, Training loss: 0.0031, Test loss: 25509.6328
In this epoch 800/5000, Training loss: 0.0022, Test loss: 25509.2363
In this epoch 850/5000, Training loss: 0.0015, Test loss: 25508.9004
In this epoch 900/5000, Training loss: 0.0011, Test loss: 25508.6172
In this epoch 950/5000, Training loss: 0.0008, Test loss: 25508.3887
In this epoch 1000/5000, Training loss: 0.0006, Test loss: 25508.1953
In this epoch 1050/5000, Training loss: 0.0005, Test loss: 25508.0391
In this epoch 1100/5000, Training loss: 0.0005, Test loss: 25507.9180
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 25507.8145
In this epoch 1200/5000, Training loss: 0.0004, Test loss: 25507.7422
In this epoch 1250/5000, Training loss: 0.0004, Test loss: 25507.6855
In this epoch 1300/5000, Training loss: 0.0004, Test loss: 25507.6426
In this epoch 1350/5000, Training loss: 0.0004, Test loss: 25507.6055
In this epoch 1400/5000, Training loss: 0.0004, Test loss: 25507.5762
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 25507.5586
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 25507.5469
In this epoch 1550/5000, Training loss: 0.0004, Test loss: 25507.5352
In this epoch 1600/5000, Training loss: 0.0004, Test loss: 25507.5293
In this epoch 1650/5000, Training loss: 0.0004, Test loss: 25507.5293
In this epoch 1700/5000, Training loss: 0.0004, Test loss: 25507.5234
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 25507.5215
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 25507.5156
In this epoch 1900/5000, Training loss: 0.0004, Test loss: 25507.5156
In this epoch 1950/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2000/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2050/5000, Training loss: 0.0004, Test loss: 25507.5156
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 25507.5156
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 25507.5156
In this epoch 2250/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2300/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2350/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2400/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2450/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2600/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2650/5000, Training loss: 0.0004, Test loss: 25507.5156
In this epoch 2700/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2750/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2800/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2850/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2900/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 2950/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 3000/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 25507.5156
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 25507.5156
In this epoch 3200/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 3250/5000, Training loss: 0.0004, Test loss: 25507.5156
In this epoch 3300/5000, Training loss: 0.0004, Test loss: 25507.5176
In this epoch 3350/5000, Training loss: 30057298.0000, Test loss: 34424232.0000
In this epoch 3400/5000, Training loss: 113364.1406, Test loss: 31329.6270
In this epoch 3450/5000, Training loss: 957.6241, Test loss: 26885.7617
In this epoch 3500/5000, Training loss: 1.6461, Test loss: 25524.1328
In this epoch 3550/5000, Training loss: 0.0134, Test loss: 25472.1055
In this epoch 3600/5000, Training loss: 0.0005, Test loss: 25468.6172
In this epoch 3650/5000, Training loss: 0.0004, Test loss: 25468.4590
In this epoch 3700/5000, Training loss: 0.0004, Test loss: 25468.4824
In this epoch 3750/5000, Training loss: 0.0004, Test loss: 25468.4824
In this epoch 3800/5000, Training loss: 0.0004, Test loss: 25468.4883
In this epoch 3850/5000, Training loss: 0.0004, Test loss: 25468.4922
In this epoch 3900/5000, Training loss: 0.0004, Test loss: 25468.4941
In this epoch 3950/5000, Training loss: 0.0004, Test loss: 25468.4941
In this epoch 4000/5000, Training loss: 0.0004, Test loss: 25468.4980
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 25468.4941
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 25468.4980
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 25468.4980
In this epoch 4200/5000, Training loss: 0.0004, Test loss: 25468.5000
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 25468.5039
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 25468.5000
In this epoch 4350/5000, Training loss: 0.0004, Test loss: 25468.5039
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 25468.5000
In this epoch 4450/5000, Training loss: 0.0004, Test loss: 25468.5059
In this epoch 4500/5000, Training loss: 0.0004, Test loss: 25468.5059
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 25468.5039
In this epoch 4600/5000, Training loss: 0.0004, Test loss: 25468.5039
In this epoch 4650/5000, Training loss: 0.0004, Test loss: 25468.5059
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 25468.5039
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 25468.5039
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 25468.5059
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 25468.5039
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 25468.5059
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 25468.5059
In this epoch 5000/5000, Training loss: 0.0004, Test loss: 25468.5059
train_acc 0.9992647058823529
test_acc 0.0013494809688581314
{'y_pred': [574.9261474609375, 23.534088134765625, 504.42437744140625, 304.81341552734375, 396.83184814453125, 350.27490234375, 320.626220703125, 101.70572662353516, 123.58757781982422, 114.36727142333984, 656.3297119140625, 459.49609375, 277.13641357421875, 44.27689743041992, 343.52899169921875, 315.5218811035156, 364.70062255859375, 240.3184356689453, 202.0115966796875, 21.54936981201172, 615.3604736328125, 370.12127685546875, 494.8643798828125, 624.5344848632812, 28.319087982177734, 90.02717590332031, 550.2276611328125, 563.1663208007812, 302.60284423828125, 70.98090362548828, -7.681674957275391, 183.75704956054688, 417.5227966308594, 487.56536865234375, 502.106689453125, 232.7423858642578, 603.2537231445312, 395.8143310546875, 42.93733215332031, 571.5789184570312, 493.70587158203125, 701.6205444335938, 457.06396484375, 619.6117553710938, 499.9367980957031, 158.96214294433594, 224.2041473388672, 187.14031982421875, 594.9690551757812, 213.17625427246094, 113.8248062133789, 208.62684631347656, 587.1609497070312, 102.5494155883789, 128.54855346679688, 731.6871337890625, -50.3775520324707, 125.62471008300781, 75.41370391845703, 613.6808471679688, 226.3608856201172, 478.3114013671875, 633.0747680664062, 413.84393310546875, 469.3687438964844, 383.63616943359375, 454.2191162109375, 686.1931762695312, 198.03167724609375, 368.3065185546875, -28.27975845336914, 53.88288497924805, 347.4679870605469, -70.13678741455078, 440.9967346191406, 546.8933715820312, 169.1166534423828, 250.47665405273438, 622.2806396484375, 695.3196411132812, 580.646240234375, 78.12186431884766, 493.862060546875, 79.8956069946289, 89.64569854736328, 126.37653350830078, 469.2291259765625, 547.1670532226562, 104.54444122314453, 42.00986099243164, 492.7185974121094, 407.7586669921875, 442.5216979980469, 47.90389633178711, 59.38021469116211, 276.5882568359375, 93.20995330810547, 203.19737243652344, 610.68310546875, 368.0005798339844, 275.57330322265625, 605.8211669921875, 331.3470764160156, 463.3517150878906, 79.09508514404297, 253.14979553222656, 180.7027130126953, 490.38873291015625, 457.86474609375, 55.28574752807617, -67.78150177001953, 183.80328369140625, 405.3407287597656, 241.5009002685547, 503.734619140625, 265.0282897949219, 362.2083740234375, 505.496826171875, 484.62109375, 484.9044189453125, 339.11553955078125, 607.21923828125, 502.0589599609375, 373.00018310546875, 258.6800231933594, 312.81463623046875, -120.35931396484375, 539.3976440429688, 320.54571533203125, 494.7716979980469, 199.30360412597656, 383.0632629394531, 505.4936828613281, 288.1898498535156, 54.021236419677734, 372.2878723144531, 140.52816772460938, 134.70404052734375, -10.10123062133789, 216.09332275390625, 569.4155883789062, 105.20496368408203, 658.0625, 840.0301513671875, 123.0120620727539, 47.944454193115234, 648.4855346679688, 631.7489624023438, 264.1180419921875, 494.294189453125, 114.99370574951172, 109.20877838134766, 155.80055236816406, 82.92813873291016, 150.71835327148438, 548.5563354492188, 94.0867691040039, 131.2480010986328, 485.2059020996094, 521.3959350585938, 108.7667007446289, 189.6576385498047, 126.50934600830078, 542.3436889648438, 523.8993530273438, 681.7589111328125, 338.77197265625, 78.42444610595703, 507.57049560546875, 513.408447265625, 297.204345703125, 487.9765625, 248.0655517578125, 384.6214904785156, 286.8743896484375, 459.4859619140625, 339.1287841796875, 274.4865417480469, 253.39988708496094, 686.8406372070312, 348.12274169921875, 352.5290832519531, 135.81439208984375, 253.7177276611328, 737.449462890625, 625.2928466796875, 487.03546142578125, 441.78204345703125, 563.96337890625, 175.8651885986328, 383.0205993652344, 512.754638671875, 463.6055908203125, 94.20247650146484, 451.8586730957031, 543.0453491210938, 317.127197265625, 256.1575927734375, 600.5325927734375, 274.3712158203125, 730.1779174804688, 206.88394165039062, 229.2380828857422, 376.17913818359375, 338.9184265136719, 578.7401733398438, 402.83233642578125, 546.0602416992188, 381.5261535644531, 290.74884033203125, 405.5634460449219, 615.8491821289062, 119.44591522216797, 250.91148376464844, 561.025146484375, 624.0704956054688, 468.1958312988281, 477.489501953125, 88.40155792236328, 111.98331451416016, 552.0292358398438, 40.68510055541992, -43.472843170166016, 314.98480224609375, 320.4651184082031, 340.6839904785156, 348.2513427734375, 690.9332275390625, 477.3886413574219, 244.9566192626953, 764.8355712890625, 359.61114501953125, 479.5723876953125, 640.4468994140625, 61.95113754272461, -21.930561065673828, 113.71967315673828, 75.6653060913086, 503.204345703125, 215.32879638671875, 424.4819641113281, 433.17620849609375, 748.6002197265625, 442.20892333984375, 428.0577392578125, 380.4485778808594, 544.43701171875, 393.8892517089844, 5.961742401123047, 473.52276611328125, 158.08651733398438, 284.2208251953125, 93.20545196533203, 317.43646240234375, 120.42536926269531, 673.1052856445312, 225.6803436279297, 194.3607635498047, 191.3196258544922, 333.7506103515625, 136.70167541503906, 555.2577514648438, 774.8148803710938, 217.28912353515625, 215.08592224121094, 159.23414611816406, 418.5151062011719, 86.61005401611328, 730.1522216796875, 242.78004455566406, 402.00799560546875, 218.03053283691406, 818.6542358398438, 504.2468566894531, 526.6660766601562, 246.03224182128906, 661.02587890625, 551.5330200195312, 428.2976989746094, 200.15553283691406, 433.90240478515625, 488.5040588378906, 527.1357421875, 551.3316040039062, 28.393939971923828, 376.09075927734375, 165.7207489013672, 605.7945556640625, 365.2203063964844, 450.0332336425781, 518.1591796875, 352.5852966308594, 90.17046356201172, 302.6794128417969, 569.169677734375, 722.9564208984375, 572.1551513671875, 154.99998474121094, 189.9931640625, 616.1533203125, 439.9990539550781, 85.67857360839844, 324.7065124511719, 488.5447998046875, 216.0542449951172, 144.66122436523438, 569.4700927734375, 404.52899169921875, 59.00948715209961, 528.9828491210938, 329.2528381347656, 164.4088592529297, 297.724365234375, 612.95849609375, 48.90407943725586, 408.50390625, 266.68310546875, 554.2920532226562, 85.05095672607422, 98.65396118164062, 418.4060363769531, 313.1339111328125, 271.96771240234375, 276.21331787109375, 412.30194091796875, 247.4426727294922, 126.52387237548828, 232.52011108398438, 484.27459716796875, 146.6393280029297, 50.36912155151367, 235.59385681152344, 572.070068359375, 542.469482421875, 441.08306884765625, 99.37857818603516, 154.6429901123047, 336.2218322753906, 351.1021423339844, 114.3372573852539], 'y_pred_int': [575, 24, 504, 305, 397, 350, 321, 102, 124, 114, 656, 459, 277, 44, 344, 316, 365, 240, 202, 22, 615, 370, 495, 625, 28, 90, 550, 563, 303, 71, 0, 184, 418, 488, 502, 233, 603, 396, 43, 572, 494, 702, 457, 620, 500, 159, 224, 187, 595, 213, 114, 209, 587, 103, 129, 732, 0, 126, 75, 614, 226, 478, 633, 414, 469, 384, 454, 686, 198, 368, 0, 54, 347, 0, 441, 547, 169, 250, 622, 695, 581, 78, 494, 80, 90, 126, 469, 547, 105, 42, 493, 408, 443, 48, 59, 277, 93, 203, 611, 368, 276, 606, 331, 463, 79, 253, 181, 490, 458, 55, 0, 184, 405, 242, 504, 265, 362, 505, 485, 485, 339, 607, 502, 373, 259, 313, 0, 539, 321, 495, 199, 383, 505, 288, 54, 372, 141, 135, 0, 216, 569, 105, 658, 840, 123, 48, 648, 632, 264, 494, 115, 109, 156, 83, 151, 549, 94, 131, 485, 521, 109, 190, 127, 542, 524, 682, 339, 78, 508, 513, 297, 488, 248, 385, 287, 459, 339, 274, 253, 687, 348, 353, 136, 254, 737, 625, 487, 442, 564, 176, 383, 513, 464, 94, 452, 543, 317, 256, 601, 274, 730, 207, 229, 376, 339, 579, 403, 546, 382, 291, 406, 616, 119, 251, 561, 624, 468, 477, 88, 112, 552, 41, 0, 315, 320, 341, 348, 691, 477, 245, 765, 360, 480, 640, 62, 0, 114, 76, 503, 215, 424, 433, 749, 442, 428, 380, 544, 394, 6, 474, 158, 284, 93, 317, 120, 673, 226, 194, 191, 334, 137, 555, 775, 217, 215, 159, 419, 87, 730, 243, 402, 218, 819, 504, 527, 246, 661, 552, 428, 200, 434, 489, 527, 551, 28, 376, 166, 606, 365, 450, 518, 353, 90, 303, 569, 723, 572, 155, 190, 616, 440, 86, 325, 489, 216, 145, 569, 405, 59, 529, 329, 164, 298, 613, 49, 409, 267, 554, 85, 99, 418, 313, 272, 276, 412, 247, 127, 233, 484, 147, 50, 236, 572, 542, 441, 99, 155, 336, 351, 114], 'y_true': [235, 228, 491, 376, 358, 296, 401, 45, 142, 80, 468, 338, 212, 153, 786, 325, 532, 212, 362, 193, 367, 295, 878, 530, 164, 96, 556, 420, 450, 162, 78, 260, 328, 391, 326, 193, 361, 390, 155, 440, 243, 505, 478, 510, 340, 217, 205, 224, 535, 191, 226, 224, 701, 213, 164, 778, 75, 237, 156, 456, 308, 389, 445, 599, 327, 245, 790, 548, 246, 235, 195, 166, 786, 299, 391, 629, 376, 295, 292, 456, 308, 117, 562, 187, 209, 147, 603, 445, 280, 210, 550, 185, 235, 81, 56, 200, 196, 195, 453, 331, 227, 311, 158, 552, 230, 296, 181, 634, 284, 150, 74, 291, 374, 239, 395, 130, 415, 1062, 351, 431, 608, 607, 372, 373, 425, 351, 68, 518, 449, 506, 141, 269, 521, 552, 168, 828, 185, 89, 103, 358, 907, 169, 494, 611, 113, 77, 431, 544, 244, 599, 86, 299, 145, 132, 279, 441, 156, 167, 392, 372, 144, 122, 211, 437, 485, 589, 208, 161, 718, 527, 220, 269, 161, 231, 288, 446, 163, 288, 197, 677, 199, 272, 324, 420, 467, 350, 474, 336, 447, 260, 166, 588, 731, 177, 729, 290, 445, 237, 521, 210, 504, 150, 323, 409, 542, 619, 874, 689, 509, 300, 241, 281, 98, 240, 889, 628, 455, 493, 835, 149, 507, 164, 286, 341, 339, 815, 158, 521, 286, 372, 346, 601, 419, 530, 128, 209, 252, 87, 334, 145, 370, 372, 655, 350, 821, 575, 688, 313, 108, 460, 108, 276, 222, 308, 242, 733, 162, 155, 124, 200, 160, 389, 814, 243, 301, 201, 574, 111, 748, 296, 629, 220, 495, 428, 346, 242, 445, 1027, 334, 175, 358, 561, 327, 491, 76, 324, 197, 566, 471, 259, 389, 802, 140, 344, 706, 527, 680, 155, 187, 392, 679, 78, 598, 607, 228, 484, 445, 367, 93, 425, 255, 143, 165, 494, 55, 428, 157, 523, 123, 135, 468, 264, 153, 274, 332, 130, 142, 470, 300, 92, 199, 373, 723, 329, 370, 154, 88, 440, 258, 115], 'train_acc': 0.9992647058823529, 'test_acc': 0.0013494809688581314, 'X': 'X_TANL_layer_10_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_11_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 938621.5625, Test loss: 218150.6406
In this epoch 100/5000, Training loss: 5758.4478, Test loss: 34597.2148
In this epoch 150/5000, Training loss: 39.6912, Test loss: 33936.1758
In this epoch 200/5000, Training loss: 0.2770, Test loss: 33942.3125
In this epoch 250/5000, Training loss: 0.0414, Test loss: 33945.8281
In this epoch 300/5000, Training loss: 0.0334, Test loss: 33946.1016
In this epoch 350/5000, Training loss: 0.0275, Test loss: 33946.4844
In this epoch 400/5000, Training loss: 0.0223, Test loss: 33946.8672
In this epoch 450/5000, Training loss: 0.0177, Test loss: 33947.2422
In this epoch 500/5000, Training loss: 0.0139, Test loss: 33947.5938
In this epoch 550/5000, Training loss: 0.0107, Test loss: 33947.9258
In this epoch 600/5000, Training loss: 0.0081, Test loss: 33948.2344
In this epoch 650/5000, Training loss: 0.0060, Test loss: 33948.5117
In this epoch 700/5000, Training loss: 0.0044, Test loss: 33948.7695
In this epoch 750/5000, Training loss: 0.0032, Test loss: 33949.0078
In this epoch 800/5000, Training loss: 0.0023, Test loss: 33949.2109
In this epoch 850/5000, Training loss: 0.0016, Test loss: 33949.4023
In this epoch 900/5000, Training loss: 0.0011, Test loss: 33949.5586
In this epoch 950/5000, Training loss: 0.0007, Test loss: 33949.7188
In this epoch 1000/5000, Training loss: 0.0005, Test loss: 33949.8281
In this epoch 1050/5000, Training loss: 0.0003, Test loss: 33949.9297
In this epoch 1100/5000, Training loss: 0.0002, Test loss: 33950.0195
In this epoch 1150/5000, Training loss: 0.0001, Test loss: 33950.0938
In this epoch 1200/5000, Training loss: 0.0001, Test loss: 33950.1523
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 33950.2070
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 33950.2461
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 33950.2812
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 33950.2930
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 33950.3203
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 33950.3438
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 33950.3438
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 33950.3633
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 33950.3672
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 33950.3633
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 33950.3672
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 33950.3672
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 33950.3672
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 33950.3750
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 33950.3672
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 33950.3789
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 33950.3750
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 33950.3672
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 33950.3672
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 33950.3672
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 33950.3672
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 33950.3750
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 33950.3789
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 33950.3750
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 33950.3711
In this epoch 3650/5000, Training loss: 4710772.5000, Test loss: 984845.1875
In this epoch 3700/5000, Training loss: 46554.4492, Test loss: 66561.2812
In this epoch 3750/5000, Training loss: 76.8201, Test loss: 34302.6680
In this epoch 3800/5000, Training loss: 0.6319, Test loss: 33932.8438
In this epoch 3850/5000, Training loss: 0.0082, Test loss: 33938.9141
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 33938.7344
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 33938.6953
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 33938.6797
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 33938.6875
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 33938.6875
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 33938.6953
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 33938.6992
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 33938.6992
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 33938.6992
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 33938.7070
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 33938.7070
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 33938.6992
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 33938.6992
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 33938.7070
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 33938.7070
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 33938.7070
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 33938.7070
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 33938.7031
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 33938.7031
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 33938.7109
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 33938.7031
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 33938.7070
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 33938.7031
train_acc 1.0
test_acc 0.001306228373702422
{'y_pred': [250.36126708984375, 209.1049346923828, 183.48843383789062, 318.9689636230469, 298.88720703125, 135.7381134033203, 234.0455780029297, 358.58544921875, 554.6578979492188, 460.63665771484375, 152.60276794433594, 29.25472640991211, 692.9146728515625, 344.7132568359375, 228.761962890625, 676.9866943359375, 555.9659423828125, 400.23077392578125, 237.6780548095703, 525.128173828125, 494.7789306640625, 601.694580078125, 118.93478393554688, 169.79827880859375, 307.7248840332031, 574.4449462890625, 415.783203125, 478.38525390625, 273.7088623046875, 239.69288635253906, -241.1406707763672, 32.08320617675781, 224.3769989013672, 644.2350463867188, 668.33203125, 845.8392333984375, 600.5340576171875, 174.9136505126953, 57.25640106201172, 124.44938659667969, 574.4159545898438, 149.45602416992188, 718.107666015625, 519.8433837890625, 375.29833984375, 416.75347900390625, 80.68868255615234, 634.8690185546875, 169.47496032714844, 348.83746337890625, 494.7607421875, 544.58349609375, 672.1622924804688, 291.3477783203125, 477.096923828125, -14.959303855895996, 90.79415130615234, 8.138127326965332, 766.7587280273438, 439.0797119140625, 384.8246765136719, 340.644287109375, -29.693115234375, 179.1842498779297, 694.9403076171875, 353.27410888671875, 567.8729248046875, 28.76909637451172, 597.5257568359375, 484.23699951171875, 473.42633056640625, 559.4808349609375, 190.22117614746094, 490.31817626953125, 125.8676528930664, 304.304931640625, 254.9551544189453, 391.583984375, 53.39739990234375, 734.3026123046875, 980.4132690429688, 427.391845703125, 35.6884765625, 186.78627014160156, 571.0421752929688, 256.39886474609375, 395.0238952636719, 756.4024658203125, 874.79931640625, 708.392333984375, 869.5957641601562, 109.29861450195312, 58.92494201660156, 912.5958251953125, 254.8897247314453, 235.5296173095703, 748.1768188476562, 321.1036071777344, 468.5994567871094, 571.9955444335938, 327.8086242675781, 259.32598876953125, 242.38011169433594, 674.2601318359375, 8.165314674377441, 1.1232458353042603, 61.21512222290039, 16.494735717773438, 334.2959899902344, 138.18930053710938, 98.44090270996094, 796.867919921875, 0.88713139295578, 308.79217529296875, 448.5659484863281, 126.1197280883789, 24.527297973632812, 180.4326629638672, 81.20518493652344, 548.057373046875, 399.16986083984375, 461.4093017578125, 410.8796691894531, 542.670166015625, 287.6466064453125, 378.0766296386719, -15.495329856872559, 110.80308532714844, 705.0973510742188, -39.364288330078125, 392.8363037109375, 420.9917907714844, 470.8705139160156, -75.02466583251953, 198.68991088867188, 350.86871337890625, 41.309818267822266, 314.9120178222656, 537.770751953125, 251.4579620361328, 453.18609619140625, 281.9761962890625, 654.0302734375, 521.5740966796875, 741.4774169921875, 425.4775390625, 222.59461975097656, 427.18701171875, 262.445556640625, 113.71080017089844, 197.3162384033203, 322.8363342285156, 64.02688598632812, 614.8295288085938, 77.35247802734375, 605.7453002929688, 515.4337158203125, 289.59942626953125, 686.8372192382812, 333.28912353515625, 513.034912109375, 170.9613800048828, 150.19606018066406, 503.56134033203125, 803.0048217773438, 474.32110595703125, -47.53394317626953, 86.10185241699219, 520.035400390625, 2.9617888927459717, 168.16619873046875, 78.11935424804688, 188.62486267089844, 384.1244201660156, 852.23193359375, 257.63201904296875, 339.97607421875, 454.02923583984375, -59.333465576171875, -59.31555938720703, 673.2584228515625, 476.9037170410156, 224.8221435546875, 133.96824645996094, 680.6486206054688, 257.5592041015625, -26.966007232666016, 381.5909729003906, 135.4257049560547, 492.9430847167969, 273.2101135253906, 471.2978515625, 160.99806213378906, 324.63336181640625, 167.11302185058594, 158.0101776123047, 235.6537628173828, 666.3417358398438, 80.30963134765625, 28.292816162109375, 148.606201171875, 299.7279968261719, 558.0152587890625, 488.27911376953125, 284.45928955078125, 69.48886108398438, 213.2612762451172, 216.94308471679688, 554.3273315429688, 207.67539978027344, 544.5828857421875, 605.765625, 510.7381896972656, -20.777755737304688, 550.7724609375, 41.06584930419922, 243.4860382080078, 487.0481262207031, 541.0153198242188, 303.02093505859375, 325.72113037109375, 757.120361328125, 294.13250732421875, 266.5063171386719, 42.7490348815918, -17.294418334960938, 359.0492248535156, 539.6509399414062, 57.34190368652344, 21.046401977539062, 711.0548095703125, 541.4375610351562, 506.08978271484375, 450.93682861328125, 589.396484375, 91.34877014160156, 278.82666015625, 269.12506103515625, 602.037841796875, 231.2396697998047, 19.64276695251465, 839.999755859375, 169.8400115966797, 190.1475067138672, 84.67771911621094, 654.3568725585938, 180.87991333007812, 146.29945373535156, 55.31640625, 481.0672607421875, 889.841796875, 454.3482666015625, 118.94981384277344, 271.8544616699219, 497.31939697265625, 319.12567138671875, 519.5776977539062, 551.4727783203125, 458.80145263671875, 489.1396179199219, 436.9756774902344, 244.38389587402344, 659.4503784179688, 724.581787109375, 666.8456420898438, 18.866592407226562, 403.2616271972656, 74.50489807128906, 438.0760803222656, 239.3973846435547, 760.899169921875, 718.1170654296875, 507.32733154296875, 70.46751403808594, 168.53663635253906, 291.07598876953125, 378.4532165527344, 311.05316162109375, 242.16387939453125, 135.5801544189453, 580.5385131835938, 386.16766357421875, 14.8422212600708, 196.7820587158203, 318.608154296875, 402.1819763183594, -85.00923919677734, 740.6495971679688, -4.062163829803467, 157.71469116210938, 761.665283203125, 169.93174743652344, 349.913330078125, 690.6683349609375, 96.40486145019531, 455.14019775390625, 847.8609619140625, 680.5270385742188, 492.9608154296875, 543.6541748046875, 948.2716064453125, 140.4178009033203, 13.744843482971191, 497.5443115234375, 692.1556396484375, 516.6929931640625, 411.8079528808594, 229.46250915527344, 437.8397216796875, 449.50372314453125, 285.8260498046875, 437.3971862792969, 579.230712890625, 364.88897705078125, 433.1664123535156, 326.4792175292969, 560.2666015625, 116.31271362304688, 133.02603149414062, 593.8936767578125, 301.75244140625, -105.69770812988281, 477.3870849609375, 348.85595703125, 761.8223876953125, 450.83282470703125, 564.230224609375, 685.3098754882812, 619.2539672851562, 116.81318664550781, 170.81910705566406, 151.78221130371094, 476.9543762207031, 756.7794799804688, 262.42144775390625, 343.8319396972656, 333.69635009765625, 433.523193359375, -59.61742401123047, 694.524658203125], 'y_pred_int': [250, 209, 183, 319, 299, 136, 234, 359, 555, 461, 153, 29, 693, 345, 229, 677, 556, 400, 238, 525, 495, 602, 119, 170, 308, 574, 416, 478, 274, 240, 0, 32, 224, 644, 668, 846, 601, 175, 57, 124, 574, 149, 718, 520, 375, 417, 81, 635, 169, 349, 495, 545, 672, 291, 477, 0, 91, 8, 767, 439, 385, 341, 0, 179, 695, 353, 568, 29, 598, 484, 473, 559, 190, 490, 126, 304, 255, 392, 53, 734, 980, 427, 36, 187, 571, 256, 395, 756, 875, 708, 870, 109, 59, 913, 255, 236, 748, 321, 469, 572, 328, 259, 242, 674, 8, 1, 61, 16, 334, 138, 98, 797, 1, 309, 449, 126, 25, 180, 81, 548, 399, 461, 411, 543, 288, 378, 0, 111, 705, 0, 393, 421, 471, 0, 199, 351, 41, 315, 538, 251, 453, 282, 654, 522, 741, 425, 223, 427, 262, 114, 197, 323, 64, 615, 77, 606, 515, 290, 687, 333, 513, 171, 150, 504, 803, 474, 0, 86, 520, 3, 168, 78, 189, 384, 852, 258, 340, 454, 0, 0, 673, 477, 225, 134, 681, 258, 0, 382, 135, 493, 273, 471, 161, 325, 167, 158, 236, 666, 80, 28, 149, 300, 558, 488, 284, 69, 213, 217, 554, 208, 545, 606, 511, 0, 551, 41, 243, 487, 541, 303, 326, 757, 294, 267, 43, 0, 359, 540, 57, 21, 711, 541, 506, 451, 589, 91, 279, 269, 602, 231, 20, 840, 170, 190, 85, 654, 181, 146, 55, 481, 890, 454, 119, 272, 497, 319, 520, 551, 459, 489, 437, 244, 659, 725, 667, 19, 403, 75, 438, 239, 761, 718, 507, 70, 169, 291, 378, 311, 242, 136, 581, 386, 15, 197, 319, 402, 0, 741, 0, 158, 762, 170, 350, 691, 96, 455, 848, 681, 493, 544, 948, 140, 14, 498, 692, 517, 412, 229, 438, 450, 286, 437, 579, 365, 433, 326, 560, 116, 133, 594, 302, 0, 477, 349, 762, 451, 564, 685, 619, 117, 171, 152, 477, 757, 262, 344, 334, 434, 0, 695], 'y_true': [119, 329, 250, 215, 308, 206, 180, 297, 598, 445, 97, 61, 1027, 180, 113, 665, 934, 500, 266, 393, 1009, 332, 117, 232, 337, 607, 548, 227, 160, 108, 213, 183, 209, 373, 601, 580, 543, 332, 41, 214, 461, 243, 558, 470, 351, 370, 76, 537, 109, 184, 465, 426, 441, 193, 455, 88, 238, 96, 381, 389, 214, 313, 81, 220, 401, 284, 756, 373, 331, 291, 842, 801, 209, 707, 237, 679, 634, 446, 215, 874, 445, 663, 199, 178, 608, 243, 182, 614, 500, 572, 716, 98, 201, 901, 168, 158, 715, 700, 523, 565, 274, 317, 212, 529, 60, 125, 66, 264, 249, 117, 153, 371, 270, 155, 350, 177, 298, 257, 109, 621, 182, 296, 312, 718, 200, 270, 32, 403, 601, 121, 437, 550, 792, 81, 195, 502, 156, 507, 1342, 105, 337, 286, 292, 326, 471, 333, 161, 370, 233, 212, 108, 143, 113, 643, 164, 602, 491, 351, 450, 165, 302, 294, 110, 912, 370, 352, 145, 204, 344, 64, 191, 128, 211, 605, 719, 217, 158, 536, 197, 118, 561, 460, 258, 98, 511, 101, 162, 641, 307, 794, 207, 644, 209, 268, 185, 462, 143, 823, 172, 159, 87, 343, 600, 307, 189, 227, 198, 210, 921, 202, 348, 346, 258, 167, 356, 83, 249, 448, 785, 239, 222, 664, 266, 228, 113, 77, 795, 468, 832, 200, 686, 312, 645, 352, 409, 155, 327, 667, 530, 488, 122, 841, 169, 167, 149, 324, 199, 205, 196, 387, 799, 309, 296, 292, 429, 276, 452, 425, 402, 391, 392, 240, 598, 771, 319, 170, 699, 157, 334, 294, 414, 833, 526, 104, 135, 167, 212, 415, 139, 141, 842, 173, 136, 384, 163, 343, 148, 397, 175, 308, 431, 164, 378, 638, 234, 445, 634, 258, 572, 898, 378, 187, 166, 532, 380, 314, 102, 194, 407, 433, 264, 328, 585, 233, 538, 236, 560, 175, 234, 339, 216, 259, 361, 275, 661, 299, 208, 395, 743, 123, 123, 81, 324, 934, 141, 312, 253, 289, 107, 205], 'train_acc': 1.0, 'test_acc': 0.001306228373702422, 'X': 'X_TANL_layer_11_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 0.3771, Test loss: 1.3101
In this epoch 100/5000, Training loss: 0.0019, Test loss: 1.0893
In this epoch 150/5000, Training loss: 0.0000, Test loss: 1.0900
In this epoch 200/5000, Training loss: 0.0000, Test loss: 1.0895
In this epoch 250/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 300/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 350/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 400/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 450/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 500/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 550/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 600/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 650/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 700/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 750/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 800/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 850/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 900/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 950/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 1.0894
In this epoch 3450/5000, Training loss: 20.2631, Test loss: 44.4808
In this epoch 3500/5000, Training loss: 0.0433, Test loss: 1.2254
In this epoch 3550/5000, Training loss: 0.0004, Test loss: 1.1057
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 1.1061
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 1.1061
train_acc 0.34386137543252593
test_acc 0.4190657439446367
{'y_pred': [1.386555552482605, 0.9896695017814636, 1.0240931510925293, 1.3976621627807617, 1.0402556657791138, 0.4985884726047516, 0.8997620940208435, 0.9863307476043701, 1.1706361770629883, 0.5391756296157837, 0.1545829325914383, 0.3912275433540344, 0.03261857479810715, 1.1832002401351929, 0.7631137371063232, 0.860385537147522, 1.2367099523544312, 1.062393069267273, 1.2149683237075806, 0.7957791686058044, 0.6295751929283142, 0.9851683378219604, 1.0814005136489868, 0.4267547130584717, 0.619338870048523, 0.7396559715270996, 0.6578333377838135, 0.4015820622444153, 0.627022922039032, 1.2717944383621216, 0.6189550757408142, 0.207508847117424, 1.2982475757598877, 1.691688060760498, 0.37121254205703735, 1.4466205835342407, 0.9334321022033691, 0.5584673285484314, 0.9438658952713013, -0.04092182591557503, 0.5993382930755615, 0.7531723976135254, 0.295317143201828, 0.993249773979187, 0.40581759810447693, 0.7682676911354065, 0.42312556505203247, 0.7163784503936768, 0.7105715274810791, 0.9862393736839294, 0.5999277830123901, 1.4436553716659546, 1.3133220672607422, 0.27474159002304077, 1.0215868949890137, 1.8627794981002808, 0.42965391278266907, -0.14856652915477753, 1.125427007675171, 0.47720229625701904, 1.3440253734588623, 0.5778253674507141, 0.926487922668457, 1.4287407398223877, 1.1770577430725098, 1.0746808052062988, 0.38944244384765625, 0.3797295391559601, 0.5296226143836975, 1.0889983177185059, 0.0102395536378026, 1.5038429498672485, 0.5740275979042053, 2.012444496154785, 1.4682953357696533, 0.8967893123626709, 0.5048858523368835, 0.8096573352813721, 1.3514227867126465, 1.002026081085205, 1.270584225654602, 0.7916638851165771, 0.5904836058616638, 1.584244966506958, 0.6635701656341553, 0.21338747441768646, 1.6347459554672241, 0.4720056354999542, 1.032996416091919, 0.26403969526290894, 0.5423696041107178, 1.5673788785934448, 0.5215718746185303, 0.1885969489812851, 1.2458735704421997, 1.1743597984313965, 1.131219506263733, 0.5215173959732056, 0.4726335406303406, 0.9372665882110596, 0.8059843182563782, 1.0541622638702393, 0.04134434834122658, 1.201193928718567, 1.3292958736419678, 1.0623701810836792, 0.5894330143928528, 1.156118392944336, 0.7231730222702026, 0.7387397289276123, 1.1823742389678955, 0.5380244255065918, 1.4566380977630615, 0.8497897386550903, 1.003722071647644, 0.6263601779937744, 0.7175770998001099, 1.3678078651428223, 0.736031711101532, 1.1430597305297852, 1.1140122413635254, 1.5903962850570679, 0.4651941657066345, 1.33778715133667, 0.8765645623207092, 0.8346114158630371, 0.24632667005062103, 0.4290620684623718, 0.60588538646698, 0.6806190609931946, 1.9297417402267456, 1.9493893384933472, 1.1005254983901978, 0.7009287476539612, 0.3691856265068054, 1.8140835762023926, 1.1560989618301392, -0.2939005494117737, 1.1362354755401611, 0.9393904805183411, 0.37014588713645935, 0.7383906245231628, -0.20104694366455078, 1.5184578895568848, 0.7388120889663696, 0.9955836534500122, 0.5537936687469482, 0.023139309138059616, 1.3280404806137085, 0.9507269859313965, 1.0403443574905396, 0.9863211512565613, 0.9932698011398315, -0.18515531718730927, 1.1591176986694336, 1.4827666282653809, 0.8718444108963013, 1.3183174133300781, 0.9360798001289368, 1.247237205505371, 1.2423985004425049, 0.2901553511619568, 0.4065124988555908, 0.7661468982696533, 0.6204192638397217, 0.2646145820617676, 0.8663606643676758, 1.6936593055725098, 1.1743252277374268, 1.4076478481292725, 0.4239395260810852, 0.8386102318763733, 0.6767601370811462, 1.081915259361267, -0.23367716372013092, 0.3578055799007416, 1.4870212078094482, 1.4225612878799438, 1.4072372913360596, 1.1795463562011719, 0.4004952907562256, 0.5278511643409729, 0.2090197056531906, 0.6280421614646912, 0.8176347613334656, 0.28450632095336914, 0.07787235826253891, -0.13546739518642426, 0.7140669226646423, 0.5806048512458801, 0.32991328835487366, 1.2347193956375122, 0.1802392452955246, 2.0857248306274414, 0.7802102565765381, 1.9397519826889038, 1.3323545455932617, 0.8907381296157837, 0.7660501003265381, 0.45047077536582947, 0.5545064210891724, 0.8613418936729431, 0.7806625366210938, 1.323854923248291, 1.261322259902954, 0.30951738357543945, 0.30332884192466736, 1.577934980392456, 0.11026915162801743, 0.7417463064193726, 0.31816166639328003, 0.4188382923603058, 0.5998555421829224, 1.4073069095611572, 0.5712301731109619, 1.2616679668426514, 0.7071694135665894, 0.7110424041748047, 1.6596850156784058, -0.08667777478694916, 0.8616753220558167, 1.3443344831466675, 0.5676184296607971, 0.8287410736083984, 1.006833791732788, 1.4372620582580566, 1.059069037437439, 1.7570648193359375, 0.5707089900970459, 1.3083131313323975, 1.4098602533340454, 1.2759511470794678, 1.1816712617874146, 0.7845541834831238, 0.46669450402259827, 1.4748542308807373, 1.230118751525879, 1.5692692995071411, 1.521594762802124, 0.4726335406303406, 0.9773028492927551, 0.46703779697418213, 1.3441623449325562, 1.7595696449279785, 0.7212797403335571, 0.9069769382476807, 0.4347340762615204, -0.35609734058380127, 1.1903694868087769, 2.0058531761169434, 0.4894641637802124, 0.16280236840248108, 0.8925889134407043, 0.35564273595809937, 0.9626762270927429, 0.7631114721298218, 0.10733265429735184, 0.5576159358024597, 1.0956250429153442, 1.3492120504379272, 1.1311254501342773, 1.444694995880127, 1.9838860034942627, 0.7347737550735474, 1.026001214981079, 0.3470655381679535, 1.0364331007003784, 0.32066529989242554, 0.507506251335144, 0.9654549360275269, 0.44448357820510864, 0.6995909214019775, 1.3911371231079102, 1.218183159828186, 0.7947685122489929, -0.013724888674914837, 0.534130334854126, 1.1454699039459229, 1.432896614074707, 0.9999767541885376, -5.529727786779404e-08, 1.3144525289535522, -0.015872571617364883, 0.7977862358093262, 0.6879488229751587, 1.4026541709899902, 0.9028767347335815, 1.113534927368164, 0.16359414160251617, 0.1804506927728653, 0.5925727486610413, -0.19480986893177032, -0.021988917142152786, 0.9315786361694336, 1.3693103790283203, 0.7083969116210938, 0.6303672790527344, 0.9287551045417786, 1.3496654033660889, 1.2322373390197754, 1.0704587697982788, 0.29830843210220337, 1.7768343687057495, -0.09787460416555405, 0.39513397216796875, 0.45221656560897827, 1.139192819595337, 0.13583311438560486, 0.9585205316543579, 0.8121289014816284, 0.5825240612030029, 1.4368560314178467, 1.2013994455337524, 1.177245020866394, 0.9645227193832397, 1.7697029113769531, 1.6429798603057861, 0.5080148577690125, 0.619847297668457, 1.3772207498550415, 0.5083263516426086, 0.36360839009284973, 1.1758177280426025, 0.7229539155960083, 1.1431275606155396, 0.23040492832660675, 1.5064492225646973, 1.2316917181015015, 1.1497622728347778, 0.7561030983924866, 1.7985539436340332, 0.2923431694507599, 0.958422064781189, 0.42714667320251465, 0.5418906211853027, 0.44803422689437866, 1.1523852348327637, 2.2091026306152344, 1.2591160535812378, 0.40775376558303833], 'y_pred_int': [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 2, 0, 1, 0, 1, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 0, 0, 1, 1, 2, 2, 1, 1, 0, 2, 1, 0, 1, 1, 0, 1, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 2, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 2, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 2, 0, 1, 0, 1, 2, 1, 1, 0, 0, 1, 2, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 2, 0, 1, 0, 1, 0, 1, 2, 1, 0], 'y_true': [3, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2, 0, 2, 1, 1, 1, 1, 0, 1, 0, 1, 2, 0, 1, 3, 1, 0, 1, 1, 0, 3, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 2, 1, 2, 0, 1, 0, 1, 0, 1, 1, 1, 2, 4, 2, 1, 0, 0, 0, 1, 1, 1, 2, 2, 1, 0, 3, 1, 0, 1, 7, 0, 0, 7, 0, 0, 2, 2, 0, 0, 1, 8, 0, 0, 1, 3, 1, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 2, 1, 2, 3, 1, 2, 1, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 2, 1, 0, 0, 5, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 5, 0, 1, 1, 0, 3, 1, 1, 3, 1, 3, 1, 0, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 2, 1, 0, 0, 0, 3, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 3, 2, 2, 1, 1, 0, 1, 0, 0, 6, 2, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0, 1, 0, 2, 2, 0, 1, 0, 1, 0, 2, 2, 0, 0, 0, 1, 0, 0, 2, 0, 1, 1, 1, 3, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 0, 2, 0, 0, 1, 1, 2, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1], 'train_acc': 0.34386137543252593, 'test_acc': 0.4190657439446367, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 0.6283, Test loss: 40.5167
In this epoch 100/5000, Training loss: 0.0064, Test loss: 40.9729
In this epoch 150/5000, Training loss: 0.0000, Test loss: 40.9361
In this epoch 200/5000, Training loss: 0.0000, Test loss: 40.9286
In this epoch 250/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 300/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 350/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 400/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 450/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 500/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 550/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 600/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 650/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 700/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 750/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 800/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 850/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 900/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 950/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 40.9284
In this epoch 1400/5000, Training loss: 0.7861, Test loss: 137.9604
In this epoch 1450/5000, Training loss: 0.5867, Test loss: 40.8198
In this epoch 1500/5000, Training loss: 0.0005, Test loss: 40.9526
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 40.9197
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 40.9211
In this epoch 3250/5000, Training loss: 2.3289, Test loss: 44.9024
In this epoch 3300/5000, Training loss: 0.0391, Test loss: 40.9559
In this epoch 3350/5000, Training loss: 0.0002, Test loss: 40.9149
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 40.9214
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4450/5000, Training loss: 27.6580, Test loss: 45.4040
In this epoch 4500/5000, Training loss: 0.0107, Test loss: 40.8071
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 40.9047
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 40.9212
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 40.9214
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 40.9213
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 40.9213
train_acc 1.0
test_acc 0.10882352941176471
{'y_pred': [5.491335868835449, 8.939581871032715, 17.321672439575195, 23.85391616821289, 20.7880916595459, 3.532829523086548, 4.612541198730469, 14.538844108581543, 3.2798564434051514, 1.3233567476272583, 4.102646827697754, 26.89646339416504, 8.809081077575684, 18.86741828918457, 8.04301929473877, 18.69027328491211, 21.4193172454834, 6.083510875701904, 8.250407218933105, 24.25362205505371, 24.54872703552246, 5.190116882324219, 10.602869987487793, 22.70720672607422, 4.82338285446167, 15.223258972167969, 20.711233139038086, 7.031912326812744, 12.375943183898926, 22.54625129699707, 19.51314926147461, 19.450227737426758, 9.409167289733887, 17.402130126953125, 10.017402648925781, 15.039721488952637, 8.330860137939453, 16.524621963500977, 18.35004997253418, 3.2074248790740967, 9.550555229187012, 12.701358795166016, 20.36981964111328, 18.42975425720215, 19.493053436279297, 25.227434158325195, 2.7468862533569336, 10.142313957214355, 27.065046310424805, 8.922672271728516, 18.46190071105957, 8.712028503417969, 10.900506019592285, 22.31714630126953, 5.821145057678223, 24.377824783325195, 3.960294008255005, 2.1222610473632812, 6.860616683959961, 13.97412109375, 13.533522605895996, 26.373491287231445, 9.438798904418945, 4.721737861633301, 22.45091438293457, 2.7020885944366455, 18.199573516845703, 11.666033744812012, 11.84399127960205, 21.904308319091797, 21.976572036743164, 10.97175121307373, 15.753007888793945, 25.291379928588867, 5.264098167419434, 19.521564483642578, 8.604308128356934, 2.6224772930145264, 4.7472662925720215, 19.010332107543945, 13.304473876953125, 4.3169450759887695, 14.384133338928223, 29.101423263549805, 18.823957443237305, 7.6203293800354, 22.357633590698242, 7.044800281524658, 20.823162078857422, 18.850994110107422, 12.180394172668457, 16.942607879638672, 4.9486589431762695, 12.709195137023926, 2.0528576374053955, 6.094758033752441, 21.105546951293945, 11.487082481384277, 9.250542640686035, 6.889373302459717, 4.6887946128845215, 21.550809860229492, 23.581539154052734, 20.684572219848633, 15.588064193725586, 12.030709266662598, 22.829246520996094, 14.740508079528809, 20.194780349731445, 18.132143020629883, 24.74554443359375, 4.5176191329956055, 20.168548583984375, 22.390958786010742, 11.02222728729248, 21.061784744262695, 24.012964248657227, 13.317081451416016, 23.896284103393555, 19.557100296020508, 19.900480270385742, 6.136272430419922, 4.639730930328369, 4.859315872192383, 4.788612365722656, 4.322160243988037, 11.512214660644531, 4.01649808883667, 20.77387809753418, 2.281815528869629, 10.05346965789795, 17.32949447631836, 25.199222564697266, 9.958884239196777, 21.96739387512207, 31.1452579498291, 17.236913681030273, 3.543309211730957, 13.446715354919434, 6.791202545166016, 17.82074737548828, 17.617958068847656, 15.759696006774902, 5.980234146118164, 10.363961219787598, 9.177694320678711, 21.642377853393555, 6.04194450378418, 22.346323013305664, 23.255441665649414, 21.793071746826172, 24.096094131469727, 10.052252769470215, 10.777873039245605, 18.988317489624023, 22.204177856445312, 20.820932388305664, 15.086188316345215, 7.789028644561768, 22.511157989501953, 7.162265300750732, 20.173114776611328, 10.06700325012207, 20.278560638427734, 17.460451126098633, 2.646930456161499, 18.273115158081055, 17.37169075012207, 5.936842918395996, 4.0296406745910645, 14.467881202697754, 6.042422771453857, 18.11183738708496, 5.527090549468994, 3.594963550567627, 22.5750732421875, 6.876982688903809, 14.822632789611816, 8.139039039611816, 9.123498916625977, 9.350764274597168, 12.128392219543457, 4.857087135314941, 25.08176612854004, 19.0899658203125, 4.541681289672852, 6.057781219482422, 9.889206886291504, 17.592159271240234, 22.392412185668945, 5.20286750793457, 22.02110481262207, 9.664145469665527, 29.0, 4.3726115226745605, 18.65413475036621, 4.34206485748291, 16.154569625854492, 18.68467903137207, 7.909847259521484, 14.058533668518066, 18.948808670043945, 16.72719383239746, 5.342177391052246, 17.33055877685547, 21.185327529907227, 4.551680564880371, 11.276605606079102, 18.13287925720215, 6.170886993408203, 5.685676097869873, 22.830739974975586, 3.0937798023223877, 6.112591743469238, 17.87250328063965, 10.035317420959473, 3.840379476547241, 18.216760635375977, 8.89155387878418, 22.685306549072266, 9.403679847717285, 6.2941575050354, 13.993027687072754, 19.546768188476562, 23.369976043701172, 3.8784639835357666, 21.034950256347656, 17.376916885375977, 19.12403678894043, 10.684304237365723, 10.292929649353027, 10.804498672485352, 10.189188957214355, 7.385780334472656, 2.6384389400482178, 19.302364349365234, 7.004225730895996, 7.8663740158081055, 23.904680252075195, 25.867910385131836, 21.136606216430664, 19.546838760375977, 19.018171310424805, 3.5165579319000244, 11.42670726776123, 4.748815536499023, 18.97953987121582, 17.986711502075195, 27.62021827697754, 21.33591651916504, 17.63569450378418, 22.809295654296875, 20.64700698852539, 14.907400131225586, 17.74313735961914, 15.68848705291748, 4.356751918792725, 5.140669345855713, 23.564298629760742, 5.366140842437744, 19.958541870117188, 7.299266338348389, 9.82750415802002, 19.43435287475586, 11.30008602142334, 6.725717067718506, 18.8840389251709, 20.419357299804688, 14.807419776916504, 19.585281372070312, 18.399335861206055, 3.192816972732544, 13.657666206359863, 18.569135665893555, 3.4378104209899902, 11.18057918548584, 12.557290077209473, 7.689599990844727, 17.34737205505371, 9.272403717041016, 8.366093635559082, 12.778090476989746, 8.74475383758545, 4.3406171798706055, 16.01920509338379, 25.995582580566406, 21.0034122467041, 21.41705894470215, 11.601943016052246, 23.453142166137695, 21.846805572509766, 8.68737506866455, 5.223752021789551, 6.52338981628418, 16.326274871826172, 15.504018783569336, 8.391679763793945, 5.88124942779541, 5.094881057739258, 21.585662841796875, 8.520434379577637, 24.855932235717773, 8.656286239624023, 21.064865112304688, 19.775577545166016, 23.333192825317383, 25.635433197021484, 6.864660263061523, 24.19506072998047, 8.782740592956543, 22.026968002319336, 5.148473739624023, 21.22186851501465, 3.173234224319458, 7.763796329498291, 17.764066696166992, 22.63522720336914, 20.202112197875977, 4.879161834716797, 18.887109756469727, 24.107324600219727, 6.019536018371582, 9.656814575195312, 20.010448455810547, 22.49268913269043, 17.93841552734375, 4.68503475189209, 17.78581428527832, 18.20582389831543, 20.722097396850586, 26.133230209350586, 21.06083869934082, 19.74396324157715, 12.70340633392334, 11.709986686706543, 23.351863861083984, 7.190312385559082, 15.543866157531738, 18.347240447998047, 6.9975810050964355], 'y_pred_int': [5, 9, 17, 24, 21, 4, 5, 15, 3, 1, 4, 27, 9, 19, 8, 19, 21, 6, 8, 24, 25, 5, 11, 23, 5, 15, 21, 7, 12, 23, 20, 19, 9, 17, 10, 15, 8, 17, 18, 3, 10, 13, 20, 18, 19, 25, 3, 10, 27, 9, 18, 9, 11, 22, 6, 24, 4, 2, 7, 14, 14, 26, 9, 5, 22, 3, 18, 12, 12, 22, 22, 11, 16, 25, 5, 20, 9, 3, 5, 19, 13, 4, 14, 29, 19, 8, 22, 7, 21, 19, 12, 17, 5, 13, 2, 6, 21, 11, 9, 7, 5, 22, 24, 21, 16, 12, 23, 15, 20, 18, 25, 5, 20, 22, 11, 21, 24, 13, 24, 20, 20, 6, 5, 5, 5, 4, 12, 4, 21, 2, 10, 17, 25, 10, 22, 31, 17, 4, 13, 7, 18, 18, 16, 6, 10, 9, 22, 6, 22, 23, 22, 24, 10, 11, 19, 22, 21, 15, 8, 23, 7, 20, 10, 20, 17, 3, 18, 17, 6, 4, 14, 6, 18, 6, 4, 23, 7, 15, 8, 9, 9, 12, 5, 25, 19, 5, 6, 10, 18, 22, 5, 22, 10, 29, 4, 19, 4, 16, 19, 8, 14, 19, 17, 5, 17, 21, 5, 11, 18, 6, 6, 23, 3, 6, 18, 10, 4, 18, 9, 23, 9, 6, 14, 20, 23, 4, 21, 17, 19, 11, 10, 11, 10, 7, 3, 19, 7, 8, 24, 26, 21, 20, 19, 4, 11, 5, 19, 18, 28, 21, 18, 23, 21, 15, 18, 16, 4, 5, 24, 5, 20, 7, 10, 19, 11, 7, 19, 20, 15, 20, 18, 3, 14, 19, 3, 11, 13, 8, 17, 9, 8, 13, 9, 4, 16, 26, 21, 21, 12, 23, 22, 9, 5, 7, 16, 16, 8, 6, 5, 22, 9, 25, 9, 21, 20, 23, 26, 7, 24, 9, 22, 5, 21, 3, 8, 18, 23, 20, 5, 19, 24, 6, 10, 20, 22, 18, 5, 18, 18, 21, 26, 21, 20, 13, 12, 23, 7, 16, 18, 7], 'y_true': [5, 11, 14, 28, 23, 5, 6, 12, 4, 2, 8, 14, 9, 18, 8, 13, 19, 4, 14, 23, 35, 5, 10, 32, 4, 29, 22, 6, 15, 22, 26, 30, 9, 15, 11, 13, 6, 10, 17, 4, 7, 10, 33, 12, 13, 30, 3, 11, 46, 8, 14, 7, 17, 25, 6, 34, 3, 3, 4, 14, 10, 29, 7, 7, 28, 5, 14, 9, 10, 27, 18, 6, 15, 22, 8, 28, 7, 6, 3, 14, 7, 3, 25, 28, 22, 9, 14, 8, 9, 17, 10, 10, 8, 17, 6, 10, 17, 11, 7, 6, 6, 16, 30, 21, 12, 6, 21, 10, 18, 13, 45, 5, 9, 18, 9, 20, 35, 13, 19, 42, 13, 7, 6, 4, 4, 5, 13, 5, 25, 3, 9, 20, 39, 5, 43, 39, 10, 7, 15, 8, 14, 10, 14, 4, 9, 10, 16, 4, 15, 25, 21, 29, 8, 11, 16, 11, 27, 9, 9, 23, 7, 23, 9, 51, 35, 5, 18, 20, 5, 3, 17, 5, 14, 5, 6, 18, 9, 10, 9, 6, 10, 14, 6, 31, 20, 6, 8, 14, 20, 38, 5, 29, 10, 29, 4, 11, 2, 10, 22, 7, 17, 15, 26, 6, 20, 32, 7, 12, 14, 6, 8, 49, 4, 7, 9, 11, 5, 16, 8, 27, 7, 8, 9, 12, 21, 5, 24, 11, 16, 6, 13, 8, 9, 6, 3, 37, 9, 6, 13, 40, 32, 16, 13, 5, 9, 5, 13, 18, 40, 48, 8, 35, 22, 8, 16, 11, 7, 7, 26, 4, 29, 6, 5, 18, 8, 9, 11, 21, 14, 14, 12, 3, 15, 13, 3, 8, 11, 7, 17, 7, 6, 13, 10, 3, 14, 19, 9, 24, 8, 49, 14, 8, 4, 5, 14, 24, 9, 7, 4, 17, 9, 35, 9, 14, 12, 30, 23, 7, 23, 8, 23, 4, 20, 3, 8, 17, 49, 9, 4, 14, 37, 5, 7, 29, 38, 10, 6, 12, 23, 32, 39, 25, 22, 19, 11, 19, 6, 11, 10, 9], 'train_acc': 1.0, 'test_acc': 0.10882352941176471, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 14187.9990, Test loss: 14412.9033
In this epoch 100/5000, Training loss: 7953.2075, Test loss: 12614.5771
In this epoch 150/5000, Training loss: 4569.0498, Test loss: 12146.8398
In this epoch 200/5000, Training loss: 2501.0542, Test loss: 11966.6270
In this epoch 250/5000, Training loss: 1310.0828, Test loss: 11908.8369
In this epoch 300/5000, Training loss: 660.5911, Test loss: 11902.9619
In this epoch 350/5000, Training loss: 322.6949, Test loss: 11916.5127
In this epoch 400/5000, Training loss: 153.8389, Test loss: 11934.6621
In this epoch 450/5000, Training loss: 72.2179, Test loss: 11951.1611
In this epoch 500/5000, Training loss: 33.7472, Test loss: 11963.9492
In this epoch 550/5000, Training loss: 15.8930, Test loss: 11972.9180
In this epoch 600/5000, Training loss: 7.6378, Test loss: 11978.7100
In this epoch 650/5000, Training loss: 3.7844, Test loss: 11982.1738
In this epoch 700/5000, Training loss: 1.9445, Test loss: 11984.0459
In this epoch 750/5000, Training loss: 1.0366, Test loss: 11984.9385
In this epoch 800/5000, Training loss: 0.5709, Test loss: 11985.2549
In this epoch 850/5000, Training loss: 0.3230, Test loss: 11985.2764
In this epoch 900/5000, Training loss: 0.1867, Test loss: 11985.1709
In this epoch 950/5000, Training loss: 0.1099, Test loss: 11985.0322
In this epoch 1000/5000, Training loss: 0.0659, Test loss: 11984.9014
In this epoch 1050/5000, Training loss: 0.0405, Test loss: 11984.8027
In this epoch 1100/5000, Training loss: 0.0257, Test loss: 11984.7334
In this epoch 1150/5000, Training loss: 0.0171, Test loss: 11984.6992
In this epoch 1200/5000, Training loss: 0.0121, Test loss: 11984.6855
In this epoch 1250/5000, Training loss: 0.0092, Test loss: 11984.6895
In this epoch 1300/5000, Training loss: 0.0075, Test loss: 11984.7002
In this epoch 1350/5000, Training loss: 0.0065, Test loss: 11984.7178
In this epoch 1400/5000, Training loss: 0.0059, Test loss: 11984.7383
In this epoch 1450/5000, Training loss: 0.0055, Test loss: 11984.7588
In this epoch 1500/5000, Training loss: 0.0052, Test loss: 11984.7793
In this epoch 1550/5000, Training loss: 0.0051, Test loss: 11984.7998
In this epoch 1600/5000, Training loss: 0.0049, Test loss: 11984.8184
In this epoch 1650/5000, Training loss: 0.0048, Test loss: 11984.8369
In this epoch 1700/5000, Training loss: 0.0047, Test loss: 11984.8535
In this epoch 1750/5000, Training loss: 0.0046, Test loss: 11984.8662
In this epoch 1800/5000, Training loss: 0.0045, Test loss: 11984.8828
In this epoch 1850/5000, Training loss: 0.0043, Test loss: 11984.8975
In this epoch 1900/5000, Training loss: 0.0042, Test loss: 11984.9102
In this epoch 1950/5000, Training loss: 0.0041, Test loss: 11984.9219
In this epoch 2000/5000, Training loss: 0.0040, Test loss: 11984.9365
In this epoch 2050/5000, Training loss: 0.0039, Test loss: 11984.9541
In this epoch 2100/5000, Training loss: 0.0038, Test loss: 11984.9668
In this epoch 2150/5000, Training loss: 0.0037, Test loss: 11984.9766
In this epoch 2200/5000, Training loss: 0.0036, Test loss: 11984.9932
In this epoch 2250/5000, Training loss: 0.0035, Test loss: 11985.0029
In this epoch 2300/5000, Training loss: 0.0034, Test loss: 11985.0186
In this epoch 2350/5000, Training loss: 0.0033, Test loss: 11985.0352
In this epoch 2400/5000, Training loss: 0.0032, Test loss: 11985.0498
In this epoch 2450/5000, Training loss: 0.0031, Test loss: 11985.0625
In this epoch 2500/5000, Training loss: 0.0030, Test loss: 11985.0781
In this epoch 2550/5000, Training loss: 0.0029, Test loss: 11985.0928
In this epoch 2600/5000, Training loss: 0.0028, Test loss: 11985.1094
In this epoch 2650/5000, Training loss: 0.0027, Test loss: 11985.1211
In this epoch 2700/5000, Training loss: 0.0026, Test loss: 11985.1367
In this epoch 2750/5000, Training loss: 0.0025, Test loss: 11985.1484
In this epoch 2800/5000, Training loss: 0.0024, Test loss: 11985.1660
In this epoch 2850/5000, Training loss: 0.0023, Test loss: 11985.1807
In this epoch 2900/5000, Training loss: 0.0022, Test loss: 11985.2012
In this epoch 2950/5000, Training loss: 0.0021, Test loss: 11985.2148
In this epoch 3000/5000, Training loss: 0.0020, Test loss: 11985.2266
In this epoch 3050/5000, Training loss: 0.0020, Test loss: 11985.2432
In this epoch 3100/5000, Training loss: 0.0019, Test loss: 11985.2607
In this epoch 3150/5000, Training loss: 0.0018, Test loss: 11985.2725
In this epoch 3200/5000, Training loss: 0.0017, Test loss: 11985.2871
In this epoch 3250/5000, Training loss: 0.0016, Test loss: 11985.3047
In this epoch 3300/5000, Training loss: 0.0016, Test loss: 11985.3242
In this epoch 3350/5000, Training loss: 0.0015, Test loss: 11985.3359
In this epoch 3400/5000, Training loss: 0.0014, Test loss: 11985.3496
In this epoch 3450/5000, Training loss: 0.0014, Test loss: 11985.3652
In this epoch 3500/5000, Training loss: 0.0013, Test loss: 11985.3809
In this epoch 3550/5000, Training loss: 0.0012, Test loss: 11985.3965
In this epoch 3600/5000, Training loss: 0.0012, Test loss: 11985.4121
In this epoch 3650/5000, Training loss: 0.0011, Test loss: 11985.4277
In this epoch 3700/5000, Training loss: 0.0011, Test loss: 11985.4395
In this epoch 3750/5000, Training loss: 0.0010, Test loss: 11985.4551
In this epoch 3800/5000, Training loss: 0.0010, Test loss: 11985.4697
In this epoch 3850/5000, Training loss: 0.0009, Test loss: 11985.4805
In this epoch 3900/5000, Training loss: 0.0009, Test loss: 11985.4951
In this epoch 3950/5000, Training loss: 0.0009, Test loss: 11985.5078
In this epoch 4000/5000, Training loss: 0.0008, Test loss: 11985.5225
In this epoch 4050/5000, Training loss: 0.0008, Test loss: 11985.5352
In this epoch 4100/5000, Training loss: 0.0007, Test loss: 11985.5498
In this epoch 4150/5000, Training loss: 0.0007, Test loss: 11985.5586
In this epoch 4200/5000, Training loss: 0.0007, Test loss: 11985.5723
In this epoch 4250/5000, Training loss: 0.0006, Test loss: 11985.5889
In this epoch 4300/5000, Training loss: 0.0006, Test loss: 11985.6025
In this epoch 4350/5000, Training loss: 0.0006, Test loss: 11985.6152
In this epoch 4400/5000, Training loss: 0.0006, Test loss: 11985.6250
In this epoch 4450/5000, Training loss: 0.0006, Test loss: 11985.6328
In this epoch 4500/5000, Training loss: 0.0005, Test loss: 11985.6475
In this epoch 4550/5000, Training loss: 0.0005, Test loss: 11985.6543
In this epoch 4600/5000, Training loss: 0.0005, Test loss: 11985.6621
In this epoch 4650/5000, Training loss: 0.0005, Test loss: 11985.6748
In this epoch 4700/5000, Training loss: 0.0005, Test loss: 11985.6836
In this epoch 4750/5000, Training loss: 0.0005, Test loss: 11985.6943
In this epoch 4800/5000, Training loss: 0.0005, Test loss: 11985.7012
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 11985.7119
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 11985.7168
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 11985.7285
In this epoch 5000/5000, Training loss: 0.0004, Test loss: 11985.7344
train_acc 0.9992647058823529
test_acc 0.011764705882352941
{'y_pred': [186.4133758544922, 390.3421936035156, 111.20487213134766, 280.1134338378906, 97.81436920166016, 117.38419342041016, 558.4447021484375, 210.8474884033203, 526.673095703125, 471.3182678222656, 557.34716796875, 497.0801696777344, 176.56346130371094, 573.5123901367188, 599.1913452148438, 194.06553649902344, 543.3400268554688, 52.50111389160156, 561.85986328125, 166.5959014892578, 500.5049743652344, 485.5990905761719, 527.2561645507812, 288.5550231933594, 102.37777709960938, 155.00001525878906, 516.4774169921875, 50.05097961425781, 511.58135986328125, 158.8695831298828, 508.7324523925781, 556.7083740234375, 504.8526611328125, 408.4507751464844, 196.4750213623047, 556.8626098632812, 68.94398498535156, 323.7336120605469, 71.47991943359375, 538.0205688476562, 550.5111083984375, 610.9331665039062, 243.0642852783203, 207.5360565185547, 219.67494201660156, 68.02462768554688, 207.98379516601562, 581.6923828125, 633.1024169921875, 439.1748352050781, 92.47417449951172, 235.56222534179688, 95.37753295898438, 262.5630798339844, 476.15972900390625, 546.586181640625, 261.4645690917969, 285.5078125, 505.1528015136719, 151.00379943847656, 238.3693389892578, 520.9032592773438, 537.983642578125, 477.46380615234375, 64.03762817382812, 100.5609130859375, 157.42373657226562, 271.1495056152344, 255.86639404296875, 168.2648468017578, 562.7750244140625, 507.6012268066406, 68.364501953125, 81.9234619140625, 539.834228515625, 645.0889892578125, 418.7679443359375, 143.4387969970703, 507.62548828125, 551.7606201171875, 392.42669677734375, 459.9771728515625, 165.9322967529297, 593.7835693359375, 474.9569396972656, 331.87786865234375, 167.4053192138672, 386.0983581542969, 566.6068115234375, 274.5911560058594, 481.8415222167969, 264.12548828125, 114.43737030029297, 143.9548797607422, 452.88018798828125, 550.8244018554688, 432.9580078125, 535.747314453125, 593.5538330078125, 139.1385955810547, 262.51751708984375, 396.9623107910156, 517.0944213867188, 483.1895751953125, 476.0111389160156, 546.2659301757812, 64.18090057373047, 157.3191375732422, 248.3252410888672, 541.8531494140625, 279.6217346191406, 596.3055419921875, 581.6810913085938, 521.2946166992188, 148.79808044433594, 512.88916015625, 550.0479125976562, 118.4206314086914, 493.8256530761719, 529.015380859375, 392.4099426269531, 555.6498413085938, 654.3731079101562, 262.5261535644531, 299.9007263183594, 137.42901611328125, 219.65206909179688, 322.46343994140625, 118.86836242675781, 155.91795349121094, 192.97019958496094, 155.52122497558594, 220.1951446533203, 199.5595245361328, 547.718505859375, 539.2476806640625, 574.3286743164062, 489.4783020019531, 621.7343139648438, 114.21527099609375, 560.3587646484375, 461.7677307128906, 501.1713562011719, 473.4854736328125, 459.7327880859375, 473.6512451171875, 123.03948211669922, 23.487825393676758, 358.4324645996094, 107.08063507080078, 419.3107604980469, 504.13873291015625, 214.76272583007812, 536.53271484375, 197.7174835205078, 494.6091003417969, 562.48974609375, 496.85723876953125, 514.6093139648438, 456.1617736816406, 97.75144958496094, 258.9999694824219, 77.65686798095703, 199.03346252441406, 252.85977172851562, 316.7933654785156, 393.4882507324219, 408.3252258300781, 244.85752868652344, 423.3070068359375, 133.34739685058594, 258.6552734375, 127.33882141113281, 331.6332702636719, 541.5011596679688, 99.10736083984375, 470.27734375, 435.2427673339844, 250.483642578125, 269.8346862792969, 223.69851684570312, 30.006202697753906, 155.17881774902344, 244.50070190429688, 505.1224670410156, 99.23384094238281, 419.9485168457031, 216.7707061767578, 72.55744934082031, 199.1079559326172, 230.11231994628906, 476.09478759765625, 512.1015014648438, 529.50341796875, 573.6931762695312, 197.4847869873047, 553.9346313476562, 453.43353271484375, 269.5928649902344, 338.6749572753906, 109.32087707519531, 530.3676147460938, 529.007080078125, 523.1680297851562, 576.509765625, 529.087890625, 482.6679992675781, 204.34022521972656, 425.64471435546875, 487.2955627441406, 503.3598327636719, 206.6849365234375, 483.5399475097656, 313.626953125, 162.0056915283203, 540.096923828125, 479.69244384765625, 419.0865783691406, 185.17178344726562, 545.3272094726562, 216.237060546875, 536.9244384765625, 566.1473388671875, 480.8129577636719, 453.7533264160156, 259.1697082519531, 522.109130859375, 672.3223266601562, 284.5138244628906, 205.77195739746094, 535.1923828125, 558.06494140625, 159.708251953125, 492.00274658203125, 527.4495849609375, 488.5476989746094, 308.19921875, 172.5863494873047, 296.36273193359375, 289.5510559082031, 652.2420654296875, 126.52379608154297, 508.50054931640625, 348.8517150878906, 138.29531860351562, 250.98121643066406, 185.38328552246094, 500.9053649902344, 276.1575622558594, 524.2394409179688, 595.6869506835938, 553.304443359375, 255.52476501464844, 260.2981262207031, 91.05813598632812, 52.09384536743164, 544.5770263671875, 607.3749389648438, 539.3449096679688, 545.19482421875, 72.39569854736328, 423.61578369140625, 241.2899627685547, 84.40104675292969, 320.9906921386719, 547.2804565429688, 574.2013549804688, 79.47503662109375, 104.82567596435547, 448.8861389160156, 229.0840301513672, 620.9305419921875, 175.4252471923828, 550.7405395507812, 215.80841064453125, 218.00961303710938, 450.91650390625, 206.52606201171875, 241.45721435546875, 89.48898315429688, 192.55821228027344, 435.2413024902344, 514.494873046875, 100.25718688964844, 572.3350830078125, 582.8604125976562, 292.5948486328125, 441.4239196777344, 504.1399230957031, 187.42649841308594, 241.0361785888672, 20.679243087768555, 499.662841796875, 45.573707580566406, 522.2667236328125, 54.139461517333984, 192.70986938476562, 186.6548309326172, 202.0655517578125, 456.5125427246094, 519.9183349609375, 256.51123046875, 128.99331665039062, 152.89373779296875, 221.48985290527344, 306.7619323730469, 510.3642883300781, 541.61572265625, 138.23135375976562, 126.30703735351562, 559.3861694335938, 110.75098419189453, 107.24162292480469, 99.89895629882812, 378.2685241699219, 206.33738708496094, 559.4378051757812, 245.37753295898438, 114.36711120605469, 197.28012084960938, 83.05500793457031, 582.6055908203125, 115.55342864990234, 154.12191772460938, 581.557861328125, 569.1725463867188, 61.99360275268555, 544.8591918945312, 169.94430541992188, 627.82568359375, 577.1100463867188, 580.3171997070312, 86.50198364257812, 131.40963745117188, 181.96766662597656, 493.5888977050781, 591.0791625976562, 236.41696166992188, 209.65878295898438, 622.2218017578125], 'y_pred_int': [186, 390, 111, 280, 98, 117, 558, 211, 527, 471, 557, 497, 177, 574, 599, 194, 543, 53, 562, 167, 501, 486, 527, 289, 102, 155, 516, 50, 512, 159, 509, 557, 505, 408, 196, 557, 69, 324, 71, 538, 551, 611, 243, 208, 220, 68, 208, 582, 633, 439, 92, 236, 95, 263, 476, 547, 261, 286, 505, 151, 238, 521, 538, 477, 64, 101, 157, 271, 256, 168, 563, 508, 68, 82, 540, 645, 419, 143, 508, 552, 392, 460, 166, 594, 475, 332, 167, 386, 567, 275, 482, 264, 114, 144, 453, 551, 433, 536, 594, 139, 263, 397, 517, 483, 476, 546, 64, 157, 248, 542, 280, 596, 582, 521, 149, 513, 550, 118, 494, 529, 392, 556, 654, 263, 300, 137, 220, 322, 119, 156, 193, 156, 220, 200, 548, 539, 574, 489, 622, 114, 560, 462, 501, 473, 460, 474, 123, 23, 358, 107, 419, 504, 215, 537, 198, 495, 562, 497, 515, 456, 98, 259, 78, 199, 253, 317, 393, 408, 245, 423, 133, 259, 127, 332, 542, 99, 470, 435, 250, 270, 224, 30, 155, 245, 505, 99, 420, 217, 73, 199, 230, 476, 512, 530, 574, 197, 554, 453, 270, 339, 109, 530, 529, 523, 577, 529, 483, 204, 426, 487, 503, 207, 484, 314, 162, 540, 480, 419, 185, 545, 216, 537, 566, 481, 454, 259, 522, 672, 285, 206, 535, 558, 160, 492, 527, 489, 308, 173, 296, 290, 652, 127, 509, 349, 138, 251, 185, 501, 276, 524, 596, 553, 256, 260, 91, 52, 545, 607, 539, 545, 72, 424, 241, 84, 321, 547, 574, 79, 105, 449, 229, 621, 175, 551, 216, 218, 451, 207, 241, 89, 193, 435, 514, 100, 572, 583, 293, 441, 504, 187, 241, 21, 500, 46, 522, 54, 193, 187, 202, 457, 520, 257, 129, 153, 221, 307, 510, 542, 138, 126, 559, 111, 107, 100, 378, 206, 559, 245, 114, 197, 83, 583, 116, 154, 582, 569, 62, 545, 170, 628, 577, 580, 87, 131, 182, 494, 591, 236, 210, 622], 'y_true': [212, 401, 160, 294, 108, 113, 569, 237, 461, 494, 634, 290, 156, 837, 706, 144, 560, 90, 458, 200, 504, 394, 459, 284, 99, 155, 412, 60, 428, 164, 586, 514, 445, 327, 300, 554, 87, 335, 113, 792, 389, 464, 231, 207, 243, 92, 172, 431, 498, 400, 80, 260, 39, 319, 599, 375, 244, 286, 614, 141, 247, 670, 591, 687, 81, 103, 195, 285, 259, 202, 464, 563, 88, 76, 464, 980, 409, 123, 602, 574, 284, 351, 163, 465, 607, 257, 168, 331, 446, 301, 621, 262, 96, 175, 371, 342, 365, 539, 688, 158, 307, 339, 598, 404, 359, 434, 76, 185, 212, 406, 312, 389, 807, 660, 128, 643, 656, 82, 391, 911, 506, 493, 506, 228, 334, 124, 239, 313, 109, 184, 169, 145, 221, 124, 815, 593, 716, 498, 723, 109, 842, 469, 832, 616, 376, 384, 149, 68, 358, 102, 667, 447, 221, 768, 182, 367, 685, 395, 487, 381, 99, 264, 88, 186, 305, 285, 339, 367, 214, 355, 159, 239, 135, 293, 1342, 92, 833, 309, 244, 291, 215, 32, 167, 209, 515, 136, 330, 218, 60, 235, 234, 364, 389, 507, 511, 142, 583, 326, 249, 297, 101, 378, 449, 738, 874, 481, 292, 217, 360, 527, 467, 209, 530, 276, 180, 805, 488, 310, 141, 765, 234, 569, 542, 466, 490, 259, 338, 726, 258, 233, 484, 656, 176, 707, 486, 624, 211, 164, 314, 262, 459, 113, 535, 299, 168, 254, 134, 334, 280, 1032, 796, 460, 210, 182, 100, 81, 686, 517, 513, 701, 82, 334, 199, 80, 280, 415, 617, 115, 103, 324, 237, 644, 163, 523, 197, 241, 367, 167, 239, 89, 188, 302, 471, 148, 607, 770, 258, 391, 397, 200, 260, 35, 629, 73, 367, 65, 216, 201, 199, 685, 448, 222, 116, 160, 243, 280, 677, 697, 98, 123, 641, 125, 81, 89, 325, 165, 681, 205, 76, 206, 94, 596, 117, 168, 569, 385, 76, 432, 223, 898, 857, 466, 89, 127, 203, 444, 477, 203, 194, 665], 'train_acc': 0.9992647058823529, 'test_acc': 0.011764705882352941, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 28.1530, Test loss: 9.9872
In this epoch 100/5000, Training loss: 0.3400, Test loss: 0.9419
In this epoch 150/5000, Training loss: 0.0845, Test loss: 0.9149
In this epoch 200/5000, Training loss: 0.0513, Test loss: 0.9262
In this epoch 250/5000, Training loss: 0.0319, Test loss: 0.9405
In this epoch 300/5000, Training loss: 0.0201, Test loss: 0.9534
In this epoch 350/5000, Training loss: 0.0128, Test loss: 0.9644
In this epoch 400/5000, Training loss: 0.0081, Test loss: 0.9733
In this epoch 450/5000, Training loss: 0.0052, Test loss: 0.9805
In this epoch 500/5000, Training loss: 0.0033, Test loss: 0.9861
In this epoch 550/5000, Training loss: 0.0021, Test loss: 0.9905
In this epoch 600/5000, Training loss: 0.0013, Test loss: 0.9939
In this epoch 650/5000, Training loss: 0.0008, Test loss: 0.9964
In this epoch 700/5000, Training loss: 0.0005, Test loss: 0.9984
In this epoch 750/5000, Training loss: 0.0003, Test loss: 0.9999
In this epoch 800/5000, Training loss: 0.0002, Test loss: 1.0010
In this epoch 850/5000, Training loss: 0.0001, Test loss: 1.0018
In this epoch 900/5000, Training loss: 0.0001, Test loss: 1.0024
In this epoch 950/5000, Training loss: 0.0000, Test loss: 1.0029
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 1.0033
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 1.0035
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 1.0037
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 1.0039
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 1.0040
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 1.0040
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 1.0041
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 1.0041
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 1.0041
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 1.0046
In this epoch 2350/5000, Training loss: 1.7693, Test loss: 41.6473
In this epoch 2400/5000, Training loss: 0.2790, Test loss: 1.0030
In this epoch 2450/5000, Training loss: 0.0015, Test loss: 1.0044
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 1.0039
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 1.0042
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 1.0042
train_acc 0.3457417820069204
test_acc 0.38826124567474046
{'y_pred': [1.450598120689392, 0.7566854953765869, -0.10677805542945862, -0.3145979940891266, 0.15899886190891266, 2.936904191970825, 0.26815375685691833, 0.9963087439537048, 0.3459508717060089, 0.9432693719863892, 2.12113881111145, 1.1804944276809692, 0.8238417506217957, 2.082362174987793, -0.5565282106399536, 0.49855849146842957, 0.1302913874387741, 0.6082441210746765, 0.5241975784301758, 1.2723519802093506, -0.4212186634540558, 0.6129932999610901, 2.3611326217651367, 1.0595844984054565, 2.6139917373657227, 0.5116593241691589, 1.8295433521270752, 1.7952690124511719, 2.600231409072876, 0.2913568913936615, 1.0949501991271973, 1.60466730594635, 0.03474196046590805, -0.3672126829624176, 0.8760776519775391, 0.38005781173706055, 1.4563231468200684, 1.206985354423523, 1.5549310445785522, 0.24652917683124542, 0.770024836063385, 0.16804735362529755, -0.3475557863712311, 2.599307060241699, 0.3612988591194153, -0.05199404060840607, -0.03450923413038254, 2.2480199337005615, 0.1778881996870041, 0.024668917059898376, 1.7497693300247192, 2.109663963317871, -0.461103230714798, 0.5790058970451355, -0.41511932015419006, 0.15201543271541595, 0.27022626996040344, 0.6471832990646362, 1.5214804410934448, 2.558077096939087, 1.2518190145492554, 1.9879127740859985, 1.6891462802886963, 4.080061912536621, 1.2235811948776245, 0.924635648727417, 1.418599009513855, 2.3281409740448, 0.042907845228910446, 3.710106134414673, 3.77882719039917, 1.3935984373092651, -0.013651303946971893, 2.218644142150879, -0.3142044246196747, 1.588442325592041, 0.515093207359314, 0.5706097483634949, 0.44481799006462097, -0.1574796438217163, 2.5168683528900146, 0.1702776402235031, -0.1570321023464203, 0.7014846205711365, -0.1919151395559311, 0.15369312465190887, 1.1565401554107666, 0.8275948166847229, -0.22318115830421448, 1.2777631282806396, 1.8466891050338745, 0.449423223733902, 1.382789969444275, 0.7857325077056885, 1.9354342222213745, 0.25508931279182434, 2.5649280548095703, 1.3443697690963745, 1.8455604314804077, 0.8665866255760193, 1.451949954032898, 1.8212612867355347, 0.07639595866203308, 0.3888232409954071, 1.4003149271011353, -0.32949313521385193, 0.5949529409408569, 3.068084955215454, 0.06564577668905258, 0.6833157539367676, 0.2051623910665512, -0.8635081052780151, 1.4513756036758423, 0.6193646192550659, -0.438117653131485, -0.267240971326828, 0.4418548047542572, 1.9751650094985962, 3.300790548324585, 0.2286452203989029, 1.1079039573669434, 1.3080838918685913, 0.059631697833538055, 0.28300097584724426, 1.9687882661819458, 0.8631092309951782, 0.46426478028297424, 1.5252830982208252, 1.9346270561218262, 0.8336014747619629, -0.3849477469921112, 0.12988883256912231, 1.8234647512435913, 0.12312284111976624, -0.1266649067401886, 1.947041630744934, -0.0376262441277504, 0.27269402146339417, 0.6962728500366211, 1.8015401363372803, -0.15581488609313965, -0.7064734101295471, 0.05366123467683792, -0.23511728644371033, 1.0027214288711548, 0.08730224519968033, 0.06450926512479782, 0.21050028502941132, 1.6816085577011108, 0.7534971237182617, 0.1545153111219406, 2.0437700748443604, 0.15479548275470734, 0.6166464686393738, -0.3876749873161316, 1.3091500997543335, 0.252456933259964, 0.9013410806655884, -0.5573012232780457, -0.04665026813745499, -0.8269351720809937, -0.054635025560855865, 2.241495370864868, 0.2592453956604004, 0.26958659291267395, 0.6448823809623718, 0.7239623069763184, 1.8296092748641968, -0.7108201384544373, -0.3542554974555969, -0.36657479405403137, 0.8693221807479858, 0.8474247455596924, 1.3195466995239258, 2.095712900161743, 1.0818804502487183, 0.07934873551130295, 0.37547382712364197, -0.42960771918296814, 0.9497247934341431, -0.11864469200372696, 1.5971843004226685, 1.74541437625885, -0.08093499392271042, -0.17715372145175934, 0.02031252533197403, -0.1130564883351326, -0.010717801749706268, -0.2651406526565552, 1.2450981140136719, -0.1660929173231125, 0.50270676612854, 1.7592447996139526, 2.8888132572174072, -0.05460778623819351, 0.43069514632225037, -1.0039302110671997, 1.0470820665359497, 0.41316503286361694, 2.017660140991211, 1.5500901937484741, 1.502099633216858, 1.845253348350525, 1.1060640811920166, 1.3283048868179321, 2.0971944332122803, -0.058969371020793915, 0.2725105583667755, 0.6091635227203369, 0.28510400652885437, -1.1861462593078613, 2.6920573711395264, 1.0903054475784302, 1.3314176797866821, 0.7021864652633667, 0.3600477874279022, 0.08364249765872955, 0.8620457649230957, -0.25428974628448486, -0.8341631889343262, -0.12299106270074844, 0.768842875957489, 1.7221858501434326, 2.477199077606201, 0.7322425842285156, 1.588142991065979, 1.3868321180343628, 0.9059112071990967, -0.0036304632667452097, -0.3103346526622772, 3.108513832092285, 3.1757144927978516, 0.02735055610537529, 1.1166903972625732, 1.6204134225845337, 0.45800527930259705, 2.389714241027832, 2.6166467666625977, 1.95391047000885, 2.7038495540618896, 1.2715455293655396, 0.3899517059326172, 3.3543741703033447, 1.5658211708068848, -0.5641682744026184, 0.9531638622283936, 1.1692941188812256, 0.7905851602554321, 1.390270709991455, 0.10339849442243576, 1.0270459651947021, 1.1186103820800781, 0.3848492503166199, 1.4116231203079224, 0.44259926676750183, 0.3498932421207428, 1.0542534589767456, 0.19235852360725403, 0.3290702998638153, 0.9505466818809509, 1.1707632541656494, -0.5603986978530884, 1.5688831806182861, 1.7035000324249268, 0.06102810055017471, 0.8365282416343689, -0.5236724019050598, 0.10278604924678802, 0.8721986413002014, 0.15096233785152435, -0.1748584359884262, -0.09089649468660355, 1.6473824977874756, 0.04018142446875572, -0.6599140167236328, 0.33149442076683044, -0.1670491099357605, 2.0489501953125, -0.5055583715438843, -0.259439617395401, 1.8814946413040161, 0.17642386257648468, 2.979440689086914, 0.2129729837179184, 0.5965728759765625, 1.7723339796066284, 0.9266140460968018, 1.6596119403839111, 2.313157558441162, 0.8946069478988647, 0.060258399695158005, 1.0100504159927368, 0.8482495546340942, -0.36537429690361023, 0.31398648023605347, 1.9109400510787964, 0.3113149404525757, 1.319017767906189, 0.8857054710388184, -0.23752401769161224, 0.8166154623031616, 0.07425730675458908, 0.9314346313476562, 2.2119553089141846, -0.3641830384731293, 1.694804072380066, 0.7598814368247986, -0.13102677464485168, 0.12350624799728394, 0.2537117302417755, 1.761246681213379, 0.0965966209769249, 1.773805856704712, 0.3580401539802551, 1.391564130783081, -0.27197059988975525, 0.7168655395507812, 0.06308584660291672, 2.588747978210449, 0.329620897769928, 0.4839971959590912, 2.160153388977051, 2.222477912902832, 2.0127878189086914, 0.3879665732383728, 0.617984414100647, 1.660657286643982, -1.2749712467193604, 1.9591652154922485, 1.1965253353118896, 0.35205844044685364, 1.9800945520401, 2.511338233947754, 0.2526129186153412, -0.2512839138507843, 1.6235860586166382, 0.1495344489812851, -0.000269972049864009, 1.5210679769515991, -0.3149832487106323], 'y_pred_int': [1, 1, 0, 0, 0, 3, 0, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 1, 1, 1, 0, 1, 2, 1, 3, 1, 2, 2, 3, 0, 1, 2, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 0, 3, 0, 0, 0, 2, 0, 0, 2, 2, 0, 1, 0, 0, 0, 1, 2, 3, 1, 2, 2, 4, 1, 1, 1, 2, 0, 4, 4, 1, 0, 2, 0, 2, 1, 1, 0, 0, 3, 0, 0, 1, 0, 0, 1, 1, 0, 1, 2, 0, 1, 1, 2, 0, 3, 1, 2, 1, 1, 2, 0, 0, 1, 0, 1, 3, 0, 1, 0, 0, 1, 1, 0, 0, 0, 2, 3, 0, 1, 1, 0, 0, 2, 1, 0, 2, 2, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 2, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 1, 2, 1, 0, 0, 0, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 3, 0, 0, 0, 1, 0, 2, 2, 2, 2, 1, 1, 2, 0, 0, 1, 0, 0, 3, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 2, 2, 1, 2, 1, 1, 0, 0, 3, 3, 0, 1, 2, 0, 2, 3, 2, 3, 1, 0, 3, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 2, 2, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 3, 0, 1, 2, 1, 2, 2, 1, 0, 1, 1, 0, 0, 2, 0, 1, 1, 0, 1, 0, 1, 2, 0, 2, 1, 0, 0, 0, 2, 0, 2, 0, 1, 0, 1, 0, 3, 0, 0, 2, 2, 2, 0, 1, 2, 0, 2, 1, 0, 2, 3, 0, 0, 2, 0, 0, 2, 0], 'y_true': [1, 1, 0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 1, 3, 1, 1, 1, 1, 1, 3, 0, 1, 3, 5, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 3, 1, 0, 0, 4, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 2, 2, 0, 1, 1, 14, 0, 1, 1, 3, 0, 2, 5, 1, 0, 2, 0, 2, 1, 1, 1, 0, 3, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 2, 1, 1, 0, 3, 1, 1, 0, 1, 2, 0, 1, 1, 0, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 1, 1, 0, 0, 5, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 2, 0, 0, 0, 1, 1, 1, 2, 1, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 5, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 1, 7, 1, 1, 1, 0, 0, 0, 4, 1, 3, 2, 1, 0, 1, 8, 0, 2, 1, 1, 4, 2, 0, 1, 0, 0, 1, 0, 1, 3, 0, 1, 1, 0, 3, 0, 0, 0, 1, 0, 3, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 1, 0, 3, 0, 1, 1, 1, 1, 3, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 2, 0, 0, 1, 2, 3, 0, 0, 1, 0, 2, 0, 1, 1, 6, 0, 0, 3, 0, 0, 1, 0], 'train_acc': 0.3457417820069204, 'test_acc': 0.38826124567474046, 'X': 'X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 9.2543, Test loss: 17.3371
In this epoch 100/5000, Training loss: 1.2964, Test loss: 16.5320
In this epoch 150/5000, Training loss: 0.5261, Test loss: 16.3232
In this epoch 200/5000, Training loss: 0.2565, Test loss: 16.2577
In this epoch 250/5000, Training loss: 0.1328, Test loss: 16.2465
In this epoch 300/5000, Training loss: 0.0708, Test loss: 16.2491
In this epoch 350/5000, Training loss: 0.0384, Test loss: 16.2549
In this epoch 400/5000, Training loss: 0.0209, Test loss: 16.2602
In this epoch 450/5000, Training loss: 0.0114, Test loss: 16.2634
In this epoch 500/5000, Training loss: 0.0063, Test loss: 16.2646
In this epoch 550/5000, Training loss: 0.0034, Test loss: 16.2640
In this epoch 600/5000, Training loss: 0.0019, Test loss: 16.2624
In this epoch 650/5000, Training loss: 0.0010, Test loss: 16.2602
In this epoch 700/5000, Training loss: 0.0006, Test loss: 16.2578
In this epoch 750/5000, Training loss: 0.0003, Test loss: 16.2554
In this epoch 800/5000, Training loss: 0.0002, Test loss: 16.2532
In this epoch 850/5000, Training loss: 0.0001, Test loss: 16.2512
In this epoch 900/5000, Training loss: 0.0001, Test loss: 16.2494
In this epoch 950/5000, Training loss: 0.0000, Test loss: 16.2479
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 16.2466
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 16.2454
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 16.2444
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 16.2435
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 16.2427
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 16.2420
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 16.2413
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 16.2407
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 16.2401
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 16.2396
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 16.2391
In this epoch 1550/5000, Training loss: 0.0002, Test loss: 16.2437
In this epoch 1600/5000, Training loss: 6.3864, Test loss: 16.2246
In this epoch 1650/5000, Training loss: 0.0108, Test loss: 16.2241
In this epoch 1700/5000, Training loss: 0.0003, Test loss: 16.2436
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 16.2376
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 16.2371
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 16.2369
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 16.2366
In this epoch 1950/5000, Training loss: 2.0334, Test loss: 20.6492
In this epoch 2000/5000, Training loss: 2.4293, Test loss: 18.8241
In this epoch 2050/5000, Training loss: 0.0124, Test loss: 16.2837
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 16.2339
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 16.2359
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 16.2356
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 16.2354
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 16.2353
In this epoch 2350/5000, Training loss: 70.7264, Test loss: 129.3170
In this epoch 2400/5000, Training loss: 0.0001, Test loss: 17.0227
In this epoch 2450/5000, Training loss: 0.0001, Test loss: 16.2212
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 16.2351
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 16.2347
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 16.2347
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 16.2347
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 16.2362
In this epoch 2750/5000, Training loss: 8.5633, Test loss: 33.7443
In this epoch 2800/5000, Training loss: 0.0098, Test loss: 16.2312
In this epoch 2850/5000, Training loss: 0.0001, Test loss: 16.2370
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 16.2339
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 16.2343
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 16.2343
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 16.2342
In this epoch 3100/5000, Training loss: 0.0017, Test loss: 16.2218
In this epoch 3150/5000, Training loss: 8.6787, Test loss: 17.8971
In this epoch 3200/5000, Training loss: 0.0301, Test loss: 16.2460
In this epoch 3250/5000, Training loss: 0.0002, Test loss: 16.2359
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 16.2337
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 16.2341
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 16.2341
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 16.2339
In this epoch 3500/5000, Training loss: 52.7630, Test loss: 71.7269
In this epoch 3550/5000, Training loss: 0.3682, Test loss: 16.5130
In this epoch 3600/5000, Training loss: 0.0007, Test loss: 16.2338
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 16.2338
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 16.2339
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 16.2340
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 16.2340
In this epoch 3850/5000, Training loss: 68.2445, Test loss: 127.5714
In this epoch 3900/5000, Training loss: 1.1025, Test loss: 17.7402
In this epoch 3950/5000, Training loss: 0.0063, Test loss: 16.2457
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 16.2331
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 16.2338
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 16.2339
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 16.2339
In this epoch 4200/5000, Training loss: 0.7210, Test loss: 17.1856
In this epoch 4250/5000, Training loss: 0.0016, Test loss: 17.7404
In this epoch 4300/5000, Training loss: 0.0009, Test loss: 16.2212
In this epoch 4350/5000, Training loss: 0.0001, Test loss: 16.2351
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 16.2337
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 16.2339
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 16.2339
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 16.2346
In this epoch 4600/5000, Training loss: 31.9544, Test loss: 44.3885
In this epoch 4650/5000, Training loss: 0.0768, Test loss: 16.5196
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 16.2390
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 16.2346
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 16.2338
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 16.2339
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 16.2345
In this epoch 4950/5000, Training loss: 8.6967, Test loss: 43.4976
In this epoch 5000/5000, Training loss: 0.0455, Test loss: 16.2619
train_acc 1.0
test_acc 0.04167820069204152
{'y_pred': [34.766761779785156, 18.851776123046875, 4.018503189086914, 2.9854562282562256, 9.852635383605957, 4.8134236335754395, 20.690427780151367, 6.9737043380737305, 6.494837284088135, 31.012489318847656, 9.357455253601074, 18.450244903564453, 23.142885208129883, 1.874342679977417, 7.5651726722717285, 5.086391925811768, 26.67205047607422, 6.333117961883545, 16.557119369506836, 13.027244567871094, 15.305116653442383, 10.866124153137207, 23.691120147705078, 4.747791290283203, 35.53096389770508, 23.568756103515625, 10.0143461227417, 3.184786796569824, 8.158540725708008, 4.379395484924316, 8.673788070678711, 36.91028594970703, 6.74224853515625, 10.416523933410645, 8.487582206726074, 18.710649490356445, 4.268697261810303, 7.3403401374816895, 17.540922164916992, 8.777363777160645, 7.847321033477783, 13.952897071838379, 30.353515625, 21.102149963378906, 7.630320072174072, 19.830432891845703, 12.015728950500488, 5.011088848114014, 17.699167251586914, 15.252406120300293, 19.436664581298828, 20.970651626586914, 8.018767356872559, 6.162058353424072, 5.290589332580566, 26.159229278564453, 15.528339385986328, 6.303704738616943, 8.9430570602417, 14.767851829528809, 15.407493591308594, 17.814746856689453, 13.87525463104248, 9.607170104980469, 10.594884872436523, 18.3979434967041, 8.11939525604248, 10.5907564163208, 13.925870895385742, 5.451956748962402, 21.088184356689453, 11.791953086853027, 20.47629165649414, 4.83908224105835, 24.60711669921875, 13.709168434143066, 15.509865760803223, 10.459033012390137, 18.21438980102539, 10.00298023223877, 16.19112777709961, 21.033733367919922, 17.791561126708984, 8.278810501098633, 6.980912685394287, 9.6592378616333, 3.9566550254821777, 26.87310028076172, 25.200685501098633, 4.718017578125, 16.559709548950195, 8.819315910339355, 6.489068984985352, 7.302455425262451, 15.200765609741211, 6.187352657318115, 4.5499162673950195, 11.73580265045166, 4.458364009857178, 8.488899230957031, 13.01791000366211, 5.305134296417236, 22.021804809570312, 8.764066696166992, 8.193233489990234, 14.145590782165527, 5.8714752197265625, 9.868851661682129, 8.824010848999023, 10.639579772949219, 32.06772232055664, 36.47368240356445, 5.258679389953613, 10.3750581741333, 2.2026422023773193, 3.7795703411102295, 10.158365249633789, 6.3578643798828125, 10.12949275970459, 4.786181926727295, 4.149513244628906, 19.889068603515625, 11.673544883728027, 22.68324851989746, 15.257512092590332, 7.0921854972839355, 12.278605461120605, 7.548624515533447, 13.894637107849121, 7.598104953765869, 11.268200874328613, 2.6195156574249268, 8.420281410217285, 9.81562328338623, 11.401176452636719, 1.7876476049423218, 41.48761749267578, 13.741072654724121, 20.250652313232422, 9.977082252502441, 2.2235496044158936, 15.046431541442871, 11.171570777893066, 14.257235527038574, 22.959239959716797, 15.80080509185791, 6.729790210723877, 11.390400886535645, 22.04625701904297, 11.54549789428711, 12.222525596618652, 13.742945671081543, 1.6870113611221313, 24.317262649536133, 13.890762329101562, 7.7120232582092285, 2.635830879211426, 7.993682384490967, 21.133207321166992, 12.23896312713623, 15.563255310058594, 3.386138677597046, 29.99142074584961, 4.889893531799316, 6.0141987800598145, 3.7056384086608887, 0.9039526581764221, 19.940963745117188, 9.754493713378906, 59.60798645019531, 9.208642959594727, 12.84807014465332, 8.99576187133789, 24.377649307250977, 7.523809432983398, 7.4236931800842285, 7.522210121154785, 7.437086582183838, 12.153240203857422, 13.836358070373535, 10.827580451965332, 40.673587799072266, 3.888634204864502, 10.932852745056152, 4.044768333435059, 7.391951084136963, 4.99372673034668, 12.817684173583984, 10.114962577819824, 3.5392184257507324, 20.656574249267578, 2.432072639465332, 8.187264442443848, 17.407384872436523, 5.688265800476074, 5.985222339630127, 5.7383575439453125, 8.470664024353027, 18.90195083618164, 5.380922794342041, 8.417996406555176, 20.808307647705078, 18.097270965576172, 32.28432083129883, 10.874959945678711, 21.09244155883789, 7.102138996124268, 15.835740089416504, 36.494590759277344, 20.08915138244629, 30.50624656677246, 1.5031673908233643, 3.8465120792388916, 16.994548797607422, 15.053156852722168, 24.838836669921875, 39.198829650878906, 22.16097640991211, 12.46127700805664, 4.762839317321777, 13.886332511901855, 17.175006866455078, 35.73823928833008, 8.52726936340332, 7.870002269744873, 11.18430233001709, 11.11332893371582, 12.74354076385498, 11.77734088897705, 9.060361862182617, 15.108495712280273, 4.569764137268066, 4.161190509796143, 16.018821716308594, 4.303892612457275, 58.99281692504883, 6.5249128341674805, 13.061577796936035, 41.34321212768555, 33.20585250854492, 9.304141998291016, 11.508326530456543, 6.052568435668945, 28.34805679321289, 50.90351486206055, 22.830028533935547, 8.387677192687988, 14.107453346252441, 3.738910436630249, 10.022515296936035, 6.231010913848877, 5.223236560821533, 1.5692641735076904, 8.739039421081543, 4.543225288391113, 3.149472236633301, 3.3923256397247314, 16.67379379272461, 4.355526447296143, 14.468337059020996, 4.508589267730713, 7.297496318817139, 9.009749412536621, 11.47339916229248, 7.511075496673584, 1.9145797491073608, 18.817550659179688, 17.038516998291016, 6.92850923538208, 4.508630275726318, 6.75744104385376, 6.708158016204834, 16.223628997802734, 13.4712495803833, 16.39129066467285, 12.128277778625488, 14.866375923156738, 28.755935668945312, 10.293344497680664, 8.370539665222168, 16.819597244262695, 18.689048767089844, 29.487857818603516, 1.9874628782272339, 3.9951133728027344, 10.739581108093262, 16.402162551879883, 28.361251831054688, 20.096240997314453, 13.362762451171875, 6.348926544189453, 29.987802505493164, 13.6209716796875, 14.639501571655273, 13.416101455688477, 5.854204177856445, 14.342537879943848, 13.424079895019531, 9.150015830993652, 8.331926345825195, 8.265600204467773, 9.834834098815918, 30.706937789916992, 37.63223648071289, 39.75896072387695, 15.536017417907715, 23.823444366455078, 8.07021713256836, 28.173694610595703, 21.772762298583984, 4.341482162475586, 12.893818855285645, 3.9594807624816895, 9.698556900024414, 17.02733039855957, 11.895413398742676, 11.257136344909668, 7.817739009857178, 11.959160804748535, 13.35368537902832, 12.823972702026367, 7.145303249359131, 8.125754356384277, 5.485416889190674, 9.149490356445312, 7.885212421417236, 6.9145121574401855, 64.98162078857422, 24.629451751708984, 25.48552703857422, 7.44473123550415, 23.100404739379883, 1.4547193050384521, 20.18645668029785, 19.711572647094727, 26.19198226928711, 7.770443439483643, 36.409027099609375, 14.345726013183594, 10.417532920837402], 'y_pred_int': [35, 19, 4, 3, 10, 5, 21, 7, 6, 31, 9, 18, 23, 2, 8, 5, 27, 6, 17, 13, 15, 11, 24, 5, 36, 24, 10, 3, 8, 4, 9, 37, 7, 10, 8, 19, 4, 7, 18, 9, 8, 14, 30, 21, 8, 20, 12, 5, 18, 15, 19, 21, 8, 6, 5, 26, 16, 6, 9, 15, 15, 18, 14, 10, 11, 18, 8, 11, 14, 5, 21, 12, 20, 5, 25, 14, 16, 10, 18, 10, 16, 21, 18, 8, 7, 10, 4, 27, 25, 5, 17, 9, 6, 7, 15, 6, 5, 12, 4, 8, 13, 5, 22, 9, 8, 14, 6, 10, 9, 11, 32, 36, 5, 10, 2, 4, 10, 6, 10, 5, 4, 20, 12, 23, 15, 7, 12, 8, 14, 8, 11, 3, 8, 10, 11, 2, 41, 14, 20, 10, 2, 15, 11, 14, 23, 16, 7, 11, 22, 12, 12, 14, 2, 24, 14, 8, 3, 8, 21, 12, 16, 3, 30, 5, 6, 4, 1, 20, 10, 60, 9, 13, 9, 24, 8, 7, 8, 7, 12, 14, 11, 41, 4, 11, 4, 7, 5, 13, 10, 4, 21, 2, 8, 17, 6, 6, 6, 8, 19, 5, 8, 21, 18, 32, 11, 21, 7, 16, 36, 20, 31, 2, 4, 17, 15, 25, 39, 22, 12, 5, 14, 17, 36, 9, 8, 11, 11, 13, 12, 9, 15, 5, 4, 16, 4, 59, 7, 13, 41, 33, 9, 12, 6, 28, 51, 23, 8, 14, 4, 10, 6, 5, 2, 9, 5, 3, 3, 17, 4, 14, 5, 7, 9, 11, 8, 2, 19, 17, 7, 5, 7, 7, 16, 13, 16, 12, 15, 29, 10, 8, 17, 19, 29, 2, 4, 11, 16, 28, 20, 13, 6, 30, 14, 15, 13, 6, 14, 13, 9, 8, 8, 10, 31, 38, 40, 16, 24, 8, 28, 22, 4, 13, 4, 10, 17, 12, 11, 8, 12, 13, 13, 7, 8, 5, 9, 8, 7, 65, 25, 25, 7, 23, 1, 20, 20, 26, 8, 36, 14, 10], 'y_true': [31, 15, 4, 3, 10, 4, 22, 6, 10, 26, 7, 15, 14, 1, 7, 4, 19, 5, 18, 13, 10, 10, 21, 4, 28, 19, 12, 2, 11, 5, 8, 39, 6, 11, 7, 20, 4, 9, 12, 7, 9, 17, 29, 25, 10, 19, 12, 7, 20, 15, 20, 14, 5, 6, 9, 35, 18, 7, 10, 15, 16, 19, 10, 9, 9, 16, 7, 8, 13, 5, 24, 15, 15, 5, 20, 15, 13, 11, 15, 11, 16, 23, 16, 8, 8, 13, 4, 30, 35, 4, 17, 6, 8, 7, 21, 5, 4, 10, 5, 8, 11, 5, 10, 8, 7, 11, 5, 11, 7, 10, 25, 34, 5, 9, 1, 4, 11, 8, 10, 4, 4, 18, 10, 30, 15, 4, 14, 6, 17, 7, 13, 2, 11, 12, 8, 2, 43, 13, 16, 11, 3, 14, 9, 15, 22, 13, 5, 10, 22, 14, 10, 11, 2, 21, 14, 9, 2, 9, 23, 15, 14, 4, 24, 5, 8, 4, 2, 19, 8, 76, 7, 13, 8, 28, 10, 8, 6, 9, 13, 13, 9, 49, 2, 9, 6, 8, 5, 12, 7, 3, 18, 3, 9, 14, 7, 5, 4, 8, 32, 6, 10, 21, 20, 22, 19, 25, 9, 15, 31, 21, 17, 2, 3, 12, 15, 26, 37, 26, 16, 6, 11, 16, 28, 7, 8, 12, 10, 11, 6, 8, 14, 3, 3, 14, 5, 46, 7, 14, 49, 44, 4, 10, 7, 32, 63, 19, 7, 12, 6, 8, 7, 5, 3, 6, 4, 4, 6, 10, 3, 16, 4, 8, 9, 24, 8, 3, 19, 19, 9, 6, 8, 4, 11, 9, 20, 10, 18, 29, 9, 10, 13, 15, 23, 2, 3, 12, 12, 25, 18, 13, 6, 26, 12, 11, 12, 7, 11, 10, 9, 6, 5, 9, 22, 32, 29, 13, 24, 8, 28, 23, 3, 13, 3, 9, 16, 15, 12, 8, 16, 10, 12, 6, 10, 6, 10, 7, 4, 100, 19, 18, 6, 36, 3, 21, 23, 28, 7, 27, 15, 8], 'train_acc': 1.0, 'test_acc': 0.04167820069204152, 'X': 'X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 768.9472, Test loss: 1308.8226
In this epoch 100/5000, Training loss: 142.4411, Test loss: 528.6942
In this epoch 150/5000, Training loss: 79.4194, Test loss: 475.5560
In this epoch 200/5000, Training loss: 52.7179, Test loss: 449.2335
In this epoch 250/5000, Training loss: 38.1772, Test loss: 435.2654
In this epoch 300/5000, Training loss: 28.8003, Test loss: 426.3031
In this epoch 350/5000, Training loss: 22.2252, Test loss: 419.9039
In this epoch 400/5000, Training loss: 17.4075, Test loss: 415.0400
In this epoch 450/5000, Training loss: 13.7841, Test loss: 411.2097
In this epoch 500/5000, Training loss: 11.0106, Test loss: 408.1285
In this epoch 550/5000, Training loss: 8.8595, Test loss: 405.6162
In this epoch 600/5000, Training loss: 7.1729, Test loss: 403.5573
In this epoch 650/5000, Training loss: 5.8382, Test loss: 401.8619
In this epoch 700/5000, Training loss: 4.7732, Test loss: 400.4618
In this epoch 750/5000, Training loss: 3.9173, Test loss: 399.3087
In this epoch 800/5000, Training loss: 3.2249, Test loss: 398.3583
In this epoch 850/5000, Training loss: 2.6617, Test loss: 397.5767
In this epoch 900/5000, Training loss: 2.2014, Test loss: 396.9331
In this epoch 950/5000, Training loss: 1.8235, Test loss: 396.4068
In this epoch 1000/5000, Training loss: 1.5124, Test loss: 395.9790
In this epoch 1050/5000, Training loss: 1.2555, Test loss: 395.6290
In this epoch 1100/5000, Training loss: 1.0428, Test loss: 395.3473
In this epoch 1150/5000, Training loss: 0.8666, Test loss: 395.1203
In this epoch 1200/5000, Training loss: 0.7203, Test loss: 394.9378
In this epoch 1250/5000, Training loss: 0.5987, Test loss: 394.7931
In this epoch 1300/5000, Training loss: 0.4976, Test loss: 394.6779
In this epoch 1350/5000, Training loss: 0.4136, Test loss: 394.5883
In this epoch 1400/5000, Training loss: 0.3436, Test loss: 394.5205
In this epoch 1450/5000, Training loss: 0.2854, Test loss: 394.4664
In this epoch 1500/5000, Training loss: 0.2370, Test loss: 394.4246
In this epoch 1550/5000, Training loss: 0.1968, Test loss: 394.3972
In this epoch 1600/5000, Training loss: 0.1633, Test loss: 394.3743
In this epoch 1650/5000, Training loss: 0.1354, Test loss: 394.3593
In this epoch 1700/5000, Training loss: 0.1123, Test loss: 394.3479
In this epoch 1750/5000, Training loss: 0.0931, Test loss: 394.3425
In this epoch 1800/5000, Training loss: 0.0771, Test loss: 394.3391
In this epoch 1850/5000, Training loss: 0.0639, Test loss: 394.3400
In this epoch 1900/5000, Training loss: 0.0529, Test loss: 394.3432
In this epoch 1950/5000, Training loss: 0.0438, Test loss: 394.3434
In this epoch 2000/5000, Training loss: 0.0363, Test loss: 394.3505
In this epoch 2050/5000, Training loss: 0.0300, Test loss: 394.3546
In this epoch 2100/5000, Training loss: 0.0249, Test loss: 394.3616
In this epoch 2150/5000, Training loss: 0.0206, Test loss: 394.3675
In this epoch 2200/5000, Training loss: 0.0170, Test loss: 394.3752
In this epoch 2250/5000, Training loss: 0.0141, Test loss: 394.3829
In this epoch 2300/5000, Training loss: 0.0117, Test loss: 394.3900
In this epoch 2350/5000, Training loss: 0.0097, Test loss: 394.3988
In this epoch 2400/5000, Training loss: 0.0081, Test loss: 394.4062
In this epoch 2450/5000, Training loss: 0.0067, Test loss: 394.4148
In this epoch 2500/5000, Training loss: 0.0056, Test loss: 394.4222
In this epoch 2550/5000, Training loss: 0.0046, Test loss: 394.4310
In this epoch 2600/5000, Training loss: 0.0039, Test loss: 394.4392
In this epoch 2650/5000, Training loss: 0.0032, Test loss: 394.4464
In this epoch 2700/5000, Training loss: 0.0027, Test loss: 394.4539
In this epoch 2750/5000, Training loss: 0.0023, Test loss: 394.4609
In this epoch 2800/5000, Training loss: 0.0019, Test loss: 394.4676
In this epoch 2850/5000, Training loss: 0.0016, Test loss: 394.4744
In this epoch 2900/5000, Training loss: 0.0014, Test loss: 394.4810
In this epoch 2950/5000, Training loss: 0.0012, Test loss: 394.4870
In this epoch 3000/5000, Training loss: 0.0010, Test loss: 394.4915
In this epoch 3050/5000, Training loss: 0.0009, Test loss: 394.4959
In this epoch 3100/5000, Training loss: 0.0008, Test loss: 394.5015
In this epoch 3150/5000, Training loss: 0.0007, Test loss: 394.5060
In this epoch 3200/5000, Training loss: 0.0006, Test loss: 394.5102
In this epoch 3250/5000, Training loss: 0.0005, Test loss: 394.5132
In this epoch 3300/5000, Training loss: 0.0005, Test loss: 394.5175
In this epoch 3350/5000, Training loss: 0.0004, Test loss: 394.5207
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 394.5243
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 394.5263
In this epoch 3500/5000, Training loss: 0.0003, Test loss: 394.5292
In this epoch 3550/5000, Training loss: 0.0003, Test loss: 394.5311
In this epoch 3600/5000, Training loss: 0.0003, Test loss: 394.5338
In this epoch 3650/5000, Training loss: 0.0003, Test loss: 394.5346
In this epoch 3700/5000, Training loss: 0.0003, Test loss: 394.5383
In this epoch 3750/5000, Training loss: 0.0003, Test loss: 394.5393
In this epoch 3800/5000, Training loss: 0.0002, Test loss: 394.5399
In this epoch 3850/5000, Training loss: 0.0002, Test loss: 394.5432
In this epoch 3900/5000, Training loss: 0.0002, Test loss: 394.5441
In this epoch 3950/5000, Training loss: 0.0002, Test loss: 394.5453
In this epoch 4000/5000, Training loss: 0.0002, Test loss: 394.5473
In this epoch 4050/5000, Training loss: 0.0002, Test loss: 394.5485
In this epoch 4100/5000, Training loss: 0.0002, Test loss: 394.5500
In this epoch 4150/5000, Training loss: 0.0002, Test loss: 394.5511
In this epoch 4200/5000, Training loss: 0.0002, Test loss: 394.5519
In this epoch 4250/5000, Training loss: 0.0002, Test loss: 394.5542
In this epoch 4300/5000, Training loss: 0.0002, Test loss: 394.5557
In this epoch 4350/5000, Training loss: 0.0002, Test loss: 394.5549
In this epoch 4400/5000, Training loss: 0.0002, Test loss: 394.5576
In this epoch 4450/5000, Training loss: 0.0001, Test loss: 394.5582
In this epoch 4500/5000, Training loss: 0.0001, Test loss: 394.5607
In this epoch 4550/5000, Training loss: 0.0001, Test loss: 394.5608
In this epoch 4600/5000, Training loss: 0.0001, Test loss: 394.5626
In this epoch 4650/5000, Training loss: 0.0001, Test loss: 394.5629
In this epoch 4700/5000, Training loss: 0.0001, Test loss: 394.5641
In this epoch 4750/5000, Training loss: 0.0001, Test loss: 394.5660
In this epoch 4800/5000, Training loss: 0.0001, Test loss: 394.5679
In this epoch 4850/5000, Training loss: 0.0001, Test loss: 394.5693
In this epoch 4900/5000, Training loss: 0.0001, Test loss: 394.5691
In this epoch 4950/5000, Training loss: 0.0001, Test loss: 394.5709
In this epoch 5000/5000, Training loss: 0.0001, Test loss: 394.5718
train_acc 1.0
test_acc 0.05588235294117647
{'y_pred': [100.80254364013672, 184.30349731445312, 229.3096923828125, 555.2140502929688, 360.4621887207031, 148.84536743164062, 208.14450073242188, 413.2462158203125, 755.1980590820312, 198.35842895507812, 98.83440399169922, 520.4102783203125, 391.0813903808594, 448.2373352050781, 159.21392822265625, 55.66654968261719, 402.9938659667969, 612.6520385742188, 311.1784973144531, 256.98883056640625, 454.8281555175781, 303.6553039550781, 780.9429321289062, 292.9929504394531, 415.2997131347656, 240.87240600585938, 104.72735595703125, 574.3795776367188, 144.37124633789062, 114.5905532836914, 811.3495483398438, 442.0775146484375, 495.50994873046875, 408.3309631347656, 167.75616455078125, 92.24436950683594, 342.57366943359375, 775.8356323242188, 265.9041748046875, 689.2364501953125, 675.6215209960938, 652.4797973632812, 530.6636962890625, 287.91510009765625, 353.8945617675781, 335.14129638671875, 143.2745819091797, 951.2078247070312, 272.0657653808594, 311.4342346191406, 342.5205383300781, 319.188232421875, 440.1645812988281, 187.85040283203125, 577.2754516601562, 725.2329711914062, 338.0912780761719, 204.26626586914062, 281.66729736328125, 842.6649780273438, 673.698974609375, 157.5908660888672, 814.0955810546875, 420.9472351074219, 605.103759765625, 135.35865783691406, 98.61966705322266, 483.3243103027344, 471.3542175292969, 193.481689453125, 126.39096069335938, 224.73504638671875, 160.79385375976562, 389.9983215332031, 716.2044067382812, 161.07931518554688, 358.8321838378906, 163.42649841308594, 509.7043151855469, 161.4217987060547, 591.430419921875, 603.4520874023438, 404.2821350097656, 338.16552734375, 143.48065185546875, 611.9172973632812, 299.3505859375, 594.5553588867188, 1392.7413330078125, 79.33522033691406, 278.2632141113281, 137.30511474609375, 550.3474731445312, 278.7298583984375, 202.484375, 234.78176879882812, 182.00653076171875, 295.5025939941406, 451.7237243652344, 463.5542907714844, 140.78884887695312, 122.23387145996094, 287.3852233886719, 245.79983520507812, 403.0829772949219, 665.08251953125, 190.25509643554688, 148.74346923828125, 890.578857421875, 609.8458862304688, 793.3557739257812, 441.4955749511719, 375.1107482910156, 484.8056335449219, 178.26919555664062, 533.9933471679688, 120.72171020507812, 567.2874755859375, 214.919921875, 333.1967468261719, 162.77194213867188, 140.25823974609375, 377.49249267578125, 533.6847534179688, 529.0763549804688, 366.1286315917969, 643.6514892578125, 161.24285888671875, 512.4722900390625, 312.429443359375, 62.8565673828125, 407.2463684082031, 388.3338928222656, 371.4971008300781, 393.0906982421875, 459.2052307128906, 414.2821960449219, 256.2379455566406, 473.61083984375, 853.3963623046875, 201.373046875, 653.6962280273438, 352.9167175292969, 306.0140075683594, 440.4217224121094, 334.25018310546875, 253.49331665039062, 778.385986328125, 212.406005859375, 619.4478759765625, 171.3041229248047, 431.7171936035156, 116.22583770751953, 315.7326354980469, 154.2778778076172, 124.49143981933594, 470.5215148925781, 828.3318481445312, 111.08853149414062, 458.6695556640625, 362.63629150390625, 339.6212158203125, 465.9092712402344, 173.09658813476562, 228.91310119628906, 229.48353576660156, 281.3892517089844, 334.9984436035156, 229.19642639160156, 132.76565551757812, 317.3492736816406, 380.5779724121094, 202.51638793945312, 268.2505187988281, 304.3450622558594, 700.46142578125, 339.70550537109375, 70.82939910888672, 274.146728515625, 536.4194946289062, 545.2379150390625, 299.80889892578125, 497.769287109375, 104.51840209960938, 771.6923217773438, 212.66217041015625, 182.6512908935547, 78.95372772216797, 177.3655242919922, 292.7738037109375, 711.8479614257812, 277.3729553222656, 464.9410705566406, 317.3049011230469, 127.42648315429688, 820.3486938476562, 346.3348693847656, 829.5335083007812, 145.28163146972656, 57.964176177978516, 443.6889953613281, 100.92021179199219, 155.0019989013672, 218.70370483398438, 375.5163269042969, 467.8331604003906, 116.68171691894531, 258.5362243652344, 57.269203186035156, 129.90200805664062, 534.2952270507812, 513.2679443359375, 432.3564758300781, 363.6905517578125, 634.9082641601562, 199.33700561523438, 889.4556274414062, 727.1007080078125, 105.88921356201172, 148.72665405273438, 248.30316162109375, 312.4945373535156, 207.16415405273438, 447.5889587402344, 304.0712585449219, 537.0235595703125, 467.6187438964844, 479.0956115722656, 294.4615173339844, 385.7398681640625, 153.26988220214844, 943.798583984375, 403.1490478515625, 239.47378540039062, 162.14315795898438, 108.483154296875, 224.0895233154297, 247.64907836914062, 246.82949829101562, 681.6419067382812, 495.7923889160156, 399.80218505859375, 263.8290710449219, 571.7410278320312, 100.40127563476562, 681.4218139648438, 715.834228515625, 211.4197998046875, 162.25180053710938, 203.54544067382812, 339.3130798339844, 530.5467529296875, 454.19866943359375, 767.2145385742188, 60.18666076660156, 159.2794952392578, 347.33184814453125, 490.3080749511719, 755.5142211914062, 337.7933654785156, 1012.6989135742188, 448.4193115234375, 911.1126098632812, 331.6170959472656, 89.98062133789062, 152.392333984375, 429.2572937011719, 328.5498962402344, 364.0605773925781, 434.6881408691406, 405.7960510253906, 409.7320251464844, 1008.2357788085938, 169.9652099609375, 648.2803344726562, 400.9169921875, 305.5061340332031, 487.7562561035156, 430.8440246582031, 300.7962341308594, 211.8885040283203, 144.5380401611328, 122.74298095703125, 103.74011993408203, 191.79339599609375, 331.1719665527344, 208.48289489746094, 455.5494689941406, 530.2398071289062, 505.0458679199219, 155.52838134765625, 313.02911376953125, 399.0886535644531, 289.0302734375, 179.98765563964844, 433.9394226074219, 496.5509948730469, 206.02967834472656, 520.8492431640625, 552.43505859375, 455.3252868652344, 162.78945922851562, 510.28533935546875, 384.96868896484375, 575.0480346679688, 696.9685668945312, 64.18553924560547, 189.00930786132812, 820.0610961914062, 92.42500305175781, 455.8529357910156, 350.2005920410156, 79.16584777832031, 239.04908752441406, 505.4319763183594, 710.2282104492188, 94.6646728515625, 242.18954467773438, 104.09963989257812, 427.4452209472656, 432.3258056640625, 176.58970642089844, 467.974853515625, 571.864990234375, 198.01449584960938, 379.4435119628906, 533.6863403320312, 373.00054931640625, 210.19288635253906, 157.84231567382812, 293.86669921875, 372.4806213378906, 378.3557434082031, 884.349609375, 236.29180908203125, 305.52166748046875, 382.7867736816406, 319.7335510253906, 361.91143798828125, 171.7086181640625], 'y_pred_int': [101, 184, 229, 555, 360, 149, 208, 413, 755, 198, 99, 520, 391, 448, 159, 56, 403, 613, 311, 257, 455, 304, 781, 293, 415, 241, 105, 574, 144, 115, 811, 442, 496, 408, 168, 92, 343, 776, 266, 689, 676, 652, 531, 288, 354, 335, 143, 951, 272, 311, 343, 319, 440, 188, 577, 725, 338, 204, 282, 843, 674, 158, 814, 421, 605, 135, 99, 483, 471, 193, 126, 225, 161, 390, 716, 161, 359, 163, 510, 161, 591, 603, 404, 338, 143, 612, 299, 595, 1393, 79, 278, 137, 550, 279, 202, 235, 182, 296, 452, 464, 141, 122, 287, 246, 403, 665, 190, 149, 891, 610, 793, 441, 375, 485, 178, 534, 121, 567, 215, 333, 163, 140, 377, 534, 529, 366, 644, 161, 512, 312, 63, 407, 388, 371, 393, 459, 414, 256, 474, 853, 201, 654, 353, 306, 440, 334, 253, 778, 212, 619, 171, 432, 116, 316, 154, 124, 471, 828, 111, 459, 363, 340, 466, 173, 229, 229, 281, 335, 229, 133, 317, 381, 203, 268, 304, 700, 340, 71, 274, 536, 545, 300, 498, 105, 772, 213, 183, 79, 177, 293, 712, 277, 465, 317, 127, 820, 346, 830, 145, 58, 444, 101, 155, 219, 376, 468, 117, 259, 57, 130, 534, 513, 432, 364, 635, 199, 889, 727, 106, 149, 248, 312, 207, 448, 304, 537, 468, 479, 294, 386, 153, 944, 403, 239, 162, 108, 224, 248, 247, 682, 496, 400, 264, 572, 100, 681, 716, 211, 162, 204, 339, 531, 454, 767, 60, 159, 347, 490, 756, 338, 1013, 448, 911, 332, 90, 152, 429, 329, 364, 435, 406, 410, 1008, 170, 648, 401, 306, 488, 431, 301, 212, 145, 123, 104, 192, 331, 208, 456, 530, 505, 156, 313, 399, 289, 180, 434, 497, 206, 521, 552, 455, 163, 510, 385, 575, 697, 64, 189, 820, 92, 456, 350, 79, 239, 505, 710, 95, 242, 104, 427, 432, 177, 468, 572, 198, 379, 534, 373, 210, 158, 294, 372, 378, 884, 236, 306, 383, 320, 362, 172], 'y_true': [103, 165, 227, 564, 367, 145, 203, 410, 778, 203, 104, 517, 389, 443, 161, 58, 396, 624, 292, 262, 463, 312, 790, 295, 445, 249, 105, 569, 131, 113, 821, 448, 498, 395, 169, 88, 342, 773, 268, 687, 665, 649, 521, 307, 367, 330, 142, 935, 274, 339, 351, 334, 443, 193, 575, 718, 341, 214, 276, 835, 678, 159, 828, 423, 583, 141, 93, 484, 461, 194, 134, 208, 154, 373, 723, 164, 360, 164, 523, 149, 591, 607, 401, 334, 141, 624, 291, 608, 1695, 76, 284, 129, 536, 278, 195, 243, 183, 301, 458, 463, 147, 117, 288, 238, 407, 631, 189, 153, 881, 643, 783, 448, 366, 494, 181, 537, 125, 572, 217, 337, 154, 144, 367, 543, 557, 376, 643, 164, 493, 317, 68, 401, 397, 373, 392, 449, 409, 269, 478, 863, 204, 634, 363, 299, 440, 340, 258, 775, 205, 590, 163, 431, 111, 326, 155, 134, 453, 842, 114, 471, 372, 346, 462, 173, 228, 239, 292, 337, 232, 136, 321, 372, 197, 263, 308, 694, 347, 69, 264, 548, 534, 296, 495, 103, 758, 211, 182, 80, 179, 296, 724, 279, 509, 331, 128, 795, 342, 805, 149, 57, 433, 102, 159, 196, 375, 459, 116, 269, 61, 128, 500, 550, 464, 343, 638, 197, 907, 752, 107, 158, 246, 302, 217, 457, 286, 537, 466, 485, 296, 389, 161, 934, 414, 237, 169, 111, 231, 244, 243, 682, 507, 404, 256, 563, 99, 669, 688, 204, 167, 206, 338, 521, 445, 761, 59, 162, 364, 488, 768, 344, 999, 465, 898, 347, 80, 150, 424, 331, 351, 437, 417, 406, 1016, 171, 613, 387, 317, 468, 428, 296, 215, 149, 122, 103, 187, 332, 213, 466, 508, 511, 154, 309, 409, 294, 182, 452, 515, 208, 513, 520, 466, 163, 507, 378, 586, 765, 60, 189, 824, 97, 445, 367, 76, 228, 497, 711, 99, 241, 108, 428, 430, 180, 464, 566, 199, 389, 523, 373, 211, 154, 299, 370, 392, 904, 235, 312, 393, 312, 361, 181], 'train_acc': 1.0, 'test_acc': 0.05588235294117647, 'X': 'X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700', 'Y': 'Y_num_tokens_muc1700'}
