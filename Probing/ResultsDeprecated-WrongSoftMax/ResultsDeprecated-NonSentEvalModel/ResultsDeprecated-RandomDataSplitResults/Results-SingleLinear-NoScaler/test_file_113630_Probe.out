Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 25.4778, Test loss: 15.9746
In this epoch 100/5000, Training loss: 0.0575, Test loss: 1.4415
In this epoch 150/5000, Training loss: 0.0047, Test loss: 1.2604
In this epoch 200/5000, Training loss: 0.0020, Test loss: 1.2644
In this epoch 250/5000, Training loss: 0.0013, Test loss: 1.2688
In this epoch 300/5000, Training loss: 0.0010, Test loss: 1.2720
In this epoch 350/5000, Training loss: 0.0008, Test loss: 1.2742
In this epoch 400/5000, Training loss: 0.0007, Test loss: 1.2757
In this epoch 450/5000, Training loss: 0.0006, Test loss: 1.2766
In this epoch 500/5000, Training loss: 0.0006, Test loss: 1.2772
In this epoch 550/5000, Training loss: 0.0005, Test loss: 1.2776
In this epoch 600/5000, Training loss: 0.0005, Test loss: 1.2779
In this epoch 650/5000, Training loss: 0.0005, Test loss: 1.2782
In this epoch 700/5000, Training loss: 0.0005, Test loss: 1.2785
In this epoch 750/5000, Training loss: 0.0004, Test loss: 1.2787
In this epoch 800/5000, Training loss: 0.0004, Test loss: 1.2790
In this epoch 850/5000, Training loss: 0.0004, Test loss: 1.2792
In this epoch 900/5000, Training loss: 0.0004, Test loss: 1.2794
In this epoch 950/5000, Training loss: 0.0004, Test loss: 1.2796
In this epoch 1000/5000, Training loss: 0.0004, Test loss: 1.2797
In this epoch 1050/5000, Training loss: 0.0004, Test loss: 1.2799
In this epoch 1100/5000, Training loss: 0.0004, Test loss: 1.2800
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 1.2801
In this epoch 1200/5000, Training loss: 0.0004, Test loss: 1.2802
In this epoch 1250/5000, Training loss: 0.0004, Test loss: 1.2803
In this epoch 1300/5000, Training loss: 0.0004, Test loss: 1.2804
In this epoch 1350/5000, Training loss: 0.0004, Test loss: 1.2805
In this epoch 1400/5000, Training loss: 0.0004, Test loss: 1.2805
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 1.2806
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 1.2806
In this epoch 1550/5000, Training loss: 0.0004, Test loss: 1.2806
In this epoch 1600/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 1650/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 1700/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 1900/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 1950/5000, Training loss: 0.0295, Test loss: 1.3409
In this epoch 2000/5000, Training loss: 0.2058, Test loss: 1.5516
In this epoch 2050/5000, Training loss: 0.0077, Test loss: 1.3061
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 1.2812
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 1.2806
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 1.2805
In this epoch 2250/5000, Training loss: 1.6601, Test loss: 9.7392
In this epoch 2300/5000, Training loss: 0.1624, Test loss: 1.4608
In this epoch 2350/5000, Training loss: 0.0022, Test loss: 1.2880
In this epoch 2400/5000, Training loss: 0.0004, Test loss: 1.2810
In this epoch 2450/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 1.2806
In this epoch 2550/5000, Training loss: 73.7778, Test loss: 75.2491
In this epoch 2600/5000, Training loss: 0.0369, Test loss: 1.2950
In this epoch 2650/5000, Training loss: 0.0031, Test loss: 1.2878
In this epoch 2700/5000, Training loss: 0.0004, Test loss: 1.2810
In this epoch 2750/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 2800/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 2850/5000, Training loss: 0.0323, Test loss: 1.3700
In this epoch 2900/5000, Training loss: 3.4446, Test loss: 4.8977
In this epoch 2950/5000, Training loss: 0.0185, Test loss: 1.2751
In this epoch 3000/5000, Training loss: 0.0005, Test loss: 1.2812
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 1.2809
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 3200/5000, Training loss: 180.0118, Test loss: 115.7787
In this epoch 3250/5000, Training loss: 0.0186, Test loss: 1.5144
In this epoch 3300/5000, Training loss: 0.0009, Test loss: 1.2836
In this epoch 3350/5000, Training loss: 0.0004, Test loss: 1.2815
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 3500/5000, Training loss: 0.9173, Test loss: 3.6755
In this epoch 3550/5000, Training loss: 0.8811, Test loss: 1.5079
In this epoch 3600/5000, Training loss: 0.0099, Test loss: 1.3088
In this epoch 3650/5000, Training loss: 0.0050, Test loss: 1.2979
In this epoch 3700/5000, Training loss: 2.9858, Test loss: 7.1325
In this epoch 3750/5000, Training loss: 0.0133, Test loss: 1.3393
In this epoch 3800/5000, Training loss: 0.0004, Test loss: 1.2800
In this epoch 3850/5000, Training loss: 0.0098, Test loss: 1.3118
In this epoch 3900/5000, Training loss: 3.8122, Test loss: 2.1689
In this epoch 3950/5000, Training loss: 0.0317, Test loss: 1.3132
In this epoch 4000/5000, Training loss: 0.0005, Test loss: 1.2816
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 1.2810
In this epoch 4100/5000, Training loss: 14.4339, Test loss: 24.5687
In this epoch 4150/5000, Training loss: 0.2849, Test loss: 2.2031
In this epoch 4200/5000, Training loss: 0.0065, Test loss: 1.2998
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 1.2809
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 1.2808
In this epoch 4350/5000, Training loss: 0.3557, Test loss: 1.8530
In this epoch 4400/5000, Training loss: 4.5828, Test loss: 5.2944
In this epoch 4450/5000, Training loss: 0.0135, Test loss: 1.2878
In this epoch 4500/5000, Training loss: 0.0006, Test loss: 1.2787
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 1.2807
In this epoch 4600/5000, Training loss: 0.0037, Test loss: 1.2967
In this epoch 4650/5000, Training loss: 2.2481, Test loss: 6.4749
In this epoch 4700/5000, Training loss: 0.0279, Test loss: 1.3502
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 1.2813
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 1.2806
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 1.2808
In this epoch 4900/5000, Training loss: 32.6435, Test loss: 53.9792
In this epoch 4950/5000, Training loss: 0.3735, Test loss: 2.0272
In this epoch 5000/5000, Training loss: 0.0062, Test loss: 1.2994
train_acc 0.3462743295847751
test_acc 0.4054498269896194
{'y_pred': [1.1858071088790894, 0.7060046792030334, 2.0564029216766357, 0.8835356831550598, 0.9168335795402527, 0.8071320652961731, 1.1428793668746948, 1.6570464372634888, 1.3342593908309937, 1.013688564300537, 1.2278484106063843, 1.3924384117126465, 0.27442729473114014, 1.3454285860061646, 1.648297667503357, 1.7910312414169312, 1.5418871641159058, 0.7374290823936462, 0.39111268520355225, 1.6861743927001953, 0.7506573796272278, 0.01679876446723938, 1.3716269731521606, 1.2637481689453125, 0.2905797064304352, 1.5984183549880981, 0.5358660221099854, 1.008796215057373, 1.7561531066894531, 0.9436091780662537, 1.4461685419082642, 0.2880276143550873, 1.6141812801361084, 1.2875149250030518, 0.8701733350753784, 1.39323890209198, 0.971356213092804, 0.8448330163955688, 2.135291576385498, 0.3673354983329773, 0.04017281532287598, 0.8555353283882141, 1.298898458480835, 1.098053216934204, 0.1516728699207306, 1.022501826286316, 1.1906782388687134, 0.6126026511192322, 1.2636164426803589, 0.2733227610588074, 1.2478638887405396, 1.5987224578857422, 1.3034244775772095, -0.27573394775390625, 0.9446600079536438, 1.2757271528244019, 1.0622745752334595, 1.4713507890701294, 1.2665927410125732, 1.0079213380813599, 0.335146963596344, 0.6867469549179077, -0.005495368968695402, 1.3302946090698242, 1.3096871376037598, 1.669705867767334, 0.07587980479001999, 1.6060709953308105, 1.6090950965881348, 1.2803280353546143, -0.6410166621208191, 0.4732745885848999, 1.9314839839935303, 0.8277506828308105, 0.23260816931724548, 0.59603351354599, 0.702808678150177, 0.4622044563293457, 1.40091073513031, 1.0113861560821533, 1.1759252548217773, 1.415973424911499, 0.5890835523605347, 0.5569661855697632, 0.889311671257019, 1.3980658054351807, 0.06057574599981308, 0.8060964345932007, 1.5144253969192505, 0.5009782314300537, 0.7821021676063538, 0.6037941575050354, 1.0771163702011108, 0.9950409531593323, 0.965194582939148, 1.2550228834152222, 1.3729839324951172, 1.127212643623352, 1.262492299079895, -0.13157179951667786, 0.8415818810462952, 0.758441686630249, 0.32832568883895874, 1.42140531539917, 0.7185312509536743, 1.4074010848999023, 0.7717415690422058, 0.48009243607521057, 1.7148700952529907, 0.3497903347015381, 1.19334077835083, 1.0275691747665405, 1.4457427263259888, 0.9007635712623596, 1.623397707939148, 0.7565140128135681, 0.8814048171043396, 0.8098582029342651, 1.299014925956726, 1.7147175073623657, 1.6142027378082275, 0.965806782245636, 2.1148626804351807, 0.7241381406784058, 1.1561928987503052, 1.1021524667739868, 0.47877612709999084, 0.4304576814174652, 1.0139163732528687, -0.04409056901931763, 0.6458399295806885, 1.4981955289840698, 0.7934793829917908, 1.064497709274292, 0.8922591209411621, 0.9238035082817078, 0.7481528520584106, 1.280064582824707, 1.423364281654358, 1.3304446935653687, 0.3278401494026184, 0.6697506308555603, 0.008813619613647461, 0.8807034492492676, 1.4853787422180176, 0.6668740510940552, 1.240408182144165, 1.3226135969161987, 0.3339415192604065, 0.7588491439819336, 1.265202522277832, 0.786666214466095, 0.4687140882015228, 0.9919325709342957, 1.1939682960510254, 0.7971439361572266, 1.7731786966323853, 0.9188005924224854, 0.9873000383377075, 0.8230114579200745, 1.0994783639907837, 0.9544592499732971, 0.8531050682067871, 1.4386652708053589, 0.7736515998840332, 0.7370014786720276, 1.1286697387695312, 1.513336181640625, 1.5748687982559204, 1.4827290773391724, 0.8439820408821106, -0.019074151292443275, 0.9615384340286255, 1.1853889226913452, 0.9076401591300964, 0.717021644115448, 0.8886319398880005, 0.5353816747665405, 0.5154162645339966, 0.8070136904716492, 1.4151949882507324, 0.8232383131980896, 1.0288292169570923, 1.6088584661483765, 0.7901156544685364, 0.8015808463096619, 0.7205536365509033, 0.9113307595252991, 1.2319988012313843, 1.0254558324813843, 0.8227010369300842, 0.5769810676574707, 0.9184760451316833, -0.08846312016248703, 0.8789191842079163, 1.2275253534317017, 0.22573983669281006, 1.2295483350753784, 0.7646268010139465, 0.6665894389152527, 0.6185507774353027, 1.243350863456726, 0.787570059299469, 1.187568187713623, 1.7963491678237915, 1.9157947301864624, 0.9638442397117615, 0.5964412689208984, 0.9859501719474792, 1.3134548664093018, 1.4036755561828613, 0.5430911183357239, 0.8382413387298584, 1.2020542621612549, 1.2466402053833008, 0.9976823925971985, 0.7816261649131775, 0.9310318231582642, 0.6690669655799866, 1.1703563928604126, 0.5557134747505188, 1.293874740600586, 0.3258490562438965, 1.4755488634109497, 1.2106413841247559, 0.643298864364624, 0.8117803335189819, 1.1189708709716797, 0.8195867538452148, 0.44721555709838867, 0.6147898435592651, 0.8228407502174377, 1.3108049631118774, 0.5262001156806946, 1.3277255296707153, 1.175296425819397, 1.2261468172073364, 1.3192529678344727, 0.7732377648353577, 0.9991722702980042, 1.2961057424545288, 0.600804328918457, 1.652410864830017, 1.3869866132736206, 0.5854702591896057, 1.0369406938552856, 1.109462857246399, 0.6051773428916931, 0.5452425479888916, 1.0966700315475464, 0.38515499234199524, 0.5699887871742249, 1.1771469116210938, 0.9244346022605896, 1.0867475271224976, 0.45491546392440796, 0.9684886336326599, 1.1896623373031616, 2.125203847885132, 1.803510069847107, 1.399489402770996, 0.6324193477630615, 1.1945488452911377, 1.282737374305725, 0.3425186276435852, 1.632841944694519, 1.200632095336914, 1.2623168230056763, 1.140921711921692, 0.5892155766487122, 1.3955652713775635, 1.0074841976165771, 1.1858055591583252, 1.9486292600631714, 0.6491962671279907, 1.6603362560272217, 0.5310942530632019, 1.534224033355713, 1.6373016834259033, 0.7329038977622986, 1.3289602994918823, 1.198860764503479, 0.9160914421081543, 0.47326990962028503, 0.6767829060554504, 0.7778218388557434, 0.8351956009864807, 1.6481894254684448, 0.616214394569397, 1.1218429803848267, 1.1196725368499756, 0.04861369729042053, 1.2581367492675781, 1.664908528327942, 0.5964992046356201, 1.3668973445892334, 0.8318427205085754, 0.5363731980323792, 1.2271579504013062, 1.1137834787368774, 1.3403269052505493, 0.5045509934425354, 0.789959192276001, 1.4057568311691284, 0.8902745842933655, 1.2806580066680908, 1.2879295349121094, 0.6339479088783264, 1.858601689338684, 1.455876350402832, 1.2618969678878784, 0.5693802237510681, 0.9574928879737854, 1.0559124946594238, 0.9721980690956116, 1.9601444005966187, 1.0695379972457886, 0.42774108052253723, 0.9129598140716553, 1.3809894323349, 0.8159922957420349, 0.5255991816520691, 1.6810302734375, 0.6314576864242554, 0.6869780421257019, 1.348661184310913, 1.1600050926208496, 1.020811915397644, 1.0470119714736938, 1.246683120727539, 0.3460528254508972, 1.216010570526123, 0.5628024935722351, 0.9538774490356445, 0.7964916825294495, 0.8943895697593689, 0.7058419585227966, 1.3300882577896118, 0.9314858317375183, 1.1593154668807983], 'y_pred_int': [1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 2, 2, 1, 0, 2, 1, 0, 1, 1, 0, 2, 1, 1, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 2, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'y_true': [0, 2, 1, 1, 0, 0, 1, 0, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 0, 3, 1, 2, 1, 5, 0, 2, 2, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 3, 0, 0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 3, 3, 0, 2, 3, 0, 0, 1, 1, 0, 1, 0, 1, 0, 3, 0, 0, 2, 2, 1, 2, 0, 0, 1, 1, 1, 1, 0, 0, 0, 3, 0, 2, 1, 3, 0, 2, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 4, 0, 2, 0, 0, 3, 1, 0, 4, 2, 8, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 2, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 2, 1, 4, 0, 4, 1, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 3, 1, 4, 0, 0, 0, 0, 1, 1, 1, 3, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 3, 1, 0, 2, 1, 1, 1, 0, 2, 0, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 0, 0, 1, 0, 2, 1, 0, 0, 2, 0, 0, 0, 1, 3, 0, 0, 1, 3, 0, 1, 1, 0, 4, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 3, 0, 0, 0, 0, 2, 0, 1, 0, 0, 4, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 3, 1, 2, 1, 1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 2, 2, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1], 'train_acc': 0.3462743295847751, 'test_acc': 0.4054498269896194, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 21.4986, Test loss: 110.9327
In this epoch 100/5000, Training loss: 0.7782, Test loss: 107.1335
In this epoch 150/5000, Training loss: 0.2977, Test loss: 107.7850
In this epoch 200/5000, Training loss: 0.2183, Test loss: 108.0110
In this epoch 250/5000, Training loss: 0.1806, Test loss: 108.0788
In this epoch 300/5000, Training loss: 0.1495, Test loss: 108.1012
In this epoch 350/5000, Training loss: 0.1220, Test loss: 108.1198
In this epoch 400/5000, Training loss: 0.0984, Test loss: 108.1475
In this epoch 450/5000, Training loss: 0.0786, Test loss: 108.1857
In this epoch 500/5000, Training loss: 0.0625, Test loss: 108.2325
In this epoch 550/5000, Training loss: 0.0496, Test loss: 108.2852
In this epoch 600/5000, Training loss: 0.0396, Test loss: 108.3410
In this epoch 650/5000, Training loss: 0.0320, Test loss: 108.3975
In this epoch 700/5000, Training loss: 0.0264, Test loss: 108.4527
In this epoch 750/5000, Training loss: 0.0222, Test loss: 108.5049
In this epoch 800/5000, Training loss: 0.0193, Test loss: 108.5530
In this epoch 850/5000, Training loss: 0.0172, Test loss: 108.5964
In this epoch 900/5000, Training loss: 0.0158, Test loss: 108.6346
In this epoch 950/5000, Training loss: 0.0149, Test loss: 108.6677
In this epoch 1000/5000, Training loss: 0.0143, Test loss: 108.6958
In this epoch 1050/5000, Training loss: 0.0139, Test loss: 108.7193
In this epoch 1100/5000, Training loss: 0.0136, Test loss: 108.7386
In this epoch 1150/5000, Training loss: 0.0135, Test loss: 108.7544
In this epoch 1200/5000, Training loss: 0.0134, Test loss: 108.7669
In this epoch 1250/5000, Training loss: 0.0133, Test loss: 108.7768
In this epoch 1300/5000, Training loss: 0.0133, Test loss: 108.7845
In this epoch 1350/5000, Training loss: 0.0133, Test loss: 108.7904
In this epoch 1400/5000, Training loss: 0.0132, Test loss: 108.7949
In this epoch 1450/5000, Training loss: 3.3635, Test loss: 109.8013
In this epoch 1500/5000, Training loss: 0.0430, Test loss: 108.8763
In this epoch 1550/5000, Training loss: 0.0134, Test loss: 108.8056
In this epoch 1600/5000, Training loss: 15.3733, Test loss: 126.9479
In this epoch 1650/5000, Training loss: 0.1705, Test loss: 108.8648
In this epoch 1700/5000, Training loss: 0.0191, Test loss: 108.8273
In this epoch 1750/5000, Training loss: 3.1360, Test loss: 108.9535
In this epoch 1800/5000, Training loss: 0.0516, Test loss: 108.7972
In this epoch 1850/5000, Training loss: 0.0133, Test loss: 108.8088
In this epoch 1900/5000, Training loss: 67.8756, Test loss: 191.9891
In this epoch 1950/5000, Training loss: 0.0354, Test loss: 108.8748
In this epoch 2000/5000, Training loss: 0.0169, Test loss: 108.8213
In this epoch 2050/5000, Training loss: 0.0132, Test loss: 108.8062
In this epoch 2100/5000, Training loss: 1.0208, Test loss: 110.3529
In this epoch 2150/5000, Training loss: 1.8991, Test loss: 109.1128
In this epoch 2200/5000, Training loss: 0.0213, Test loss: 108.8020
In this epoch 2250/5000, Training loss: 0.0133, Test loss: 108.8085
In this epoch 2300/5000, Training loss: 0.0132, Test loss: 108.8067
In this epoch 2350/5000, Training loss: 0.0132, Test loss: 108.8064
In this epoch 2400/5000, Training loss: 43.8962, Test loss: 159.2887
In this epoch 2450/5000, Training loss: 0.0206, Test loss: 108.8398
In this epoch 2500/5000, Training loss: 0.0202, Test loss: 108.7978
In this epoch 2550/5000, Training loss: 0.5909, Test loss: 113.0098
In this epoch 2600/5000, Training loss: 0.1084, Test loss: 108.8464
In this epoch 2650/5000, Training loss: 0.0138, Test loss: 108.8028
In this epoch 2700/5000, Training loss: 20.7855, Test loss: 111.3591
In this epoch 2750/5000, Training loss: 0.0227, Test loss: 108.8438
In this epoch 2800/5000, Training loss: 0.0149, Test loss: 108.8131
In this epoch 2850/5000, Training loss: 0.0132, Test loss: 108.8070
In this epoch 2900/5000, Training loss: 17.9679, Test loss: 109.7336
In this epoch 2950/5000, Training loss: 0.2504, Test loss: 108.8251
In this epoch 3000/5000, Training loss: 0.0139, Test loss: 108.8074
In this epoch 3050/5000, Training loss: 0.0133, Test loss: 108.8061
In this epoch 3100/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 3150/5000, Training loss: 0.0133, Test loss: 108.8087
In this epoch 3200/5000, Training loss: 0.0266, Test loss: 108.8930
In this epoch 3250/5000, Training loss: 2.7563, Test loss: 108.7946
In this epoch 3300/5000, Training loss: 0.0185, Test loss: 108.8024
In this epoch 3350/5000, Training loss: 0.0134, Test loss: 108.8046
In this epoch 3400/5000, Training loss: 0.0132, Test loss: 108.8070
In this epoch 3450/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 3500/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 3550/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 3600/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 3650/5000, Training loss: 235.7732, Test loss: 169.1314
In this epoch 3700/5000, Training loss: 0.7867, Test loss: 108.8057
In this epoch 3750/5000, Training loss: 0.0192, Test loss: 108.8025
In this epoch 3800/5000, Training loss: 0.0133, Test loss: 108.8066
In this epoch 3850/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 3900/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 3950/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 4000/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 4050/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 4100/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 4150/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 4200/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 4250/5000, Training loss: 0.0132, Test loss: 108.8068
In this epoch 4300/5000, Training loss: 157.4029, Test loss: 371.0492
In this epoch 4350/5000, Training loss: 0.3519, Test loss: 110.0998
In this epoch 4400/5000, Training loss: 0.0235, Test loss: 108.8293
In this epoch 4450/5000, Training loss: 0.0132, Test loss: 108.8106
In this epoch 4500/5000, Training loss: 0.0132, Test loss: 108.8111
In this epoch 4550/5000, Training loss: 0.0132, Test loss: 108.8109
In this epoch 4600/5000, Training loss: 0.0132, Test loss: 108.8109
In this epoch 4650/5000, Training loss: 0.0132, Test loss: 108.8109
In this epoch 4700/5000, Training loss: 0.0132, Test loss: 108.8109
In this epoch 4750/5000, Training loss: 0.0132, Test loss: 108.8109
In this epoch 4800/5000, Training loss: 0.0132, Test loss: 108.8109
In this epoch 4850/5000, Training loss: 0.0132, Test loss: 108.8109
In this epoch 4900/5000, Training loss: 0.0132, Test loss: 108.8109
In this epoch 4950/5000, Training loss: 0.0132, Test loss: 108.8109
In this epoch 5000/5000, Training loss: 425.2161, Test loss: 310.5614
train_acc 0.0
test_acc 0.011764705882352941
{'y_pred': [28.89054298400879, 32.08949661254883, 32.278358459472656, 33.29161834716797, 30.351421356201172, 33.9791145324707, 27.801937103271484, 32.84725570678711, 25.682437896728516, 20.998746871948242, 29.46249008178711, 36.03642272949219, 19.80939483642578, 15.647375106811523, 27.214765548706055, 25.408607482910156, 28.024425506591797, 33.23875427246094, 29.844289779663086, 27.479623794555664, 32.16553497314453, 27.41012954711914, 16.119300842285156, 31.50924301147461, 31.656892776489258, 24.907155990600586, 32.389244079589844, 27.278390884399414, 23.389780044555664, 23.014589309692383, 34.134788513183594, 24.906227111816406, 30.31899642944336, 28.31768035888672, 26.928043365478516, 26.75103187561035, 32.57169723510742, 23.90916633605957, 25.36427879333496, 28.02988624572754, 18.99516487121582, 26.926708221435547, 32.96752166748047, 31.18394660949707, 31.423837661743164, 27.6243839263916, 20.698192596435547, 21.940784454345703, 25.211774826049805, 27.100400924682617, 31.855152130126953, 29.009050369262695, 20.770740509033203, 29.641677856445312, 30.19145965576172, 25.95815658569336, 24.7249755859375, 27.921924591064453, 32.45458984375, 20.27652931213379, 29.544462203979492, 21.349388122558594, 26.264354705810547, 33.518531799316406, 26.845626831054688, 27.630863189697266, 26.126142501831055, 22.87653350830078, 28.826292037963867, 24.682071685791016, 30.496646881103516, 34.00117111206055, 29.701255798339844, 25.81713104248047, 21.238000869750977, 27.556392669677734, 21.730663299560547, 30.112640380859375, 31.135894775390625, 31.94620132446289, 30.365116119384766, 23.76366424560547, 29.37401580810547, 30.152938842773438, 23.055288314819336, 24.465023040771484, 28.482969284057617, 36.26616668701172, 20.132518768310547, 21.53934097290039, 30.264572143554688, 27.39512062072754, 30.417556762695312, 29.631731033325195, 26.813432693481445, 27.444032669067383, 22.340076446533203, 32.421451568603516, 28.21706771850586, 31.156814575195312, 34.397850036621094, 21.319345474243164, 23.664764404296875, 27.27922821044922, 24.599668502807617, 20.631216049194336, 30.51468276977539, 31.339508056640625, 29.929229736328125, 29.70041847229004, 30.116668701171875, 33.84385681152344, 33.1702766418457, 23.86176300048828, 22.33046531677246, 20.86540412902832, 22.55773162841797, 28.556438446044922, 31.152801513671875, 26.585811614990234, 32.84541320800781, 24.41897964477539, 22.385848999023438, 33.76277542114258, 29.978836059570312, 24.17705535888672, 29.277101516723633, 31.5595645904541, 22.599639892578125, 23.73118019104004, 27.86163902282715, 21.739078521728516, 29.68282127380371, 28.526626586914062, 35.202545166015625, 28.722286224365234, 28.120033264160156, 25.566055297851562, 21.745624542236328, 26.61853790283203, 23.183305740356445, 30.256372451782227, 15.903881072998047, 27.793115615844727, 25.33556365966797, 34.125186920166016, 23.630508422851562, 28.895858764648438, 35.294403076171875, 30.682357788085938, 25.07869529724121, 25.90493392944336, 29.802654266357422, 31.785972595214844, 30.61595916748047, 30.781572341918945, 29.597503662109375, 18.541688919067383, 27.16681671142578, 25.197900772094727, 30.87314796447754, 28.28839683532715, 25.22283172607422, 24.522846221923828, 29.671438217163086, 32.43388748168945, 21.803668975830078, 26.45980453491211, 28.061128616333008, 30.833118438720703, 27.531841278076172, 32.28364562988281, 27.676570892333984, 31.64598846435547, 22.26686668395996, 26.096303939819336, 20.532970428466797, 35.7217903137207, 24.215126037597656, 34.109130859375, 25.431394577026367, 21.495668411254883, 34.8746452331543, 27.605743408203125, 26.26319122314453, 29.600326538085938, 27.97508430480957, 26.372879028320312, 28.81220245361328, 30.805572509765625, 22.24671173095703, 29.529815673828125, 21.838598251342773, 36.48093795776367, 23.12723159790039, 29.474163055419922, 30.667999267578125, 36.48627471923828, 25.162620544433594, 34.2588996887207, 29.469823837280273, 32.85316467285156, 27.500072479248047, 26.000926971435547, 26.249313354492188, 26.351137161254883, 30.79648780822754, 27.477201461791992, 22.131084442138672, 29.372236251831055, 26.186288833618164, 24.2548885345459, 33.63902282714844, 30.864002227783203, 30.726320266723633, 28.192808151245117, 25.143234252929688, 29.728002548217773, 30.808074951171875, 31.84856605529785, 21.9698486328125, 28.4932861328125, 27.635705947875977, 25.032848358154297, 31.33272933959961, 31.67810821533203, 27.80299186706543, 32.12615966796875, 27.156246185302734, 24.935453414916992, 32.50349807739258, 20.82140350341797, 29.607492446899414, 19.65816307067871, 34.60194396972656, 23.598400115966797, 30.104473114013672, 32.20271301269531, 28.04269027709961, 32.8837890625, 24.075733184814453, 36.618282318115234, 27.88844871520996, 19.40201187133789, 20.149272918701172, 31.152801513671875, 17.996654510498047, 32.347408294677734, 26.775047302246094, 27.060745239257812, 32.15647506713867, 17.924930572509766, 23.870607376098633, 29.899383544921875, 22.747669219970703, 26.142024993896484, 32.86265182495117, 33.76420974731445, 27.535335540771484, 28.73103904724121, 29.830677032470703, 28.02216339111328, 35.07984161376953, 24.587276458740234, 33.34171676635742, 26.22913932800293, 29.022048950195312, 28.90944480895996, 21.548107147216797, 26.785234451293945, 31.00246238708496, 24.759862899780273, 30.335433959960938, 31.99097442626953, 19.897794723510742, 28.792240142822266, 23.545602798461914, 28.122299194335938, 34.115089416503906, 31.953079223632812, 31.416032791137695, 33.98398971557617, 30.759653091430664, 27.143266677856445, 24.573957443237305, 32.55591583251953, 19.823532104492188, 28.063343048095703, 32.636993408203125, 36.946632385253906, 28.461668014526367, 26.9481258392334, 29.542951583862305, 24.876298904418945, 28.254732131958008, 25.234542846679688, 23.59026336669922, 30.403961181640625, 25.382633209228516, 28.662822723388672, 24.903675079345703, 23.54930877685547, 23.837642669677734, 28.9876651763916, 29.406044006347656, 21.250263214111328, 29.844921112060547, 34.79127502441406, 27.87018394470215, 27.06889533996582, 36.049278259277344, 22.676116943359375, 30.892621994018555, 28.411197662353516, 24.59521484375, 29.960906982421875, 29.063884735107422, 26.149734497070312, 24.836048126220703, 30.176498413085938, 25.292619705200195, 29.451801300048828, 25.657852172851562, 24.659709930419922, 22.77299690246582, 29.914310455322266, 27.092796325683594, 24.96331024169922, 32.37248992919922, 32.17798614501953, 26.569814682006836, 32.80400466918945, 24.6622314453125, 29.26259994506836, 29.864999771118164, 32.500083923339844, 24.503520965576172, 24.367250442504883, 27.890592575073242, 25.100130081176758], 'y_pred_int': [29, 32, 32, 33, 30, 34, 28, 33, 26, 21, 29, 36, 20, 16, 27, 25, 28, 33, 30, 27, 32, 27, 16, 32, 32, 25, 32, 27, 23, 23, 34, 25, 30, 28, 27, 27, 33, 24, 25, 28, 19, 27, 33, 31, 31, 28, 21, 22, 25, 27, 32, 29, 21, 30, 30, 26, 25, 28, 32, 20, 30, 21, 26, 34, 27, 28, 26, 23, 29, 25, 30, 34, 30, 26, 21, 28, 22, 30, 31, 32, 30, 24, 29, 30, 23, 24, 28, 36, 20, 22, 30, 27, 30, 30, 27, 27, 22, 32, 28, 31, 34, 21, 24, 27, 25, 21, 31, 31, 30, 30, 30, 34, 33, 24, 22, 21, 23, 29, 31, 27, 33, 24, 22, 34, 30, 24, 29, 32, 23, 24, 28, 22, 30, 29, 35, 29, 28, 26, 22, 27, 23, 30, 16, 28, 25, 34, 24, 29, 35, 31, 25, 26, 30, 32, 31, 31, 30, 19, 27, 25, 31, 28, 25, 25, 30, 32, 22, 26, 28, 31, 28, 32, 28, 32, 22, 26, 21, 36, 24, 34, 25, 21, 35, 28, 26, 30, 28, 26, 29, 31, 22, 30, 22, 36, 23, 29, 31, 36, 25, 34, 29, 33, 28, 26, 26, 26, 31, 27, 22, 29, 26, 24, 34, 31, 31, 28, 25, 30, 31, 32, 22, 28, 28, 25, 31, 32, 28, 32, 27, 25, 33, 21, 30, 20, 35, 24, 30, 32, 28, 33, 24, 37, 28, 19, 20, 31, 18, 32, 27, 27, 32, 18, 24, 30, 23, 26, 33, 34, 28, 29, 30, 28, 35, 25, 33, 26, 29, 29, 22, 27, 31, 25, 30, 32, 20, 29, 24, 28, 34, 32, 31, 34, 31, 27, 25, 33, 20, 28, 33, 37, 28, 27, 30, 25, 28, 25, 24, 30, 25, 29, 25, 24, 24, 29, 29, 21, 30, 35, 28, 27, 36, 23, 31, 28, 25, 30, 29, 26, 25, 30, 25, 29, 26, 25, 23, 30, 27, 25, 32, 32, 27, 33, 25, 29, 30, 33, 25, 24, 28, 25], 'y_true': [7, 14, 30, 2, 3, 3, 39, 5, 14, 26, 23, 4, 12, 6, 6, 9, 15, 31, 12, 20, 4, 3, 8, 3, 15, 35, 12, 3, 11, 15, 24, 19, 8, 20, 30, 14, 2, 8, 3, 22, 13, 10, 8, 13, 8, 13, 33, 6, 14, 7, 6, 24, 28, 10, 11, 4, 15, 16, 29, 7, 5, 26, 17, 6, 4, 15, 10, 35, 11, 14, 1, 16, 35, 29, 23, 14, 9, 8, 33, 8, 20, 6, 19, 10, 7, 22, 20, 23, 20, 4, 26, 26, 29, 18, 11, 13, 6, 6, 7, 21, 9, 27, 5, 8, 13, 18, 6, 11, 6, 33, 21, 2, 8, 11, 25, 12, 16, 10, 5, 7, 14, 10, 4, 9, 8, 5, 22, 22, 2, 12, 21, 13, 28, 14, 15, 11, 43, 8, 46, 51, 30, 22, 4, 9, 2, 7, 6, 38, 25, 13, 12, 6, 7, 9, 6, 10, 9, 6, 15, 28, 16, 27, 3, 11, 15, 5, 9, 16, 12, 5, 5, 20, 4, 12, 5, 8, 32, 5, 8, 5, 1, 17, 2, 10, 12, 9, 2, 8, 2, 8, 22, 11, 3, 5, 37, 10, 22, 5, 9, 15, 10, 25, 8, 12, 34, 14, 18, 34, 7, 6, 6, 7, 13, 9, 11, 5, 20, 10, 29, 27, 15, 13, 8, 40, 31, 2, 4, 15, 2, 18, 39, 5, 15, 8, 18, 12, 24, 14, 4, 19, 38, 30, 17, 15, 14, 3, 24, 7, 3, 7, 9, 19, 6, 16, 11, 14, 2, 23, 10, 7, 5, 46, 14, 17, 16, 14, 13, 16, 19, 34, 4, 2, 20, 20, 19, 10, 9, 11, 6, 14, 23, 12, 10, 15, 12, 8, 8, 4, 7, 6, 8, 24, 35, 7, 8, 7, 9, 14, 20, 10, 8, 6, 12, 9, 32, 8, 12, 3, 15, 8, 9, 8, 3, 21, 9, 11, 26, 7, 60, 3, 8, 15, 15, 5, 15, 19, 15, 20, 20, 11, 28, 17, 14, 8, 13, 3, 14, 11, 4, 4], 'train_acc': 0.0, 'test_acc': 0.011764705882352941, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 33797.4375, Test loss: 49356.1602
In this epoch 100/5000, Training loss: 19413.4629, Test loss: 49422.2461
In this epoch 150/5000, Training loss: 10822.4004, Test loss: 49743.1641
In this epoch 200/5000, Training loss: 6080.7002, Test loss: 50029.1406
In this epoch 250/5000, Training loss: 3527.7715, Test loss: 50249.6992
In this epoch 300/5000, Training loss: 2131.9121, Test loss: 50419.9141
In this epoch 350/5000, Training loss: 1344.8663, Test loss: 50556.6016
In this epoch 400/5000, Training loss: 886.3436, Test loss: 50670.8438
In this epoch 450/5000, Training loss: 611.0375, Test loss: 50768.9180
In this epoch 500/5000, Training loss: 441.1880, Test loss: 50854.2188
In this epoch 550/5000, Training loss: 333.7619, Test loss: 50928.5898
In this epoch 600/5000, Training loss: 264.1854, Test loss: 50993.1055
In this epoch 650/5000, Training loss: 218.0260, Test loss: 51048.5000
In this epoch 700/5000, Training loss: 186.5976, Test loss: 51095.4648
In this epoch 750/5000, Training loss: 164.5658, Test loss: 51134.7188
In this epoch 800/5000, Training loss: 148.5986, Test loss: 51167.0000
In this epoch 850/5000, Training loss: 136.5872, Test loss: 51193.1016
In this epoch 900/5000, Training loss: 127.1816, Test loss: 51213.7305
In this epoch 950/5000, Training loss: 119.5104, Test loss: 51229.6133
In this epoch 1000/5000, Training loss: 113.0073, Test loss: 51241.3945
In this epoch 1050/5000, Training loss: 107.3032, Test loss: 51249.6523
In this epoch 1100/5000, Training loss: 102.1562, Test loss: 51254.9062
In this epoch 1150/5000, Training loss: 97.4081, Test loss: 51257.6250
In this epoch 1200/5000, Training loss: 92.9551, Test loss: 51258.2227
In this epoch 1250/5000, Training loss: 88.7292, Test loss: 51257.0938
In this epoch 1300/5000, Training loss: 84.6865, Test loss: 51254.5234
In this epoch 1350/5000, Training loss: 80.7988, Test loss: 51250.8242
In this epoch 1400/5000, Training loss: 77.0483, Test loss: 51246.2305
In this epoch 1450/5000, Training loss: 73.4243, Test loss: 51240.9766
In this epoch 1500/5000, Training loss: 69.9210, Test loss: 51235.2422
In this epoch 1550/5000, Training loss: 66.5352, Test loss: 51229.1992
In this epoch 1600/5000, Training loss: 63.2660, Test loss: 51222.9766
In this epoch 1650/5000, Training loss: 60.1135, Test loss: 51216.6875
In this epoch 1700/5000, Training loss: 57.0784, Test loss: 51210.4531
In this epoch 1750/5000, Training loss: 54.1617, Test loss: 51204.3359
In this epoch 1800/5000, Training loss: 51.3643, Test loss: 51198.4141
In this epoch 1850/5000, Training loss: 48.6871, Test loss: 51192.7305
In this epoch 1900/5000, Training loss: 46.1305, Test loss: 51187.3320
In this epoch 1950/5000, Training loss: 43.6945, Test loss: 51182.2500
In this epoch 2000/5000, Training loss: 41.3791, Test loss: 51177.5078
In this epoch 2050/5000, Training loss: 39.1833, Test loss: 51173.1133
In this epoch 2100/5000, Training loss: 37.1061, Test loss: 51169.0938
In this epoch 2150/5000, Training loss: 35.1458, Test loss: 51165.4375
In this epoch 2200/5000, Training loss: 33.3006, Test loss: 51162.1484
In this epoch 2250/5000, Training loss: 31.5683, Test loss: 51159.2227
In this epoch 2300/5000, Training loss: 29.9460, Test loss: 51156.6602
In this epoch 2350/5000, Training loss: 28.4308, Test loss: 51154.4258
In this epoch 2400/5000, Training loss: 27.0196, Test loss: 51152.5430
In this epoch 2450/5000, Training loss: 25.7087, Test loss: 51150.9531
In this epoch 2500/5000, Training loss: 24.4946, Test loss: 51149.6719
In this epoch 2550/5000, Training loss: 23.3733, Test loss: 51148.6719
In this epoch 2600/5000, Training loss: 22.3407, Test loss: 51147.9414
In this epoch 2650/5000, Training loss: 21.3926, Test loss: 51147.4492
In this epoch 2700/5000, Training loss: 20.5250, Test loss: 51147.1836
In this epoch 2750/5000, Training loss: 19.7333, Test loss: 51147.1289
In this epoch 2800/5000, Training loss: 19.0133, Test loss: 51147.2734
In this epoch 2850/5000, Training loss: 18.3606, Test loss: 51147.5703
In this epoch 2900/5000, Training loss: 17.7710, Test loss: 51148.0234
In this epoch 2950/5000, Training loss: 17.2401, Test loss: 51148.6133
In this epoch 3000/5000, Training loss: 16.7639, Test loss: 51149.3203
In this epoch 3050/5000, Training loss: 16.3382, Test loss: 51150.1250
In this epoch 3100/5000, Training loss: 15.9590, Test loss: 51151.0117
In this epoch 3150/5000, Training loss: 15.6226, Test loss: 51151.9648
In this epoch 3200/5000, Training loss: 15.3254, Test loss: 51152.9766
In this epoch 3250/5000, Training loss: 15.0637, Test loss: 51154.0352
In this epoch 3300/5000, Training loss: 14.8343, Test loss: 51155.1172
In this epoch 3350/5000, Training loss: 14.6340, Test loss: 51156.2109
In this epoch 3400/5000, Training loss: 14.4599, Test loss: 51157.3203
In this epoch 3450/5000, Training loss: 14.3093, Test loss: 51158.4180
In this epoch 3500/5000, Training loss: 14.1795, Test loss: 51159.5000
In this epoch 3550/5000, Training loss: 14.0682, Test loss: 51160.5820
In this epoch 3600/5000, Training loss: 13.9731, Test loss: 51161.6367
In this epoch 3650/5000, Training loss: 13.8925, Test loss: 51162.6523
In this epoch 3700/5000, Training loss: 13.8243, Test loss: 51163.6367
In this epoch 3750/5000, Training loss: 13.7669, Test loss: 51164.5703
In this epoch 3800/5000, Training loss: 13.7189, Test loss: 51165.4727
In this epoch 3850/5000, Training loss: 13.6790, Test loss: 51166.3242
In this epoch 3900/5000, Training loss: 13.6460, Test loss: 51167.1367
In this epoch 3950/5000, Training loss: 13.6189, Test loss: 51167.8945
In this epoch 4000/5000, Training loss: 13.5966, Test loss: 51168.6133
In this epoch 4050/5000, Training loss: 13.5786, Test loss: 51169.2773
In this epoch 4100/5000, Training loss: 13.5640, Test loss: 51169.8945
In this epoch 4150/5000, Training loss: 13.5523, Test loss: 51170.4648
In this epoch 4200/5000, Training loss: 13.5429, Test loss: 51170.9961
In this epoch 4250/5000, Training loss: 13.5355, Test loss: 51171.4727
In this epoch 4300/5000, Training loss: 13.5296, Test loss: 51171.9141
In this epoch 4350/5000, Training loss: 13.5251, Test loss: 51172.3125
In this epoch 4400/5000, Training loss: 13.5215, Test loss: 51172.6836
In this epoch 4450/5000, Training loss: 13.5188, Test loss: 51173.0000
In this epoch 4500/5000, Training loss: 13.5167, Test loss: 51173.2891
In this epoch 4550/5000, Training loss: 13.5151, Test loss: 51173.5586
In this epoch 4600/5000, Training loss: 13.5139, Test loss: 51173.7891
In this epoch 4650/5000, Training loss: 13.5130, Test loss: 51174.0000
In this epoch 4700/5000, Training loss: 13.5124, Test loss: 51174.1758
In this epoch 4750/5000, Training loss: 13.5119, Test loss: 51174.3438
In this epoch 4800/5000, Training loss: 13.5115, Test loss: 51174.4844
In this epoch 4850/5000, Training loss: 13.5113, Test loss: 51174.6133
In this epoch 4900/5000, Training loss: 27.6628, Test loss: 51239.4961
In this epoch 4950/5000, Training loss: 14.1142, Test loss: 51169.7539
In this epoch 5000/5000, Training loss: 13.5109, Test loss: 51174.5000
train_acc 0.9970588235294118
test_acc 0.0029411764705882353
{'y_pred': [447.0689392089844, 351.9829406738281, 298.718994140625, 257.4612731933594, 370.5292053222656, 297.6142883300781, 373.0292663574219, 391.1051940917969, 366.48834228515625, 393.6988220214844, 373.8169250488281, 402.5011901855469, 281.20635986328125, 488.56060791015625, 348.6712341308594, 313.4529724121094, 397.1774597167969, 300.0081787109375, 282.8286437988281, 366.0295104980469, 357.4006042480469, 311.555419921875, 377.6731872558594, 354.3119201660156, 284.2733459472656, 361.27734375, 389.8263854980469, 344.8808288574219, 340.71685791015625, 340.2322692871094, 383.6973876953125, 368.368408203125, 318.1554260253906, 214.1943359375, 321.766357421875, 215.21405029296875, 343.88201904296875, 300.9303894042969, 427.3544006347656, 395.56707763671875, 256.7046203613281, 358.73284912109375, 283.9769592285156, 390.42364501953125, 369.8071594238281, 409.07501220703125, 343.1567687988281, 442.9394226074219, 381.1419982910156, 423.1729431152344, 377.549560546875, 283.6442565917969, 399.3702087402344, 396.6773376464844, 402.7975769042969, 297.2940979003906, 375.48846435546875, 407.3272705078125, 388.6792907714844, 380.8878479003906, 263.8653564453125, 327.2764587402344, 446.1196594238281, 381.3573303222656, 345.4482727050781, 409.50372314453125, 365.2538757324219, 422.3229675292969, 457.0462341308594, 502.0687255859375, 310.2257385253906, 400.9118957519531, 467.0530090332031, 371.0202941894531, 326.8581237792969, 383.8503723144531, 366.96368408203125, 334.7841491699219, 362.0509948730469, 378.108642578125, 390.3864440917969, 400.8603820800781, 348.3404235839844, 383.8820495605469, 539.4384155273438, 340.8441467285156, 354.1435241699219, 378.884033203125, 290.1194152832031, 450.64410400390625, 409.851318359375, 404.15521240234375, 326.4034729003906, 333.6521301269531, 370.12371826171875, 439.0347900390625, 262.650390625, 343.0096130371094, 356.1059265136719, 278.0970458984375, 411.6765441894531, 436.60955810546875, 241.6433563232422, 368.8202819824219, 331.1394348144531, 416.8822326660156, 457.5458679199219, 415.5453186035156, 439.4105224609375, 416.4429016113281, 351.6828918457031, 340.0823059082031, 345.4798583984375, 362.2173767089844, 368.2811584472656, 363.72735595703125, 401.1470031738281, 383.2793884277344, 375.33331298828125, 440.0252380371094, 414.1324157714844, 296.43853759765625, 357.1903381347656, 424.685791015625, 307.31048583984375, 290.8478088378906, 378.7542724609375, 247.1488037109375, 335.4271545410156, 237.5821990966797, 366.2490234375, 423.0201721191406, 397.4941101074219, 454.2973327636719, 404.6551818847656, 380.5128479003906, 365.0582580566406, 425.5921325683594, 374.6593933105469, 391.3421936035156, 261.205078125, 384.6210021972656, 416.4170227050781, 383.1255187988281, 332.7145080566406, 266.3975524902344, 369.9134826660156, 308.5367126464844, 359.4144287109375, 444.677978515625, 332.73553466796875, 333.4105529785156, 300.8374328613281, 430.331787109375, 423.424560546875, 439.9385681152344, 396.66326904296875, 344.5946044921875, 389.83343505859375, 293.8367004394531, 406.3551940917969, 427.0732727050781, 417.75262451171875, 325.9185485839844, 324.1134033203125, 297.4976501464844, 366.7272644042969, 419.2276306152344, 403.9561462402344, 303.2055969238281, 311.8599853515625, 348.5505676269531, 321.9010314941406, 444.63177490234375, 414.8508605957031, 382.8703918457031, 311.94476318359375, 352.123291015625, 374.2685546875, 282.2793273925781, 382.0461120605469, 384.3695373535156, 318.6986083984375, 413.6029968261719, 289.1356201171875, 361.29669189453125, 385.8524169921875, 376.58978271484375, 343.9521484375, 399.1244812011719, 319.8728942871094, 295.2146301269531, 411.849365234375, 312.8409729003906, 328.2711181640625, 393.3763732910156, 334.05157470703125, 424.56182861328125, 335.68646240234375, 324.7190856933594, 361.8683166503906, 347.3571472167969, 338.6053771972656, 375.1513977050781, 373.02984619140625, 330.0352478027344, 477.6776428222656, 213.69735717773438, 399.35162353515625, 364.30645751953125, 294.0265197753906, 374.83355712890625, 442.4859924316406, 387.02642822265625, 250.11654663085938, 298.6825866699219, 493.0273742675781, 454.8883361816406, 367.7576599121094, 385.9208068847656, 376.6162414550781, 448.0409240722656, 404.3625793457031, 413.2629089355469, 407.4994201660156, 379.1277770996094, 310.166748046875, 420.8108215332031, 309.3055725097656, 399.4917907714844, 422.1431579589844, 328.7047119140625, 381.4920959472656, 293.5320129394531, 296.2140808105469, 346.777099609375, 323.7392578125, 381.2246398925781, 432.33868408203125, 360.64190673828125, 299.1678771972656, 394.9515075683594, 319.3543395996094, 334.6231994628906, 363.4664611816406, 375.3384094238281, 297.6746520996094, 343.2242431640625, 363.8807067871094, 397.0963134765625, 374.3764953613281, 309.972412109375, 472.20098876953125, 343.1502685546875, 400.0639343261719, 186.60540771484375, 369.1607971191406, 342.32989501953125, 435.8631896972656, 465.2875671386719, 205.6175994873047, 368.4449462890625, 372.7108154296875, 373.4931640625, 337.87371826171875, 305.187255859375, 357.193115234375, 301.2467346191406, 373.9654541015625, 267.0874328613281, 335.0130310058594, 487.962158203125, 393.5732116699219, 287.150634765625, 405.7245788574219, 452.4360656738281, 435.52667236328125, 372.8617858886719, 389.83575439453125, 351.2366638183594, 350.23114013671875, 356.3831481933594, 364.9754943847656, 449.05548095703125, 378.0918884277344, 556.8619995117188, 477.5581970214844, 365.5863342285156, 322.84820556640625, 311.57171630859375, 361.9737243652344, 345.0348205566406, 383.5126647949219, 342.3174743652344, 351.0081481933594, 440.31597900390625, 431.0908508300781, 413.4824523925781, 355.9472351074219, 412.08697509765625, 236.79486083984375, 391.4413757324219, 445.07196044921875, 303.81494140625, 414.26788330078125, 332.5680236816406, 247.46279907226562, 325.6524658203125, 394.7950134277344, 357.5265808105469, 320.9852600097656, 359.7373962402344, 307.0106506347656, 335.81488037109375, 280.1901550292969, 423.1911926269531, 362.3209228515625, 410.3971862792969, 454.6766662597656, 360.5978698730469, 443.02215576171875, 509.0361633300781, 330.3287658691406, 464.28155517578125, 319.8096008300781, 361.0464172363281, 445.5494689941406, 418.5726013183594, 406.6354675292969, 388.5176696777344, 491.8155822753906, 454.7955017089844, 348.3195495605469, 347.1065673828125, 358.8488464355469, 382.9029235839844, 380.6029357910156, 357.8431396484375, 386.45538330078125, 359.6555480957031], 'y_pred_int': [447, 352, 299, 257, 371, 298, 373, 391, 366, 394, 374, 403, 281, 489, 349, 313, 397, 300, 283, 366, 357, 312, 378, 354, 284, 361, 390, 345, 341, 340, 384, 368, 318, 214, 322, 215, 344, 301, 427, 396, 257, 359, 284, 390, 370, 409, 343, 443, 381, 423, 378, 284, 399, 397, 403, 297, 375, 407, 389, 381, 264, 327, 446, 381, 345, 410, 365, 422, 457, 502, 310, 401, 467, 371, 327, 384, 367, 335, 362, 378, 390, 401, 348, 384, 539, 341, 354, 379, 290, 451, 410, 404, 326, 334, 370, 439, 263, 343, 356, 278, 412, 437, 242, 369, 331, 417, 458, 416, 439, 416, 352, 340, 345, 362, 368, 364, 401, 383, 375, 440, 414, 296, 357, 425, 307, 291, 379, 247, 335, 238, 366, 423, 397, 454, 405, 381, 365, 426, 375, 391, 261, 385, 416, 383, 333, 266, 370, 309, 359, 445, 333, 333, 301, 430, 423, 440, 397, 345, 390, 294, 406, 427, 418, 326, 324, 297, 367, 419, 404, 303, 312, 349, 322, 445, 415, 383, 312, 352, 374, 282, 382, 384, 319, 414, 289, 361, 386, 377, 344, 399, 320, 295, 412, 313, 328, 393, 334, 425, 336, 325, 362, 347, 339, 375, 373, 330, 478, 214, 399, 364, 294, 375, 442, 387, 250, 299, 493, 455, 368, 386, 377, 448, 404, 413, 407, 379, 310, 421, 309, 399, 422, 329, 381, 294, 296, 347, 324, 381, 432, 361, 299, 395, 319, 335, 363, 375, 298, 343, 364, 397, 374, 310, 472, 343, 400, 187, 369, 342, 436, 465, 206, 368, 373, 373, 338, 305, 357, 301, 374, 267, 335, 488, 394, 287, 406, 452, 436, 373, 390, 351, 350, 356, 365, 449, 378, 557, 478, 366, 323, 312, 362, 345, 384, 342, 351, 440, 431, 413, 356, 412, 237, 391, 445, 304, 414, 333, 247, 326, 395, 358, 321, 360, 307, 336, 280, 423, 362, 410, 455, 361, 443, 509, 330, 464, 320, 361, 446, 419, 407, 389, 492, 455, 348, 347, 359, 383, 381, 358, 386, 360], 'y_true': [168, 456, 163, 656, 706, 407, 522, 209, 531, 261, 187, 612, 649, 89, 621, 162, 224, 340, 155, 443, 828, 281, 738, 350, 463, 409, 171, 456, 250, 676, 252, 164, 447, 392, 119, 643, 239, 247, 436, 90, 39, 150, 220, 216, 243, 550, 585, 286, 316, 414, 212, 513, 384, 259, 607, 94, 426, 241, 115, 765, 278, 387, 765, 1042, 262, 45, 167, 130, 187, 221, 528, 173, 466, 542, 81, 1062, 202, 59, 569, 391, 502, 556, 230, 212, 1006, 500, 221, 412, 32, 196, 200, 367, 367, 801, 373, 709, 233, 459, 523, 421, 385, 112, 840, 326, 878, 350, 162, 338, 132, 513, 248, 529, 260, 265, 624, 169, 611, 380, 80, 1695, 131, 354, 154, 176, 305, 585, 154, 831, 449, 200, 82, 260, 73, 197, 370, 516, 194, 580, 409, 207, 508, 172, 504, 743, 163, 761, 186, 741, 534, 494, 223, 224, 711, 233, 187, 577, 334, 187, 156, 577, 66, 170, 326, 189, 106, 768, 126, 447, 488, 78, 149, 200, 335, 193, 161, 877, 279, 338, 714, 217, 423, 574, 460, 351, 319, 203, 439, 237, 561, 169, 134, 135, 433, 168, 337, 344, 572, 200, 258, 431, 407, 392, 96, 136, 327, 201, 521, 194, 131, 415, 78, 292, 678, 60, 498, 214, 446, 321, 168, 385, 312, 376, 203, 686, 687, 202, 690, 264, 173, 332, 119, 361, 714, 156, 962, 391, 452, 361, 266, 645, 193, 413, 97, 191, 68, 215, 551, 603, 69, 143, 607, 339, 408, 458, 268, 540, 239, 431, 209, 428, 328, 355, 901, 491, 280, 417, 95, 351, 80, 154, 159, 563, 481, 328, 550, 370, 904, 312, 476, 455, 310, 385, 226, 586, 129, 342, 463, 57, 76, 260, 257, 545, 583, 552, 496, 510, 382, 161, 324, 302, 242, 197, 672, 211, 519, 367, 324, 337, 378, 394, 610, 382, 75, 212, 169, 302, 596, 203, 187, 363, 445, 254, 220, 205, 141, 102, 715, 521, 502, 253, 317, 118, 191, 898, 172, 200, 134, 269, 471, 510], 'train_acc': 0.9970588235294118, 'test_acc': 0.0029411764705882353, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 51.9964, Test loss: 20.3175
In this epoch 100/5000, Training loss: 0.7973, Test loss: 1.6181
In this epoch 150/5000, Training loss: 0.2287, Test loss: 1.4908
In this epoch 200/5000, Training loss: 0.1154, Test loss: 1.5271
In this epoch 250/5000, Training loss: 0.0581, Test loss: 1.5559
In this epoch 300/5000, Training loss: 0.0294, Test loss: 1.5772
In this epoch 350/5000, Training loss: 0.0149, Test loss: 1.5925
In this epoch 400/5000, Training loss: 0.0077, Test loss: 1.6032
In this epoch 450/5000, Training loss: 0.0040, Test loss: 1.6107
In this epoch 500/5000, Training loss: 0.0021, Test loss: 1.6158
In this epoch 550/5000, Training loss: 0.0011, Test loss: 1.6194
In this epoch 600/5000, Training loss: 0.0006, Test loss: 1.6220
In this epoch 650/5000, Training loss: 0.0003, Test loss: 1.6237
In this epoch 700/5000, Training loss: 0.0002, Test loss: 1.6250
In this epoch 750/5000, Training loss: 0.0001, Test loss: 1.6260
In this epoch 800/5000, Training loss: 0.0000, Test loss: 1.6267
In this epoch 850/5000, Training loss: 0.0000, Test loss: 1.6272
In this epoch 900/5000, Training loss: 0.0000, Test loss: 1.6275
In this epoch 950/5000, Training loss: 0.0000, Test loss: 1.6278
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 1.6280
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 1.6281
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 1.6282
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 1.6283
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 1.6283
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 1.6284
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 1.6284
train_acc 0.3483780276816609
test_acc 0.40515570934256057
{'y_pred': [0.486458957195282, 0.774844765663147, 0.6089076995849609, 1.6154338121414185, 1.3984040021896362, 1.6339749097824097, -0.021793240681290627, 1.2407337427139282, 0.7089146375656128, 2.037058115005493, 1.0806269645690918, 1.388669490814209, 0.7173143029212952, 0.8572976589202881, 0.6107595562934875, 0.6952797174453735, 0.51434326171875, 0.7678139209747314, 1.2892991304397583, 0.2348252683877945, 0.9168964624404907, 0.8738613128662109, 0.9491618871688843, 0.9022737741470337, -0.02534562163054943, 0.8769268989562988, 1.0854922533035278, 0.7907402515411377, 1.1644740104675293, 1.8906047344207764, 1.688544511795044, 0.18129093945026398, 0.5457396507263184, 0.9486281275749207, 1.084879755973816, 1.0165579319000244, 0.08888467401266098, 1.068618893623352, 1.4568994045257568, 1.173779845237732, 1.6016207933425903, 1.406049132347107, 0.0728728324174881, 1.0161904096603394, 1.6552098989486694, 0.35526224970817566, 1.1095410585403442, 1.2410811185836792, -0.13583342730998993, 0.747286319732666, 1.8242197036743164, 0.9488307237625122, 0.8175528645515442, 0.9395157098770142, 0.9284306168556213, 0.5498968362808228, 1.7885000705718994, 0.9598572254180908, 1.1119154691696167, 1.3587523698806763, 1.0395411252975464, 0.9885779023170471, 1.3341565132141113, 0.35076475143432617, 1.0243600606918335, 0.8293415307998657, -0.054958369582891464, 0.40069013833999634, 1.6087568998336792, 0.41264262795448303, 0.7014228701591492, 0.09532508999109268, 0.6545765399932861, 1.2828181982040405, 0.3930988311767578, 0.9850488901138306, 0.9369632005691528, 0.6078576445579529, 1.0778933763504028, 1.232185959815979, 0.8651643395423889, 0.09260203689336777, 1.3261537551879883, 1.49729323387146, 1.0091978311538696, 0.33663561940193176, 1.5102647542953491, 1.4693228006362915, 0.24010437726974487, 1.2729848623275757, 1.060986876487732, 1.8239995241165161, 1.4566282033920288, 0.7701486349105835, 0.15067431330680847, 0.14397494494915009, 1.3247255086898804, 1.314038634300232, 0.12333018332719803, 0.9465107917785645, 1.1208701133728027, 0.8241814374923706, 0.7685049176216125, -0.3098921775817871, -0.9783837795257568, 0.8656736612319946, 1.3868098258972168, 1.6329728364944458, 1.636137843132019, 0.43318408727645874, 0.7652715444564819, 1.2899163961410522, 0.5010737180709839, 1.598333477973938, 2.085362672805786, 0.6858795285224915, 1.1484479904174805, 1.400324821472168, 0.983529806137085, 1.153286337852478, 1.480912685394287, 0.08416757732629776, 1.2584248781204224, 0.8380283713340759, 1.1565431356430054, 1.153079867362976, 0.1456727832555771, 1.3594287633895874, 1.4069944620132446, 1.9215506315231323, 1.506543517112732, 0.899204432964325, 1.1761122941970825, 1.6488838195800781, 0.5485501885414124, 1.0489414930343628, 0.7709935903549194, 1.5096144676208496, 1.3552805185317993, 0.7847793102264404, 0.14221470057964325, 0.7782007455825806, 0.5140630602836609, 0.9716719388961792, 1.2661408185958862, 0.3917731046676636, 0.3297799229621887, 0.9512518644332886, 0.9316614866256714, -0.06734009832143784, 1.260701298713684, 0.49369174242019653, -0.31615352630615234, 1.1721476316452026, 0.1615816205739975, 1.565233588218689, 1.3268197774887085, 1.8301056623458862, 1.3887850046157837, 0.5163958072662354, 0.7481561303138733, 1.2050647735595703, 1.258567214012146, 1.168552279472351, 1.3647677898406982, 1.0881632566452026, -0.0703730508685112, 1.3592852354049683, 1.1420729160308838, 0.5330485105514526, 0.42379117012023926, 1.4655457735061646, 0.7303558588027954, 1.4299579858779907, 1.8642463684082031, 0.7230800986289978, 0.5425350666046143, 1.0404865741729736, 1.874679446220398, 0.6460886597633362, 0.848154604434967, 0.5788753032684326, 0.558593213558197, 1.3314234018325806, 0.6255728006362915, -0.31382182240486145, 0.45083844661712646, 0.5232225656509399, 1.5262080430984497, 1.0063270330429077, 0.9782229661941528, 0.46329206228256226, 1.615754246711731, 0.3149428367614746, 0.9871429204940796, 1.9971476793289185, 1.6089969873428345, 0.543424129486084, 0.06475739181041718, 0.9900535941123962, 0.33891138434410095, 0.37412315607070923, 0.887006402015686, -0.806103527545929, -0.029145577922463417, -0.028886327520012856, 1.2843828201293945, 0.4366477429866791, 1.0631402730941772, -0.3035210967063904, 1.5885155200958252, 0.5344131588935852, 1.910650372505188, 1.456718921661377, 1.3117698431015015, 1.1640005111694336, 0.6290554404258728, 0.422412246465683, 0.11387500911951065, 1.4627923965454102, 1.1888338327407837, 0.10469505935907364, 1.2861294746398926, 0.2911880612373352, 0.44390302896499634, 1.32427978515625, 0.6959181427955627, 1.0656780004501343, -0.0481368787586689, 1.1792914867401123, 0.2997341752052307, 0.5411103963851929, 0.31961220502853394, 0.07876677066087723, 0.6436635851860046, 0.3092426061630249, 0.8009918928146362, 0.19268126785755157, 0.8201998472213745, 1.3338475227355957, -0.20695826411247253, 1.330918788909912, 0.9761569499969482, 0.5474586486816406, 0.8473351001739502, 0.49944525957107544, 1.8935061693191528, 0.7046405076980591, 0.8270589709281921, 0.9999996423721313, 1.997341513633728, -0.16663359105587006, 0.4641962945461273, 1.183496117591858, 0.8500770926475525, 0.4689527153968811, 2.576890230178833, 0.9165339469909668, 0.4574997127056122, -0.44082003831863403, 0.6314585208892822, 0.5198286771774292, 0.5883209705352783, 1.6393251419067383, 0.6552037000656128, 1.129841923713684, 1.8975526094436646, 0.8224018812179565, 0.11997868865728378, 1.2835946083068848, 0.851282000541687, 1.012963891029358, 1.4548166990280151, 0.9261741042137146, 1.351051926612854, 1.0826753377914429, 0.38129615783691406, 0.7621409893035889, 1.132277011871338, 0.5412052869796753, 1.1967172622680664, 1.3977526426315308, 0.7140713334083557, 1.3641321659088135, 0.47931790351867676, 1.1024600267410278, 1.034774661064148, 0.6238740086555481, 0.9946074485778809, 0.8766411542892456, 1.1295664310455322, -0.04841902479529381, 0.5148605108261108, 1.3516370058059692, 0.028960874304175377, 1.1919442415237427, 0.9946326017379761, 0.30021190643310547, 1.5807160139083862, 0.7800639867782593, 1.259940266609192, 0.9019574522972107, -0.1528063267469406, 0.17285433411598206, 0.23711062967777252, 1.2305309772491455, 1.1360604763031006, 0.9218398332595825, 0.4234398603439331, 0.2957920730113983, 1.443246603012085, 0.7936716675758362, 0.008350466378033161, 0.9021607637405396, 0.5935923457145691, 1.398036241531372, 1.5769360065460205, 1.0410045385360718, 0.499376118183136, 0.9619612097740173, 1.2175238132476807, 0.6041358709335327, 0.41573065519332886, 0.6780780553817749, 0.7078176140785217, 0.5710680484771729, 1.5128639936447144, 1.9233607053756714, 0.7046018838882446, 1.4191944599151611, 0.5891969203948975, 0.2135007530450821, 0.37728995084762573, 1.498091697692871, -0.22759290039539337, 1.0900272130966187, 0.9488680958747864, 0.9369727373123169, 0.6379282474517822, 0.22494186460971832], 'y_pred_int': [0, 1, 1, 2, 1, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 2, 0, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 0, 1, 1, 2, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 2, 2, 0, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 2, 1, 1, 0, 2, 0, 1, 2, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 2, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 1, 1, 2, 0, 0, 1, 1, 0, 3, 1, 0, 0, 1, 1, 1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0], 'y_true': [2, 0, 1, 4, 0, 2, 2, 0, 0, 3, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 3, 2, 0, 1, 1, 1, 2, 0, 3, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 0, 2, 2, 1, 1, 0, 1, 2, 3, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 3, 1, 1, 3, 0, 3, 1, 0, 0, 5, 0, 0, 0, 0, 0, 3, 1, 1, 0, 0, 0, 9, 1, 0, 1, 0, 0, 1, 2, 3, 3, 2, 1, 0, 0, 0, 1, 0, 2, 1, 1, 0, 2, 0, 0, 2, 0, 0, 1, 3, 2, 1, 1, 0, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 0, 2, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 3, 2, 7, 1, 1, 0, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 1, 3, 2, 0, 2, 0, 0, 0, 3, 1, 2, 1, 1, 2, 4, 1, 1, 0, 0, 1, 1, 2, 0, 4, 1, 4, 1, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 0, 8, 1, 0, 2, 0, 3, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 2, 0, 1, 3, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 2, 1, 0, 1, 2, 0, 1, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 1], 'train_acc': 0.3483780276816609, 'test_acc': 0.40515570934256057, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 86.8115, Test loss: 136.6333
In this epoch 100/5000, Training loss: 32.0782, Test loss: 105.8745
In this epoch 150/5000, Training loss: 15.2151, Test loss: 109.9574
In this epoch 200/5000, Training loss: 7.1743, Test loss: 112.9714
In this epoch 250/5000, Training loss: 3.4569, Test loss: 115.0677
In this epoch 300/5000, Training loss: 1.7556, Test loss: 116.4796
In this epoch 350/5000, Training loss: 0.9798, Test loss: 117.4078
In this epoch 400/5000, Training loss: 0.6254, Test loss: 118.0021
In this epoch 450/5000, Training loss: 0.4614, Test loss: 118.3700
In this epoch 500/5000, Training loss: 0.3824, Test loss: 118.5870
In this epoch 550/5000, Training loss: 0.3406, Test loss: 118.7059
In this epoch 600/5000, Training loss: 0.3151, Test loss: 118.7626
In this epoch 650/5000, Training loss: 0.2965, Test loss: 118.7810
In this epoch 700/5000, Training loss: 0.2809, Test loss: 118.7767
In this epoch 750/5000, Training loss: 0.2667, Test loss: 118.7599
In this epoch 800/5000, Training loss: 0.2531, Test loss: 118.7365
In this epoch 850/5000, Training loss: 0.2400, Test loss: 118.7105
In this epoch 900/5000, Training loss: 0.2273, Test loss: 118.6843
In this epoch 950/5000, Training loss: 0.2150, Test loss: 118.6591
In this epoch 1000/5000, Training loss: 0.2031, Test loss: 118.6359
In this epoch 1050/5000, Training loss: 0.1917, Test loss: 118.6153
In this epoch 1100/5000, Training loss: 0.1807, Test loss: 118.5974
In this epoch 1150/5000, Training loss: 0.1702, Test loss: 118.5825
In this epoch 1200/5000, Training loss: 0.1601, Test loss: 118.5705
In this epoch 1250/5000, Training loss: 0.1505, Test loss: 118.5616
In this epoch 1300/5000, Training loss: 0.1415, Test loss: 118.5557
In this epoch 1350/5000, Training loss: 0.1329, Test loss: 118.5526
In this epoch 1400/5000, Training loss: 0.1248, Test loss: 118.5523
In this epoch 1450/5000, Training loss: 0.1171, Test loss: 118.5546
In this epoch 1500/5000, Training loss: 0.1100, Test loss: 118.5594
In this epoch 1550/5000, Training loss: 0.1032, Test loss: 118.5666
In this epoch 1600/5000, Training loss: 0.0969, Test loss: 118.5759
In this epoch 1650/5000, Training loss: 0.0910, Test loss: 118.5872
In this epoch 1700/5000, Training loss: 0.0855, Test loss: 118.6004
In this epoch 1750/5000, Training loss: 1.3119, Test loss: 119.9808
In this epoch 1800/5000, Training loss: 0.0761, Test loss: 120.2215
In this epoch 1850/5000, Training loss: 0.0723, Test loss: 118.5732
In this epoch 1900/5000, Training loss: 0.0688, Test loss: 118.6573
In this epoch 1950/5000, Training loss: 0.0653, Test loss: 118.6752
In this epoch 2000/5000, Training loss: 0.0621, Test loss: 118.6928
In this epoch 2050/5000, Training loss: 0.0590, Test loss: 118.7105
In this epoch 2100/5000, Training loss: 0.0561, Test loss: 118.7288
In this epoch 2150/5000, Training loss: 8.8500, Test loss: 134.1892
In this epoch 2200/5000, Training loss: 1.1754, Test loss: 124.8051
In this epoch 2250/5000, Training loss: 0.0669, Test loss: 118.7990
In this epoch 2300/5000, Training loss: 0.0471, Test loss: 118.8055
In this epoch 2350/5000, Training loss: 0.0451, Test loss: 118.8143
In this epoch 2400/5000, Training loss: 0.0432, Test loss: 118.8305
In this epoch 2450/5000, Training loss: 0.0414, Test loss: 118.8477
In this epoch 2500/5000, Training loss: 0.0397, Test loss: 118.8652
In this epoch 2550/5000, Training loss: 0.0380, Test loss: 118.8829
In this epoch 2600/5000, Training loss: 66.2939, Test loss: 121.3546
In this epoch 2650/5000, Training loss: 0.1234, Test loss: 119.4292
In this epoch 2700/5000, Training loss: 0.0373, Test loss: 118.8802
In this epoch 2750/5000, Training loss: 0.0326, Test loss: 118.9417
In this epoch 2800/5000, Training loss: 0.0314, Test loss: 118.9614
In this epoch 2850/5000, Training loss: 0.0302, Test loss: 118.9766
In this epoch 2900/5000, Training loss: 0.0290, Test loss: 118.9922
In this epoch 2950/5000, Training loss: 0.0279, Test loss: 119.0079
In this epoch 3000/5000, Training loss: 100.6511, Test loss: 324.8563
In this epoch 3050/5000, Training loss: 2.7532, Test loss: 118.9172
In this epoch 3100/5000, Training loss: 0.0350, Test loss: 119.0447
In this epoch 3150/5000, Training loss: 0.0241, Test loss: 119.0652
In this epoch 3200/5000, Training loss: 0.0232, Test loss: 119.0781
In this epoch 3250/5000, Training loss: 0.0223, Test loss: 119.0913
In this epoch 3300/5000, Training loss: 0.0215, Test loss: 119.1050
In this epoch 3350/5000, Training loss: 0.0207, Test loss: 119.1189
In this epoch 3400/5000, Training loss: 0.0199, Test loss: 119.1334
In this epoch 3450/5000, Training loss: 26.5073, Test loss: 123.3726
In this epoch 3500/5000, Training loss: 0.3030, Test loss: 119.1590
In this epoch 3550/5000, Training loss: 0.0188, Test loss: 119.1756
In this epoch 3600/5000, Training loss: 0.0172, Test loss: 119.1817
In this epoch 3650/5000, Training loss: 0.0166, Test loss: 119.1956
In this epoch 3700/5000, Training loss: 0.0160, Test loss: 119.2078
In this epoch 3750/5000, Training loss: 0.0153, Test loss: 119.2204
In this epoch 3800/5000, Training loss: 0.0147, Test loss: 119.2334
In this epoch 3850/5000, Training loss: 35.9083, Test loss: 129.1335
In this epoch 3900/5000, Training loss: 0.9619, Test loss: 119.2927
In this epoch 3950/5000, Training loss: 0.0173, Test loss: 119.3247
In this epoch 4000/5000, Training loss: 0.0127, Test loss: 119.2812
In this epoch 4050/5000, Training loss: 0.0123, Test loss: 119.2910
In this epoch 4100/5000, Training loss: 0.0118, Test loss: 119.3023
In this epoch 4150/5000, Training loss: 0.0113, Test loss: 119.3139
In this epoch 4200/5000, Training loss: 0.0109, Test loss: 119.3281
In this epoch 4250/5000, Training loss: 20.7621, Test loss: 120.5648
In this epoch 4300/5000, Training loss: 0.0435, Test loss: 119.2356
In this epoch 4350/5000, Training loss: 0.0115, Test loss: 119.3290
In this epoch 4400/5000, Training loss: 0.0094, Test loss: 119.3708
In this epoch 4450/5000, Training loss: 0.0090, Test loss: 119.3790
In this epoch 4500/5000, Training loss: 0.0087, Test loss: 119.3898
In this epoch 4550/5000, Training loss: 0.0083, Test loss: 119.4007
In this epoch 4600/5000, Training loss: 2.5275, Test loss: 122.6973
In this epoch 4650/5000, Training loss: 1.7352, Test loss: 125.7105
In this epoch 4700/5000, Training loss: 0.0083, Test loss: 119.3588
In this epoch 4750/5000, Training loss: 0.0072, Test loss: 119.4438
In this epoch 4800/5000, Training loss: 0.0069, Test loss: 119.4506
In this epoch 4850/5000, Training loss: 0.0066, Test loss: 119.4609
In this epoch 4900/5000, Training loss: 0.0063, Test loss: 119.4710
In this epoch 4950/5000, Training loss: 0.0061, Test loss: 119.4822
In this epoch 5000/5000, Training loss: 1.0755, Test loss: 160.1810
train_acc 0.006617647058823529
test_acc 0.020588235294117647
{'y_pred': [19.660005569458008, 22.58299446105957, 22.026161193847656, 23.16509246826172, 27.731718063354492, 26.028398513793945, 22.627403259277344, 19.01527976989746, 23.92351531982422, 16.927452087402344, 14.68880558013916, 27.3725528717041, 16.423185348510742, 26.301162719726562, 22.48768424987793, 29.94354820251465, 21.713857650756836, 17.771883010864258, 22.081668853759766, 8.033827781677246, 25.081174850463867, 11.18193531036377, 23.36514663696289, 22.564289093017578, 21.097726821899414, 20.01130485534668, 21.541913986206055, 16.736467361450195, 19.152143478393555, 23.432437896728516, 14.764351844787598, 18.23554039001465, 23.93513298034668, 16.96599578857422, 13.492541313171387, 12.91141414642334, 12.36555004119873, 24.087614059448242, 15.367108345031738, 11.891441345214844, 14.531085014343262, 22.239566802978516, 37.48198318481445, 24.940031051635742, 16.46661376953125, 18.18592643737793, 24.685422897338867, 19.366031646728516, 17.97932243347168, 22.279006958007812, 17.06320571899414, 24.691362380981445, 15.115706443786621, 24.670209884643555, 18.370559692382812, 23.5695858001709, 13.721637725830078, 15.21798038482666, 19.032880783081055, 17.188718795776367, 13.886120796203613, 22.48197364807129, 20.129182815551758, 23.852741241455078, 19.6630859375, 25.856653213500977, 15.97287368774414, 14.305468559265137, 19.227420806884766, 23.15325355529785, 20.04937171936035, 27.841691970825195, 16.92808723449707, 20.495393753051758, 20.96342658996582, 20.54493522644043, 22.65662956237793, 28.144075393676758, 27.845502853393555, 19.644311904907227, 12.092613220214844, 19.314149856567383, 12.657971382141113, 17.043052673339844, 19.88848304748535, 21.637767791748047, 21.84294319152832, 27.56194496154785, 13.053765296936035, 16.683929443359375, 22.993484497070312, 20.333322525024414, 20.651329040527344, 19.52765464782715, 17.42351531982422, 22.433563232421875, 19.748001098632812, 13.430434226989746, 23.54800796508789, 19.643999099731445, 17.190074920654297, 20.01082420349121, 8.344523429870605, 21.877243041992188, 13.821669578552246, 21.617795944213867, 24.499197006225586, 25.17180061340332, 19.30427360534668, 23.636186599731445, 24.51148223876953, 24.67713165283203, 16.749269485473633, 13.545523643493652, 15.409438133239746, 17.382261276245117, 19.75217628479004, 21.699094772338867, 23.34462547302246, 20.84870719909668, 15.271154403686523, 20.913429260253906, 18.538711547851562, 24.256685256958008, 20.461992263793945, 26.636363983154297, 24.15446662902832, 16.21670150756836, 18.00517463684082, 18.11992073059082, 15.429582595825195, 24.6625919342041, 20.782529830932617, 27.621652603149414, 19.860586166381836, 15.47910213470459, 22.874937057495117, 16.762426376342773, 14.143970489501953, 24.568363189697266, 35.02755355834961, 38.892757415771484, 23.08580780029297, 21.78461265563965, 24.70195960998535, 19.061695098876953, 12.521336555480957, 21.33058738708496, 22.11866569519043, 17.15717315673828, 19.837902069091797, 26.790523529052734, 24.06351661682129, 20.933801651000977, 20.554288864135742, 23.133289337158203, 17.53870964050293, 22.70915412902832, 18.28582763671875, 28.445560455322266, 25.409799575805664, 25.796066284179688, 18.486961364746094, 23.057226181030273, 17.58746910095215, 20.125436782836914, 14.628766059875488, 21.003910064697266, 16.33707046508789, 23.98030662536621, 17.79435157775879, 22.269384384155273, 14.375062942504883, 19.861459732055664, 17.343839645385742, 16.53250503540039, 11.242424964904785, 36.73322296142578, 17.853973388671875, 24.923721313476562, 24.445703506469727, 18.171457290649414, 22.18196678161621, 23.658720016479492, 26.79326057434082, 19.453187942504883, 17.089981079101562, 20.28778648376465, 18.692283630371094, 27.86642837524414, 24.292593002319336, 24.111143112182617, 25.462331771850586, 23.791181564331055, 4.747682571411133, 15.288359642028809, 20.086973190307617, 10.177555084228516, 21.271583557128906, 23.477754592895508, 27.691015243530273, 17.907623291015625, 24.022920608520508, 26.628578186035156, 11.48816204071045, 23.55264663696289, 20.675737380981445, 24.35647201538086, 20.94162368774414, 22.20139503479004, 17.73980140686035, 19.703874588012695, 14.145177841186523, 13.885720252990723, 16.916223526000977, 16.528615951538086, 18.14185905456543, 18.68347930908203, 21.02324867248535, 20.53806495666504, 16.674453735351562, 12.720413208007812, 18.976945877075195, 15.096034049987793, 20.517210006713867, 28.68001365661621, 21.859289169311523, 22.335573196411133, 23.755910873413086, 23.085172653198242, 24.370208740234375, 20.580827713012695, 16.4246768951416, 21.541648864746094, 12.98971939086914, 21.82223892211914, 17.937946319580078, 18.861661911010742, 31.02013397216797, 20.646394729614258, 9.61185359954834, 18.879491806030273, 26.43696403503418, 15.900457382202148, 19.20448875427246, 30.956663131713867, 9.196619987487793, 17.40781593322754, 29.095216751098633, 18.88155174255371, 24.295499801635742, 21.0535945892334, 19.714496612548828, 14.835799217224121, 28.71790885925293, 23.69013023376465, 28.259132385253906, 21.155113220214844, 22.3378963470459, 29.105384826660156, 17.0944881439209, 17.803499221801758, 18.584680557250977, 23.094682693481445, 20.78696060180664, 27.930078506469727, 11.904932975769043, 17.33914566040039, 20.8102970123291, 23.321855545043945, 12.50011920928955, 17.32046890258789, 12.64051342010498, 21.74863624572754, 23.27890396118164, 24.196022033691406, 19.27090835571289, 23.366853713989258, 18.473093032836914, 23.24279022216797, 17.70317840576172, 33.432804107666016, 21.015727996826172, 28.864473342895508, 21.265579223632812, 15.32303524017334, 18.26129150390625, 9.583942413330078, 25.29207420349121, 17.290002822875977, 23.281984329223633, 18.54056739807129, 25.842710494995117, 21.279016494750977, 27.16652488708496, 19.711435317993164, 24.46558952331543, 17.098785400390625, 18.440793991088867, 22.543413162231445, 22.182327270507812, 25.249996185302734, 32.00752639770508, 23.025657653808594, 26.612001419067383, 10.57340145111084, 25.454710006713867, 20.279067993164062, 23.005550384521484, 21.786684036254883, 11.235062599182129, 21.139556884765625, 18.007081985473633, 24.601552963256836, 8.332598686218262, 10.723424911499023, 24.684982299804688, 25.234037399291992, 15.001482009887695, 23.69283103942871, 21.989707946777344, 18.079137802124023, 24.819622039794922, 21.0513916015625, 19.29808235168457, 24.691200256347656, 26.7386531829834, 26.29656219482422, 13.731330871582031, 21.13035011291504, 26.267127990722656, 29.365598678588867, 21.817031860351562, 12.553288459777832, 15.92137336730957, 26.067113876342773, 29.47340202331543, 17.470338821411133, 20.59578514099121, 16.708337783813477], 'y_pred_int': [20, 23, 22, 23, 28, 26, 23, 19, 24, 17, 15, 27, 16, 26, 22, 30, 22, 18, 22, 8, 25, 11, 23, 23, 21, 20, 22, 17, 19, 23, 15, 18, 24, 17, 13, 13, 12, 24, 15, 12, 15, 22, 37, 25, 16, 18, 25, 19, 18, 22, 17, 25, 15, 25, 18, 24, 14, 15, 19, 17, 14, 22, 20, 24, 20, 26, 16, 14, 19, 23, 20, 28, 17, 20, 21, 21, 23, 28, 28, 20, 12, 19, 13, 17, 20, 22, 22, 28, 13, 17, 23, 20, 21, 20, 17, 22, 20, 13, 24, 20, 17, 20, 8, 22, 14, 22, 24, 25, 19, 24, 25, 25, 17, 14, 15, 17, 20, 22, 23, 21, 15, 21, 19, 24, 20, 27, 24, 16, 18, 18, 15, 25, 21, 28, 20, 15, 23, 17, 14, 25, 35, 39, 23, 22, 25, 19, 13, 21, 22, 17, 20, 27, 24, 21, 21, 23, 18, 23, 18, 28, 25, 26, 18, 23, 18, 20, 15, 21, 16, 24, 18, 22, 14, 20, 17, 17, 11, 37, 18, 25, 24, 18, 22, 24, 27, 19, 17, 20, 19, 28, 24, 24, 25, 24, 5, 15, 20, 10, 21, 23, 28, 18, 24, 27, 11, 24, 21, 24, 21, 22, 18, 20, 14, 14, 17, 17, 18, 19, 21, 21, 17, 13, 19, 15, 21, 29, 22, 22, 24, 23, 24, 21, 16, 22, 13, 22, 18, 19, 31, 21, 10, 19, 26, 16, 19, 31, 9, 17, 29, 19, 24, 21, 20, 15, 29, 24, 28, 21, 22, 29, 17, 18, 19, 23, 21, 28, 12, 17, 21, 23, 13, 17, 13, 22, 23, 24, 19, 23, 18, 23, 18, 33, 21, 29, 21, 15, 18, 10, 25, 17, 23, 19, 26, 21, 27, 20, 24, 17, 18, 23, 22, 25, 32, 23, 27, 11, 25, 20, 23, 22, 11, 21, 18, 25, 8, 11, 25, 25, 15, 24, 22, 18, 25, 21, 19, 25, 27, 26, 14, 21, 26, 29, 22, 13, 16, 26, 29, 17, 21, 17], 'y_true': [3, 34, 36, 12, 10, 25, 17, 7, 40, 7, 2, 3, 49, 1, 11, 19, 30, 13, 5, 8, 16, 18, 11, 9, 63, 3, 20, 8, 8, 6, 13, 9, 41, 14, 5, 12, 7, 10, 13, 37, 4, 18, 9, 20, 2, 7, 23, 8, 11, 7, 23, 5, 21, 9, 8, 11, 6, 12, 4, 12, 13, 15, 7, 35, 11, 14, 19, 10, 7, 7, 5, 22, 16, 16, 6, 4, 6, 42, 10, 13, 13, 20, 22, 4, 7, 10, 8, 6, 19, 4, 8, 12, 11, 7, 4, 19, 13, 10, 13, 5, 26, 10, 17, 17, 15, 23, 20, 32, 10, 12, 19, 7, 10, 3, 17, 9, 7, 5, 5, 7, 14, 15, 14, 29, 35, 14, 11, 6, 20, 4, 6, 29, 38, 5, 19, 10, 14, 14, 9, 28, 22, 24, 29, 9, 3, 17, 18, 2, 9, 10, 28, 12, 17, 7, 13, 21, 42, 7, 11, 7, 15, 9, 24, 5, 6, 6, 21, 27, 3, 10, 3, 12, 31, 15, 14, 20, 3, 23, 40, 12, 7, 5, 21, 9, 11, 10, 22, 7, 8, 6, 19, 10, 7, 14, 6, 10, 8, 4, 6, 33, 16, 9, 14, 17, 15, 6, 24, 6, 6, 11, 6, 9, 43, 25, 15, 30, 4, 33, 11, 6, 37, 17, 4, 35, 5, 4, 5, 16, 17, 8, 35, 15, 3, 46, 9, 11, 15, 22, 6, 15, 5, 15, 6, 31, 26, 48, 14, 12, 10, 24, 5, 17, 10, 11, 6, 8, 20, 9, 6, 11, 17, 9, 4, 14, 10, 17, 15, 6, 8, 38, 10, 7, 38, 22, 3, 12, 7, 2, 8, 18, 12, 24, 11, 11, 26, 9, 23, 14, 20, 23, 11, 5, 20, 5, 5, 15, 18, 18, 5, 12, 11, 26, 2, 13, 29, 10, 21, 8, 12, 4, 3, 26, 4, 5, 22, 4, 9, 12, 18, 15, 10, 10, 15, 15, 23, 23, 17, 9, 7, 27, 8, 4, 31, 28, 6, 7, 13, 2, 42, 11], 'train_acc': 0.006617647058823529, 'test_acc': 0.020588235294117647, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 44248.2852, Test loss: 49886.1289
In this epoch 100/5000, Training loss: 37754.3906, Test loss: 48821.3438
In this epoch 150/5000, Training loss: 31879.2969, Test loss: 48829.3789
In this epoch 200/5000, Training loss: 26569.0156, Test loss: 49008.6992
In this epoch 250/5000, Training loss: 21924.5918, Test loss: 49284.2344
In this epoch 300/5000, Training loss: 17943.7871, Test loss: 49634.3789
In this epoch 350/5000, Training loss: 14581.6357, Test loss: 50038.1523
In this epoch 400/5000, Training loss: 11775.7598, Test loss: 50475.5078
In this epoch 450/5000, Training loss: 9457.7930, Test loss: 50928.7305
In this epoch 500/5000, Training loss: 7559.5415, Test loss: 51383.4727
In this epoch 550/5000, Training loss: 6016.6899, Test loss: 51828.8828
In this epoch 600/5000, Training loss: 4770.8853, Test loss: 52257.1758
In this epoch 650/5000, Training loss: 3770.7068, Test loss: 52663.0898
In this epoch 700/5000, Training loss: 2971.8201, Test loss: 53043.3906
In this epoch 750/5000, Training loss: 2336.6382, Test loss: 53396.3320
In this epoch 800/5000, Training loss: 1833.7242, Test loss: 53721.2656
In this epoch 850/5000, Training loss: 1437.0687, Test loss: 54018.3555
In this epoch 900/5000, Training loss: 1125.3422, Test loss: 54288.3203
In this epoch 950/5000, Training loss: 881.1908, Test loss: 54532.3008
In this epoch 1000/5000, Training loss: 690.5807, Test loss: 54751.6758
In this epoch 1050/5000, Training loss: 542.2315, Test loss: 54948.0469
In this epoch 1100/5000, Training loss: 427.1208, Test loss: 55123.0469
In this epoch 1150/5000, Training loss: 338.0642, Test loss: 55278.4062
In this epoch 1200/5000, Training loss: 269.3654, Test loss: 55415.7773
In this epoch 1250/5000, Training loss: 216.5231, Test loss: 55536.8242
In this epoch 1300/5000, Training loss: 175.9926, Test loss: 55643.1484
In this epoch 1350/5000, Training loss: 144.9930, Test loss: 55736.1875
In this epoch 1400/5000, Training loss: 121.3482, Test loss: 55817.3594
In this epoch 1450/5000, Training loss: 103.3611, Test loss: 55887.9648
In this epoch 1500/5000, Training loss: 89.7118, Test loss: 55949.1758
In this epoch 1550/5000, Training loss: 79.3775, Test loss: 56002.0586
In this epoch 1600/5000, Training loss: 71.5671, Test loss: 56047.6367
In this epoch 1650/5000, Training loss: 65.6716, Test loss: 56086.7500
In this epoch 1700/5000, Training loss: 61.2230, Test loss: 56120.2188
In this epoch 1750/5000, Training loss: 57.8634, Test loss: 56148.7539
In this epoch 1800/5000, Training loss: 55.3199, Test loss: 56172.9766
In this epoch 1850/5000, Training loss: 53.3848, Test loss: 56193.4844
In this epoch 1900/5000, Training loss: 51.9013, Test loss: 56210.7617
In this epoch 1950/5000, Training loss: 50.7510, Test loss: 56225.2305
In this epoch 2000/5000, Training loss: 49.8450, Test loss: 56237.3086
In this epoch 2050/5000, Training loss: 49.1169, Test loss: 56247.3320
In this epoch 2100/5000, Training loss: 48.5172, Test loss: 56255.5820
In this epoch 2150/5000, Training loss: 48.0093, Test loss: 56262.3438
In this epoch 2200/5000, Training loss: 47.5664, Test loss: 56267.8203
In this epoch 2250/5000, Training loss: 47.1684, Test loss: 56272.2266
In this epoch 2300/5000, Training loss: 46.8009, Test loss: 56275.7070
In this epoch 2350/5000, Training loss: 46.4535, Test loss: 56278.4258
In this epoch 2400/5000, Training loss: 46.1188, Test loss: 56280.4883
In this epoch 2450/5000, Training loss: 45.7910, Test loss: 56282.0234
In this epoch 2500/5000, Training loss: 45.4665, Test loss: 56283.0938
In this epoch 2550/5000, Training loss: 45.1425, Test loss: 56283.7969
In this epoch 2600/5000, Training loss: 44.8171, Test loss: 56284.1953
In this epoch 2650/5000, Training loss: 44.4889, Test loss: 56284.3320
In this epoch 2700/5000, Training loss: 44.1571, Test loss: 56284.2734
In this epoch 2750/5000, Training loss: 43.8210, Test loss: 56284.0117
In this epoch 2800/5000, Training loss: 43.4801, Test loss: 56283.6172
In this epoch 2850/5000, Training loss: 43.1344, Test loss: 56283.1172
In this epoch 2900/5000, Training loss: 42.7834, Test loss: 56282.5312
In this epoch 2950/5000, Training loss: 42.4272, Test loss: 56281.8555
In this epoch 3000/5000, Training loss: 42.0657, Test loss: 56281.1289
In this epoch 3050/5000, Training loss: 41.6990, Test loss: 56280.3555
In this epoch 3100/5000, Training loss: 41.3269, Test loss: 56279.5469
In this epoch 3150/5000, Training loss: 40.9497, Test loss: 56278.7070
In this epoch 3200/5000, Training loss: 40.5672, Test loss: 56277.8789
In this epoch 3250/5000, Training loss: 40.1796, Test loss: 56277.0195
In this epoch 3300/5000, Training loss: 39.7870, Test loss: 56276.1758
In this epoch 3350/5000, Training loss: 39.3895, Test loss: 56275.3125
In this epoch 3400/5000, Training loss: 38.9870, Test loss: 56274.4844
In this epoch 3450/5000, Training loss: 38.5798, Test loss: 56273.6523
In this epoch 3500/5000, Training loss: 38.1680, Test loss: 56272.8359
In this epoch 3550/5000, Training loss: 37.7516, Test loss: 56272.0469
In this epoch 3600/5000, Training loss: 37.3308, Test loss: 56271.2656
In this epoch 3650/5000, Training loss: 36.9057, Test loss: 56270.5195
In this epoch 3700/5000, Training loss: 36.4765, Test loss: 56269.8008
In this epoch 3750/5000, Training loss: 36.0433, Test loss: 56269.1133
In this epoch 3800/5000, Training loss: 35.6064, Test loss: 56268.4492
In this epoch 3850/5000, Training loss: 35.1657, Test loss: 56267.8203
In this epoch 3900/5000, Training loss: 34.7215, Test loss: 56267.2422
In this epoch 3950/5000, Training loss: 34.2741, Test loss: 56266.6992
In this epoch 4000/5000, Training loss: 33.8235, Test loss: 56266.1992
In this epoch 4050/5000, Training loss: 33.3701, Test loss: 56265.7656
In this epoch 4100/5000, Training loss: 32.9139, Test loss: 56265.3672
In this epoch 4150/5000, Training loss: 383.6668, Test loss: 57739.1289
In this epoch 4200/5000, Training loss: 33.0164, Test loss: 56273.8945
In this epoch 4250/5000, Training loss: 31.6326, Test loss: 56264.3555
In this epoch 4300/5000, Training loss: 31.2149, Test loss: 56264.1055
In this epoch 4350/5000, Training loss: 30.7979, Test loss: 56264.3477
In this epoch 4400/5000, Training loss: 30.3787, Test loss: 56264.2969
In this epoch 4450/5000, Training loss: 29.9575, Test loss: 56264.3008
In this epoch 4500/5000, Training loss: 310.1787, Test loss: 55557.9961
In this epoch 4550/5000, Training loss: 29.1835, Test loss: 56324.9727
In this epoch 4600/5000, Training loss: 28.7885, Test loss: 56265.4023
In this epoch 4650/5000, Training loss: 28.4055, Test loss: 56265.3789
In this epoch 4700/5000, Training loss: 28.0271, Test loss: 56265.2266
In this epoch 4750/5000, Training loss: 27.6471, Test loss: 56265.5312
In this epoch 4800/5000, Training loss: 27.2656, Test loss: 56265.9414
In this epoch 4850/5000, Training loss: 26.9014, Test loss: 56276.1172
In this epoch 4900/5000, Training loss: 33.0157, Test loss: 56461.5078
In this epoch 4950/5000, Training loss: 26.1777, Test loss: 56277.1016
In this epoch 5000/5000, Training loss: 25.8293, Test loss: 56268.9297
train_acc 0.9970588235294118
test_acc 0.0
{'y_pred': [364.271484375, 400.0303649902344, 207.3768310546875, 370.4183349609375, 348.6134338378906, 426.740966796875, 433.434326171875, 250.6918182373047, 361.5721740722656, 339.02325439453125, 386.2224426269531, 351.1965637207031, 356.60296630859375, 445.5199890136719, 341.1011047363281, 441.4167175292969, 437.0285949707031, 197.60557556152344, 395.6031494140625, 326.2176818847656, 544.5557861328125, 447.1216735839844, 298.5860900878906, 431.8719482421875, 444.1434631347656, 410.0249328613281, 315.2737731933594, 399.88665771484375, 404.2057800292969, 248.01829528808594, 483.4412536621094, 522.9139404296875, 464.7314453125, 137.8852081298828, 474.1637268066406, 407.63299560546875, 443.0155029296875, 492.27105712890625, 336.7046813964844, 461.3638000488281, 398.03900146484375, 305.2618713378906, 476.0055847167969, 284.7131042480469, 276.4726867675781, 487.5254821777344, 539.7698364257812, 333.7269592285156, 387.8625793457031, 323.8971252441406, 396.47979736328125, 370.2036437988281, 414.6300048828125, 219.74595642089844, 417.05780029296875, 448.6905212402344, 499.4529724121094, 496.9715270996094, 334.80999755859375, 307.9644775390625, 315.43572998046875, 236.13275146484375, 348.5018005371094, 297.94769287109375, 348.1851806640625, 423.8170166015625, 266.1492004394531, 485.5592346191406, 364.7613525390625, 320.3250427246094, 500.9595642089844, 310.7108459472656, 254.48158264160156, 428.7476806640625, 279.0426940917969, 424.6427001953125, 283.3287658691406, 369.8261413574219, 379.7242126464844, 498.0034484863281, 516.1000366210938, 365.038818359375, 308.5550537109375, 345.8225402832031, 291.1846008300781, 263.24200439453125, 295.2154846191406, 385.1318664550781, 257.1051940917969, 318.7949523925781, 483.7914733886719, 351.4382019042969, 345.4657897949219, 433.2958068847656, 550.2962646484375, 430.6838684082031, 385.1108093261719, 544.1943359375, 388.7973327636719, 187.53440856933594, 321.9701843261719, 433.0252380371094, 490.4543151855469, 423.8882751464844, 374.7320251464844, 319.6976623535156, 473.3676452636719, 323.69073486328125, 487.0802917480469, 381.7734069824219, 346.8102722167969, 466.9334716796875, 350.9217834472656, 493.66412353515625, 452.6548156738281, 404.8683166503906, 392.4606018066406, 211.12083435058594, 320.9197692871094, 330.3628234863281, 350.7035827636719, 292.3383483886719, 343.25457763671875, 566.610595703125, 307.56060791015625, 501.35101318359375, 418.917724609375, 506.245361328125, 406.2745666503906, 484.85552978515625, 379.1952819824219, 344.586669921875, 240.3135528564453, 466.496337890625, 372.3769836425781, 369.8174133300781, 588.8638916015625, 325.659423828125, 328.70965576171875, 341.5019836425781, 446.4106750488281, 302.5188293457031, 220.8223419189453, 254.06529235839844, 405.2584228515625, 414.40594482421875, 308.8700866699219, 182.2387237548828, 502.01336669921875, 279.4137268066406, 451.5870666503906, 218.73609924316406, 431.77398681640625, 284.19677734375, 243.76377868652344, 454.5206604003906, 296.2963562011719, 306.0256042480469, 494.82177734375, 250.66810607910156, 529.6128540039062, 236.41751098632812, 323.7361755371094, 352.17999267578125, 410.54058837890625, 484.71514892578125, 198.55810546875, 299.8108215332031, 412.8699645996094, 365.6449890136719, 375.1044006347656, 552.0136108398438, 432.0921630859375, 246.0292205810547, 303.80572509765625, 503.7039489746094, 296.1133117675781, 464.4164733886719, 322.5306701660156, 451.97369384765625, 457.06915283203125, 251.01121520996094, 384.9871520996094, 297.1015930175781, 314.5189514160156, 463.8929748535156, 369.2819519042969, 195.17156982421875, 347.48455810546875, 368.6325988769531, 264.54559326171875, 176.81350708007812, 438.72503662109375, 420.2214660644531, 370.8952941894531, 223.8389892578125, 753.7147827148438, 406.064208984375, 357.1597595214844, 440.37432861328125, 387.2048645019531, 449.0664978027344, 384.6158447265625, 169.17808532714844, 372.64910888671875, 284.8849182128906, 378.7032775878906, 462.199951171875, 358.5622253417969, 489.56689453125, 317.0403747558594, 291.4730224609375, 346.3989562988281, 514.4541625976562, 289.5169372558594, 422.41375732421875, 307.5408630371094, 448.4927062988281, 501.9621276855469, 364.9842529296875, 353.8144836425781, 140.6982879638672, 369.8841857910156, 387.1950988769531, 343.26300048828125, 412.236572265625, 323.1611633300781, 306.9374694824219, 392.63671875, 344.797119140625, 400.7074890136719, 296.76446533203125, 383.92755126953125, 371.6438903808594, 435.9229431152344, 624.8683471679688, 296.73870849609375, 313.0472412109375, 393.21160888671875, 503.1215515136719, 451.6593933105469, 564.6709594726562, 180.6149444580078, 452.58782958984375, 339.9013977050781, 399.2317810058594, 279.2039794921875, 221.6870574951172, 399.2437438964844, 343.8725280761719, 257.81475830078125, 320.3393859863281, 509.19012451171875, 241.5543670654297, 430.6849365234375, 233.78729248046875, 510.6268615722656, 221.75868225097656, 369.69647216796875, 368.7574768066406, 350.2419738769531, 299.0499572753906, 320.0426940917969, 342.0379638671875, 304.61932373046875, 492.3745422363281, 386.8335266113281, 476.6584167480469, 295.8423156738281, 331.1995544433594, 365.9459228515625, 381.600830078125, 316.5758972167969, 502.410400390625, 311.4541320800781, 540.530029296875, 407.1889343261719, 316.7036437988281, 460.23492431640625, 425.9134521484375, 483.93304443359375, 281.8697509765625, 449.56494140625, 449.9814147949219, 443.4325256347656, 328.7121276855469, 383.9522399902344, 340.6944885253906, 287.1803283691406, 457.693115234375, 435.3797912597656, 397.6322021484375, 409.88714599609375, 372.77587890625, 416.0845031738281, 505.9508972167969, 462.09185791015625, 235.66839599609375, 476.80322265625, 424.2608947753906, 391.52178955078125, 317.85919189453125, 488.2676696777344, 293.33447265625, 722.9296875, 408.40313720703125, 405.3904113769531, 337.7344665527344, 358.779541015625, 446.43133544921875, 419.8625183105469, 288.9306640625, 406.6147155761719, 372.2760009765625, 344.0093688964844, 387.4345703125, 175.6517333984375, 377.5802307128906, 270.25054931640625, 412.3171081542969, 355.4674987792969, 338.88616943359375, 307.1357727050781, 392.6199951171875, 454.0588073730469, 315.4789733886719, 463.3404846191406, 358.0341491699219, 275.1727600097656, 273.24078369140625, 524.5582885742188, 263.88226318359375, 364.72021484375, 322.6435241699219, 383.79559326171875, 344.0534362792969, 331.7317810058594, 378.87451171875, 273.1665954589844, 172.83322143554688], 'y_pred_int': [364, 400, 207, 370, 349, 427, 433, 251, 362, 339, 386, 351, 357, 446, 341, 441, 437, 198, 396, 326, 545, 447, 299, 432, 444, 410, 315, 400, 404, 248, 483, 523, 465, 138, 474, 408, 443, 492, 337, 461, 398, 305, 476, 285, 276, 488, 540, 334, 388, 324, 396, 370, 415, 220, 417, 449, 499, 497, 335, 308, 315, 236, 349, 298, 348, 424, 266, 486, 365, 320, 501, 311, 254, 429, 279, 425, 283, 370, 380, 498, 516, 365, 309, 346, 291, 263, 295, 385, 257, 319, 484, 351, 345, 433, 550, 431, 385, 544, 389, 188, 322, 433, 490, 424, 375, 320, 473, 324, 487, 382, 347, 467, 351, 494, 453, 405, 392, 211, 321, 330, 351, 292, 343, 567, 308, 501, 419, 506, 406, 485, 379, 345, 240, 466, 372, 370, 589, 326, 329, 342, 446, 303, 221, 254, 405, 414, 309, 182, 502, 279, 452, 219, 432, 284, 244, 455, 296, 306, 495, 251, 530, 236, 324, 352, 411, 485, 199, 300, 413, 366, 375, 552, 432, 246, 304, 504, 296, 464, 323, 452, 457, 251, 385, 297, 315, 464, 369, 195, 347, 369, 265, 177, 439, 420, 371, 224, 754, 406, 357, 440, 387, 449, 385, 169, 373, 285, 379, 462, 359, 490, 317, 291, 346, 514, 290, 422, 308, 448, 502, 365, 354, 141, 370, 387, 343, 412, 323, 307, 393, 345, 401, 297, 384, 372, 436, 625, 297, 313, 393, 503, 452, 565, 181, 453, 340, 399, 279, 222, 399, 344, 258, 320, 509, 242, 431, 234, 511, 222, 370, 369, 350, 299, 320, 342, 305, 492, 387, 477, 296, 331, 366, 382, 317, 502, 311, 541, 407, 317, 460, 426, 484, 282, 450, 450, 443, 329, 384, 341, 287, 458, 435, 398, 410, 373, 416, 506, 462, 236, 477, 424, 392, 318, 488, 293, 723, 408, 405, 338, 359, 446, 420, 289, 407, 372, 344, 387, 176, 378, 270, 412, 355, 339, 307, 393, 454, 315, 463, 358, 275, 273, 525, 264, 365, 323, 384, 344, 332, 379, 273, 173], 'y_true': [295, 244, 565, 60, 137, 539, 572, 354, 180, 521, 215, 76, 399, 536, 266, 317, 762, 114, 428, 90, 783, 643, 226, 204, 752, 148, 823, 207, 770, 223, 202, 216, 177, 161, 385, 621, 347, 466, 778, 659, 45, 154, 101, 88, 188, 125, 607, 276, 1342, 541, 239, 517, 283, 498, 343, 814, 318, 186, 799, 934, 889, 434, 321, 326, 319, 351, 921, 548, 396, 217, 361, 344, 409, 236, 284, 532, 294, 324, 715, 80, 160, 214, 552, 340, 508, 342, 528, 421, 176, 350, 272, 212, 163, 343, 327, 254, 69, 464, 171, 371, 599, 469, 250, 128, 233, 905, 405, 155, 244, 148, 123, 260, 390, 407, 159, 254, 74, 253, 148, 308, 446, 131, 412, 470, 484, 355, 187, 619, 203, 321, 312, 164, 346, 634, 180, 687, 145, 210, 237, 232, 301, 131, 243, 300, 424, 535, 129, 417, 162, 350, 143, 84, 665, 143, 296, 415, 162, 89, 97, 32, 81, 297, 601, 96, 89, 588, 100, 350, 628, 747, 173, 280, 238, 587, 97, 497, 99, 173, 170, 197, 157, 408, 351, 336, 765, 133, 447, 85, 105, 269, 445, 201, 222, 585, 441, 81, 237, 245, 337, 113, 328, 156, 138, 339, 537, 321, 297, 314, 278, 1062, 111, 340, 211, 266, 460, 513, 168, 456, 174, 224, 235, 409, 103, 179, 603, 317, 300, 89, 188, 574, 458, 367, 415, 444, 175, 122, 402, 145, 396, 217, 1297, 334, 78, 344, 880, 455, 100, 104, 244, 445, 791, 863, 235, 318, 678, 257, 478, 155, 247, 294, 599, 298, 107, 173, 641, 523, 312, 77, 335, 714, 663, 221, 158, 348, 912, 376, 292, 163, 621, 163, 168, 726, 200, 228, 531, 280, 154, 280, 127, 456, 74, 161, 511, 629, 384, 422, 165, 580, 135, 143, 439, 275, 446, 165, 256, 164, 831, 380, 133, 260, 205, 614, 680, 350, 657, 162, 203, 462, 135, 60, 250, 175, 274, 643, 172, 801, 556, 343, 634, 167, 389, 359, 202, 80, 540, 465, 452, 665, 123, 62], 'train_acc': 0.9970588235294118, 'test_acc': 0.0, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 1631.3273, Test loss: 1378.4999
In this epoch 100/5000, Training loss: 9.4425, Test loss: 108.7348
In this epoch 150/5000, Training loss: 0.0999, Test loss: 94.3128
In this epoch 200/5000, Training loss: 0.0000, Test loss: 93.5418
In this epoch 250/5000, Training loss: 0.0000, Test loss: 93.4848
In this epoch 300/5000, Training loss: 0.0000, Test loss: 93.4765
In this epoch 350/5000, Training loss: 0.0000, Test loss: 93.4738
In this epoch 400/5000, Training loss: 0.0000, Test loss: 93.4718
In this epoch 450/5000, Training loss: 0.0000, Test loss: 93.4704
In this epoch 500/5000, Training loss: 0.0000, Test loss: 93.4695
In this epoch 550/5000, Training loss: 0.0000, Test loss: 93.4690
In this epoch 600/5000, Training loss: 0.0000, Test loss: 93.4686
In this epoch 650/5000, Training loss: 0.0000, Test loss: 93.4684
In this epoch 700/5000, Training loss: 0.0000, Test loss: 93.4683
In this epoch 750/5000, Training loss: 0.0000, Test loss: 93.4683
In this epoch 800/5000, Training loss: 0.0000, Test loss: 93.4683
In this epoch 850/5000, Training loss: 0.0000, Test loss: 93.4683
In this epoch 900/5000, Training loss: 0.0000, Test loss: 93.4683
In this epoch 950/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 93.4683
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 93.4683
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 93.4683
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 93.4683
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 93.4682
In this epoch 2550/5000, Training loss: 33450.3789, Test loss: 270465.1250
In this epoch 2600/5000, Training loss: 2055.3159, Test loss: 314.2713
In this epoch 2650/5000, Training loss: 0.0145, Test loss: 105.6169
In this epoch 2700/5000, Training loss: 0.0277, Test loss: 91.6460
In this epoch 2750/5000, Training loss: 0.0001, Test loss: 91.8698
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 91.8483
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 91.8504
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 91.8505
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 91.8505
train_acc 0.34274221453287196
test_acc 0.27427335640138406
{'y_pred': [-4.818687915802002, -10.352741241455078, 10.412349700927734, 8.533249855041504, -19.242170333862305, -3.4379427433013916, 7.525939464569092, 4.779537200927734, 0.36834704875946045, 2.1115028858184814, -6.9997029304504395, 4.071711540222168, -3.806358814239502, -4.229024887084961, 10.166590690612793, -9.371926307678223, -5.116928577423096, -4.913028240203857, -5.486486434936523, 18.959882736206055, 2.1790668964385986, -10.172802925109863, -5.132091999053955, -10.868644714355469, -16.515836715698242, -4.151363372802734, 0.7765177488327026, 8.147405624389648, -1.7097359895706177, -1.6266456842422485, -4.1967997550964355, 21.07815170288086, 9.00674819946289, 2.584848642349243, 1.181835412979126, -3.7489004135131836, -3.530691146850586, -7.561191082000732, -9.046927452087402, -2.0535638332366943, -3.9749820232391357, -4.015616416931152, -0.7344942092895508, -10.833171844482422, 5.734842777252197, -7.494200229644775, -12.38841438293457, 1.9180656671524048, -21.065317153930664, -2.444722890853882, -5.8566107749938965, -0.05912197008728981, 7.9517083168029785, -13.97268009185791, -1.4013243913650513, 5.650629043579102, 12.949921607971191, 13.46284294128418, 8.653854370117188, 4.59440803527832, 1.7908262014389038, -18.057287216186523, 5.966248989105225, -6.191193103790283, 8.602998733520508, 9.837968826293945, -3.211296796798706, -0.36972010135650635, -13.009553909301758, -11.75057315826416, 5.133863925933838, 7.657614707946777, -7.769578456878662, 9.826501846313477, -3.692389488220215, 7.003175258636475, -10.004610061645508, -9.614542007446289, 1.9060275554656982, -11.510002136230469, 4.969635486602783, -7.52250337600708, 9.002482414245605, 12.284960746765137, 4.764404773712158, -15.669961929321289, -5.778295993804932, 3.3274452686309814, -6.909585475921631, -5.9263386726379395, 5.2401556968688965, -1.0197845697402954, 11.00532341003418, -12.943244934082031, -6.589353561401367, 4.661159038543701, 5.0781450271606445, 1.0411018133163452, -11.677141189575195, -6.84260892868042, 2.404040575027466, 10.538002014160156, -4.890374839305878e-06, 6.61601448059082, -15.226191520690918, 0.6073387861251831, -0.4031716585159302, -4.638795375823975, 0.12408769130706787, -8.997172355651855, -6.730217456817627, 16.790645599365234, -0.5323141813278198, -3.471390962600708, -6.872781276702881, -4.063758850097656, -1.0553115606307983, 0.5683315992355347, 13.80491828918457, 4.568021774291992, -8.41712760925293, 2.717911958694458, -7.996617794036865, 2.490865468978882, -6.644120693206787, -0.2924079895019531, -9.895715713500977, -5.299405097961426, -1.5592557191848755, 6.853884696960449, 13.31370735168457, -0.8194471597671509, 21.711971282958984, -5.255783557891846, 13.021917343139648, 5.725241184234619, -12.722930908203125, -10.745277404785156, 4.675323009490967, 10.202150344848633, 1.0340222120285034, -13.334047317504883, 8.688779830932617, -1.948512315750122, 6.145466327667236, -0.10404717922210693, -9.649188041687012, 4.329347610473633, 2.4945037364959717, 11.52822494506836, 0.6034919023513794, -4.391788005828857, -7.271606922149658, -1.1635037660598755, 11.255241394042969, 14.022371292114258, -0.4396463632583618, 16.280807495117188, 5.077892780303955, 16.13625717163086, -4.7675580978393555, 2.0101354122161865, -35.89215850830078, 9.45615291595459, -11.56386947631836, 16.583843231201172, -22.974254608154297, 6.9500837326049805, 23.365514755249023, 5.277219295501709, -11.539704322814941, 13.91915225982666, -3.5145108699798584, 26.826772689819336, -5.743247985839844, 8.79310417175293, 3.8838393688201904, 10.602065086364746, -3.6616928577423096, -1.6306103467941284, -11.288071632385254, 8.463981628417969, 10.74215030670166, 9.616792678833008, 7.28452730178833, -2.256666898727417, 2.4409339427948, 1.93199622631073, -29.1203670501709, -11.756075859069824, 9.021705627441406, -11.785942077636719, 4.906673908233643, 0.18990743160247803, 4.389812469482422, 1.4994093179702759, 12.107073783874512, -15.734084129333496, -0.30867087841033936, -11.746491432189941, -2.462963581085205, 2.407923698425293, 5.661192893981934, -9.802751541137695, -4.4859724044799805, 8.024956703186035, 7.158740520477295, 2.9761860370635986, 10.853135108947754, 10.108920097351074, 2.3992722034454346, -3.5462300777435303, -6.721242427825928, -11.203452110290527, 0.05750071629881859, 18.771474838256836, -8.30937671661377, 7.7457194328308105, -5.667881488800049, -6.8661885261535645, -14.28353214263916, 7.261112689971924, 5.580883502960205, 5.765035152435303, 4.436100482940674, 8.231311798095703, -6.547745704650879, 2.566892623901367, -1.2512470483779907, 1.8669987916946411, -0.41156256198883057, 1.7258762121200562, -3.612560749053955, 8.225095748901367, -16.48363494873047, -7.394061088562012, 0.756527304649353, 5.02070426940918, -5.114574432373047, -5.149954795837402, 3.117757797241211, 2.6625630855560303, 3.520148992538452, 0.9009933471679688, -11.914484024047852, 8.194955825805664, 4.551119804382324, 14.562973022460938, -10.712729454040527, 0.5715843439102173, -4.0227251052856445, 1.9877690076828003, 11.61292552947998, -1.275835633277893, 1.1079849004745483, 1.770358681678772, -12.607090950012207, -8.79458236694336, 1.8015693426132202, 1.507764220237732, 9.957969665527344, -2.08851957321167, -7.087767124176025, 2.1988399028778076, 5.03930139541626, -5.770798206329346, 8.756126403808594, -7.186910629272461, -14.447261810302734, -6.880067825317383, 0.3904837369918823, -8.754993438720703, -7.058042526245117, -9.96790885925293, -21.37120246887207, -19.603975296020508, 2.9153645038604736, -6.935478687286377, -3.7457122802734375, 0.6388329267501831, -19.07763671875, 29.975175857543945, -7.65885591506958, -4.661845684051514, -2.436760187149048, -21.70557975769043, -20.082120895385742, 7.950798988342285, -8.483859062194824, 0.40963876247406006, 1.960150122642517, 9.679526329040527, -5.629945755004883, -6.974876403808594, 9.95096492767334, 2.0836057662963867, -15.067181587219238, -1.5077165365219116, -4.995366096496582, -5.748790264129639, -0.7122923135757446, 9.786815643310547, 3.0130579471588135, 1.0105541944503784, 14.873760223388672, 19.622987747192383, 24.974687576293945, 16.441930770874023, -14.618362426757812, 4.1569414138793945, 9.98920726776123, -6.771762371063232, -1.4736937284469604, -13.692303657531738, 0.6334289312362671, -19.425952911376953, 19.3983211517334, 2.4025747776031494, 5.236438274383545, -3.3050758838653564, -4.110053539276123, 4.437882900238037, -3.54263973236084, -15.035167694091797, -14.68453311920166, 14.505940437316895, -10.392308235168457, -14.60670280456543, -12.662269592285156, 7.5986809730529785, -1.1824146509170532, -5.487196445465088, 10.91752815246582, -6.271905422210693, -3.9955923557281494, -8.820032119750977, 5.402602672576904, -11.402926445007324, 4.29982852935791, -4.602766513824463], 'y_pred_int': [0, 0, 10, 9, 0, 0, 8, 5, 0, 2, 0, 4, 0, 0, 10, 0, 0, 0, 0, 19, 2, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 21, 9, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 2, 0, 0, 0, 0, 8, 0, 0, 6, 13, 13, 9, 5, 2, 0, 6, 0, 9, 10, 0, 0, 0, 0, 5, 8, 0, 10, 0, 7, 0, 0, 2, 0, 5, 0, 9, 12, 5, 0, 0, 3, 0, 0, 5, 0, 11, 0, 0, 5, 5, 1, 0, 0, 2, 11, 0, 7, 0, 1, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 1, 14, 5, 0, 3, 0, 2, 0, 0, 0, 0, 0, 7, 13, 0, 22, 0, 13, 6, 0, 0, 5, 10, 1, 0, 9, 0, 6, 0, 0, 4, 2, 12, 1, 0, 0, 0, 11, 14, 0, 16, 5, 16, 0, 2, 0, 9, 0, 17, 0, 7, 23, 5, 0, 14, 0, 27, 0, 9, 4, 11, 0, 0, 0, 8, 11, 10, 7, 0, 2, 2, 0, 0, 9, 0, 5, 0, 4, 1, 12, 0, 0, 0, 0, 2, 6, 0, 0, 8, 7, 3, 11, 10, 2, 0, 0, 0, 0, 19, 0, 8, 0, 0, 0, 7, 6, 6, 4, 8, 0, 3, 0, 2, 0, 2, 0, 8, 0, 0, 1, 5, 0, 0, 3, 3, 4, 1, 0, 8, 5, 15, 0, 1, 0, 2, 12, 0, 1, 2, 0, 0, 2, 2, 10, 0, 0, 2, 5, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 30, 0, 0, 0, 0, 0, 8, 0, 0, 2, 10, 0, 0, 10, 2, 0, 0, 0, 0, 0, 10, 3, 1, 15, 20, 25, 16, 0, 4, 10, 0, 0, 0, 1, 0, 19, 2, 5, 0, 0, 4, 0, 0, 0, 15, 0, 0, 0, 8, 0, 0, 11, 0, 0, 0, 5, 0, 4, 0], 'y_true': [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 3, 0, 1, 1, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 2, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1, 0, 0, 1, 1, 0, 1, 0, 2, 3, 1, 0, 0, 2, 0, 1, 0, 0, 2, 1, 2, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 2, 1, 0, 0, 1, 1, 4, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 2, 1, 1, 1, 0, 6, 1, 2, 0, 0, 1, 0, 0, 0, 3, 6, 0, 1, 0, 1, 1, 2, 1, 0, 1, 1, 2, 1, 0, 1, 0, 0, 2, 0, 0, 1, 0, 3, 0, 2, 1, 4, 0, 0, 0, 0, 1, 2, 2, 1, 0, 2, 1, 3, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 3, 0, 0, 1, 1, 1, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 3, 0, 1, 1, 0, 2, 0, 1, 1, 1, 3, 0, 0, 2, 0, 5, 1, 0, 0, 9, 0, 0, 1, 2, 1, 1, 0, 1, 0, 1, 3, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 3, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 3, 2, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 5, 2, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 3, 0, 1, 1, 1, 1, 0, 1, 3, 0, 1, 2, 0, 0, 0, 1, 3, 1, 0, 1, 1, 1, 0], 'train_acc': 0.34274221453287196, 'test_acc': 0.27427335640138406, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 653.3634, Test loss: 2117.5979
In this epoch 100/5000, Training loss: 16.8465, Test loss: 174.1867
In this epoch 150/5000, Training loss: 0.0050, Test loss: 158.8288
In this epoch 200/5000, Training loss: 0.0004, Test loss: 159.2898
In this epoch 250/5000, Training loss: 0.0000, Test loss: 159.3109
In this epoch 300/5000, Training loss: 0.0000, Test loss: 159.3163
In this epoch 350/5000, Training loss: 0.0000, Test loss: 159.3186
In this epoch 400/5000, Training loss: 0.0000, Test loss: 159.3199
In this epoch 450/5000, Training loss: 0.0000, Test loss: 159.3209
In this epoch 500/5000, Training loss: 0.0000, Test loss: 159.3215
In this epoch 550/5000, Training loss: 0.0000, Test loss: 159.3219
In this epoch 600/5000, Training loss: 0.0000, Test loss: 159.3221
In this epoch 650/5000, Training loss: 0.0000, Test loss: 159.3223
In this epoch 700/5000, Training loss: 0.0000, Test loss: 159.3223
In this epoch 750/5000, Training loss: 0.0000, Test loss: 159.3223
In this epoch 800/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 850/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 900/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 950/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 159.3224
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 159.3194
In this epoch 2050/5000, Training loss: 4793.8613, Test loss: 1761.5383
In this epoch 2100/5000, Training loss: 2.6169, Test loss: 175.8112
In this epoch 2150/5000, Training loss: 0.0006, Test loss: 160.4409
In this epoch 2200/5000, Training loss: 0.0008, Test loss: 159.5553
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 159.5704
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 159.5718
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 159.5715
In this epoch 3250/5000, Training loss: 0.0599, Test loss: 160.1532
In this epoch 3300/5000, Training loss: 89.1806, Test loss: 1024.5170
In this epoch 3350/5000, Training loss: 4.0582, Test loss: 162.0056
In this epoch 3400/5000, Training loss: 0.0270, Test loss: 159.2848
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 159.5784
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 159.5713
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 159.5716
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 159.5744
In this epoch 3650/5000, Training loss: 7575.7266, Test loss: 12524.9648
In this epoch 3700/5000, Training loss: 78.6140, Test loss: 195.3645
In this epoch 3750/5000, Training loss: 0.1772, Test loss: 158.9500
In this epoch 3800/5000, Training loss: 0.0024, Test loss: 159.6215
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 159.5762
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 159.5724
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 159.5709
In this epoch 4000/5000, Training loss: 0.0035, Test loss: 159.6994
In this epoch 4050/5000, Training loss: 1330.0254, Test loss: 2184.8979
In this epoch 4100/5000, Training loss: 0.0130, Test loss: 160.7397
In this epoch 4150/5000, Training loss: 0.0546, Test loss: 159.4536
In this epoch 4200/5000, Training loss: 0.0001, Test loss: 159.5434
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 159.5703
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 159.5630
In this epoch 4350/5000, Training loss: 20.0727, Test loss: 399.1655
In this epoch 4400/5000, Training loss: 14.4066, Test loss: 170.0119
In this epoch 4450/5000, Training loss: 36.8521, Test loss: 334.2004
In this epoch 4500/5000, Training loss: 1.3647, Test loss: 159.2659
In this epoch 4550/5000, Training loss: 0.0117, Test loss: 159.4710
In this epoch 4600/5000, Training loss: 0.0001, Test loss: 159.5552
In this epoch 4650/5000, Training loss: 2.0182, Test loss: 165.3540
In this epoch 4700/5000, Training loss: 106.3772, Test loss: 614.3254
In this epoch 4750/5000, Training loss: 4.1494, Test loss: 159.6081
In this epoch 4800/5000, Training loss: 0.0243, Test loss: 159.3562
In this epoch 4850/5000, Training loss: 0.0002, Test loss: 159.5520
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 159.5714
In this epoch 4950/5000, Training loss: 0.0119, Test loss: 159.7753
In this epoch 5000/5000, Training loss: 2017.4706, Test loss: 1265.7771
train_acc 0.0
test_acc 0.0029411764705882353
{'y_pred': [24.97878646850586, 45.47625732421875, 46.18456268310547, 48.63549041748047, 32.72138214111328, 50.9891357421875, 50.21424865722656, 55.896522521972656, 40.17350769042969, 43.176544189453125, 57.55643081665039, 37.327640533447266, 77.92742156982422, 51.74987030029297, 35.35127258300781, 68.70472717285156, 61.37903594970703, 59.47626495361328, 40.921226501464844, 38.45397186279297, 42.10292053222656, 65.96231842041016, 53.131019592285156, 32.984619140625, 39.448486328125, 47.144596099853516, 65.63094329833984, 45.28754806518555, 39.053314208984375, 51.026451110839844, 57.868282318115234, 52.14977264404297, 52.65656280517578, 48.175209045410156, 44.52779769897461, 49.05360794067383, 32.76255416870117, 37.67152404785156, 45.16560745239258, 31.399629592895508, 36.496482849121094, 62.486175537109375, 40.62751007080078, 39.3260498046875, 48.747314453125, 72.53905487060547, 63.96521759033203, 38.54574203491211, 67.39908599853516, 53.872154235839844, 43.49923324584961, 59.46734619140625, 58.19908142089844, 43.79091262817383, 41.5408935546875, 59.718170166015625, 49.70496368408203, 46.911373138427734, 28.965274810791016, 36.08823776245117, 31.110347747802734, 22.995372772216797, 53.49821853637695, 41.91658020019531, 44.63978958129883, 42.07000732421875, 44.4659309387207, 34.96913146972656, 55.77253341674805, 48.9470329284668, 50.10930633544922, 36.52107620239258, 48.502044677734375, 61.51945495605469, 48.853050231933594, 31.583528518676758, 59.30838394165039, 49.521690368652344, 41.19757080078125, 30.0805606842041, 56.51697540283203, 35.796966552734375, 52.75436019897461, 48.664039611816406, 74.75562286376953, 57.53289794921875, 41.10294723510742, 43.4272575378418, 45.21996307373047, 73.11524200439453, 49.00205612182617, 28.447696685791016, 55.68888473510742, 56.23754119873047, 55.24251174926758, 41.560272216796875, 75.6769790649414, 35.21354675292969, 57.561737060546875, 44.26296615600586, 44.87397384643555, 40.449825286865234, 42.54731369018555, 58.04534149169922, 40.63947296142578, 51.76032257080078, 44.44675827026367, 38.301998138427734, 35.19364929199219, 40.809593200683594, 77.79883575439453, 69.44689178466797, 40.06257629394531, 31.565893173217773, 68.9157485961914, 70.75505828857422, 67.59505462646484, 37.08665084838867, 40.79246520996094, 34.4381103515625, 40.02287673950195, 33.654823303222656, 49.97209167480469, 65.07918548583984, 49.31272506713867, 42.89619064331055, 59.222442626953125, 38.76782989501953, 46.44342041015625, 36.10758972167969, 39.594425201416016, 33.671295166015625, 67.2492904663086, 43.38035583496094, 34.56121063232422, 42.970157623291016, 40.628761291503906, 59.68196105957031, 49.08781433105469, 26.236923217773438, 47.254730224609375, 27.92239761352539, 46.026161193847656, 38.86056137084961, 55.03479766845703, 58.339683532714844, 42.2224235534668, 64.07855987548828, 44.87520980834961, 22.181652069091797, 68.85616302490234, 41.32876968383789, 47.02392578125, 38.03080749511719, 31.44890594482422, 39.72068786621094, 47.93133544921875, 42.528411865234375, 45.361175537109375, 51.920204162597656, 76.88912200927734, 42.56853485107422, 52.49658203125, 60.00830841064453, 48.18192672729492, 45.221797943115234, 53.252197265625, 45.910037994384766, 45.59644317626953, 53.12492752075195, 49.78041458129883, 54.114158630371094, 32.85982131958008, 44.04840850830078, 16.257587432861328, 49.37548065185547, 43.524993896484375, 51.67469787597656, 43.96905517578125, 51.63196563720703, 43.36256408691406, 26.92643928527832, 52.015254974365234, 50.88106155395508, 48.91657638549805, 33.605201721191406, 42.242408752441406, 30.35279083251953, 52.492225646972656, 52.27461242675781, 38.972999572753906, 33.1096305847168, 32.563865661621094, 30.243507385253906, 36.10847473144531, 36.16703796386719, 47.19116973876953, 40.080570220947266, 69.32573699951172, 36.68099594116211, 44.39778137207031, 44.97442626953125, 53.27971649169922, 34.69206237792969, 57.06758117675781, 73.08116149902344, 39.71982192993164, 43.680389404296875, 36.25506591796875, 40.82567596435547, 49.40447998046875, 66.59937286376953, 48.91692352294922, 47.868534088134766, 68.9191665649414, 38.109092712402344, 44.79927062988281, 29.430591583251953, 23.048324584960938, 45.076202392578125, 46.718360900878906, 36.497379302978516, 42.222625732421875, 31.272918701171875, 49.564208984375, 61.05058288574219, 51.186134338378906, 41.58049011230469, 45.55176544189453, 44.064353942871094, 40.27434158325195, 47.68407440185547, 40.3079948425293, 57.124229431152344, 53.78870391845703, 44.42009735107422, 86.62979888916016, 32.63076400756836, 53.84632873535156, 57.69291687011719, 43.716064453125, 51.804901123046875, 49.821781158447266, 44.476619720458984, 29.100305557250977, 56.57609939575195, 70.53803253173828, 59.040283203125, 40.407257080078125, 67.72241973876953, 34.79246520996094, 42.69289016723633, 44.00914001464844, 30.46651840209961, 34.593421936035156, 55.59526824951172, 54.28426742553711, 30.11127281188965, 51.802398681640625, 50.918277740478516, 51.84819793701172, 46.06499481201172, 40.657814025878906, 32.335609436035156, 48.884464263916016, 34.97696304321289, 13.571188926696777, 26.4241886138916, 38.83823013305664, 48.14206314086914, 48.20512771606445, 31.892276763916016, 39.40321350097656, 35.444339752197266, 30.18832778930664, 34.75171661376953, 48.126068115234375, 44.46720886230469, 53.769630432128906, 43.46763610839844, 30.019851684570312, 49.18039321899414, 53.422027587890625, 52.49627685546875, 42.5688591003418, 33.2333984375, 38.865333557128906, 52.85152053833008, 45.341426849365234, 34.523681640625, 41.2515754699707, 34.61543655395508, 45.247154235839844, 60.84248352050781, 32.680789947509766, 51.17584991455078, 45.335533142089844, 47.40154266357422, 60.45243835449219, 43.58462905883789, 45.54088592529297, 26.033306121826172, 40.32613754272461, 34.5564079284668, 56.767330169677734, 61.6269416809082, 37.77301025390625, 42.32172393798828, 65.4281005859375, 60.49384307861328, 38.96298599243164, 73.28939819335938, 38.2049560546875, 40.51956558227539, 44.70022964477539, 57.551971435546875, 36.75120162963867, 36.52264404296875, 40.54485321044922, 51.60786437988281, 45.980224609375, 24.589242935180664, 45.81591796875, 52.01763153076172, 56.593849182128906, 53.50215148925781, 54.04124069213867, 41.10268020629883, 38.4802131652832, 38.9769287109375, 56.542091369628906, 50.207706451416016, 64.76956176757812, 44.377906799316406, 49.61023712158203, 51.3474235534668, 55.06022262573242, 35.2811279296875, 33.36537170410156, 18.296018600463867], 'y_pred_int': [25, 45, 46, 49, 33, 51, 50, 56, 40, 43, 58, 37, 78, 52, 35, 69, 61, 59, 41, 38, 42, 66, 53, 33, 39, 47, 66, 45, 39, 51, 58, 52, 53, 48, 45, 49, 33, 38, 45, 31, 36, 62, 41, 39, 49, 73, 64, 39, 67, 54, 43, 59, 58, 44, 42, 60, 50, 47, 29, 36, 31, 23, 53, 42, 45, 42, 44, 35, 56, 49, 50, 37, 49, 62, 49, 32, 59, 50, 41, 30, 57, 36, 53, 49, 75, 58, 41, 43, 45, 73, 49, 28, 56, 56, 55, 42, 76, 35, 58, 44, 45, 40, 43, 58, 41, 52, 44, 38, 35, 41, 78, 69, 40, 32, 69, 71, 68, 37, 41, 34, 40, 34, 50, 65, 49, 43, 59, 39, 46, 36, 40, 34, 67, 43, 35, 43, 41, 60, 49, 26, 47, 28, 46, 39, 55, 58, 42, 64, 45, 22, 69, 41, 47, 38, 31, 40, 48, 43, 45, 52, 77, 43, 52, 60, 48, 45, 53, 46, 46, 53, 50, 54, 33, 44, 16, 49, 44, 52, 44, 52, 43, 27, 52, 51, 49, 34, 42, 30, 52, 52, 39, 33, 33, 30, 36, 36, 47, 40, 69, 37, 44, 45, 53, 35, 57, 73, 40, 44, 36, 41, 49, 67, 49, 48, 69, 38, 45, 29, 23, 45, 47, 36, 42, 31, 50, 61, 51, 42, 46, 44, 40, 48, 40, 57, 54, 44, 87, 33, 54, 58, 44, 52, 50, 44, 29, 57, 71, 59, 40, 68, 35, 43, 44, 30, 35, 56, 54, 30, 52, 51, 52, 46, 41, 32, 49, 35, 14, 26, 39, 48, 48, 32, 39, 35, 30, 35, 48, 44, 54, 43, 30, 49, 53, 52, 43, 33, 39, 53, 45, 35, 41, 35, 45, 61, 33, 51, 45, 47, 60, 44, 46, 26, 40, 35, 57, 62, 38, 42, 65, 60, 39, 73, 38, 41, 45, 58, 37, 37, 41, 52, 46, 25, 46, 52, 57, 54, 54, 41, 38, 39, 57, 50, 65, 44, 50, 51, 55, 35, 33, 18], 'y_true': [12, 11, 53, 57, 9, 5, 13, 34, 10, 39, 29, 8, 6, 42, 11, 8, 7, 12, 15, 11, 8, 6, 11, 24, 6, 6, 5, 15, 8, 11, 19, 45, 40, 24, 27, 11, 3, 10, 23, 7, 12, 10, 11, 26, 11, 7, 5, 10, 9, 27, 2, 7, 7, 17, 5, 7, 39, 22, 13, 5, 26, 13, 5, 49, 28, 51, 4, 12, 3, 10, 31, 20, 17, 14, 7, 4, 14, 26, 16, 7, 29, 8, 32, 20, 6, 8, 10, 29, 8, 7, 16, 13, 7, 9, 49, 10, 9, 16, 5, 8, 11, 9, 15, 5, 8, 14, 15, 5, 10, 8, 10, 5, 13, 12, 7, 7, 5, 24, 4, 13, 8, 23, 7, 7, 18, 10, 40, 12, 37, 13, 13, 11, 6, 15, 46, 29, 19, 5, 15, 7, 8, 10, 23, 3, 10, 12, 11, 6, 13, 14, 9, 10, 21, 20, 10, 35, 9, 11, 19, 7, 6, 29, 14, 11, 10, 19, 14, 11, 9, 7, 16, 10, 14, 24, 21, 8, 22, 4, 15, 7, 10, 10, 18, 9, 11, 18, 16, 63, 8, 15, 19, 7, 11, 32, 15, 14, 28, 27, 10, 19, 21, 13, 35, 16, 13, 5, 21, 11, 10, 3, 9, 7, 23, 2, 6, 5, 13, 10, 20, 14, 10, 34, 9, 2, 9, 4, 9, 21, 23, 23, 15, 6, 15, 18, 9, 9, 6, 14, 7, 9, 4, 7, 19, 18, 11, 6, 9, 10, 13, 5, 2, 10, 22, 14, 13, 25, 18, 7, 8, 20, 12, 11, 38, 38, 9, 3, 34, 13, 18, 15, 13, 9, 13, 4, 9, 18, 27, 24, 31, 15, 22, 20, 14, 28, 19, 12, 13, 22, 14, 34, 5, 18, 11, 6, 20, 15, 11, 6, 5, 20, 9, 22, 2, 25, 15, 21, 12, 14, 31, 5, 3, 5, 19, 24, 16, 11, 9, 20, 1, 12, 4, 11, 9, 16, 14, 6, 19, 28, 6, 2, 8, 25, 7, 13, 16, 25, 31, 11, 44, 12], 'train_acc': 0.0, 'test_acc': 0.0029411764705882353, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 836.3813, Test loss: 16605.8848
In this epoch 100/5000, Training loss: 1.5750, Test loss: 17410.5859
In this epoch 150/5000, Training loss: 0.0329, Test loss: 17448.6523
In this epoch 200/5000, Training loss: 0.0003, Test loss: 17450.8301
In this epoch 250/5000, Training loss: 0.0000, Test loss: 17451.3672
In this epoch 300/5000, Training loss: 0.0000, Test loss: 17451.4590
In this epoch 350/5000, Training loss: 0.0000, Test loss: 17451.4688
In this epoch 400/5000, Training loss: 0.0000, Test loss: 17451.4805
In this epoch 450/5000, Training loss: 0.0000, Test loss: 17451.4824
In this epoch 500/5000, Training loss: 0.0000, Test loss: 17451.4863
In this epoch 550/5000, Training loss: 0.0000, Test loss: 17451.4863
In this epoch 600/5000, Training loss: 0.0000, Test loss: 17451.4863
In this epoch 650/5000, Training loss: 0.0000, Test loss: 17451.4863
In this epoch 700/5000, Training loss: 0.0000, Test loss: 17451.4922
In this epoch 750/5000, Training loss: 0.0000, Test loss: 17451.4922
In this epoch 800/5000, Training loss: 0.0000, Test loss: 17451.4902
In this epoch 850/5000, Training loss: 0.0000, Test loss: 17451.4883
In this epoch 900/5000, Training loss: 0.0000, Test loss: 17451.4863
In this epoch 950/5000, Training loss: 0.0000, Test loss: 17451.4883
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 17451.4883
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 17451.4883
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 17451.4883
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 17451.4883
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 17451.4863
In this epoch 1250/5000, Training loss: 13215.5557, Test loss: 17215.6777
In this epoch 1300/5000, Training loss: 100.6149, Test loss: 17229.6094
In this epoch 1350/5000, Training loss: 0.7283, Test loss: 17469.4219
In this epoch 1400/5000, Training loss: 0.0034, Test loss: 17453.3906
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 17451.5156
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 17451.4863
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 17451.5098
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 17451.5039
In this epoch 1650/5000, Training loss: 9.9223, Test loss: 17603.8965
In this epoch 1700/5000, Training loss: 30.2894, Test loss: 17233.7695
In this epoch 1750/5000, Training loss: 2.7067, Test loss: 17480.3965
In this epoch 1800/5000, Training loss: 0.0010, Test loss: 17450.1660
In this epoch 1850/5000, Training loss: 0.0001, Test loss: 17451.3887
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 17451.5215
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 17451.5039
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 17451.4941
In this epoch 2050/5000, Training loss: 44749.2461, Test loss: 46156.6133
In this epoch 2100/5000, Training loss: 39.7740, Test loss: 17222.9746
In this epoch 2150/5000, Training loss: 0.1335, Test loss: 17438.2148
In this epoch 2200/5000, Training loss: 0.0056, Test loss: 17450.5117
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 17451.3359
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 17451.4941
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 17451.5039
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 17451.5039
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 17451.5059
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 17451.5039
In this epoch 2550/5000, Training loss: 0.4864, Test loss: 17422.8477
In this epoch 2600/5000, Training loss: 702.5712, Test loss: 19184.3945
In this epoch 2650/5000, Training loss: 0.2561, Test loss: 17514.0410
In this epoch 2700/5000, Training loss: 0.0036, Test loss: 17449.3047
In this epoch 2750/5000, Training loss: 0.0001, Test loss: 17451.6406
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 17451.4688
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 17451.4961
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 17451.5000
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 17451.5000
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 17451.4941
In this epoch 3050/5000, Training loss: 0.0114, Test loss: 17455.7656
In this epoch 3100/5000, Training loss: 1835.7307, Test loss: 18513.1953
In this epoch 3150/5000, Training loss: 1.0656, Test loss: 17377.9453
In this epoch 3200/5000, Training loss: 0.0507, Test loss: 17454.5098
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 17451.1836
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 17451.5273
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 17451.4961
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 17451.5000
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 17451.5938
In this epoch 3500/5000, Training loss: 23119.1035, Test loss: 40543.6289
In this epoch 3550/5000, Training loss: 104.8694, Test loss: 17685.9824
In this epoch 3600/5000, Training loss: 0.2584, Test loss: 17473.2949
In this epoch 3650/5000, Training loss: 0.0040, Test loss: 17453.1777
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 17451.4805
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 17451.4746
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 17451.4805
In this epoch 3850/5000, Training loss: 30659.8008, Test loss: 49810.5078
In this epoch 3900/5000, Training loss: 2.6202, Test loss: 17791.5898
In this epoch 3950/5000, Training loss: 0.3247, Test loss: 17444.6895
In this epoch 4000/5000, Training loss: 0.0044, Test loss: 17452.0508
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 17451.3438
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 17451.4863
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 17451.4980
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 17451.5000
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 17451.4512
In this epoch 4300/5000, Training loss: 66.5872, Test loss: 16994.1465
In this epoch 4350/5000, Training loss: 300.5341, Test loss: 17128.3887
In this epoch 4400/5000, Training loss: 1.3173, Test loss: 17471.2891
In this epoch 4450/5000, Training loss: 0.0096, Test loss: 17448.8379
In this epoch 4500/5000, Training loss: 0.0690, Test loss: 17442.6445
In this epoch 4550/5000, Training loss: 1572.8684, Test loss: 18695.9863
In this epoch 4600/5000, Training loss: 2.7169, Test loss: 17379.5527
In this epoch 4650/5000, Training loss: 0.0231, Test loss: 17457.8184
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 17451.2188
In this epoch 4750/5000, Training loss: 0.0164, Test loss: 17455.9922
In this epoch 4800/5000, Training loss: 2438.1240, Test loss: 17657.1172
In this epoch 4850/5000, Training loss: 7.2334, Test loss: 17478.6230
In this epoch 4900/5000, Training loss: 0.0764, Test loss: 17457.0449
In this epoch 4950/5000, Training loss: 0.0005, Test loss: 17451.9238
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 17451.4062
train_acc 1.0
test_acc 0.01764705882352941
{'y_pred': [523.8621826171875, 247.05691528320312, 158.64260864257812, 119.02847290039062, 122.77333068847656, 491.49444580078125, 133.9580078125, 489.449462890625, 270.49530029296875, 282.3173522949219, 444.66986083984375, 481.22552490234375, 492.43695068359375, 448.1370849609375, 267.4378356933594, 556.5227661132812, 252.74273681640625, 325.87762451171875, 502.1201171875, 444.3548583984375, 465.8939514160156, 190.3718719482422, 532.6908569335938, 428.68292236328125, 475.8525390625, 481.77215576171875, 206.7277069091797, 382.6337890625, 111.33186340332031, 275.5555419921875, 538.4368286132812, 239.20986938476562, 495.4942626953125, 147.05230712890625, 245.2218017578125, 397.14111328125, 105.9911880493164, 146.51986694335938, 525.1083374023438, 407.35821533203125, 283.0845947265625, 526.6190185546875, 306.7532653808594, 504.05145263671875, 520.9099731445312, 433.9281005859375, 471.276123046875, 109.57394409179688, 498.5215148925781, 122.73533630371094, 478.20196533203125, 558.1067504882812, 529.6682739257812, 512.3604736328125, 188.65689086914062, 120.9597396850586, 518.6957397460938, 476.1221008300781, 507.76654052734375, 432.06524658203125, 235.10629272460938, 158.33383178710938, 434.70526123046875, 841.00244140625, 207.40341186523438, 193.86129760742188, 148.45513916015625, 200.8633575439453, 509.20318603515625, 528.6163940429688, 228.8471221923828, 143.59237670898438, 164.7282257080078, 503.21795654296875, 104.23338317871094, 172.62628173828125, 216.61154174804688, 500.8959655761719, 328.8272705078125, 572.764892578125, 368.5953369140625, 216.5908203125, 498.18817138671875, 216.86480712890625, 507.1216735839844, 462.36767578125, 151.64459228515625, 313.67742919921875, 415.0426940917969, 454.09564208984375, 467.90380859375, 74.642333984375, 207.62326049804688, 429.0906066894531, 430.87860107421875, 525.3768310546875, 113.48904418945312, 524.1145629882812, 516.2310180664062, 356.1315002441406, 144.40725708007812, 391.2000732421875, 528.8212280273438, 455.55303955078125, 158.99371337890625, 473.0499267578125, 534.2493286132812, 244.99722290039062, 482.5218200683594, 460.2408447265625, 470.2317810058594, 346.4456481933594, 488.29888916015625, 203.70542907714844, 537.3148803710938, 496.2901611328125, 532.0504760742188, 271.59490966796875, 479.3118896484375, 467.9503479003906, 165.30233764648438, 162.4947967529297, 484.69488525390625, 479.2938232421875, 529.5734252929688, 105.80252075195312, 478.96331787109375, 294.5461120605469, 503.2991943359375, 496.38726806640625, 369.6968994140625, 519.9730224609375, 94.44712829589844, 249.434814453125, 175.58828735351562, 509.5841064453125, 544.097412109375, 550.6727294921875, 474.8674011230469, 181.86203002929688, 421.1935119628906, 96.03199768066406, 541.171142578125, 534.4443969726562, 137.95590209960938, 138.41868591308594, 471.0184020996094, 473.040771484375, 552.132080078125, 448.54937744140625, 202.23109436035156, 534.0609741210938, 463.438720703125, 442.2257080078125, 497.0400085449219, 303.7926025390625, 361.50244140625, 298.6128845214844, 499.6238708496094, 363.0133361816406, 421.0471496582031, 204.76931762695312, 86.27462768554688, 12.803784370422363, 276.4825744628906, 476.0992431640625, 369.11181640625, 346.6820373535156, 519.975341796875, 233.86212158203125, 471.0669250488281, 509.8914794921875, 263.53668212890625, 510.865234375, 476.00555419921875, 197.5227813720703, 216.9272003173828, 240.78477478027344, 216.57366943359375, 491.046630859375, 368.01068115234375, 547.3362426757812, 104.4039077758789, 84.30531311035156, 136.37353515625, 178.98385620117188, 557.2750854492188, 475.8892822265625, 496.5604248046875, 528.2593994140625, 522.0802612304688, 518.3336181640625, 214.56234741210938, 493.42138671875, 526.9435424804688, 138.3867645263672, 488.4198913574219, 427.33184814453125, 540.616455078125, 129.65045166015625, 498.7473449707031, 475.5985107421875, 53.4193115234375, 520.61376953125, 277.074462890625, 173.52427673339844, 452.11724853515625, 508.473876953125, 538.7289428710938, 408.8026123046875, 164.67758178710938, 238.91770935058594, 291.801513671875, 26.763235092163086, 506.5987854003906, 229.45700073242188, 194.6486358642578, 515.67138671875, 353.6724548339844, 289.2357177734375, 285.39801025390625, 398.1041564941406, 380.1037902832031, 513.2254028320312, 459.584228515625, 98.85641479492188, 467.7708435058594, 482.5944519042969, 194.30950927734375, 582.6829833984375, 552.2958374023438, 477.65692138671875, 522.3684692382812, 503.49859619140625, 289.08154296875, 495.7000732421875, 472.29119873046875, 128.408935546875, 330.0782470703125, 582.6213989257812, 543.61083984375, 99.38561248779297, 543.5377807617188, 499.22784423828125, 490.62890625, 130.8082275390625, 469.7080078125, 283.49798583984375, 203.91600036621094, 219.759521484375, 501.03643798828125, 182.4847412109375, 264.10107421875, 479.0400695800781, 130.8863067626953, 459.8330078125, 222.11117553710938, 92.2904052734375, 132.29421997070312, 392.665771484375, 181.7349853515625, 497.39727783203125, 193.9021453857422, 452.93170166015625, 107.40744018554688, 431.0479736328125, 211.13339233398438, 111.80133056640625, 544.6422119140625, 239.81033325195312, 117.7869873046875, 492.4844665527344, 147.77200317382812, 332.97503662109375, 432.419921875, 211.9708251953125, 586.8440551757812, 460.98114013671875, 199.4347381591797, 476.0538024902344, 475.6414489746094, 204.13174438476562, 433.28802490234375, 116.60453796386719, 509.7777099609375, 508.33966064453125, 239.85369873046875, 479.5509948730469, 277.79205322265625, 151.90634155273438, 462.02435302734375, 375.68505859375, 477.3143615722656, 59.924163818359375, 450.4727478027344, 517.9385375976562, 523.6589965820312, 162.57888793945312, 243.18072509765625, 520.14697265625, 492.105712890625, 538.9212036132812, 101.48612976074219, 476.52667236328125, 547.6043090820312, 156.9848175048828, 454.6344909667969, 388.27655029296875, 555.9959106445312, 525.3416748046875, 450.95233154296875, 131.8783416748047, 471.5140075683594, 221.62461853027344, 144.23480224609375, 491.59698486328125, 451.50921630859375, 514.6902465820312, 468.38446044921875, 284.9993896484375, 485.3894348144531, 95.15483093261719, 434.0226745605469, 449.5245666503906, 485.02557373046875, 167.54376220703125, 504.92669677734375, 195.4407958984375, 263.39404296875, 327.36712646484375, 498.4072570800781, 55.73626708984375, 492.8094787597656, 516.8497314453125, 345.8107604980469, 233.30322265625, 425.57562255859375, 105.47789001464844, 93.70956420898438, 106.67911529541016], 'y_pred_int': [524, 247, 159, 119, 123, 491, 134, 489, 270, 282, 445, 481, 492, 448, 267, 557, 253, 326, 502, 444, 466, 190, 533, 429, 476, 482, 207, 383, 111, 276, 538, 239, 495, 147, 245, 397, 106, 147, 525, 407, 283, 527, 307, 504, 521, 434, 471, 110, 499, 123, 478, 558, 530, 512, 189, 121, 519, 476, 508, 432, 235, 158, 435, 841, 207, 194, 148, 201, 509, 529, 229, 144, 165, 503, 104, 173, 217, 501, 329, 573, 369, 217, 498, 217, 507, 462, 152, 314, 415, 454, 468, 75, 208, 429, 431, 525, 113, 524, 516, 356, 144, 391, 529, 456, 159, 473, 534, 245, 483, 460, 470, 346, 488, 204, 537, 496, 532, 272, 479, 468, 165, 162, 485, 479, 530, 106, 479, 295, 503, 496, 370, 520, 94, 249, 176, 510, 544, 551, 475, 182, 421, 96, 541, 534, 138, 138, 471, 473, 552, 449, 202, 534, 463, 442, 497, 304, 362, 299, 500, 363, 421, 205, 86, 13, 276, 476, 369, 347, 520, 234, 471, 510, 264, 511, 476, 198, 217, 241, 217, 491, 368, 547, 104, 84, 136, 179, 557, 476, 497, 528, 522, 518, 215, 493, 527, 138, 488, 427, 541, 130, 499, 476, 53, 521, 277, 174, 452, 508, 539, 409, 165, 239, 292, 27, 507, 229, 195, 516, 354, 289, 285, 398, 380, 513, 460, 99, 468, 483, 194, 583, 552, 478, 522, 503, 289, 496, 472, 128, 330, 583, 544, 99, 544, 499, 491, 131, 470, 283, 204, 220, 501, 182, 264, 479, 131, 460, 222, 92, 132, 393, 182, 497, 194, 453, 107, 431, 211, 112, 545, 240, 118, 492, 148, 333, 432, 212, 587, 461, 199, 476, 476, 204, 433, 117, 510, 508, 240, 480, 278, 152, 462, 376, 477, 60, 450, 518, 524, 163, 243, 520, 492, 539, 101, 477, 548, 157, 455, 388, 556, 525, 451, 132, 472, 222, 144, 492, 452, 515, 468, 285, 485, 95, 434, 450, 485, 168, 505, 195, 263, 327, 498, 56, 493, 517, 346, 233, 426, 105, 94, 107], 'y_true': [705, 217, 141, 142, 125, 385, 172, 486, 205, 235, 347, 599, 715, 380, 258, 528, 263, 301, 619, 775, 708, 161, 693, 328, 292, 417, 201, 324, 121, 256, 459, 200, 428, 130, 280, 297, 112, 165, 601, 333, 251, 514, 270, 507, 533, 413, 562, 103, 562, 122, 392, 1060, 657, 644, 193, 117, 711, 407, 505, 373, 212, 164, 351, 840, 194, 207, 158, 175, 824, 447, 274, 145, 180, 547, 106, 150, 237, 583, 313, 733, 316, 186, 344, 241, 577, 420, 191, 279, 326, 384, 292, 65, 198, 370, 302, 511, 128, 689, 596, 302, 161, 350, 577, 334, 134, 436, 628, 220, 572, 347, 756, 298, 488, 197, 548, 332, 821, 243, 610, 361, 184, 117, 417, 660, 1026, 99, 901, 281, 678, 456, 331, 773, 113, 258, 175, 544, 473, 1297, 412, 225, 294, 109, 905, 641, 121, 157, 536, 367, 367, 509, 201, 477, 481, 699, 422, 285, 235, 273, 605, 321, 633, 163, 93, 37, 270, 462, 283, 266, 513, 258, 815, 846, 240, 911, 874, 204, 286, 266, 184, 661, 358, 770, 102, 84, 155, 187, 718, 1238, 633, 561, 686, 396, 200, 474, 481, 156, 371, 569, 701, 109, 523, 490, 45, 334, 264, 172, 443, 621, 765, 506, 181, 208, 249, 32, 373, 193, 188, 452, 324, 254, 243, 350, 327, 403, 441, 131, 427, 350, 227, 980, 655, 502, 429, 372, 235, 611, 842, 89, 246, 863, 542, 76, 507, 378, 385, 138, 934, 277, 205, 163, 396, 208, 291, 369, 113, 450, 158, 89, 123, 337, 155, 487, 181, 709, 82, 574, 214, 121, 442, 244, 118, 493, 101, 296, 456, 200, 502, 384, 167, 476, 370, 187, 438, 100, 589, 363, 239, 471, 262, 143, 446, 296, 355, 90, 375, 727, 445, 173, 233, 706, 656, 614, 80, 707, 621, 122, 322, 295, 934, 458, 350, 134, 425, 239, 156, 448, 435, 381, 656, 268, 389, 88, 546, 351, 723, 158, 593, 204, 237, 264, 603, 57, 407, 799, 334, 230, 329, 110, 108, 88], 'train_acc': 1.0, 'test_acc': 0.01764705882352941, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_1_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 6562.4702, Test loss: 1023.4294
In this epoch 100/5000, Training loss: 8.9073, Test loss: 430.2333
In this epoch 150/5000, Training loss: 0.0126, Test loss: 255.7867
In this epoch 200/5000, Training loss: 0.0042, Test loss: 258.4211
In this epoch 250/5000, Training loss: 0.0006, Test loss: 258.6765
In this epoch 300/5000, Training loss: 0.0005, Test loss: 258.6703
In this epoch 350/5000, Training loss: 0.0004, Test loss: 258.6711
In this epoch 400/5000, Training loss: 0.0003, Test loss: 258.6700
In this epoch 450/5000, Training loss: 0.0002, Test loss: 258.6689
In this epoch 500/5000, Training loss: 0.0002, Test loss: 258.6678
In this epoch 550/5000, Training loss: 0.0001, Test loss: 258.6667
In this epoch 600/5000, Training loss: 0.0001, Test loss: 258.6658
In this epoch 650/5000, Training loss: 0.0001, Test loss: 258.6650
In this epoch 700/5000, Training loss: 0.0001, Test loss: 258.6643
In this epoch 750/5000, Training loss: 0.0000, Test loss: 258.6637
In this epoch 800/5000, Training loss: 0.0000, Test loss: 258.6631
In this epoch 850/5000, Training loss: 0.0000, Test loss: 258.6626
In this epoch 900/5000, Training loss: 0.0000, Test loss: 258.6622
In this epoch 950/5000, Training loss: 0.0000, Test loss: 258.6620
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 258.6616
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 258.6613
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 258.6611
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 258.6611
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 258.6609
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 258.6609
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 258.6607
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 258.6606
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 258.6606
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 258.6606
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 258.6606
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 258.6606
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 258.6605
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 258.6605
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 258.6605
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 258.6605
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 258.6605
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 258.6605
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 258.6605
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 258.6605
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 258.6605
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 258.6605
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 258.6605
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 258.6604
In this epoch 4400/5000, Training loss: 266558.6875, Test loss: 538976.9375
In this epoch 4450/5000, Training loss: 1093.1444, Test loss: 437.1667
In this epoch 4500/5000, Training loss: 15.5323, Test loss: 263.0975
In this epoch 4550/5000, Training loss: 0.0776, Test loss: 268.0346
In this epoch 4600/5000, Training loss: 0.0004, Test loss: 268.9900
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 269.0466
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 269.0466
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 269.0488
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 269.0506
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 269.0523
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 269.0538
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 269.0551
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 269.0563
train_acc 1.0
test_acc 0.018209342560553635
{'y_pred': [24.584821701049805, 3.5596461296081543, -4.259435176849365, 1.8431601524353027, 4.671451091766357, 23.005407333374023, -5.494689464569092, 27.172515869140625, 40.334964752197266, 26.178306579589844, -2.8313660621643066, 3.5816855430603027, 16.05380630493164, 10.763519287109375, -5.6824822425842285, 38.821563720703125, 4.045779705047607, 9.495159149169922, 3.2717652320861816, 43.594642639160156, 19.8450927734375, 5.958653926849365, -4.784822940826416, 17.983959197998047, 21.279834747314453, 0.194430872797966, 0.0532875582575798, 37.46281814575195, 27.596391677856445, 31.77395248413086, 36.93708419799805, -5.452137470245361, 15.022442817687988, -7.206199645996094, 18.91570472717285, 11.153741836547852, 36.908424377441406, 4.6605544090271, 11.284074783325195, -8.571617126464844, 21.81673812866211, 12.269420623779297, 36.34514617919922, 35.4263916015625, 16.252994537353516, 25.13136100769043, 4.695641994476318, 15.538318634033203, -3.1329751014709473, 50.97156524658203, -35.32452392578125, -4.2010579109191895, 14.985160827636719, -19.14539909362793, 4.0371270179748535, 31.434648513793945, 20.182907104492188, 2.953761577606201, -16.98212432861328, 14.353124618530273, 1.5903334617614746, 4.419878005981445, 12.310506820678711, 39.687408447265625, 12.922563552856445, 27.311105728149414, 28.532514572143555, -5.464534282684326, 4.203999042510986, 2.256701946258545, 47.39113235473633, 27.710718154907227, -8.959890365600586, 18.579708099365234, -8.512916564941406, -24.9940185546875, -4.341249942779541, 20.036449432373047, 20.2545166015625, 3.921382427215576, 22.469911575317383, -6.180775165557861, 30.35370635986328, -15.87863540649414, -7.202439785003662, 31.95749282836914, 17.126127243041992, -1.921947956085205, -4.687729358673096, -7.114670276641846, 19.20114517211914, 2.430476665496826, 7.360955715179443, 42.806907653808594, 42.60823059082031, 23.72491455078125, 10.297435760498047, 10.696895599365234, -7.8890461921691895, 13.353340148925781, 34.935943603515625, -3.423067808151245, 30.156482696533203, 8.279232025146484, 6.942553997039795, 2.714587688446045, -0.188567116856575, -3.820253849029541, 20.014442443847656, 38.26994705200195, 2.3249282836914062, 36.823997497558594, 19.663658142089844, -12.39482307434082, 35.60651779174805, -17.710609436035156, 5.871337413787842, -14.807809829711914, 8.080196380615234, 38.59931945800781, 4.819211483001709, -8.169275283813477, -4.9984564781188965, 0.6873851418495178, 13.108476638793945, 47.69377136230469, 8.406475067138672, 3.441636562347412, 61.91999816894531, -10.88058090209961, 11.861467361450195, 1.5014376640319824, -17.080394744873047, 23.06442642211914, 20.509746551513672, 6.841342449188232, 28.20662498474121, 30.34883689880371, 15.000001907348633, 25.426740646362305, -4.908085823059082, 6.587684154510498, -13.514059066772461, 25.452983856201172, -22.929967880249023, 5.213443279266357, 3.444793224334717, 22.82108497619629, 25.201852798461914, -5.99747896194458, 5.869837760925293, -5.083691120147705, 33.140769958496094, -18.653583526611328, -0.6878246665000916, 23.47730827331543, 0.7196441292762756, 32.98066329956055, 14.817585945129395, 4.454676151275635, 35.05931854248047, 31.79684066772461, -0.15141339600086212, -4.905154705047607, 1.5225310325622559, -0.3085255026817322, -6.550316333770752, 30.567123413085938, 41.576385498046875, 16.129892349243164, 0.690822184085846, 13.0411376953125, 3.1379542350769043, 0.5694041848182678, 30.206005096435547, 30.815711975097656, -2.5602498054504395, 6.435818672180176, 9.59304428100586, -13.388313293457031, 19.037353515625, -19.077322006225586, 6.0579705238342285, 14.327156066894531, 24.654483795166016, 3.5776267051696777, -1.241936206817627, 22.12251091003418, 6.264675617218018, 21.093530654907227, -16.90826416015625, -25.38910675048828, -3.942810535430908, 16.60883140563965, 25.715147018432617, 6.758580684661865, 9.795967102050781, 0.17667917907238007, -27.108436584472656, -12.900493621826172, 4.052981853485107, 17.596763610839844, 25.726539611816406, 35.212547302246094, 13.268129348754883, 31.522085189819336, -3.1239571571350098, 10.948783874511719, 16.84597396850586, 26.643953323364258, -1.0576558113098145, 19.840660095214844, -6.632212162017822, -2.921835422515869, 29.212631225585938, 19.936307907104492, 3.3544440269470215, 12.47567367553711, -9.563565254211426, -10.671607971191406, 3.9170289039611816, -10.74367904663086, 11.563800811767578, 35.665611267089844, -8.410076141357422, 17.32474708557129, 29.87047576904297, -5.612148761749268, 27.09889030456543, -13.755935668945312, 19.695831298828125, 12.985258102416992, 25.507410049438477, 16.90345001220703, 21.91079330444336, 20.137956619262695, 20.05128288269043, 7.493622303009033, -21.800004959106445, -3.319183826446533, -11.978578567504883, 11.578580856323242, 33.41798782348633, 49.49732971191406, 26.953174591064453, -1.8294949531555176, -8.041492462158203, 30.49326515197754, -2.686516284942627, 37.734249114990234, -1.6350598335266113, -2.3396401405334473, 9.071585655212402, 11.438028335571289, 5.609645366668701, 38.130592346191406, 24.06021499633789, 28.95604133605957, 0.40137726068496704, 9.487995147705078, -22.658288955688477, 3.1664481163024902, 9.128763198852539, 7.903802394866943, 14.90163803100586, 7.4315409660339355, -2.524787187576294, 1.7154021263122559, 1.903120517730713, 17.049026489257812, 10.892223358154297, 17.047016143798828, 7.190700054168701, 14.148574829101562, -14.799230575561523, 9.584627151489258, -20.865198135375977, 29.999393463134766, 1.5719008445739746, 28.89599609375, -15.494782447814941, 41.22402572631836, -1.261974811553955, 51.54958724975586, 17.45853614807129, 14.175865173339844, 5.008517742156982, 22.43887710571289, 42.94499969482422, 19.00074577331543, 5.847157001495361, 40.01310729980469, 47.21965789794922, -7.262144565582275, 18.4576473236084, 32.769859313964844, 10.067205429077148, 41.739646911621094, 37.023048400878906, 7.466860294342041, 24.349294662475586, -10.585020065307617, 32.45609664916992, -5.008360385894775, -4.438686847686768, -24.72329330444336, 10.250164031982422, 2.6325716972351074, 63.105308532714844, 13.343955039978027, 40.1547737121582, 28.870431900024414, 10.175271987915039, 29.385894775390625, 34.24342346191406, 35.000221252441406, 19.975521087646484, 22.552711486816406, -0.8642944693565369, -9.767280578613281, 30.71242904663086, 13.164360046386719, 0.10345416516065598, 26.228681564331055, 14.215555191040039, 3.246997356414795, 37.45967483520508, 31.685029983520508, -9.510831832885742, 0.9695163369178772, -2.817901849746704, 30.20362091064453, 41.86279296875, 37.09613037109375, -16.279159545898438, 2.7499537467956543, 9.08791732788086, 22.26665496826172, -1.8567051887512207, 1.7599730491638184], 'y_pred_int': [25, 4, 0, 2, 5, 23, 0, 27, 40, 26, 0, 4, 16, 11, 0, 39, 4, 9, 3, 44, 20, 6, 0, 18, 21, 0, 0, 37, 28, 32, 37, 0, 15, 0, 19, 11, 37, 5, 11, 0, 22, 12, 36, 35, 16, 25, 5, 16, 0, 51, 0, 0, 15, 0, 4, 31, 20, 3, 0, 14, 2, 4, 12, 40, 13, 27, 29, 0, 4, 2, 47, 28, 0, 19, 0, 0, 0, 20, 20, 4, 22, 0, 30, 0, 0, 32, 17, 0, 0, 0, 19, 2, 7, 43, 43, 24, 10, 11, 0, 13, 35, 0, 30, 8, 7, 3, 0, 0, 20, 38, 2, 37, 20, 0, 36, 0, 6, 0, 8, 39, 5, 0, 0, 1, 13, 48, 8, 3, 62, 0, 12, 2, 0, 23, 21, 7, 28, 30, 15, 25, 0, 7, 0, 25, 0, 5, 3, 23, 25, 0, 6, 0, 33, 0, 0, 23, 1, 33, 15, 4, 35, 32, 0, 0, 2, 0, 0, 31, 42, 16, 1, 13, 3, 1, 30, 31, 0, 6, 10, 0, 19, 0, 6, 14, 25, 4, 0, 22, 6, 21, 0, 0, 0, 17, 26, 7, 10, 0, 0, 0, 4, 18, 26, 35, 13, 32, 0, 11, 17, 27, 0, 20, 0, 0, 29, 20, 3, 12, 0, 0, 4, 0, 12, 36, 0, 17, 30, 0, 27, 0, 20, 13, 26, 17, 22, 20, 20, 7, 0, 0, 0, 12, 33, 49, 27, 0, 0, 30, 0, 38, 0, 0, 9, 11, 6, 38, 24, 29, 0, 9, 0, 3, 9, 8, 15, 7, 0, 2, 2, 17, 11, 17, 7, 14, 0, 10, 0, 30, 2, 29, 0, 41, 0, 52, 17, 14, 5, 22, 43, 19, 6, 40, 47, 0, 18, 33, 10, 42, 37, 7, 24, 0, 32, 0, 0, 0, 10, 3, 63, 13, 40, 29, 10, 29, 34, 35, 20, 23, 0, 0, 31, 13, 0, 26, 14, 3, 37, 32, 0, 1, 0, 30, 42, 37, 0, 3, 9, 22, 0, 2], 'y_true': [8, 11, 13, 11, 4, 10, 18, 24, 29, 18, 1, 8, 22, 6, 6, 16, 7, 3, 11, 18, 8, 5, 9, 20, 6, 2, 21, 8, 9, 14, 11, 3, 9, 13, 10, 16, 50, 4, 35, 6, 11, 6, 20, 20, 19, 12, 14, 6, 7, 25, 6, 12, 6, 14, 15, 23, 19, 38, 8, 15, 31, 11, 11, 42, 2, 17, 5, 11, 14, 7, 9, 15, 2, 8, 8, 10, 5, 10, 28, 6, 11, 4, 18, 14, 14, 22, 7, 12, 6, 9, 19, 5, 4, 15, 13, 33, 6, 13, 7, 5, 11, 34, 23, 9, 8, 6, 11, 6, 12, 14, 21, 15, 18, 6, 11, 9, 3, 9, 6, 16, 21, 5, 6, 3, 9, 26, 7, 3, 13, 12, 20, 11, 8, 14, 17, 14, 12, 21, 15, 47, 14, 2, 10, 19, 30, 4, 7, 9, 12, 6, 19, 7, 19, 8, 9, 23, 5, 15, 19, 22, 12, 9, 8, 8, 5, 7, 7, 15, 14, 66, 7, 9, 4, 8, 17, 25, 9, 18, 22, 7, 21, 10, 20, 9, 34, 8, 8, 25, 7, 26, 16, 7, 7, 25, 10, 8, 8, 6, 8, 7, 8, 39, 10, 28, 5, 32, 9, 9, 33, 15, 3, 14, 8, 11, 37, 24, 8, 9, 59, 5, 13, 5, 3, 23, 5, 4, 13, 7, 18, 7, 21, 22, 9, 11, 22, 23, 12, 13, 5, 4, 6, 6, 22, 13, 13, 2, 11, 10, 6, 25, 5, 13, 36, 21, 3, 17, 15, 21, 6, 22, 15, 11, 11, 7, 3, 10, 10, 7, 13, 11, 6, 7, 9, 12, 8, 8, 9, 11, 3, 13, 11, 26, 5, 16, 8, 7, 6, 4, 19, 10, 22, 12, 23, 6, 17, 12, 10, 27, 19, 5, 11, 9, 9, 5, 11, 8, 8, 6, 28, 16, 10, 11, 10, 29, 44, 10, 16, 19, 6, 4, 21, 9, 4, 13, 51, 5, 27, 21, 7, 4, 20, 5, 16, 32, 9, 3, 2, 27, 7, 4], 'train_acc': 1.0, 'test_acc': 0.018209342560553635, 'X': 'X_TANL_layer_1_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_2_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 36225.5820, Test loss: 7437.5205
In this epoch 100/5000, Training loss: 225.9742, Test loss: 288.8128
In this epoch 150/5000, Training loss: 1.6400, Test loss: 287.4468
In this epoch 200/5000, Training loss: 0.0097, Test loss: 282.6687
In this epoch 250/5000, Training loss: 0.0003, Test loss: 282.3871
In this epoch 300/5000, Training loss: 0.0002, Test loss: 282.4009
In this epoch 350/5000, Training loss: 0.0002, Test loss: 282.4027
In this epoch 400/5000, Training loss: 0.0002, Test loss: 282.4036
In this epoch 450/5000, Training loss: 0.0001, Test loss: 282.4045
In this epoch 500/5000, Training loss: 0.0001, Test loss: 282.4055
In this epoch 550/5000, Training loss: 0.0001, Test loss: 282.4063
In this epoch 600/5000, Training loss: 0.0001, Test loss: 282.4072
In this epoch 650/5000, Training loss: 0.0000, Test loss: 282.4079
In this epoch 700/5000, Training loss: 0.0000, Test loss: 282.4086
In this epoch 750/5000, Training loss: 0.0000, Test loss: 282.4094
In this epoch 800/5000, Training loss: 0.0000, Test loss: 282.4100
In this epoch 850/5000, Training loss: 0.0000, Test loss: 282.4104
In this epoch 900/5000, Training loss: 0.0000, Test loss: 282.4110
In this epoch 950/5000, Training loss: 0.0000, Test loss: 282.4115
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 282.4118
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 282.4122
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 282.4124
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 282.4126
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 282.4128
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 282.4131
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 282.4131
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 282.4132
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 282.4132
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 282.4133
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 282.4134
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 282.4136
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 282.4136
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 282.4136
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 282.4136
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 282.4136
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 282.4136
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 282.4136
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 282.4135
In this epoch 4200/5000, Training loss: 41131.9727, Test loss: 587275.2500
In this epoch 4250/5000, Training loss: 5540.5303, Test loss: 396.8626
In this epoch 4300/5000, Training loss: 38.5350, Test loss: 293.7122
In this epoch 4350/5000, Training loss: 0.1490, Test loss: 279.6421
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 279.4426
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 279.5249
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 279.5304
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 279.5304
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 279.5303
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 279.5302
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 279.5299
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 279.5298
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 279.5298
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 279.5297
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 279.5296
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 279.5296
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 279.5295
train_acc 1.0
test_acc 0.020224913494809687
{'y_pred': [-4.1989521980285645, -11.998824119567871, 49.12593460083008, 11.114043235778809, 15.395315170288086, 33.38325119018555, -2.1502578258514404, 57.33903503417969, -1.0951504707336426, 16.350406646728516, 21.178070068359375, 20.985088348388672, 18.895544052124023, 36.428443908691406, 31.09210205078125, -3.351241111755371, 48.289485931396484, -3.434650421142578, 22.209854125976562, 43.393802642822266, 28.085742950439453, 5.909987449645996, 34.529624938964844, -18.009267807006836, -6.542844295501709, 12.377492904663086, -19.46000862121582, 24.994003295898438, 28.494522094726562, 27.438474655151367, 28.999954223632812, 11.661931991577148, -20.654556274414062, 12.035473823547363, 20.349353790283203, 7.114856243133545, 28.36180877685547, 7.367313861846924, 5.336817741394043, 10.701980590820312, 17.774742126464844, 27.940086364746094, 9.21322250366211, 25.099044799804688, 22.892454147338867, -11.724770545959473, 9.863460540771484, 5.494259357452393, 25.42704200744629, -14.820186614990234, 7.483811378479004, -4.40252685546875, 17.36252212524414, 10.31135368347168, 41.687286376953125, 21.718154907226562, 7.9144463539123535, 0.20214562118053436, 40.13557052612305, -3.321566581726074, 39.07223129272461, 31.306169509887695, 10.049461364746094, 7.881527900695801, 32.129817962646484, 4.606259346008301, -15.83426570892334, 12.966060638427734, 8.098617553710938, 2.325484275817871, 6.000067234039307, 2.9965896606445312, 5.610434532165527, 2.0948123931884766, 9.16452407836914, 11.609804153442383, 9.619489669799805, 11.751161575317383, 21.158096313476562, -31.5919189453125, 38.24543380737305, 37.148563385009766, 16.039939880371094, 51.529293060302734, -6.648216724395752, -1.8053934574127197, 52.487159729003906, 12.345888137817383, 15.989629745483398, 0.1836901158094406, 25.52517318725586, -13.133549690246582, -12.254883766174316, -13.448887825012207, 0.7612181305885315, 2.064849853515625, -10.463996887207031, 17.50597381591797, 22.8448429107666, 10.525618553161621, 4.731998920440674, 20.805749893188477, -21.34515953063965, 20.619964599609375, 4.069177627563477, 36.330379486083984, -5.64815092086792, 3.756288528442383, -4.3374457359313965, -19.526758193969727, -4.082633972167969, -6.741762161254883, -0.10562510043382645, -15.993471145629883, 2.977400302886963, 7.301142692565918, 2.2349815368652344, 10.489571571350098, 6.451030731201172, -9.145689010620117, -18.435617446899414, -1.7097749710083008, 13.257094383239746, 13.855789184570312, -1.7236216068267822, 35.54815673828125, 13.994450569152832, 28.830547332763672, 9.160260200500488, 2.6013975143432617, 33.41734313964844, 4.510886192321777, 21.014694213867188, -14.327882766723633, 26.010753631591797, 37.13222122192383, 32.137115478515625, 34.755428314208984, 23.534210205078125, 0.5627947449684143, 18.88623046875, 6.123653411865234, 2.7259209156036377, 6.026864051818848, 12.260835647583008, 3.5444416999816895, 9.289421081542969, -10.634431838989258, -3.9827795028686523, 16.196420669555664, -0.7608603835105896, 10.565224647521973, 10.494327545166016, 14.955093383789062, -8.889902114868164, 45.15396499633789, 19.035076141357422, 13.914453506469727, -0.8212422728538513, -3.3087501525878906, -2.5342445373535156, 2.8608884811401367, -1.4481992721557617, -9.905494689941406, 4.137791633605957, -0.9552411437034607, 25.140987396240234, 31.69052505493164, 2.053034543991089, -20.486661911010742, -6.08294677734375, 20.470508575439453, 19.467504501342773, -13.299288749694824, 31.835205078125, 13.620548248291016, 26.58279037475586, -3.6448144912719727, 10.556259155273438, -12.983019828796387, 13.68724536895752, 17.21874237060547, 30.671186447143555, -0.05546946078538895, 19.130205154418945, 38.20389938354492, 26.55586814880371, 10.37248706817627, -13.553424835205078, 14.291529655456543, 40.36496353149414, 37.90059280395508, 5.190028667449951, 18.767724990844727, 11.068065643310547, 17.52472686767578, 18.31380844116211, -10.438199043273926, -5.691286087036133, 3.375901222229004, 20.55906105041504, 37.999061584472656, 0.5321634411811829, -0.12817950546741486, 19.40921401977539, -1.041513442993164, 30.566265106201172, 10.927041053771973, 69.58523559570312, 43.266563415527344, -21.718645095825195, 38.30770492553711, 43.43113327026367, 10.437950134277344, 19.721498489379883, 0.439761221408844, 10.229415893554688, 2.2926597595214844, 12.91011905670166, 17.371002197265625, 8.634639739990234, -6.446735382080078, -2.4592337608337402, 19.465639114379883, 14.73929214477539, -4.985487937927246, -9.80395793914795, 17.233083724975586, -11.24868392944336, 17.561737060546875, 21.152240753173828, 22.3023681640625, 2.050830364227295, 0.39839714765548706, 26.885000228881836, 4.547483921051025, 14.633058547973633, -3.759230136871338, 28.014026641845703, 14.280343055725098, 20.45551872253418, 3.2620511054992676, 25.071548461914062, 5.726447105407715, 0.8741133809089661, 17.49673843383789, 10.830592155456543, 27.80769920349121, 19.940574645996094, -8.008315086364746, 59.49860763549805, -8.384170532226562, -10.142212867736816, 13.644309043884277, -3.146242141723633, 4.833045959472656, 37.561798095703125, 25.112167358398438, 23.02283477783203, 20.23093032836914, -1.657404899597168, 19.262969970703125, 12.52682113647461, 10.617175102233887, 13.517402648925781, -8.504486083984375, -13.753061294555664, 13.849373817443848, 46.23290252685547, 40.058082580566406, 39.84406661987305, 1.5459098815917969, 22.223115921020508, 12.699956893920898, 9.364546775817871, 22.479482650756836, -8.808757781982422, 1.489527702331543, 9.02280044555664, -28.28316879272461, 34.81509780883789, 23.773942947387695, 31.285030364990234, 13.50596809387207, -11.50967788696289, -7.419540882110596, 4.408918380737305, 4.702357292175293, 0.1962624043226242, 18.129863739013672, 37.18535232543945, 33.58561325073242, 7.45306396484375, 13.415742874145508, -10.917505264282227, 17.2930908203125, 22.96951675415039, 6.565274238586426, 14.900802612304688, 47.09251022338867, 25.41182518005371, 30.1207275390625, 11.524104118347168, 54.75387191772461, -1.2917404174804688, 16.932640075683594, 22.319496154785156, 9.77022933959961, -10.068550109863281, -9.685040473937988, -13.199804306030273, 1.7471814155578613, -0.26007360219955444, 8.76563549041748, 19.507225036621094, -34.05542755126953, 23.53603744506836, 31.344921112060547, -7.8762969970703125, 60.161190032958984, 18.63032341003418, 25.83525848388672, -10.340463638305664, -5.241318225860596, 14.811742782592773, 1.5084962844848633, 20.627716064453125, -7.471564769744873, -17.795822143554688, 51.234378814697266, 27.257699966430664, 17.398754119873047, -16.06805419921875, 0.981570303440094, 27.175434112548828, 30.258777618408203, 14.453454971313477, -9.50178337097168, 2.2308998107910156, 28.35346794128418], 'y_pred_int': [0, 0, 49, 11, 15, 33, 0, 57, 0, 16, 21, 21, 19, 36, 31, 0, 48, 0, 22, 43, 28, 6, 35, 0, 0, 12, 0, 25, 28, 27, 29, 12, 0, 12, 20, 7, 28, 7, 5, 11, 18, 28, 9, 25, 23, 0, 10, 5, 25, 0, 7, 0, 17, 10, 42, 22, 8, 0, 40, 0, 39, 31, 10, 8, 32, 5, 0, 13, 8, 2, 6, 3, 6, 2, 9, 12, 10, 12, 21, 0, 38, 37, 16, 52, 0, 0, 52, 12, 16, 0, 26, 0, 0, 0, 1, 2, 0, 18, 23, 11, 5, 21, 0, 21, 4, 36, 0, 4, 0, 0, 0, 0, 0, 0, 3, 7, 2, 10, 6, 0, 0, 0, 13, 14, 0, 36, 14, 29, 9, 3, 33, 5, 21, 0, 26, 37, 32, 35, 24, 1, 19, 6, 3, 6, 12, 4, 9, 0, 0, 16, 0, 11, 10, 15, 0, 45, 19, 14, 0, 0, 0, 3, 0, 0, 4, 0, 25, 32, 2, 0, 0, 20, 19, 0, 32, 14, 27, 0, 11, 0, 14, 17, 31, 0, 19, 38, 27, 10, 0, 14, 40, 38, 5, 19, 11, 18, 18, 0, 0, 3, 21, 38, 1, 0, 19, 0, 31, 11, 70, 43, 0, 38, 43, 10, 20, 0, 10, 2, 13, 17, 9, 0, 0, 19, 15, 0, 0, 17, 0, 18, 21, 22, 2, 0, 27, 5, 15, 0, 28, 14, 20, 3, 25, 6, 1, 17, 11, 28, 20, 0, 59, 0, 0, 14, 0, 5, 38, 25, 23, 20, 0, 19, 13, 11, 14, 0, 0, 14, 46, 40, 40, 2, 22, 13, 9, 22, 0, 1, 9, 0, 35, 24, 31, 14, 0, 0, 4, 5, 0, 18, 37, 34, 7, 13, 0, 17, 23, 7, 15, 47, 25, 30, 12, 55, 0, 17, 22, 10, 0, 0, 0, 2, 0, 9, 20, 0, 24, 31, 0, 60, 19, 26, 0, 0, 15, 2, 21, 0, 0, 51, 27, 17, 0, 1, 27, 30, 14, 0, 2, 28], 'y_true': [4, 15, 10, 4, 4, 12, 9, 16, 5, 7, 4, 21, 10, 15, 22, 8, 23, 23, 10, 13, 21, 32, 12, 6, 4, 9, 9, 14, 16, 40, 29, 4, 4, 7, 18, 22, 60, 7, 6, 12, 16, 23, 6, 36, 27, 8, 8, 5, 6, 5, 4, 25, 9, 12, 18, 4, 2, 15, 48, 2, 46, 19, 8, 8, 26, 6, 15, 13, 6, 8, 11, 3, 8, 4, 11, 15, 4, 11, 46, 6, 2, 11, 38, 16, 13, 4, 17, 7, 18, 7, 15, 10, 5, 13, 5, 14, 5, 10, 10, 21, 8, 5, 15, 9, 5, 30, 6, 10, 22, 11, 9, 7, 16, 7, 5, 4, 9, 5, 20, 5, 11, 20, 10, 9, 13, 16, 23, 13, 5, 9, 19, 47, 8, 6, 10, 31, 23, 17, 6, 8, 5, 5, 13, 9, 36, 8, 6, 6, 16, 5, 26, 4, 14, 6, 2, 28, 17, 6, 4, 4, 16, 21, 9, 8, 12, 2, 10, 11, 10, 6, 4, 10, 21, 40, 37, 3, 27, 7, 3, 6, 33, 14, 8, 5, 10, 14, 35, 10, 5, 10, 39, 10, 7, 10, 5, 17, 1, 5, 8, 9, 28, 9, 5, 2, 11, 4, 17, 11, 14, 12, 10, 22, 29, 7, 17, 37, 14, 8, 5, 25, 18, 11, 7, 36, 28, 12, 5, 12, 30, 17, 12, 10, 25, 11, 15, 6, 29, 5, 31, 6, 10, 9, 25, 18, 4, 14, 8, 18, 20, 14, 18, 4, 5, 13, 8, 6, 17, 9, 10, 16, 19, 16, 13, 5, 9, 7, 6, 8, 20, 13, 11, 2, 8, 19, 10, 25, 22, 20, 7, 22, 27, 13, 25, 6, 4, 4, 5, 7, 17, 25, 14, 46, 41, 42, 3, 15, 12, 17, 4, 28, 17, 8, 17, 23, 16, 19, 6, 9, 9, 8, 7, 16, 7, 6, 13, 15, 8, 27, 6, 8, 16, 13, 1, 6, 11, 7, 14, 6, 14, 16, 11, 13, 7, 15, 19, 25, 6, 4, 11, 15], 'train_acc': 1.0, 'test_acc': 0.020224913494809687, 'X': 'X_TANL_layer_2_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_3_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 76153.4844, Test loss: 34151.0039
In this epoch 100/5000, Training loss: 480.3477, Test loss: 713.3394
In this epoch 150/5000, Training loss: 0.0023, Test loss: 385.3417
In this epoch 200/5000, Training loss: 0.0045, Test loss: 383.9718
In this epoch 250/5000, Training loss: 0.0001, Test loss: 383.9708
In this epoch 300/5000, Training loss: 0.0000, Test loss: 383.9722
In this epoch 350/5000, Training loss: 0.0000, Test loss: 383.9736
In this epoch 400/5000, Training loss: 0.0000, Test loss: 383.9747
In this epoch 450/5000, Training loss: 0.0000, Test loss: 383.9759
In this epoch 500/5000, Training loss: 0.0000, Test loss: 383.9767
In this epoch 550/5000, Training loss: 0.0000, Test loss: 383.9776
In this epoch 600/5000, Training loss: 0.0000, Test loss: 383.9782
In this epoch 650/5000, Training loss: 0.0000, Test loss: 383.9789
In this epoch 700/5000, Training loss: 0.0000, Test loss: 383.9792
In this epoch 750/5000, Training loss: 0.0000, Test loss: 383.9795
In this epoch 800/5000, Training loss: 0.0000, Test loss: 383.9799
In this epoch 850/5000, Training loss: 0.0000, Test loss: 383.9800
In this epoch 900/5000, Training loss: 0.0000, Test loss: 383.9802
In this epoch 950/5000, Training loss: 0.0000, Test loss: 383.9803
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 383.9804
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 383.9804
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 383.9804
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 383.9805
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 383.9805
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 383.9805
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 383.9805
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 383.9805
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 383.9805
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 383.9805
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 383.9806
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 383.9805
In this epoch 2600/5000, Training loss: 10912.1885, Test loss: 40706.4375
In this epoch 2650/5000, Training loss: 121486.8281, Test loss: 63585.6523
In this epoch 2700/5000, Training loss: 209.3837, Test loss: 1030.6486
In this epoch 2750/5000, Training loss: 2.3932, Test loss: 388.1001
In this epoch 2800/5000, Training loss: 0.0134, Test loss: 385.7978
In this epoch 2850/5000, Training loss: 0.0001, Test loss: 385.7982
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 385.7984
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 385.7984
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 385.7984
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 385.7985
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 385.7984
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 385.7984
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 385.7985
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 385.7985
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 385.7985
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 385.7985
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 385.7985
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 385.7985
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 385.7985
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 385.7985
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 385.7985
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 385.7986
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 385.7986
train_acc 1.0
test_acc 0.019342560553633217
{'y_pred': [3.9043514728546143, 46.13632583618164, 39.247886657714844, 7.091929912567139, 8.494933128356934, -27.955745697021484, 27.7830810546875, 6.533584117889404, 19.1207332611084, -1.758217453956604, 41.60517883300781, 18.228708267211914, 42.994873046875, 38.05161666870117, 23.415449142456055, 37.682701110839844, 15.869576454162598, 0.1521238088607788, -20.192180633544922, -20.91603660583496, 18.7272891998291, 17.06910514831543, 30.17383575439453, -8.146929740905762, 4.131088733673096, -50.528499603271484, -4.901041507720947, 38.90208053588867, 19.174169540405273, 12.1598539352417, 10.049891471862793, 14.719740867614746, -21.319673538208008, -30.541810989379883, -13.677117347717285, -17.41462516784668, 30.758182525634766, -16.71558952331543, -4.765038967132568, 18.261302947998047, 14.40179443359375, 1.2914727926254272, 17.620311737060547, 8.445323944091797, -13.708810806274414, 36.89263153076172, -13.171719551086426, 26.972440719604492, 4.16461706161499, -30.16389274597168, 15.982115745544434, -9.778615951538086, 4.476377010345459, 6.856654644012451, 34.82708740234375, 21.887828826904297, 44.7890739440918, 32.224143981933594, 21.013185501098633, 6.187009334564209, -1.8776575326919556, 22.142292022705078, -0.17259466648101807, 2.2135822772979736, 26.869739532470703, -14.743992805480957, 24.1431827545166, -8.026915550231934, -7.666805267333984, -5.754908084869385, 26.566362380981445, 10.952775001525879, 15.2052583694458, -11.827601432800293, 8.993531227111816, 46.23289489746094, 41.544158935546875, 7.411050319671631, 60.02751922607422, 0.7503894567489624, 24.856002807617188, 40.08759689331055, -1.2226554155349731, -15.351356506347656, -17.994768142700195, 6.7388834953308105, -5.46836519241333, 48.45539093017578, -13.09261417388916, 18.241121292114258, 0.21218621730804443, 8.967961311340332, 10.011600494384766, 7.46425199508667, 21.55060386657715, 23.98073959350586, 21.674480438232422, 16.57611083984375, 1.9433473348617554, 31.599855422973633, 11.368611335754395, 9.427470207214355, 34.23396301269531, -3.8900763988494873, 55.498329162597656, -29.074953079223633, 0.8807748556137085, 1.6670812368392944, 14.68700885772705, 4.436513423919678, 12.765332221984863, 13.361176490783691, 0.6511834859848022, 67.24459838867188, 55.986480712890625, 16.19782829284668, 40.62275695800781, 5.437794208526611, -17.02661895751953, -11.47726058959961, -0.6462761163711548, 5.331648349761963, 24.39119529724121, 27.6092586517334, 18.776729583740234, 21.8089542388916, 12.765175819396973, -22.250423431396484, 24.950199127197266, 2.186868906021118, 45.88629150390625, 22.990036010742188, 11.320666313171387, 26.16372299194336, 26.436981201171875, -16.752153396606445, 13.978035926818848, -1.6800657510757446, 43.72711181640625, 13.083868980407715, 6.250362873077393, 24.789133071899414, 9.568181037902832, -0.7119995355606079, 31.07590103149414, 40.18743896484375, 51.913604736328125, 47.211334228515625, 15.450093269348145, 14.008805274963379, 16.176698684692383, 4.024784564971924, 1.702384352684021, 40.846309661865234, 40.963592529296875, 6.763103008270264, 48.90708923339844, 12.868071556091309, -22.179157257080078, 31.90367317199707, -5.684056758880615, 43.25904846191406, 7.784648418426514, 33.019493103027344, -10.167229652404785, -10.3273286819458, 26.48822593688965, 38.17816162109375, -6.936300754547119, -2.555806875228882, 6.75639009475708, 22.60276222229004, 21.793569564819336, 44.420440673828125, 10.673129081726074, -19.783039093017578, 12.255744934082031, 5.103259563446045, 8.06590747833252, 55.683502197265625, 18.54976463317871, -0.538063645362854, 21.946880340576172, -5.755737781524658, 11.65251350402832, 53.57897186279297, 18.091867446899414, -9.116534233093262, -9.253718376159668, 12.92194652557373, -14.109320640563965, -13.08094310760498, 12.940834045410156, 40.393150329589844, 54.51395034790039, -3.1428139209747314, 26.74947166442871, -6.625059604644775, 11.274327278137207, 11.459245681762695, 8.185347557067871, 13.841317176818848, 27.069520950317383, 18.8105525970459, 38.38576126098633, -3.2506110668182373, 34.82858657836914, 17.530141830444336, -8.98380184173584, 18.668315887451172, -3.8991940021514893, 36.98535919189453, 14.51752758026123, 15.587721824645996, 9.282395362854004, 6.6122145652771, -2.995504140853882, -4.421261310577393, 22.251249313354492, 2.5558197498321533, 37.56599807739258, 7.919692516326904, -7.794962406158447, 17.86254119873047, -12.309956550598145, 31.342714309692383, 12.235962867736816, 20.2688045501709, 14.949943542480469, 15.760685920715332, -6.779242992401123, -8.18439769744873, 23.48551368713379, 17.117338180541992, 11.500714302062988, -5.067018985748291, 23.837509155273438, 12.269811630249023, 30.119407653808594, 29.989770889282227, 33.38426208496094, -15.653045654296875, -25.858606338500977, -3.7509829998016357, 22.887720108032227, 28.692869186401367, 17.785825729370117, 40.15194320678711, 55.76893997192383, 13.996365547180176, 19.927431106567383, 14.902363777160645, 20.240264892578125, 1.4940322637557983, 4.91151762008667, 35.578006744384766, 18.85646629333496, 47.77205276489258, -14.029643058776855, 3.5776865482330322, 31.922412872314453, 39.420372009277344, -26.199398040771484, 33.83745574951172, 14.164145469665527, 57.10011672973633, 20.825044631958008, 34.01374816894531, 23.54268455505371, 7.105212688446045, 22.5881290435791, 12.15298843383789, 25.365407943725586, 26.80177879333496, 15.3053560256958, 15.151045799255371, 5.086005687713623, 19.88688087463379, -17.93878936767578, -12.078934669494629, 24.993032455444336, 31.756208419799805, 47.38931655883789, 18.19809913635254, 7.225983619689941, 41.79887390136719, 1.0001004934310913, 25.525814056396484, 10.921212196350098, 11.499140739440918, 9.296365737915039, -5.724781513214111, 11.41244888305664, -6.10620641708374, 33.82758331298828, -7.385591983795166, 9.130892753601074, 30.695463180541992, 5.496461391448975, 31.782316207885742, -10.013886451721191, -9.825531005859375, -5.134157180786133, 16.147497177124023, -0.9088560342788696, 22.31136131286621, 19.86391830444336, 7.48883581161499, 13.260233879089355, 42.42304611206055, 9.189003944396973, 6.513444423675537, -1.5588880777359009, 5.378175258636475, 16.93173599243164, -1.4903894662857056, 28.342676162719727, -9.55762004852295, 27.839120864868164, 19.469125747680664, 14.941054344177246, -21.30862808227539, -4.750044345855713, -10.47176456451416, 34.329647064208984, 9.15379810333252, 29.27818489074707, -9.661520957946777, 19.653532028198242, 5.466642379760742, 13.806279182434082, -8.1198091506958, 18.8409423828125, 52.526641845703125, 53.56362533569336, 36.074405670166016, 9.632285118103027, 14.897891998291016, 33.591190338134766, 42.222625732421875], 'y_pred_int': [4, 46, 39, 7, 8, 0, 28, 7, 19, 0, 42, 18, 43, 38, 23, 38, 16, 0, 0, 0, 19, 17, 30, 0, 4, 0, 0, 39, 19, 12, 10, 15, 0, 0, 0, 0, 31, 0, 0, 18, 14, 1, 18, 8, 0, 37, 0, 27, 4, 0, 16, 0, 4, 7, 35, 22, 45, 32, 21, 6, 0, 22, 0, 2, 27, 0, 24, 0, 0, 0, 27, 11, 15, 0, 9, 46, 42, 7, 60, 1, 25, 40, 0, 0, 0, 7, 0, 48, 0, 18, 0, 9, 10, 7, 22, 24, 22, 17, 2, 32, 11, 9, 34, 0, 55, 0, 1, 2, 15, 4, 13, 13, 1, 67, 56, 16, 41, 5, 0, 0, 0, 5, 24, 28, 19, 22, 13, 0, 25, 2, 46, 23, 11, 26, 26, 0, 14, 0, 44, 13, 6, 25, 10, 0, 31, 40, 52, 47, 15, 14, 16, 4, 2, 41, 41, 7, 49, 13, 0, 32, 0, 43, 8, 33, 0, 0, 26, 38, 0, 0, 7, 23, 22, 44, 11, 0, 12, 5, 8, 56, 19, 0, 22, 0, 12, 54, 18, 0, 0, 13, 0, 0, 13, 40, 55, 0, 27, 0, 11, 11, 8, 14, 27, 19, 38, 0, 35, 18, 0, 19, 0, 37, 15, 16, 9, 7, 0, 0, 22, 3, 38, 8, 0, 18, 0, 31, 12, 20, 15, 16, 0, 0, 23, 17, 12, 0, 24, 12, 30, 30, 33, 0, 0, 0, 23, 29, 18, 40, 56, 14, 20, 15, 20, 1, 5, 36, 19, 48, 0, 4, 32, 39, 0, 34, 14, 57, 21, 34, 24, 7, 23, 12, 25, 27, 15, 15, 5, 20, 0, 0, 25, 32, 47, 18, 7, 42, 1, 26, 11, 11, 9, 0, 11, 0, 34, 0, 9, 31, 5, 32, 0, 0, 0, 16, 0, 22, 20, 7, 13, 42, 9, 7, 0, 5, 17, 0, 28, 0, 28, 19, 15, 0, 0, 0, 34, 9, 29, 0, 20, 5, 14, 0, 19, 53, 54, 36, 10, 15, 34, 42], 'y_true': [19, 17, 8, 21, 12, 9, 26, 8, 3, 4, 19, 11, 8, 6, 24, 10, 6, 26, 3, 6, 13, 12, 12, 18, 6, 15, 53, 18, 10, 9, 21, 9, 23, 8, 29, 10, 10, 3, 28, 6, 11, 3, 8, 4, 14, 4, 8, 11, 14, 11, 11, 3, 6, 7, 12, 17, 10, 17, 23, 10, 18, 40, 32, 20, 8, 10, 9, 22, 5, 8, 20, 17, 4, 2, 33, 26, 24, 34, 17, 13, 13, 15, 4, 9, 7, 10, 6, 4, 8, 14, 8, 15, 13, 46, 7, 4, 14, 38, 9, 27, 5, 6, 10, 15, 21, 13, 8, 6, 15, 5, 7, 26, 34, 11, 30, 54, 28, 10, 6, 3, 10, 10, 11, 15, 8, 6, 10, 6, 11, 10, 16, 11, 14, 10, 15, 18, 5, 9, 25, 7, 1, 18, 18, 4, 12, 25, 18, 19, 13, 36, 12, 10, 4, 6, 16, 6, 15, 10, 6, 19, 6, 34, 36, 25, 7, 7, 15, 17, 4, 5, 4, 20, 19, 19, 8, 4, 5, 29, 9, 16, 7, 5, 6, 6, 7, 16, 8, 3, 12, 16, 3, 4, 6, 23, 23, 9, 13, 9, 6, 13, 13, 11, 21, 8, 4, 6, 21, 11, 7, 7, 4, 10, 30, 3, 6, 8, 8, 12, 31, 11, 12, 21, 26, 14, 17, 25, 15, 4, 5, 12, 2, 11, 15, 60, 11, 28, 15, 6, 24, 7, 10, 3, 4, 5, 12, 26, 21, 19, 5, 14, 4, 19, 11, 21, 6, 14, 7, 39, 23, 5, 35, 36, 17, 9, 4, 8, 11, 8, 28, 4, 11, 34, 16, 19, 23, 18, 6, 11, 4, 25, 14, 13, 29, 10, 2, 26, 11, 15, 13, 10, 5, 13, 7, 6, 17, 8, 25, 26, 6, 30, 1, 5, 13, 12, 7, 6, 10, 5, 5, 20, 6, 2, 16, 13, 7, 7, 5, 37, 8, 13, 3, 9, 9, 7, 17, 4, 66, 20, 6, 11, 32, 13, 7, 14, 13, 10, 16, 17, 8, 15], 'train_acc': 1.0, 'test_acc': 0.019342560553633217, 'X': 'X_TANL_layer_3_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_4_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 99426.4453, Test loss: 33638.9844
In this epoch 100/5000, Training loss: 694.0168, Test loss: 926.5823
In this epoch 150/5000, Training loss: 0.7004, Test loss: 538.7897
In this epoch 200/5000, Training loss: 0.0129, Test loss: 544.9476
In this epoch 250/5000, Training loss: 0.0108, Test loss: 545.5072
In this epoch 300/5000, Training loss: 0.0092, Test loss: 545.3770
In this epoch 350/5000, Training loss: 0.0076, Test loss: 545.2903
In this epoch 400/5000, Training loss: 0.0063, Test loss: 545.2075
In this epoch 450/5000, Training loss: 0.0050, Test loss: 545.1271
In this epoch 500/5000, Training loss: 0.0040, Test loss: 545.0500
In this epoch 550/5000, Training loss: 0.0031, Test loss: 544.9772
In this epoch 600/5000, Training loss: 0.0024, Test loss: 544.9102
In this epoch 650/5000, Training loss: 0.0018, Test loss: 544.8483
In this epoch 700/5000, Training loss: 0.0014, Test loss: 544.7914
In this epoch 750/5000, Training loss: 0.0010, Test loss: 544.7399
In this epoch 800/5000, Training loss: 0.0007, Test loss: 544.6943
In this epoch 850/5000, Training loss: 0.0005, Test loss: 544.6536
In this epoch 900/5000, Training loss: 0.0004, Test loss: 544.6179
In this epoch 950/5000, Training loss: 0.0002, Test loss: 544.5870
In this epoch 1000/5000, Training loss: 0.0002, Test loss: 544.5600
In this epoch 1050/5000, Training loss: 0.0001, Test loss: 544.5374
In this epoch 1100/5000, Training loss: 0.0001, Test loss: 544.5181
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 544.5020
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 544.4884
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 544.4774
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 544.4684
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 544.4609
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 544.4551
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 544.4505
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 544.4469
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 544.4440
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 544.4420
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 544.4404
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 544.4390
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 544.4382
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 544.4375
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 544.4370
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 544.4366
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 544.4364
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 544.4362
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 544.4360
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 544.4360
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 544.4360
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 544.4360
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 544.4359
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 544.4359
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 544.4359
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 544.4359
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 544.4359
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 544.4359
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 544.4358
In this epoch 3850/5000, Training loss: 0.1398, Test loss: 538.8743
In this epoch 3900/5000, Training loss: 581776.0000, Test loss: 400935.8125
In this epoch 3950/5000, Training loss: 1407.8938, Test loss: 749.1682
In this epoch 4000/5000, Training loss: 13.9468, Test loss: 592.2948
In this epoch 4050/5000, Training loss: 0.0130, Test loss: 558.7021
In this epoch 4100/5000, Training loss: 0.0001, Test loss: 560.2377
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 560.1159
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 560.1162
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 560.1083
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 560.1019
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 560.0959
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 560.0903
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 560.0852
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 560.0808
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 560.0768
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 560.0729
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 560.0695
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 560.0664
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 560.0636
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 560.0613
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 560.0592
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 560.0573
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 560.0555
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 560.0541
train_acc 1.0
test_acc 0.016306228373702422
{'y_pred': [1.7631044387817383, -15.527195930480957, 30.693391799926758, 12.923195838928223, 47.876285552978516, 11.4773588180542, 29.229873657226562, 3.281553030014038, -12.933053016662598, 41.86768341064453, 14.733696937561035, -11.010052680969238, -4.054497718811035, -23.135677337646484, 21.520477294921875, 27.665119171142578, -0.20086570084095, 40.175559997558594, 46.509544372558594, 12.176786422729492, 4.65415096282959, -18.43149757385254, 40.25434112548828, -13.030608177185059, 16.235050201416016, -6.932272911071777, -17.091175079345703, 4.170190334320068, -13.33220100402832, 10.664759635925293, -6.709571838378906, 41.079490661621094, 21.89336395263672, 8.295449256896973, 13.398306846618652, 0.06138615310192108, 6.124476909637451, 60.319244384765625, 0.29703620076179504, 8.593798637390137, -17.709619522094727, -4.1590070724487305, -30.249462127685547, 30.480451583862305, 36.009117126464844, 42.05506134033203, 12.303019523620605, 47.512657165527344, 41.94918441772461, 43.12494659423828, -1.925948143005371, 13.27318286895752, 29.6346492767334, -14.4589204788208, 23.79693031311035, 34.43138885498047, -14.51208209991455, 14.913702011108398, 40.979896545410156, 70.56926727294922, 6.075945854187012, 56.33650207519531, 13.887273788452148, 9.15713882446289, 49.30827713012695, 8.410218238830566, 53.08262252807617, 13.20327091217041, -10.7089204788208, 36.11968994140625, -1.5575685501098633, -43.303009033203125, 21.57852554321289, -16.439189910888672, 34.184932708740234, -31.513107299804688, 0.5155449509620667, -8.337422370910645, -4.454206466674805, 4.133734703063965, 7.902248382568359, 10.97670841217041, -14.200655937194824, -23.732637405395508, -10.657088279724121, 2.04647159576416, 9.445279121398926, 12.921069145202637, 9.037164688110352, 23.409759521484375, 51.950531005859375, -26.962265014648438, -8.059245109558105, -41.48213195800781, 24.13751792907715, -29.81011199951172, 29.001998901367188, -7.133816719055176, 15.302767753601074, 7.970989227294922, 10.094542503356934, 31.42436981201172, -28.05006980895996, 30.659568786621094, -8.881529808044434, 7.366354942321777, -9.908308982849121, 4.47487735748291, -28.369722366333008, 23.125917434692383, 5.437800407409668, 28.682432174682617, 59.2784538269043, 24.507057189941406, -18.46702766418457, 48.308650970458984, -31.370319366455078, 16.738412857055664, 24.166221618652344, 12.327485084533691, -41.2125244140625, 3.5215749740600586, -7.294424057006836, 44.94147872924805, 31.450424194335938, 43.38688659667969, 34.09050369262695, 32.70421600341797, 41.38166809082031, -27.657249450683594, 16.485431671142578, 43.986995697021484, 35.429405212402344, 24.579612731933594, -17.00537872314453, 29.34314727783203, -0.6030701994895935, -38.488555908203125, 6.848381996154785, -14.70177936553955, 2.3584394454956055, 4.679376125335693, -11.180441856384277, 38.25726318359375, 46.49333572387695, 23.549598693847656, 11.500243186950684, 53.35762405395508, -10.077916145324707, 59.830360412597656, 14.206130981445312, 26.34554672241211, -15.652129173278809, 18.47361946105957, -11.866535186767578, 14.66273021697998, 12.521703720092773, 42.738853454589844, -3.806994915008545, 23.484413146972656, 28.683786392211914, 8.88219928741455, 53.19142532348633, -1.8779497146606445, 7.668689250946045, 54.408660888671875, 17.741859436035156, -3.437008857727051, 20.367908477783203, 27.000877380371094, 44.342994689941406, 40.99040222167969, 50.3845329284668, 64.86133575439453, 13.388318061828613, 13.31800365447998, 45.82535934448242, 45.287540435791016, 27.602497100830078, 53.81150436401367, 8.15778923034668, 77.84143829345703, 20.99254608154297, -17.641006469726562, -23.504234313964844, 0.5943098664283752, 43.55885314941406, 14.999966621398926, 6.185976028442383, -5.395013809204102, 1.6757726669311523, -15.911271095275879, 12.836938858032227, 21.6420955657959, 15.73046588897705, 33.40277099609375, 11.923975944519043, 20.320329666137695, 47.47502899169922, -0.9583749175071716, -6.411572456359863, -9.204319953918457, -29.441804885864258, 24.98654556274414, -31.982261657714844, 8.828141212463379, 42.125038146972656, 47.7805290222168, 45.98219680786133, -15.08621883392334, -31.375179290771484, -13.394932746887207, 8.92798137664795, 30.518918991088867, -32.47753143310547, -5.783642768859863, 2.7052783966064453, 28.866073608398438, -2.115248680114746, -35.088008880615234, 6.758193492889404, 21.56779670715332, 43.37020492553711, 19.196510314941406, -18.665719985961914, 10.37204360961914, 62.327659606933594, -23.99205780029297, 4.482553482055664, 29.184083938598633, -9.497967720031738, -14.424919128417969, -13.206889152526855, -12.167539596557617, -11.87350845336914, -12.142894744873047, 11.869521141052246, 31.375579833984375, 7.172729969024658, 31.698505401611328, 6.64515495300293, 8.856494903564453, 5.732821464538574, 22.384946823120117, -10.487914085388184, 5.7505598068237305, -3.7170400619506836, 3.8055849075317383, 59.680870056152344, -43.564666748046875, 30.915714263916016, 4.252939701080322, -29.945701599121094, 1.9543323516845703, 64.13439178466797, 9.934721946716309, 51.98811721801758, 14.425057411193848, 18.328996658325195, -11.549378395080566, 33.15589141845703, 17.037885665893555, 25.615304946899414, -2.2019777297973633, -7.2536420822143555, 60.231109619140625, 6.831491470336914, -22.103191375732422, -16.5443115234375, 8.222722053527832, 48.536094665527344, -2.283658981323242, 11.29894733428955, 3.8044090270996094, -5.409106254577637, -22.495643615722656, 3.1756162643432617, -34.6143798828125, 3.821009635925293, 11.236987113952637, 31.308015823364258, 28.258865356445312, -12.678860664367676, 4.51895809173584, 37.525760650634766, 12.91169548034668, 3.861372470855713, -14.303765296936035, 20.007076263427734, 32.68107223510742, 12.234206199645996, 45.16038131713867, 13.616438865661621, 20.183256149291992, 40.936973571777344, -25.87310028076172, 20.64288330078125, 36.95894241333008, -10.276103019714355, 15.359435081481934, 17.658546447753906, 7.677340507507324, 21.827604293823242, -11.552029609680176, 15.698447227478027, 6.4103899002075195, 1.5490446090698242, 7.923773765563965, -12.852463722229004, 4.75649356842041, -11.655896186828613, 54.201175689697266, 57.633506774902344, 11.29502010345459, -13.178400993347168, -0.6303548216819763, -18.076995849609375, 29.822813034057617, 5.896571159362793, 8.3117036819458, 35.479248046875, 21.876750946044922, 16.865419387817383, -37.071815490722656, -2.886598587036133, 22.057289123535156, -0.38432976603507996, -18.48214340209961, 12.075818061828613, -2.6850976943969727, 23.882171630859375, -7.786406517028809, -19.627309799194336, 25.212963104248047, 9.2095947265625, -11.108668327331543, 31.993515014648438, 6.48307991027832, 46.63163757324219, 48.307926177978516], 'y_pred_int': [2, 0, 31, 13, 48, 11, 29, 3, 0, 42, 15, 0, 0, 0, 22, 28, 0, 40, 47, 12, 5, 0, 40, 0, 16, 0, 0, 4, 0, 11, 0, 41, 22, 8, 13, 0, 6, 60, 0, 9, 0, 0, 0, 30, 36, 42, 12, 48, 42, 43, 0, 13, 30, 0, 24, 34, 0, 15, 41, 71, 6, 56, 14, 9, 49, 8, 53, 13, 0, 36, 0, 0, 22, 0, 34, 0, 1, 0, 0, 4, 8, 11, 0, 0, 0, 2, 9, 13, 9, 23, 52, 0, 0, 0, 24, 0, 29, 0, 15, 8, 10, 31, 0, 31, 0, 7, 0, 4, 0, 23, 5, 29, 59, 25, 0, 48, 0, 17, 24, 12, 0, 4, 0, 45, 31, 43, 34, 33, 41, 0, 16, 44, 35, 25, 0, 29, 0, 0, 7, 0, 2, 5, 0, 38, 46, 24, 12, 53, 0, 60, 14, 26, 0, 18, 0, 15, 13, 43, 0, 23, 29, 9, 53, 0, 8, 54, 18, 0, 20, 27, 44, 41, 50, 65, 13, 13, 46, 45, 28, 54, 8, 78, 21, 0, 0, 1, 44, 15, 6, 0, 2, 0, 13, 22, 16, 33, 12, 20, 47, 0, 0, 0, 0, 25, 0, 9, 42, 48, 46, 0, 0, 0, 9, 31, 0, 0, 3, 29, 0, 0, 7, 22, 43, 19, 0, 10, 62, 0, 4, 29, 0, 0, 0, 0, 0, 0, 12, 31, 7, 32, 7, 9, 6, 22, 0, 6, 0, 4, 60, 0, 31, 4, 0, 2, 64, 10, 52, 14, 18, 0, 33, 17, 26, 0, 0, 60, 7, 0, 0, 8, 49, 0, 11, 4, 0, 0, 3, 0, 4, 11, 31, 28, 0, 5, 38, 13, 4, 0, 20, 33, 12, 45, 14, 20, 41, 0, 21, 37, 0, 15, 18, 8, 22, 0, 16, 6, 2, 8, 0, 5, 0, 54, 58, 11, 0, 0, 0, 30, 6, 8, 35, 22, 17, 0, 0, 22, 0, 0, 12, 0, 24, 0, 0, 25, 9, 0, 32, 6, 47, 48], 'y_true': [6, 3, 19, 10, 39, 2, 7, 9, 6, 16, 25, 22, 8, 2, 7, 7, 5, 33, 20, 10, 8, 7, 32, 17, 8, 8, 8, 23, 13, 17, 3, 13, 5, 5, 3, 4, 6, 12, 13, 13, 9, 29, 21, 7, 12, 14, 19, 10, 13, 47, 14, 18, 29, 18, 10, 8, 7, 60, 4, 25, 48, 11, 4, 10, 10, 6, 8, 11, 7, 10, 5, 9, 8, 9, 15, 10, 3, 9, 15, 3, 34, 35, 2, 3, 3, 6, 29, 8, 2, 23, 6, 7, 31, 9, 23, 34, 15, 15, 25, 26, 46, 36, 10, 10, 14, 8, 12, 14, 6, 18, 2, 6, 19, 10, 7, 34, 8, 29, 17, 4, 7, 3, 5, 11, 40, 36, 16, 22, 26, 4, 8, 17, 8, 41, 9, 13, 9, 12, 13, 9, 5, 17, 12, 24, 11, 28, 11, 37, 40, 31, 19, 3, 6, 7, 14, 14, 14, 37, 18, 9, 11, 16, 19, 4, 39, 29, 11, 2, 3, 5, 12, 6, 28, 14, 10, 14, 15, 46, 4, 16, 29, 17, 19, 6, 25, 11, 43, 15, 7, 5, 7, 5, 13, 7, 10, 4, 8, 11, 13, 8, 3, 6, 17, 23, 8, 17, 14, 19, 26, 28, 11, 14, 28, 28, 25, 7, 15, 17, 11, 15, 21, 28, 11, 3, 9, 6, 14, 13, 14, 11, 6, 11, 22, 6, 5, 37, 9, 17, 16, 12, 5, 19, 5, 15, 20, 3, 6, 10, 21, 5, 8, 23, 14, 12, 22, 8, 12, 6, 13, 2, 16, 23, 3, 22, 3, 8, 8, 5, 13, 19, 19, 33, 11, 29, 2, 4, 3, 7, 13, 14, 22, 22, 1, 13, 15, 7, 9, 2, 46, 4, 15, 24, 18, 13, 28, 16, 30, 19, 9, 11, 23, 12, 7, 6, 6, 37, 8, 14, 16, 14, 9, 22, 30, 8, 20, 5, 22, 35, 12, 26, 5, 16, 7, 10, 6, 23, 18, 15, 12, 8, 11, 3, 24, 17, 27, 2, 10, 5, 14, 9], 'train_acc': 1.0, 'test_acc': 0.016306228373702422, 'X': 'X_TANL_layer_4_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_5_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 148037.8438, Test loss: 69120.9453
In this epoch 100/5000, Training loss: 901.0654, Test loss: 1096.0641
In this epoch 150/5000, Training loss: 0.0959, Test loss: 572.3450
In this epoch 200/5000, Training loss: 0.0133, Test loss: 563.4598
In this epoch 250/5000, Training loss: 0.0002, Test loss: 563.2983
In this epoch 300/5000, Training loss: 0.0001, Test loss: 563.3021
In this epoch 350/5000, Training loss: 0.0001, Test loss: 563.3044
In this epoch 400/5000, Training loss: 0.0001, Test loss: 563.3089
In this epoch 450/5000, Training loss: 0.0000, Test loss: 563.3126
In this epoch 500/5000, Training loss: 0.0000, Test loss: 563.3165
In this epoch 550/5000, Training loss: 0.0000, Test loss: 563.3198
In this epoch 600/5000, Training loss: 0.0000, Test loss: 563.3227
In this epoch 650/5000, Training loss: 0.0000, Test loss: 563.3253
In this epoch 700/5000, Training loss: 0.0000, Test loss: 563.3275
In this epoch 750/5000, Training loss: 0.0000, Test loss: 563.3293
In this epoch 800/5000, Training loss: 0.0000, Test loss: 563.3312
In this epoch 850/5000, Training loss: 0.0000, Test loss: 563.3325
In this epoch 900/5000, Training loss: 0.0000, Test loss: 563.3339
In this epoch 950/5000, Training loss: 0.0000, Test loss: 563.3348
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 563.3357
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 563.3362
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 563.3366
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 563.3371
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 563.3372
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 563.3374
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 563.3376
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 563.3377
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 563.3379
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 563.3379
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 563.3380
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 563.3380
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 563.3380
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 563.3380
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 563.3380
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 563.3380
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 563.3382
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 563.3382
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 563.3382
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 563.3383
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 563.3381
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 563.3382
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 563.3382
In this epoch 3350/5000, Training loss: 5939239.5000, Test loss: 325129.0312
In this epoch 3400/5000, Training loss: 7770.9097, Test loss: 34407.1758
In this epoch 3450/5000, Training loss: 101.9364, Test loss: 760.2968
In this epoch 3500/5000, Training loss: 0.9501, Test loss: 730.6771
In this epoch 3550/5000, Training loss: 0.0031, Test loss: 732.0427
In this epoch 3600/5000, Training loss: 0.0001, Test loss: 731.8322
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 731.8137
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 731.8190
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 731.8230
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 731.8264
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 731.8295
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 731.8328
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 731.8355
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 731.8381
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 731.8405
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 731.8423
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 731.8442
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 731.8459
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 731.8476
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 731.8489
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 731.8500
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 731.8511
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 731.8519
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 731.8529
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 731.8535
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 731.8542
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 731.8548
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 731.8555
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 731.8561
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 731.8564
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 731.8570
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 731.8573
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 731.8576
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 731.8578
train_acc 1.0
test_acc 0.01384083044982699
{'y_pred': [-0.07772284001111984, 30.83810806274414, -12.105361938476562, 14.003067970275879, -19.149137496948242, 7.821953773498535, 50.036521911621094, 42.00908660888672, 11.57107162475586, 27.157442092895508, 22.665868759155273, -22.4772891998291, 12.143213272094727, 10.172354698181152, -4.809444904327393, 26.600419998168945, -12.44729232788086, 30.795364379882812, 0.1823078989982605, -32.102012634277344, 56.948204040527344, 15.088350296020508, 29.965932846069336, 2.0415236949920654, 24.800365447998047, 34.80479431152344, 21.048986434936523, -42.29002380371094, -2.1391031742095947, 5.214343547821045, -6.45238733291626, 42.850486755371094, -9.337606430053711, -14.675174713134766, 63.93778610229492, 13.37382698059082, 60.174156188964844, 45.8206787109375, -17.59762954711914, 5.096797466278076, 47.17940139770508, 37.52404022216797, -10.028358459472656, 9.359519004821777, 1.799678087234497, 2.4715116024017334, -22.456401824951172, 40.4998779296875, 4.230815410614014, 16.592723846435547, 14.774032592773438, 9.049869537353516, -14.349678993225098, -17.457317352294922, 24.13207244873047, 3.3132855892181396, 45.030757904052734, 44.23671340942383, 15.822526931762695, -3.8114359378814697, 17.69034767150879, 5.968064785003662, 0.6230465769767761, 51.9127197265625, 25.099382400512695, 7.150717258453369, 11.223525047302246, 3.560910940170288, 39.94983673095703, -8.439964294433594, 40.62748718261719, 61.793212890625, 66.89661407470703, 41.08433532714844, 22.602771759033203, -51.737876892089844, 18.110254287719727, 33.53662109375, 18.449024200439453, 7.402053356170654, -29.337921142578125, 101.6001968383789, 16.634206771850586, 3.1250369548797607, 23.73621368408203, -15.027498245239258, 12.074042320251465, 15.960389137268066, 5.304961681365967, 6.82185697555542, 27.013324737548828, -1.5836012363433838, 42.362937927246094, 22.080196380615234, 26.31253433227539, 19.595111846923828, 39.88011932373047, -14.60505199432373, -18.753604888916016, 28.873445510864258, 43.79668426513672, 40.04393005371094, 1.9929540157318115, 27.94245147705078, 53.59555435180664, 47.98553466796875, 19.334033966064453, 22.73682403564453, -68.85887908935547, 77.312255859375, 4.825843334197998, 30.85025405883789, -8.361580848693848, -29.20349884033203, 15.468324661254883, -12.743885040283203, -5.5055317878723145, -1.9202358722686768, 11.867631912231445, 28.92017364501953, 27.135215759277344, 45.98401641845703, 49.901039123535156, 42.639617919921875, 25.971057891845703, -9.772272109985352, -13.863685607910156, 5.221192836761475, 4.994555950164795, 54.69219207763672, 31.583316802978516, 27.16336441040039, -1.6503775119781494, 9.437114715576172, 15.992059707641602, 11.693038940429688, -17.96320343017578, 11.453971862792969, 29.000064849853516, 5.389251232147217, 24.82480812072754, 23.710277557373047, 12.853195190429688, 55.36410903930664, 68.19945526123047, 30.625709533691406, 76.5791244506836, 6.05324125289917, 38.80458450317383, 2.9803102016448975, 100.0035400390625, 9.936888694763184, 40.60761260986328, -5.6372599601745605, 12.599215507507324, 11.108259201049805, -5.486314296722412, 37.76287841796875, 20.477439880371094, 12.691100120544434, 7.756957530975342, 7.3592143058776855, 69.83856964111328, 29.193370819091797, 27.149677276611328, -7.0179524421691895, 3.8653523921966553, 44.932029724121094, 24.440113067626953, -6.42948579788208, 56.73401641845703, 27.538257598876953, -45.214942932128906, 38.73860168457031, 40.389671325683594, -0.0025885633658617735, 67.69999694824219, 3.5790603160858154, 67.5253677368164, -17.33183479309082, -0.3069804310798645, 34.17259979248047, 24.71592140197754, 46.627647399902344, 14.554054260253906, 16.17449188232422, 7.729220867156982, 28.620691299438477, 15.035633087158203, 45.747772216796875, 64.91934967041016, 0.24076813459396362, 63.47685623168945, 13.000008583068848, 50.47112274169922, -11.229912757873535, 8.630020141601562, 4.150574207305908, -17.459232330322266, 25.95743179321289, 104.71996307373047, 3.575629949569702, 3.1101624965667725, -6.953266620635986, 25.22686767578125, 14.236127853393555, -13.960039138793945, 22.650985717773438, 13.221224784851074, 29.924278259277344, 45.976234436035156, 17.87154197692871, 46.3026237487793, -24.574268341064453, 62.74179458618164, -1.2557222843170166, -11.244903564453125, 20.423233032226562, 0.7064358592033386, -2.239074945449829, 6.775491237640381, -11.955881118774414, -15.060161590576172, 26.790130615234375, 30.302196502685547, 41.76832580566406, 24.117456436157227, 28.884517669677734, 51.79491424560547, -4.2188897132873535, 28.487537384033203, 30.493783950805664, 17.424137115478516, -36.46088790893555, 26.602270126342773, -67.2968978881836, -14.068679809570312, 10.23117446899414, 49.86629867553711, 15.513038635253906, 22.81671905517578, -22.95216941833496, 45.12425231933594, 25.50216293334961, -55.924766540527344, 43.162994384765625, 2.922837972640991, 61.92118453979492, -2.7144415378570557, 8.35092830657959, 21.934955596923828, 5.027612209320068, 37.87260437011719, -6.539717197418213, 47.561119079589844, 10.133415222167969, -1.994333267211914, -8.075056076049805, 65.3289794921875, 45.33574295043945, 93.77857208251953, -27.822900772094727, 41.435791015625, 41.85744857788086, 20.269180297851562, -1.7348711490631104, 36.585872650146484, -17.484169006347656, 53.20397186279297, 43.69140625, 62.592063903808594, 3.821885824203491, 15.522234916687012, -28.954364776611328, 50.584495544433594, 14.277007102966309, 4.4785895347595215, 24.17626190185547, -4.785682201385498, 30.28685188293457, 2.289597272872925, 16.003454208374023, 41.69886016845703, 38.76070022583008, 8.730337142944336, 34.21392059326172, -22.95285987854004, -26.49527931213379, 7.823797702789307, 29.433624267578125, -38.93193054199219, -33.2645378112793, 14.166118621826172, 64.52397155761719, 36.91342544555664, -2.459897994995117, 11.571208953857422, 49.993553161621094, 57.08513259887695, -22.718994140625, 1.2099688053131104, 53.35511779785156, -20.671432495117188, 26.14307403564453, -10.581554412841797, 52.154205322265625, 22.187841415405273, 23.214330673217773, -0.10275297611951828, 21.661951065063477, -13.26982593536377, 42.45663070678711, 22.978992462158203, 17.88585090637207, -9.602543830871582, 53.5274658203125, 22.036632537841797, 29.51227569580078, 7.755371570587158, -17.061891555786133, 57.700958251953125, -5.860599994659424, -10.901634216308594, 23.44416046142578, -4.842010021209717, -4.661512851715088, 44.32176971435547, -22.740215301513672, -5.673603534698486, -11.621760368347168, 66.7261734008789, 36.11182403564453, -11.66695499420166, 19.826311111450195, 21.008716583251953, -1.2285845279693604, 18.582311630249023, 43.163177490234375, 40.158668518066406, 6.562535762786865], 'y_pred_int': [0, 31, 0, 14, 0, 8, 50, 42, 12, 27, 23, 0, 12, 10, 0, 27, 0, 31, 0, 0, 57, 15, 30, 2, 25, 35, 21, 0, 0, 5, 0, 43, 0, 0, 64, 13, 60, 46, 0, 5, 47, 38, 0, 9, 2, 2, 0, 40, 4, 17, 15, 9, 0, 0, 24, 3, 45, 44, 16, 0, 18, 6, 1, 52, 25, 7, 11, 4, 40, 0, 41, 62, 67, 41, 23, 0, 18, 34, 18, 7, 0, 102, 17, 3, 24, 0, 12, 16, 5, 7, 27, 0, 42, 22, 26, 20, 40, 0, 0, 29, 44, 40, 2, 28, 54, 48, 19, 23, 0, 77, 5, 31, 0, 0, 15, 0, 0, 0, 12, 29, 27, 46, 50, 43, 26, 0, 0, 5, 5, 55, 32, 27, 0, 9, 16, 12, 0, 11, 29, 5, 25, 24, 13, 55, 68, 31, 77, 6, 39, 3, 100, 10, 41, 0, 13, 11, 0, 38, 20, 13, 8, 7, 70, 29, 27, 0, 4, 45, 24, 0, 57, 28, 0, 39, 40, 0, 68, 4, 68, 0, 0, 34, 25, 47, 15, 16, 8, 29, 15, 46, 65, 0, 63, 13, 50, 0, 9, 4, 0, 26, 105, 4, 3, 0, 25, 14, 0, 23, 13, 30, 46, 18, 46, 0, 63, 0, 0, 20, 1, 0, 7, 0, 0, 27, 30, 42, 24, 29, 52, 0, 28, 30, 17, 0, 27, 0, 0, 10, 50, 16, 23, 0, 45, 26, 0, 43, 3, 62, 0, 8, 22, 5, 38, 0, 48, 10, 0, 0, 65, 45, 94, 0, 41, 42, 20, 0, 37, 0, 53, 44, 63, 4, 16, 0, 51, 14, 4, 24, 0, 30, 2, 16, 42, 39, 9, 34, 0, 0, 8, 29, 0, 0, 14, 65, 37, 0, 12, 50, 57, 0, 1, 53, 0, 26, 0, 52, 22, 23, 0, 22, 0, 42, 23, 18, 0, 54, 22, 30, 8, 0, 58, 0, 0, 23, 0, 0, 44, 0, 0, 0, 67, 36, 0, 20, 21, 0, 19, 43, 40, 7], 'y_true': [4, 2, 25, 20, 3, 8, 8, 13, 17, 3, 3, 10, 7, 27, 23, 5, 11, 8, 6, 11, 14, 11, 10, 39, 14, 7, 3, 16, 11, 7, 36, 9, 14, 15, 4, 19, 9, 13, 5, 6, 16, 10, 13, 7, 6, 5, 22, 14, 22, 20, 4, 20, 6, 5, 6, 7, 6, 9, 6, 20, 10, 6, 14, 4, 14, 11, 7, 12, 11, 18, 12, 17, 7, 16, 16, 9, 18, 28, 7, 28, 10, 34, 36, 19, 7, 16, 9, 18, 3, 4, 34, 6, 20, 6, 7, 19, 31, 7, 4, 12, 23, 8, 10, 20, 31, 8, 7, 15, 9, 15, 28, 9, 28, 9, 4, 31, 11, 15, 18, 14, 14, 4, 15, 18, 19, 5, 10, 24, 7, 6, 11, 9, 23, 8, 10, 5, 10, 14, 29, 5, 5, 14, 21, 16, 49, 6, 6, 6, 17, 10, 10, 16, 8, 9, 9, 13, 27, 20, 10, 5, 9, 20, 34, 11, 12, 10, 32, 7, 11, 8, 19, 26, 5, 19, 3, 21, 14, 11, 39, 6, 8, 14, 10, 9, 23, 8, 5, 13, 51, 14, 35, 3, 28, 13, 39, 21, 11, 15, 3, 10, 11, 23, 17, 4, 22, 6, 8, 12, 10, 8, 7, 11, 8, 12, 19, 39, 11, 8, 9, 5, 6, 4, 12, 14, 10, 15, 9, 2, 42, 18, 8, 14, 19, 8, 10, 8, 10, 4, 19, 16, 8, 4, 18, 22, 8, 35, 11, 5, 26, 22, 32, 14, 22, 6, 13, 14, 33, 11, 100, 21, 13, 2, 16, 19, 12, 22, 16, 3, 33, 6, 24, 29, 11, 12, 15, 11, 5, 10, 6, 8, 8, 2, 26, 7, 17, 6, 6, 18, 10, 20, 16, 21, 4, 23, 11, 12, 42, 16, 33, 16, 41, 6, 22, 6, 16, 48, 37, 29, 10, 14, 8, 6, 23, 17, 32, 12, 4, 11, 11, 6, 28, 6, 10, 30, 11, 16, 28, 16, 6, 11, 39, 35, 21, 19, 5, 8, 8, 16, 41, 13], 'train_acc': 1.0, 'test_acc': 0.01384083044982699, 'X': 'X_TANL_layer_5_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_6_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 188196.7812, Test loss: 74448.8750
In this epoch 100/5000, Training loss: 1251.4207, Test loss: 1831.2208
In this epoch 150/5000, Training loss: 0.1414, Test loss: 1044.0482
In this epoch 200/5000, Training loss: 0.0082, Test loss: 1046.7393
In this epoch 250/5000, Training loss: 0.0030, Test loss: 1047.2521
In this epoch 300/5000, Training loss: 0.0024, Test loss: 1047.2764
In this epoch 350/5000, Training loss: 0.0020, Test loss: 1047.3228
In this epoch 400/5000, Training loss: 0.0016, Test loss: 1047.3689
In this epoch 450/5000, Training loss: 0.0012, Test loss: 1047.4135
In this epoch 500/5000, Training loss: 0.0010, Test loss: 1047.4559
In this epoch 550/5000, Training loss: 0.0007, Test loss: 1047.4957
In this epoch 600/5000, Training loss: 0.0005, Test loss: 1047.5321
In this epoch 650/5000, Training loss: 0.0004, Test loss: 1047.5657
In this epoch 700/5000, Training loss: 0.0003, Test loss: 1047.5958
In this epoch 750/5000, Training loss: 0.0002, Test loss: 1047.6230
In this epoch 800/5000, Training loss: 0.0001, Test loss: 1047.6471
In this epoch 850/5000, Training loss: 0.0001, Test loss: 1047.6677
In this epoch 900/5000, Training loss: 0.0001, Test loss: 1047.6852
In this epoch 950/5000, Training loss: 0.0000, Test loss: 1047.7012
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 1047.7140
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 1047.7249
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 1047.7340
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 1047.7412
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 1047.7474
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 1047.7521
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 1047.7557
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 1047.7590
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 1047.7612
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 1047.7632
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 1047.7644
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 1047.7656
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 1047.7664
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 1047.7670
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 1047.7673
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 1047.7677
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 1047.7678
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 1047.7681
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 1047.7681
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 1047.7682
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 1047.7684
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 1047.7684
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 1047.7684
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 1047.7684
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 1047.7682
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 1047.7684
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 1047.7682
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 1047.7682
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 1047.7682
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 1047.7684
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 1047.7684
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 1047.7684
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 1047.7684
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 1047.7684
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 1047.7683
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 1047.7684
In this epoch 3650/5000, Training loss: 3076.6519, Test loss: 158291.4531
In this epoch 3700/5000, Training loss: 730992.0625, Test loss: 509309.5625
In this epoch 3750/5000, Training loss: 1706.3378, Test loss: 5348.1040
In this epoch 3800/5000, Training loss: 10.8553, Test loss: 1197.3329
In this epoch 3850/5000, Training loss: 0.1181, Test loss: 1199.8882
In this epoch 3900/5000, Training loss: 0.0002, Test loss: 1198.9191
In this epoch 3950/5000, Training loss: 0.0002, Test loss: 1198.8435
In this epoch 4000/5000, Training loss: 0.0001, Test loss: 1198.8318
In this epoch 4050/5000, Training loss: 0.0001, Test loss: 1198.8201
In this epoch 4100/5000, Training loss: 0.0001, Test loss: 1198.8085
In this epoch 4150/5000, Training loss: 0.0001, Test loss: 1198.7977
In this epoch 4200/5000, Training loss: 0.0001, Test loss: 1198.7885
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 1198.7798
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 1198.7716
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 1198.7644
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 1198.7574
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 1198.7515
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 1198.7460
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 1198.7407
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 1198.7358
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 1198.7321
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 1198.7283
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 1198.7249
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 1198.7219
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 1198.7191
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 1198.7168
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 1198.7150
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 1198.7129
train_acc 1.0
test_acc 0.011773356401384083
{'y_pred': [48.27729034423828, 33.61642074584961, 9.527624130249023, 22.35666847229004, -10.241914749145508, -50.572052001953125, 32.33369064331055, -26.502805709838867, 84.3326187133789, 58.015869140625, -46.629520416259766, 27.427040100097656, 5.4076008796691895, 78.33235168457031, 18.214054107666016, 45.846717834472656, -32.696044921875, 41.281333923339844, -5.380977153778076, 5.140942096710205, -20.498044967651367, -48.47730255126953, 1.1828683614730835, 11.971426010131836, 54.17124938964844, 41.752838134765625, -1.4670301675796509, 39.96006774902344, -26.331329345703125, -27.02034568786621, -2.159261465072632, 68.45712280273438, 50.2689094543457, 20.671674728393555, -40.147239685058594, 12.825523376464844, 48.20868682861328, 68.76658630371094, 34.92566680908203, 13.694703102111816, -1.1502000093460083, 33.232086181640625, 14.166518211364746, -3.8083913326263428, -14.655290603637695, 2.4178903102874756, 38.24367904663086, 31.107357025146484, -28.98944091796875, -43.56302261352539, 28.86109161376953, 55.60710906982422, 25.662200927734375, 70.19951629638672, 42.57402038574219, 97.11396789550781, -1.3060790300369263, 24.799396514892578, -4.624782085418701, 5.947437763214111, -10.663002014160156, 41.11910629272461, -12.069195747375488, 80.93396759033203, 27.967449188232422, -18.44416046142578, 9.383806228637695, 41.83439636230469, 8.287429809570312, -0.48612943291664124, -13.50678825378418, 36.29887390136719, 18.957489013671875, 30.818605422973633, 11.326854705810547, 18.983787536621094, 53.804378509521484, -2.141944646835327, 32.697776794433594, 25.51270294189453, 7.955461025238037, 4.070785999298096, 24.58095932006836, 30.639633178710938, -7.2095561027526855, 8.021523475646973, 13.713971138000488, -41.572200775146484, 20.837467193603516, 78.49689483642578, 7.387022495269775, 21.047176361083984, -45.626617431640625, -25.398651123046875, -18.915733337402344, 31.671567916870117, 11.848071098327637, 92.32913208007812, 30.22613525390625, 47.55439376831055, 19.27304458618164, -50.1139030456543, 19.978261947631836, 47.81983184814453, 4.99973726272583, 36.98588943481445, 35.82374954223633, -33.252079010009766, 7.024921894073486, 44.656436920166016, 80.10104370117188, 0.38740381598472595, -27.392919540405273, 65.49594116210938, 56.820411682128906, -27.417030334472656, 12.082719802856445, 23.702985763549805, -13.727849960327148, -4.086716175079346, 17.25843620300293, -16.581071853637695, 8.705623626708984, -30.994853973388672, -3.791010618209839, 31.816404342651367, 62.20802307128906, 25.483489990234375, -27.529964447021484, 31.786888122558594, -30.797645568847656, 37.33530044555664, 72.40221405029297, 52.334842681884766, -11.234750747680664, 3.4383432865142822, 5.583950519561768, -27.799442291259766, -3.7441189289093018, -19.136890411376953, -15.403059005737305, -29.55992889404297, 34.89413070678711, -47.36814880371094, 117.06320190429688, -36.438148498535156, 97.66761016845703, 33.34376907348633, 27.19283103942871, 41.45838165283203, -23.405038833618164, 63.6859130859375, 9.058372497558594, 61.69207000732422, -15.111167907714844, 31.516321182250977, -53.82665252685547, 3.028761148452759, -56.98058319091797, 75.8768539428711, -10.241914749145508, 56.104000091552734, -9.505817413330078, -50.76515579223633, 66.33589172363281, 30.658973693847656, 43.82646179199219, 38.85110092163086, -7.112832546234131, -0.056762371212244034, 2.2953951358795166, 5.853050231933594, 11.78500747680664, 10.94963264465332, -27.099082946777344, 56.153263092041016, 16.46639633178711, 26.768436431884766, 14.982789993286133, -49.83648681640625, -59.750484466552734, 4.698622226715088, 22.361812591552734, 15.958613395690918, 51.3975830078125, 6.888025760650635, 49.42156219482422, -53.61238098144531, -15.605499267578125, 2.577357530593872, 30.65430450439453, 86.49826049804688, 1.3652957677841187, 75.96025085449219, 58.01852035522461, 45.61383056640625, -23.54450035095215, 10.11109733581543, 12.461036682128906, 9.120758056640625, 10.370861053466797, -58.664451599121094, 55.17780303955078, 73.02001190185547, -1.9321051836013794, 45.42533493041992, -9.339803695678711, -17.568532943725586, 32.045291900634766, -27.934974670410156, -20.719066619873047, 2.6645209789276123, -5.715369701385498, 0.7977164387702942, -4.647712230682373, 13.568922996520996, 69.48246002197266, 63.49711227416992, 74.3857650756836, -47.0070915222168, -0.8054043650627136, 35.944705963134766, 46.24115753173828, -1.8931537866592407, 117.0463638305664, 65.7064208984375, 97.28504943847656, 5.458578586578369, 24.580610275268555, 64.61631774902344, -3.7619597911834717, -18.461790084838867, 40.302825927734375, -72.52923583984375, 18.564422607421875, -5.830798625946045, 4.23612117767334, 51.853233337402344, 24.578018188476562, 45.22748565673828, 42.92167663574219, 41.02613830566406, 10.899925231933594, -18.398345947265625, 17.647235870361328, -1.5278087854385376, -1.6808468103408813, 16.727222442626953, -8.487839698791504, 54.604515075683594, 11.731313705444336, -10.045768737792969, -87.06388092041016, 13.15159797668457, 39.68741226196289, -3.8951051235198975, 32.77161407470703, 29.83096694946289, 0.9472621083259583, 56.15734100341797, 47.794044494628906, 3.2422831058502197, 7.233360767364502, 14.376998901367188, -52.29503631591797, 65.5038833618164, 79.53076171875, 20.159374237060547, 52.281944274902344, 9.57040786743164, 32.43107986450195, 118.92151641845703, 39.839969635009766, -4.600793361663818, -34.22096633911133, 17.742921829223633, 9.446674346923828, 2.4330694675445557, 15.043899536132812, -7.446248531341553, 48.26806640625, 20.127689361572266, -38.6503791809082, 4.646395683288574, 57.3898811340332, 30.515975952148438, -25.454471588134766, 24.79477310180664, -20.623586654663086, 3.1055233478546143, 80.79000854492188, -7.950541973114014, 25.9550838470459, 104.77894592285156, 71.32049560546875, 75.35175323486328, 19.966716766357422, -30.144248962402344, 8.827632904052734, 46.56190872192383, 27.644744873046875, -9.128900527954102, 25.672405242919922, 31.817455291748047, 4.119776725769043, 31.51233673095703, 11.749917984008789, 16.800159454345703, 25.60456085205078, -15.375301361083984, -6.103972911834717, 3.601248025894165, 5.923887729644775, 31.16126823425293, 89.05850219726562, 49.16075897216797, 15.752613067626953, -8.237687110900879, 90.37506103515625, 49.730262756347656, 73.14410400390625, 47.82097625732422, 36.165679931640625, 20.766082763671875, 20.996755599975586, -49.29636764526367, -57.15565490722656, -29.206436157226562, 3.389953851699829, 26.26067352294922, 11.772109985351562, 3.0906574726104736, 31.76908302307129, 5.15079927444458, 36.18498229980469, 26.60780143737793, 72.7091293334961, 76.24797821044922, 1.5110782384872437, 56.93254852294922], 'y_pred_int': [48, 34, 10, 22, 0, 0, 32, 0, 84, 58, 0, 27, 5, 78, 18, 46, 0, 41, 0, 5, 0, 0, 1, 12, 54, 42, 0, 40, 0, 0, 0, 68, 50, 21, 0, 13, 48, 69, 35, 14, 0, 33, 14, 0, 0, 2, 38, 31, 0, 0, 29, 56, 26, 70, 43, 97, 0, 25, 0, 6, 0, 41, 0, 81, 28, 0, 9, 42, 8, 0, 0, 36, 19, 31, 11, 19, 54, 0, 33, 26, 8, 4, 25, 31, 0, 8, 14, 0, 21, 78, 7, 21, 0, 0, 0, 32, 12, 92, 30, 48, 19, 0, 20, 48, 5, 37, 36, 0, 7, 45, 80, 0, 0, 65, 57, 0, 12, 24, 0, 0, 17, 0, 9, 0, 0, 32, 62, 25, 0, 32, 0, 37, 72, 52, 0, 3, 6, 0, 0, 0, 0, 0, 35, 0, 117, 0, 98, 33, 27, 41, 0, 64, 9, 62, 0, 32, 0, 3, 0, 76, 0, 56, 0, 0, 66, 31, 44, 39, 0, 0, 2, 6, 12, 11, 0, 56, 16, 27, 15, 0, 0, 5, 22, 16, 51, 7, 49, 0, 0, 3, 31, 86, 1, 76, 58, 46, 0, 10, 12, 9, 10, 0, 55, 73, 0, 45, 0, 0, 32, 0, 0, 3, 0, 1, 0, 14, 69, 63, 74, 0, 0, 36, 46, 0, 117, 66, 97, 5, 25, 65, 0, 0, 40, 0, 19, 0, 4, 52, 25, 45, 43, 41, 11, 0, 18, 0, 0, 17, 0, 55, 12, 0, 0, 13, 40, 0, 33, 30, 1, 56, 48, 3, 7, 14, 0, 66, 80, 20, 52, 10, 32, 119, 40, 0, 0, 18, 9, 2, 15, 0, 48, 20, 0, 5, 57, 31, 0, 25, 0, 3, 81, 0, 26, 105, 71, 75, 20, 0, 9, 47, 28, 0, 26, 32, 4, 32, 12, 17, 26, 0, 0, 4, 6, 31, 89, 49, 16, 0, 90, 50, 73, 48, 36, 21, 21, 0, 0, 0, 3, 26, 12, 3, 32, 5, 36, 27, 73, 76, 2, 57], 'y_true': [10, 8, 16, 5, 29, 7, 14, 16, 34, 18, 6, 13, 4, 28, 2, 7, 3, 62, 9, 9, 20, 10, 15, 6, 5, 27, 30, 23, 4, 16, 14, 10, 12, 22, 13, 6, 27, 15, 15, 23, 3, 2, 6, 10, 15, 9, 16, 7, 8, 6, 14, 37, 28, 26, 8, 25, 6, 7, 10, 13, 8, 20, 1, 19, 4, 6, 6, 23, 14, 22, 10, 5, 9, 15, 39, 10, 21, 14, 10, 7, 17, 9, 21, 5, 6, 11, 41, 11, 14, 36, 25, 4, 11, 3, 3, 9, 29, 17, 17, 16, 10, 17, 14, 10, 8, 21, 50, 5, 3, 33, 37, 7, 7, 8, 13, 10, 17, 22, 12, 18, 2, 11, 5, 8, 6, 33, 29, 19, 7, 14, 5, 16, 25, 15, 20, 13, 5, 23, 31, 7, 23, 19, 19, 13, 17, 8, 19, 6, 8, 3, 12, 10, 16, 9, 16, 14, 11, 5, 11, 28, 29, 17, 17, 4, 7, 45, 19, 17, 15, 20, 12, 9, 7, 10, 8, 8, 2, 5, 36, 9, 9, 3, 13, 30, 23, 5, 12, 10, 2, 6, 21, 44, 8, 33, 25, 8, 8, 10, 40, 11, 29, 7, 9, 13, 20, 25, 15, 16, 4, 12, 12, 6, 29, 3, 17, 9, 22, 18, 39, 15, 13, 46, 4, 5, 11, 32, 18, 12, 19, 9, 14, 100, 3, 10, 8, 6, 25, 22, 4, 11, 3, 12, 1, 6, 9, 11, 5, 42, 3, 28, 11, 3, 7, 18, 51, 10, 15, 10, 41, 17, 31, 8, 18, 21, 17, 28, 33, 19, 13, 35, 7, 21, 26, 5, 5, 6, 8, 6, 18, 5, 23, 11, 7, 9, 29, 29, 3, 15, 4, 6, 12, 13, 10, 14, 20, 8, 23, 12, 14, 6, 27, 15, 12, 12, 10, 11, 11, 4, 4, 10, 5, 18, 6, 10, 25, 16, 15, 5, 7, 32, 47, 17, 8, 12, 6, 10, 14, 6, 34, 4, 16, 4, 12, 8, 3, 3, 6, 11, 5, 31], 'train_acc': 1.0, 'test_acc': 0.011773356401384083, 'X': 'X_TANL_layer_6_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_7_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 228638.2188, Test loss: 63613.7656
In this epoch 100/5000, Training loss: 1603.3679, Test loss: 2298.2417
In this epoch 150/5000, Training loss: 4.3839, Test loss: 1445.9803
In this epoch 200/5000, Training loss: 0.0136, Test loss: 1488.0627
In this epoch 250/5000, Training loss: 0.0001, Test loss: 1492.4734
In this epoch 300/5000, Training loss: 0.0000, Test loss: 1492.1880
In this epoch 350/5000, Training loss: 0.0000, Test loss: 1492.1616
In this epoch 400/5000, Training loss: 0.0000, Test loss: 1492.1670
In this epoch 450/5000, Training loss: 0.0000, Test loss: 1492.1708
In this epoch 500/5000, Training loss: 0.0000, Test loss: 1492.1737
In this epoch 550/5000, Training loss: 0.0000, Test loss: 1492.1770
In this epoch 600/5000, Training loss: 0.0000, Test loss: 1492.1799
In this epoch 650/5000, Training loss: 0.0000, Test loss: 1492.1824
In this epoch 700/5000, Training loss: 0.0000, Test loss: 1492.1846
In this epoch 750/5000, Training loss: 0.0000, Test loss: 1492.1870
In this epoch 800/5000, Training loss: 0.0000, Test loss: 1492.1886
In this epoch 850/5000, Training loss: 0.0000, Test loss: 1492.1901
In this epoch 900/5000, Training loss: 0.0000, Test loss: 1492.1918
In this epoch 950/5000, Training loss: 0.0000, Test loss: 1492.1927
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 1492.1937
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 1492.1948
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 1492.1952
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 1492.1960
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 1492.1965
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 1492.1968
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 1492.1971
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 1492.1971
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 1492.1973
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 1492.1975
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 1492.1976
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 1492.1975
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 1492.1976
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 1492.1976
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 1492.1976
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 1492.1978
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 1492.1978
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 1492.1980
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 1492.1980
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 1492.1978
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 1492.1978
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 1492.1979
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 1492.1980
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 1492.1981
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 1492.1980
In this epoch 4100/5000, Training loss: 8262206.0000, Test loss: 2902244.7500
In this epoch 4150/5000, Training loss: 39367.0586, Test loss: 6344.8418
In this epoch 4200/5000, Training loss: 27.8979, Test loss: 1721.3331
In this epoch 4250/5000, Training loss: 1.0452, Test loss: 1491.1447
In this epoch 4300/5000, Training loss: 0.0007, Test loss: 1504.7135
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 1505.9642
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 1506.0568
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 1506.0538
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 1506.0530
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 1506.0524
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 1506.0518
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 1506.0511
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 1506.0508
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 1506.0505
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 1506.0502
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 1506.0499
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 1506.0498
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 1506.0497
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 1506.0494
train_acc 1.0
test_acc 0.01217128027681661
{'y_pred': [17.463685989379883, -33.377689361572266, 71.54629516601562, -28.637300491333008, 32.77884292602539, 9.983888626098633, 40.601497650146484, 57.86648941040039, 19.304800033569336, 7.386370658874512, 66.41781616210938, 2.1775805950164795, -24.899301528930664, 27.295148849487305, 64.10152435302734, 28.151060104370117, 67.69097137451172, 61.104835510253906, -8.319472312927246, 13.742812156677246, 67.13521575927734, -23.9603214263916, -77.97924041748047, -14.538296699523926, 59.12580108642578, 7.270096778869629, 45.612464904785156, 34.07914352416992, 54.68601989746094, 16.933944702148438, 29.340211868286133, 2.1449782848358154, 92.06026458740234, 30.65705680847168, -16.27553939819336, -35.69747543334961, 4.239859580993652, 107.17362213134766, -28.874914169311523, -48.98335266113281, 40.938236236572266, 49.35811996459961, 66.94416809082031, 22.645700454711914, 58.268619537353516, -46.93785858154297, -7.108323097229004, 57.718780517578125, 13.085631370544434, 31.36833381652832, -1.768159031867981, 17.979360580444336, 58.901466369628906, 23.832660675048828, 15.208388328552246, 16.918771743774414, -13.305384635925293, -19.11964988708496, 33.60596466064453, 30.623620986938477, 9.952580451965332, 39.234397888183594, 37.28398132324219, 23.10087776184082, -12.003020286560059, -18.26118278503418, 48.72150802612305, 26.865217208862305, -41.85240936279297, 16.565465927124023, 63.02181625366211, 30.138690948486328, 34.41780471801758, 138.93569946289062, 63.275630950927734, 113.13968658447266, 55.124542236328125, 9.829511642456055, 9.566620826721191, 52.79970169067383, 85.46436309814453, 47.14286422729492, 113.93392944335938, 19.3314266204834, 41.077293395996094, 7.769495010375977, 45.278621673583984, 62.05557632446289, -4.325844764709473, 59.74510192871094, 10.047560691833496, 91.85203552246094, 34.78355407714844, 40.07797622680664, 70.10203552246094, 50.66805648803711, -40.71467208862305, -17.678653717041016, -41.439205169677734, -8.601639747619629, 25.242128372192383, 10.641708374023438, 57.733917236328125, 151.29119873046875, 3.4909164905548096, -15.698677062988281, 10.776812553405762, -0.8419601321220398, 27.14690589904785, 81.1230239868164, 24.640178680419922, 103.63928985595703, 8.800209999084473, 14.831472396850586, 89.64815521240234, 15.048544883728027, 75.90567779541016, 61.59982681274414, 122.83601379394531, -59.5310173034668, -17.90996742248535, 35.822021484375, 15.787476539611816, 41.624176025390625, 31.661069869995117, -19.2381649017334, 81.24076843261719, -75.62288665771484, 41.657958984375, 53.05214309692383, -24.685531616210938, 23.35032081604004, 16.364912033081055, -24.52543067932129, -11.45632266998291, 6.001620292663574, 2.7961995601654053, 81.08894348144531, 45.6374626159668, -25.69322395324707, 29.20183753967285, 11.704596519470215, 2.881676435470581, 46.276954650878906, 8.000000953674316, 4.794861793518066, -9.3012056350708, 45.07601547241211, 6.919798851013184, 41.56794357299805, 34.54511642456055, 2.2687442302703857, -19.52248191833496, 33.78608322143555, -5.259280204772949, -10.101560592651367, 15.407870292663574, 81.2109375, -31.388896942138672, 44.93999481201172, 25.818910598754883, -59.13173294067383, 21.132532119750977, 87.18071746826172, 65.34208679199219, 35.71846389770508, 37.21501159667969, 32.3691520690918, 47.11653518676758, 63.13534164428711, -45.55791473388672, 39.97737121582031, -3.6651546955108643, 103.0156021118164, 15.77738094329834, 33.58642578125, 7.452696800231934, 30.876617431640625, 37.79555892944336, 67.27761840820312, 39.34918975830078, 43.714298248291016, 5.2176833152771, 83.62947845458984, -4.678267002105713, 29.172212600708008, -74.329345703125, 30.339326858520508, 26.02992057800293, 36.38615798950195, 45.323307037353516, 15.403605461120605, 10.031264305114746, 15.814970970153809, 49.264095306396484, 90.95722961425781, 7.6981916427612305, 31.798757553100586, -41.11362075805664, -1.2017112970352173, -25.731924057006836, 19.692533493041992, -16.917715072631836, 45.06884002685547, 20.609601974487305, 53.056251525878906, 39.377376556396484, 60.390750885009766, 18.88996124267578, 23.714879989624023, 78.8206787109375, 73.11268615722656, 94.8173828125, 28.61918067932129, -12.866703987121582, -33.799068450927734, 33.55378723144531, 16.98862648010254, 94.32833099365234, 4.465485572814941, 19.56287956237793, -6.9949140548706055, 45.5095100402832, 9.269448280334473, 24.84730339050293, 5.7577314376831055, 45.15728759765625, 106.24895477294922, 74.7774887084961, 33.67229080200195, 32.77030563354492, 46.80045700073242, 91.58303833007812, 18.922086715698242, 33.2485237121582, 60.47838592529297, 42.23037338256836, 33.08857345581055, -68.65557098388672, 44.35092544555664, 86.40678405761719, 41.2137336730957, 59.25629806518555, 33.94094467163086, -2.73449444770813, -21.07938003540039, -32.2032470703125, 3.001662015914917, 25.240333557128906, -6.959748268127441, -2.927321195602417, 5.541735649108887, 68.09662628173828, 16.188949584960938, 56.597938537597656, -8.073494911193848, -52.43422317504883, 18.18694496154785, 2.3757007122039795, 45.70988082885742, -14.188307762145996, 8.042767524719238, 8.854144096374512, -27.174713134765625, 11.563145637512207, 6.131518363952637, -13.756566047668457, -5.930752754211426, -35.201358795166016, -12.928236961364746, 32.748600006103516, -15.772263526916504, 9.468138694763184, 20.3475399017334, -3.9724209308624268, 67.78407287597656, 12.82242488861084, 25.67470359802246, 68.45172882080078, 39.82417297363281, 18.98746681213379, 59.83218002319336, 4.433516502380371, 58.9141845703125, 4.193164825439453, 30.657413482666016, 76.24495697021484, 0.32931119203567505, 23.93223762512207, -11.884625434875488, -52.13112258911133, 58.975711822509766, 62.64264678955078, 54.86086654663086, 77.95237731933594, -18.81322479248047, 3.3201186656951904, 95.4261703491211, -10.622794151306152, 15.845770835876465, 50.18659210205078, -18.217287063598633, -14.048359870910645, 2.881676435470581, 5.787696838378906, 18.02766990661621, -54.2788200378418, 9.047526359558105, -42.79694747924805, 23.88286018371582, 24.140138626098633, 46.99269104003906, -36.00653076171875, 20.994781494140625, 58.00004577636719, 47.37559127807617, 36.076324462890625, 67.83223724365234, -25.740571975708008, 108.1703872680664, 51.1007194519043, 64.30377960205078, 74.30326080322266, -70.89019012451172, 30.479167938232422, 21.63090705871582, 13.571784019470215, -22.735841751098633, 53.8102912902832, 63.78137969970703, 37.040279388427734, -41.071224212646484, 73.82537841796875, 5.823203086853027, 8.26030445098877, 24.56380271911621, 6.349254608154297, 68.35847473144531, 6.023711204528809, -20.180728912353516], 'y_pred_int': [17, 0, 72, 0, 33, 10, 41, 58, 19, 7, 66, 2, 0, 27, 64, 28, 68, 61, 0, 14, 67, 0, 0, 0, 59, 7, 46, 34, 55, 17, 29, 2, 92, 31, 0, 0, 4, 107, 0, 0, 41, 49, 67, 23, 58, 0, 0, 58, 13, 31, 0, 18, 59, 24, 15, 17, 0, 0, 34, 31, 10, 39, 37, 23, 0, 0, 49, 27, 0, 17, 63, 30, 34, 139, 63, 113, 55, 10, 10, 53, 85, 47, 114, 19, 41, 8, 45, 62, 0, 60, 10, 92, 35, 40, 70, 51, 0, 0, 0, 0, 25, 11, 58, 151, 3, 0, 11, 0, 27, 81, 25, 104, 9, 15, 90, 15, 76, 62, 123, 0, 0, 36, 16, 42, 32, 0, 81, 0, 42, 53, 0, 23, 16, 0, 0, 6, 3, 81, 46, 0, 29, 12, 3, 46, 8, 5, 0, 45, 7, 42, 35, 2, 0, 34, 0, 0, 15, 81, 0, 45, 26, 0, 21, 87, 65, 36, 37, 32, 47, 63, 0, 40, 0, 103, 16, 34, 7, 31, 38, 67, 39, 44, 5, 84, 0, 29, 0, 30, 26, 36, 45, 15, 10, 16, 49, 91, 8, 32, 0, 0, 0, 20, 0, 45, 21, 53, 39, 60, 19, 24, 79, 73, 95, 29, 0, 0, 34, 17, 94, 4, 20, 0, 46, 9, 25, 6, 45, 106, 75, 34, 33, 47, 92, 19, 33, 60, 42, 33, 0, 44, 86, 41, 59, 34, 0, 0, 0, 3, 25, 0, 0, 6, 68, 16, 57, 0, 0, 18, 2, 46, 0, 8, 9, 0, 12, 6, 0, 0, 0, 0, 33, 0, 9, 20, 0, 68, 13, 26, 68, 40, 19, 60, 4, 59, 4, 31, 76, 0, 24, 0, 0, 59, 63, 55, 78, 0, 3, 95, 0, 16, 50, 0, 0, 3, 6, 18, 0, 9, 0, 24, 24, 47, 0, 21, 58, 47, 36, 68, 0, 108, 51, 64, 74, 0, 30, 22, 14, 0, 54, 64, 37, 0, 74, 6, 8, 25, 6, 68, 6, 0], 'y_true': [4, 8, 10, 6, 10, 9, 46, 21, 18, 9, 15, 6, 4, 1, 42, 11, 8, 29, 6, 13, 49, 10, 7, 31, 22, 19, 9, 7, 17, 28, 10, 2, 19, 17, 3, 11, 9, 35, 8, 28, 17, 10, 13, 4, 32, 8, 15, 5, 23, 15, 5, 9, 10, 5, 17, 14, 4, 16, 14, 10, 20, 31, 5, 16, 6, 18, 6, 14, 10, 6, 14, 10, 5, 11, 11, 30, 45, 39, 6, 63, 8, 26, 46, 15, 22, 4, 14, 29, 10, 11, 14, 15, 2, 9, 10, 6, 14, 7, 9, 5, 8, 13, 19, 9, 23, 10, 10, 5, 76, 34, 14, 14, 8, 12, 14, 6, 31, 42, 14, 16, 12, 25, 15, 12, 10, 11, 20, 17, 24, 16, 5, 25, 12, 9, 27, 9, 15, 35, 4, 4, 2, 29, 29, 6, 5, 4, 9, 9, 7, 26, 27, 10, 7, 17, 31, 42, 10, 2, 5, 26, 3, 50, 11, 29, 6, 1, 7, 26, 7, 11, 7, 12, 5, 10, 10, 4, 11, 50, 10, 15, 15, 12, 6, 16, 10, 10, 8, 9, 22, 11, 15, 7, 5, 20, 6, 11, 13, 8, 9, 9, 10, 17, 6, 30, 12, 17, 15, 17, 4, 18, 5, 13, 4, 13, 23, 9, 4, 5, 9, 5, 23, 7, 14, 6, 7, 16, 4, 40, 54, 8, 21, 19, 28, 3, 38, 18, 33, 39, 9, 16, 36, 4, 28, 16, 8, 14, 6, 7, 7, 5, 6, 8, 16, 11, 34, 20, 7, 7, 40, 28, 19, 3, 7, 11, 2, 14, 15, 24, 9, 6, 19, 3, 29, 19, 4, 13, 43, 13, 18, 12, 16, 11, 25, 16, 12, 5, 32, 12, 2, 4, 6, 33, 5, 18, 22, 8, 5, 17, 7, 10, 7, 14, 7, 29, 49, 14, 9, 5, 6, 10, 30, 4, 11, 21, 5, 38, 27, 18, 7, 12, 10, 9, 23, 30, 9, 6, 10, 12, 10, 4, 12, 10, 7, 2, 5, 10, 3, 6, 13, 7], 'train_acc': 1.0, 'test_acc': 0.01217128027681661, 'X': 'X_TANL_layer_7_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_8_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 104785.2969, Test loss: 262591.9688
In this epoch 100/5000, Training loss: 676.1339, Test loss: 3395.7019
In this epoch 150/5000, Training loss: 5.4470, Test loss: 3133.2366
In this epoch 200/5000, Training loss: 0.0021, Test loss: 3203.3152
In this epoch 250/5000, Training loss: 0.0000, Test loss: 3198.5515
In this epoch 300/5000, Training loss: 0.0000, Test loss: 3198.2461
In this epoch 350/5000, Training loss: 0.0000, Test loss: 3198.2732
In this epoch 400/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 450/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 500/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 550/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 600/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 650/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 700/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 750/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 800/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 850/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 900/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 950/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 3198.2717
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 3198.2717
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 3198.2725
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 3198.2722
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 3198.2737
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 3198.2732
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 3198.2729
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 3198.2737
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 3198.2737
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 3198.2737
In this epoch 3850/5000, Training loss: 16109266.0000, Test loss: 14529172.0000
In this epoch 3900/5000, Training loss: 12886.3682, Test loss: 94521.2422
In this epoch 3950/5000, Training loss: 4.3016, Test loss: 3079.1997
In this epoch 4000/5000, Training loss: 0.5983, Test loss: 3063.8545
In this epoch 4050/5000, Training loss: 0.0152, Test loss: 3033.7498
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 3034.7952
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 3034.9136
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 3034.9199
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 3034.9202
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 3034.9199
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 3034.9199
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 3034.9202
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 3034.9202
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 3034.9204
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 3034.9204
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 3034.9202
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 3034.9202
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 3034.9204
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 3034.9202
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 3034.9204
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 3034.9202
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 3034.9202
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 3034.9204
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 3034.9204
train_acc 1.0
test_acc 0.00819204152249135
{'y_pred': [-94.48837280273438, 111.52876281738281, 43.88511657714844, 26.516834259033203, 14.41415786743164, 67.94327545166016, 81.25385284423828, 58.55161666870117, 52.68718338012695, 61.281253814697266, 71.63546752929688, 11.025203704833984, 7.3777289390563965, -22.960472106933594, -39.75297546386719, -1.2951186895370483, -2.8310399055480957, 119.08403015136719, 9.313899993896484, 22.525970458984375, -55.52040481567383, 86.10604858398438, 12.27767562866211, -11.187938690185547, 88.85600280761719, 12.166786193847656, 51.84442901611328, 71.370361328125, -31.768367767333984, 136.62652587890625, 35.59302520751953, -13.319005966186523, -24.602176666259766, -33.69686508178711, 123.12998962402344, 9.883731842041016, 81.4761962890625, -12.404239654541016, 75.00830078125, 64.355712890625, -3.584686756134033, 83.53627014160156, 26.723804473876953, 4.392262935638428, 43.49981689453125, -8.515621185302734, 48.692684173583984, 56.34944152832031, -61.00218963623047, -76.32867431640625, 45.996063232421875, -17.761198043823242, 98.85275268554688, -61.72500228881836, 24.846393585205078, 89.55670166015625, 115.1462173461914, 46.84225845336914, -84.12610626220703, -91.63690948486328, -10.001472473144531, 40.68524169921875, 65.71419525146484, -35.82152557373047, -50.68667984008789, 54.40213394165039, -120.2317886352539, 7.548768520355225, 29.53589630126953, -54.85177230834961, 2.620368480682373, 24.767009735107422, 119.2293701171875, -16.77476692199707, 72.52049255371094, 47.010196685791016, 13.709403991699219, 55.515254974365234, -23.5123291015625, -31.409809112548828, -27.64836883544922, 15.259223937988281, -35.85931396484375, 68.17219543457031, 10.837711334228516, 32.116336822509766, 61.11452102661133, 26.067291259765625, 16.341737747192383, 13.100284576416016, 3.65016508102417, 89.1937255859375, -22.256855010986328, 19.026880264282227, 8.120681762695312, -2.3426504135131836, 30.724300384521484, 121.75440216064453, 28.396059036254883, 61.66054916381836, 73.83364868164062, 28.877052307128906, -26.91680145263672, 32.59748458862305, -6.607193470001221, -89.41299438476562, 94.50421142578125, 69.41950225830078, 105.41262817382812, 30.73731231689453, 116.12589263916016, 7.782721996307373, -41.272396087646484, -37.31178283691406, -1.3303149938583374, 40.3631706237793, 16.196460723876953, 118.97828674316406, -13.46832275390625, 1.7021900415420532, 63.282222747802734, 21.960735321044922, 42.66379928588867, -27.933929443359375, -42.78339767456055, 80.11294555664062, -28.714553833007812, 79.2715835571289, 119.5472183227539, 17.560474395751953, 53.271522521972656, 20.538814544677734, 23.920059204101562, 151.66741943359375, 87.66453552246094, 127.44779205322266, -43.97193145751953, 13.311229705810547, -3.9860997200012207, -0.6025241613388062, -78.5796890258789, 5.244853496551514, 34.757747650146484, 37.68437576293945, 49.767250061035156, 3.477790355682373, 24.854610443115234, 59.20358657836914, 44.34721755981445, 0.6792894601821899, 93.48894500732422, 10.276386260986328, -38.025081634521484, -28.755727767944336, -14.473224639892578, 49.26730728149414, -3.760441303253174, -76.71800231933594, 91.96395874023438, -4.6234822273254395, -36.54050827026367, -134.44818115234375, 34.4719352722168, -55.47175979614258, 59.21390914916992, -40.10784912109375, -14.66806411743164, 119.62844848632812, 53.878440856933594, 6.984771251678467, 77.73369598388672, -29.740581512451172, 12.039478302001953, 74.11637878417969, 133.44207763671875, -41.6479606628418, 44.29523849487305, -33.51383590698242, 11.431631088256836, -9.54412841796875, 8.77273178100586, -117.74640655517578, -9.274333953857422, 58.82884216308594, 13.092365264892578, -7.297512531280518, -22.429454803466797, 33.83549880981445, 12.880203247070312, 76.10601806640625, 83.33950805664062, 20.487607955932617, 37.81428146362305, -36.81835174560547, 15.919174194335938, 54.265018463134766, 67.19818115234375, 32.31064987182617, -18.482826232910156, 132.93869018554688, 12.999858856201172, 91.54043579101562, -29.73090362548828, -43.75362014770508, 30.39352798461914, 113.31993103027344, -44.57542037963867, -34.58842086791992, 90.91385650634766, 98.87681579589844, 38.20219039916992, 105.61530303955078, -45.37569046020508, -82.86551666259766, 18.49441146850586, -53.14912796020508, -59.79277420043945, -22.720691680908203, 107.8468246459961, 113.07566833496094, 75.78704833984375, 37.954952239990234, 19.546695709228516, -8.732959747314453, 1.6041408777236938, -83.35763549804688, 18.46979522705078, -43.552642822265625, 19.694522857666016, -0.8598064184188843, -70.76448059082031, -36.06606674194336, -8.922626495361328, 17.750656127929688, 16.28826904296875, 63.73097610473633, 65.02867889404297, -55.66936492919922, 96.65785217285156, 47.904911041259766, 4.844146251678467, -20.018760681152344, -24.823591232299805, 132.56771850585938, 15.488330841064453, -31.524566650390625, 25.22632598876953, -51.9742317199707, -27.090763092041016, 91.53781127929688, -17.582077026367188, -45.634403228759766, 119.19186401367188, -60.64029312133789, -5.285908222198486, -14.510784149169922, 18.275691986083984, -79.6506576538086, 75.69180297851562, 49.64057922363281, 15.521595001220703, 99.55810546875, 63.60947036743164, 71.1985092163086, -5.606014728546143, -24.744983673095703, 0.00971280224621296, 62.27866744995117, 85.84095764160156, 39.9476432800293, 86.32538604736328, -10.193683624267578, -80.10208129882812, -8.8905029296875, -89.70181274414062, 159.81617736816406, -95.3599624633789, 12.562362670898438, -42.088706970214844, -34.055782318115234, -33.75252914428711, 128.37591552734375, 43.470245361328125, 79.86727905273438, 68.40069580078125, 51.83023452758789, -7.760441303253174, 8.726726531982422, -2.5181241035461426, 14.61941909790039, 152.25396728515625, -103.54476928710938, 86.93251037597656, -14.115402221679688, 80.55284118652344, 37.07625198364258, 118.97294616699219, 0.5586467981338501, -18.22388458251953, 72.04688262939453, 63.88773727416992, 20.827129364013672, 58.4679069519043, 28.258190155029297, -19.42229461669922, 101.57630920410156, 10.121284484863281, 53.33259582519531, 3.3125224113464355, -73.6373291015625, 29.298139572143555, 123.67271423339844, -32.02216720581055, 33.438602447509766, -15.656364440917969, -1.4634689092636108, 88.95391845703125, 40.088592529296875, 45.007408142089844, 51.80282974243164, -32.3932991027832, 9.825885772705078, 72.8297119140625, 89.66952514648438, -7.077396869659424, 33.54132843017578, 1.5759693384170532, 35.585811614990234, 8.437995910644531, 11.897037506103516, 27.072837829589844, 59.43629837036133, 2.9338793754577637, 13.643611907958984, 77.31231689453125, -18.638683319091797, 14.08175277709961, 6.661448955535889, 68.15618896484375, -1.8220943212509155], 'y_pred_int': [0, 112, 44, 27, 14, 68, 81, 59, 53, 61, 72, 11, 7, 0, 0, 0, 0, 119, 9, 23, 0, 86, 12, 0, 89, 12, 52, 71, 0, 137, 36, 0, 0, 0, 123, 10, 81, 0, 75, 64, 0, 84, 27, 4, 43, 0, 49, 56, 0, 0, 46, 0, 99, 0, 25, 90, 115, 47, 0, 0, 0, 41, 66, 0, 0, 54, 0, 8, 30, 0, 3, 25, 119, 0, 73, 47, 14, 56, 0, 0, 0, 15, 0, 68, 11, 32, 61, 26, 16, 13, 4, 89, 0, 19, 8, 0, 31, 122, 28, 62, 74, 29, 0, 33, 0, 0, 95, 69, 105, 31, 116, 8, 0, 0, 0, 40, 16, 119, 0, 2, 63, 22, 43, 0, 0, 80, 0, 79, 120, 18, 53, 21, 24, 152, 88, 127, 0, 13, 0, 0, 0, 5, 35, 38, 50, 3, 25, 59, 44, 1, 93, 10, 0, 0, 0, 49, 0, 0, 92, 0, 0, 0, 34, 0, 59, 0, 0, 120, 54, 7, 78, 0, 12, 74, 133, 0, 44, 0, 11, 0, 9, 0, 0, 59, 13, 0, 0, 34, 13, 76, 83, 20, 38, 0, 16, 54, 67, 32, 0, 133, 13, 92, 0, 0, 30, 113, 0, 0, 91, 99, 38, 106, 0, 0, 18, 0, 0, 0, 108, 113, 76, 38, 20, 0, 2, 0, 18, 0, 20, 0, 0, 0, 0, 18, 16, 64, 65, 0, 97, 48, 5, 0, 0, 133, 15, 0, 25, 0, 0, 92, 0, 0, 119, 0, 0, 0, 18, 0, 76, 50, 16, 100, 64, 71, 0, 0, 0, 62, 86, 40, 86, 0, 0, 0, 0, 160, 0, 13, 0, 0, 0, 128, 43, 80, 68, 52, 0, 9, 0, 15, 152, 0, 87, 0, 81, 37, 119, 1, 0, 72, 64, 21, 58, 28, 0, 102, 10, 53, 3, 0, 29, 124, 0, 33, 0, 0, 89, 40, 45, 52, 0, 10, 73, 90, 0, 34, 2, 36, 8, 12, 27, 59, 3, 14, 77, 0, 14, 7, 68, 0], 'y_true': [47, 14, 16, 2, 27, 18, 10, 9, 16, 6, 11, 20, 16, 2, 30, 16, 17, 7, 11, 14, 20, 14, 10, 14, 9, 34, 13, 7, 2, 22, 10, 11, 7, 22, 9, 5, 26, 14, 7, 15, 11, 9, 3, 4, 13, 17, 8, 9, 5, 20, 12, 8, 18, 5, 8, 6, 31, 6, 35, 15, 4, 50, 15, 8, 9, 5, 6, 4, 19, 5, 11, 8, 10, 8, 9, 8, 37, 26, 7, 9, 4, 15, 6, 28, 9, 3, 10, 17, 8, 11, 6, 8, 2, 18, 19, 10, 7, 11, 9, 29, 13, 29, 9, 15, 37, 9, 8, 32, 14, 34, 11, 10, 9, 8, 5, 20, 25, 23, 9, 3, 7, 4, 23, 11, 22, 10, 23, 21, 6, 25, 18, 6, 46, 16, 15, 8, 23, 4, 11, 4, 5, 14, 26, 13, 7, 4, 14, 39, 19, 10, 11, 7, 6, 23, 20, 3, 5, 4, 13, 3, 14, 9, 3, 3, 12, 11, 6, 23, 14, 16, 13, 4, 3, 14, 40, 24, 37, 7, 32, 18, 16, 4, 14, 13, 11, 4, 10, 15, 11, 33, 18, 7, 5, 9, 15, 12, 7, 6, 7, 19, 13, 20, 23, 5, 29, 5, 3, 4, 26, 8, 15, 9, 6, 9, 38, 26, 9, 9, 31, 11, 7, 5, 28, 7, 6, 17, 24, 8, 37, 6, 5, 7, 2, 19, 6, 19, 37, 20, 7, 18, 22, 24, 13, 12, 22, 6, 4, 12, 8, 9, 5, 4, 12, 3, 39, 7, 4, 7, 38, 6, 16, 8, 20, 7, 19, 35, 9, 45, 7, 23, 8, 10, 7, 11, 8, 15, 8, 4, 9, 10, 16, 8, 8, 15, 8, 7, 11, 8, 39, 7, 11, 9, 10, 10, 6, 5, 15, 12, 15, 7, 23, 3, 11, 7, 22, 16, 29, 10, 10, 5, 9, 16, 4, 16, 4, 19, 21, 13, 21, 16, 5, 8, 16, 26, 4, 8, 4, 8, 22, 7, 9, 18, 3, 3, 18, 11, 9, 33, 7, 6], 'train_acc': 1.0, 'test_acc': 0.00819204152249135, 'X': 'X_TANL_layer_8_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_9_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 495451.5938, Test loss: 137553.0156
In this epoch 100/5000, Training loss: 3425.1589, Test loss: 8052.5347
In this epoch 150/5000, Training loss: 9.3211, Test loss: 6618.6650
In this epoch 200/5000, Training loss: 0.0509, Test loss: 6659.0132
In this epoch 250/5000, Training loss: 0.0172, Test loss: 6664.0869
In this epoch 300/5000, Training loss: 0.0144, Test loss: 6663.7065
In this epoch 350/5000, Training loss: 0.0120, Test loss: 6663.6265
In this epoch 400/5000, Training loss: 0.0098, Test loss: 6663.5757
In this epoch 450/5000, Training loss: 0.0079, Test loss: 6663.5220
In this epoch 500/5000, Training loss: 0.0063, Test loss: 6663.4717
In this epoch 550/5000, Training loss: 0.0049, Test loss: 6663.4312
In this epoch 600/5000, Training loss: 0.0038, Test loss: 6663.3911
In this epoch 650/5000, Training loss: 0.0029, Test loss: 6663.3516
In this epoch 700/5000, Training loss: 0.0021, Test loss: 6663.3198
In this epoch 750/5000, Training loss: 0.0016, Test loss: 6663.2900
In this epoch 800/5000, Training loss: 0.0011, Test loss: 6663.2583
In this epoch 850/5000, Training loss: 0.0008, Test loss: 6663.2354
In this epoch 900/5000, Training loss: 0.0006, Test loss: 6663.2236
In this epoch 950/5000, Training loss: 0.0004, Test loss: 6663.2051
In this epoch 1000/5000, Training loss: 0.0003, Test loss: 6663.1885
In this epoch 1050/5000, Training loss: 0.0002, Test loss: 6663.1758
In this epoch 1100/5000, Training loss: 0.0001, Test loss: 6663.1592
In this epoch 1150/5000, Training loss: 0.0001, Test loss: 6663.1528
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 6663.1484
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 6663.1411
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 6663.1338
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 6663.1279
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 6663.1265
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 6663.1250
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 6663.1221
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 6663.1221
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 6663.1206
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 6663.1206
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 6663.1206
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 6663.1201
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 6663.1206
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 6663.1206
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 6663.1206
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 6663.1206
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 6663.1201
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 6663.1201
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 6663.1187
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 6663.1187
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 6663.1201
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 6663.1201
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 6663.1201
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 6663.1191
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 6663.1201
In this epoch 4050/5000, Training loss: 12304631.0000, Test loss: 17169016.0000
In this epoch 4100/5000, Training loss: 5499.1636, Test loss: 48448.5781
In this epoch 4150/5000, Training loss: 296.7288, Test loss: 6579.4648
In this epoch 4200/5000, Training loss: 1.3827, Test loss: 6617.1411
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 6611.0854
In this epoch 4300/5000, Training loss: 0.0001, Test loss: 6612.5205
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 6612.4253
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 6612.4092
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 6612.4023
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 6612.3940
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 6612.3882
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 6612.3833
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 6612.3774
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 6612.3750
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 6612.3706
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 6612.3662
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 6612.3633
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 6612.3613
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 6612.3589
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 6612.3560
train_acc 1.0
test_acc 0.005216262975778547
{'y_pred': [-9.201498031616211, -86.60137939453125, 18.532533645629883, 72.59139251708984, -62.89823913574219, 37.38935089111328, 19.424821853637695, -66.5899887084961, -16.298410415649414, 84.8899917602539, -54.94904708862305, -19.001781463623047, -38.2866325378418, -18.7823543548584, -34.89699172973633, 58.27154541015625, 73.33882141113281, 119.87556457519531, -108.48512268066406, -43.462059020996094, -91.73728942871094, -47.07819366455078, 81.69410705566406, 163.24038696289062, 9.893714904785156, -5.491957187652588, 0.9181592464447021, -62.999488830566406, -66.60175323486328, 118.98141479492188, -33.47190856933594, 86.60098266601562, -68.26946258544922, -51.790626525878906, 28.999853134155273, -27.83744239807129, 38.760597229003906, 41.61121368408203, 72.0555648803711, 28.14823341369629, 22.792245864868164, -47.14053726196289, -72.86529541015625, 32.797340393066406, 152.0095672607422, 22.612245559692383, 22.79826545715332, 7.098756313323975, -96.66502380371094, 33.877281188964844, -58.738094329833984, 104.40650939941406, -49.1943359375, -85.37281036376953, -41.92014694213867, 51.51552963256836, -34.31370162963867, -7.281393527984619, 55.46244812011719, 21.56616973876953, 36.70764923095703, -5.617830753326416, -47.72832489013672, -38.38251876831055, 144.89622497558594, 20.30354118347168, 53.35430145263672, 140.5043487548828, -27.634716033935547, -39.468265533447266, 16.562837600708008, -31.285917282104492, 58.56863784790039, -24.402456283569336, 12.492242813110352, -33.51746368408203, 25.639429092407227, 19.199552536010742, -14.963079452514648, 66.66067504882812, 0.991830587387085, 32.299720764160156, -85.59697723388672, -76.96050262451172, -10.458147048950195, -96.67497253417969, 136.2374725341797, -37.326904296875, -56.7043571472168, -96.2511215209961, 33.86778259277344, 105.85697937011719, -64.34307098388672, 33.51514434814453, 13.006677627563477, -0.3079703152179718, -8.746500015258789, 84.19264221191406, 88.19844055175781, -43.25694274902344, 165.9741668701172, 52.05796813964844, -51.389976501464844, -80.01105499267578, 165.84429931640625, -237.15179443359375, -94.11700439453125, 21.60786247253418, -113.57093811035156, -44.65979766845703, 27.888887405395508, 33.30467987060547, -70.68399047851562, 46.21736145019531, 51.14975357055664, 146.6042022705078, 23.638452529907227, -62.00565719604492, 45.83043670654297, 28.52490997314453, -52.3141975402832, 13.12722396850586, -239.12966918945312, -213.26808166503906, 23.54945182800293, 54.35504150390625, 47.20684051513672, -81.83187866210938, 33.258056640625, -13.718448638916016, 4.494836330413818, 53.20232009887695, -10.364904403686523, -16.24903678894043, -110.65158081054688, -113.40298461914062, -16.224733352661133, 162.474853515625, 110.74517822265625, 37.39806365966797, -40.24040222167969, 5.191178798675537, 37.37588882446289, 16.36192512512207, 58.50968933105469, 130.0565185546875, -193.1806640625, 29.57828712463379, 10.091974258422852, 36.238250732421875, -2.2594969272613525, -21.589662551879883, -13.4942626953125, 206.08279418945312, -40.953277587890625, 89.34114074707031, -120.9749755859375, 22.46793556213379, 5.9176764488220215, -43.61894607543945, 0.9702889919281006, 80.364501953125, 36.25831985473633, 80.91384887695312, 25.70078468322754, -15.477094650268555, 14.122907638549805, -59.13355255126953, 29.524782180786133, -23.970882415771484, -118.3797378540039, 13.882734298706055, -17.98829460144043, 35.52314758300781, -47.648319244384766, -36.69517517089844, 66.01383209228516, -95.70545959472656, -76.61387634277344, 1.3936097621917725, 5.934220790863037, -65.78591918945312, 19.43303680419922, 22.94603157043457, -56.718994140625, 1.165841817855835, 156.32553100585938, 15.152109146118164, -39.413841247558594, 131.18765258789062, 53.46709442138672, 22.46648406982422, -53.88356018066406, -86.61602783203125, 9.860225677490234, 53.014644622802734, 102.43473815917969, 7.74988317489624, -125.17489624023438, 15.296079635620117, -16.880870819091797, 63.3070068359375, -0.3555281460285187, 123.42620849609375, 89.86515808105469, 9.795869827270508, -22.22736167907715, 31.427003860473633, -79.70919799804688, 38.970062255859375, -2.0480940341949463, -196.1910400390625, -29.627016067504883, -120.48570251464844, 95.21365356445312, -64.63301849365234, 30.39051628112793, 157.20748901367188, -57.82112503051758, 47.38697052001953, -56.77593994140625, -20.78110694885254, -71.56863403320312, 14.479399681091309, 16.327726364135742, -12.693426132202148, -214.63096618652344, 68.11981201171875, 30.30384635925293, -169.873291015625, 80.36476135253906, -43.702205657958984, 116.11502838134766, 1.9662797451019287, -26.159265518188477, -48.424659729003906, 3.9825685024261475, -21.46820640563965, 84.59992980957031, 92.42943572998047, -9.221723556518555, 90.9515380859375, 44.67344284057617, -107.62663269042969, 115.13406372070312, 32.5641975402832, 231.918701171875, 24.972475051879883, 167.7559356689453, -58.40486145019531, 34.96855163574219, -8.625215530395508, 71.89520263671875, 44.468360900878906, -120.83230590820312, -71.37630462646484, -2.384073495864868, 72.35749816894531, 119.23641204833984, 95.05964660644531, 28.654146194458008, -112.94631958007812, -17.494413375854492, 126.62210083007812, -244.54673767089844, 111.77738952636719, -1.3413259983062744, 91.43798828125, -170.65823364257812, 176.95480346679688, -9.788288116455078, 87.51052856445312, -127.74557495117188, 153.5129852294922, 109.38105773925781, 50.048362731933594, 31.21311378479004, -17.070138931274414, 1.6357324123382568, -5.266844272613525, 89.15377807617188, 51.217552185058594, 55.22388458251953, -10.695344924926758, 91.87309265136719, -24.882314682006836, -205.42257690429688, 72.75988006591797, -65.10247802734375, -24.382829666137695, -132.6688690185547, 46.335655212402344, 53.730926513671875, 235.82073974609375, 111.52951049804688, 64.09430694580078, 92.43855285644531, -72.36495208740234, -1.18086838722229, 96.42745971679688, -169.6905059814453, -56.90575408935547, 100.158935546875, 67.02144622802734, -166.43997192382812, 133.36228942871094, 1.9804589748382568, 62.76544189453125, 4.392768383026123, -21.735130310058594, 1.0387914180755615, 38.004905700683594, 18.704320907592773, -62.910621643066406, 190.00140380859375, -108.81769561767578, 51.074974060058594, -75.76288604736328, -15.058286666870117, -1.6789648532867432, -70.01627349853516, 31.80857276916504, -96.16007995605469, 4.5940375328063965, 76.3119125366211, 215.78860473632812, 175.93112182617188, 12.629304885864258, -6.189131259918213, 101.23087310791016, -66.52943420410156, 19.5316104888916, -61.16326904296875, 14.576377868652344, 10.355318069458008, 14.997904777526855, 45.08795928955078, 50.32084655761719, -4.374838352203369, -114.07743835449219], 'y_pred_int': [0, 0, 19, 73, 0, 37, 19, 0, 0, 85, 0, 0, 0, 0, 0, 58, 73, 120, 0, 0, 0, 0, 82, 163, 10, 0, 1, 0, 0, 119, 0, 87, 0, 0, 29, 0, 39, 42, 72, 28, 23, 0, 0, 33, 152, 23, 23, 7, 0, 34, 0, 104, 0, 0, 0, 52, 0, 0, 55, 22, 37, 0, 0, 0, 145, 20, 53, 141, 0, 0, 17, 0, 59, 0, 12, 0, 26, 19, 0, 67, 1, 32, 0, 0, 0, 0, 136, 0, 0, 0, 34, 106, 0, 34, 13, 0, 0, 84, 88, 0, 166, 52, 0, 0, 166, 0, 0, 22, 0, 0, 28, 33, 0, 46, 51, 147, 24, 0, 46, 29, 0, 13, 0, 0, 24, 54, 47, 0, 33, 0, 4, 53, 0, 0, 0, 0, 0, 162, 111, 37, 0, 5, 37, 16, 59, 130, 0, 30, 10, 36, 0, 0, 0, 206, 0, 89, 0, 22, 6, 0, 1, 80, 36, 81, 26, 0, 14, 0, 30, 0, 0, 14, 0, 36, 0, 0, 66, 0, 0, 1, 6, 0, 19, 23, 0, 1, 156, 15, 0, 131, 53, 22, 0, 0, 10, 53, 102, 8, 0, 15, 0, 63, 0, 123, 90, 10, 0, 31, 0, 39, 0, 0, 0, 0, 95, 0, 30, 157, 0, 47, 0, 0, 0, 14, 16, 0, 0, 68, 30, 0, 80, 0, 116, 2, 0, 0, 4, 0, 85, 92, 0, 91, 45, 0, 115, 33, 232, 25, 168, 0, 35, 0, 72, 44, 0, 0, 0, 72, 119, 95, 29, 0, 0, 127, 0, 112, 0, 91, 0, 177, 0, 88, 0, 154, 109, 50, 31, 0, 2, 0, 89, 51, 55, 0, 92, 0, 0, 73, 0, 0, 0, 46, 54, 236, 112, 64, 92, 0, 0, 96, 0, 0, 100, 67, 0, 133, 2, 63, 4, 0, 1, 38, 19, 0, 190, 0, 51, 0, 0, 0, 0, 32, 0, 5, 76, 216, 176, 13, 0, 101, 0, 20, 0, 15, 10, 15, 45, 50, 0, 0], 'y_true': [25, 6, 8, 12, 6, 8, 6, 2, 5, 7, 8, 1, 5, 32, 6, 13, 17, 9, 15, 2, 11, 16, 13, 25, 20, 11, 8, 15, 17, 14, 11, 17, 29, 43, 29, 5, 9, 11, 5, 5, 4, 4, 47, 15, 11, 12, 15, 30, 6, 18, 5, 23, 21, 12, 5, 3, 9, 27, 6, 18, 4, 6, 18, 6, 13, 13, 10, 23, 21, 3, 15, 6, 21, 6, 3, 18, 28, 17, 6, 10, 29, 6, 8, 17, 15, 18, 6, 8, 6, 7, 11, 6, 6, 6, 9, 35, 32, 15, 14, 35, 14, 15, 3, 23, 18, 11, 25, 6, 21, 18, 3, 11, 14, 11, 5, 14, 9, 11, 30, 5, 28, 11, 29, 12, 15, 5, 26, 12, 6, 4, 15, 1, 11, 7, 13, 8, 5, 20, 5, 15, 30, 6, 15, 4, 4, 13, 9, 13, 10, 11, 7, 11, 6, 4, 6, 7, 7, 8, 6, 33, 30, 12, 10, 9, 7, 18, 9, 14, 24, 19, 9, 18, 6, 16, 25, 6, 100, 12, 9, 8, 7, 43, 5, 9, 11, 16, 33, 28, 46, 15, 7, 25, 5, 10, 10, 48, 3, 10, 19, 29, 15, 9, 15, 16, 9, 32, 21, 21, 20, 10, 10, 13, 20, 8, 10, 11, 2, 22, 9, 10, 9, 9, 7, 8, 2, 5, 22, 10, 8, 12, 5, 10, 16, 7, 15, 13, 13, 10, 17, 10, 47, 5, 30, 23, 15, 14, 22, 12, 16, 11, 7, 8, 2, 6, 8, 22, 11, 25, 14, 6, 3, 3, 20, 39, 19, 9, 11, 8, 5, 35, 11, 8, 7, 17, 10, 6, 2, 16, 5, 11, 24, 15, 10, 10, 14, 9, 17, 25, 6, 6, 8, 16, 4, 8, 62, 7, 13, 8, 17, 9, 12, 4, 18, 15, 22, 14, 26, 5, 14, 7, 17, 10, 3, 8, 11, 17, 3, 7, 10, 6, 23, 7, 9, 17, 10, 14, 9, 28, 4, 13, 7, 49, 10, 10, 11, 6, 6, 4, 7, 7], 'train_acc': 1.0, 'test_acc': 0.005216262975778547, 'X': 'X_TANL_layer_9_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_10_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 850503.1875, Test loss: 349590.2188
In this epoch 100/5000, Training loss: 5758.5405, Test loss: 17065.5742
In this epoch 150/5000, Training loss: 1.8611, Test loss: 14841.7324
In this epoch 200/5000, Training loss: 0.0080, Test loss: 14762.8037
In this epoch 250/5000, Training loss: 0.0021, Test loss: 14759.1797
In this epoch 300/5000, Training loss: 0.0016, Test loss: 14759.5752
In this epoch 350/5000, Training loss: 0.0013, Test loss: 14759.7549
In this epoch 400/5000, Training loss: 0.0010, Test loss: 14759.9219
In this epoch 450/5000, Training loss: 0.0008, Test loss: 14760.0928
In this epoch 500/5000, Training loss: 0.0006, Test loss: 14760.2441
In this epoch 550/5000, Training loss: 0.0004, Test loss: 14760.3740
In this epoch 600/5000, Training loss: 0.0003, Test loss: 14760.4990
In this epoch 650/5000, Training loss: 0.0002, Test loss: 14760.6123
In this epoch 700/5000, Training loss: 0.0002, Test loss: 14760.7178
In this epoch 750/5000, Training loss: 0.0001, Test loss: 14760.7988
In this epoch 800/5000, Training loss: 0.0001, Test loss: 14760.8711
In this epoch 850/5000, Training loss: 0.0000, Test loss: 14760.9414
In this epoch 900/5000, Training loss: 0.0000, Test loss: 14760.9883
In this epoch 950/5000, Training loss: 0.0000, Test loss: 14761.0352
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 14761.0771
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 14761.1035
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 14761.1299
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 14761.1484
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 14761.1621
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 14761.1768
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 14761.1875
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 14761.1904
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 14761.1973
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 14761.1992
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 14761.2002
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 14761.2051
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 14761.2061
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 14761.2061
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 14761.2080
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 14761.2080
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 14761.2080
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 14761.2061
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 14761.2061
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 14761.2080
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 14761.2061
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 14761.2061
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 14761.2061
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 14761.2080
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 14761.2061
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 14761.2080
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 14761.2061
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 14761.2090
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 14761.2061
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 14761.2051
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 14761.2080
In this epoch 3350/5000, Training loss: 97375680.0000, Test loss: 11523897.0000
In this epoch 3400/5000, Training loss: 506531.4062, Test loss: 24746.6797
In this epoch 3450/5000, Training loss: 4022.9463, Test loss: 14859.1064
In this epoch 3500/5000, Training loss: 17.2211, Test loss: 14726.4102
In this epoch 3550/5000, Training loss: 0.0001, Test loss: 14729.4561
In this epoch 3600/5000, Training loss: 0.0006, Test loss: 14725.7344
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 14725.7959
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 14725.8359
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 14725.8506
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 14725.8623
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 14725.8740
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 14725.8799
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 14725.8887
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 14725.8945
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 14725.9004
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 14725.9033
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 14725.9062
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 14725.9121
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 14725.9150
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 14725.9160
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 14725.9180
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 14725.9189
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 14725.9209
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 14725.9238
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 14725.9238
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 14725.9238
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 14725.9238
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 14725.9238
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 14725.9238
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 14725.9238
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 14725.9238
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 14725.9238
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 14725.9248
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 14725.9238
train_acc 1.0
test_acc 0.004368512110726644
{'y_pred': [-118.94454956054688, 44.302757263183594, 344.4105529785156, -40.849510192871094, -60.37654113769531, 74.78569793701172, 109.17769622802734, 123.20501708984375, 124.86353302001953, -126.84413146972656, 111.1944351196289, -114.46304321289062, -29.99329948425293, 58.82605743408203, 272.2185363769531, 44.581485748291016, -64.57735443115234, -112.9059066772461, 74.91742706298828, -25.991724014282227, 114.87854766845703, 191.7687530517578, 62.69060516357422, -119.74272918701172, -27.713476181030273, -4.666974067687988, -10.8357515335083, -101.92353820800781, 191.5218963623047, 141.7911834716797, -10.32314395904541, -16.75493812561035, 163.2455596923828, 95.78050994873047, -24.272783279418945, -218.30641174316406, 22.991743087768555, -16.039011001586914, -19.031442642211914, 6.774764060974121, -162.62771606445312, -166.44027709960938, 5.04007625579834, -127.4157943725586, 11.073248863220215, -66.9319076538086, -4.881644248962402, -214.7332305908203, 70.42046356201172, 120.76000213623047, 74.39774322509766, 30.675161361694336, 95.75874328613281, 255.72975158691406, 143.63560485839844, -9.092804908752441, 195.0102996826172, 333.2677917480469, 144.8203887939453, 86.42505645751953, -140.97332763671875, 76.2931137084961, -120.93408966064453, -108.51847076416016, 6.32279109954834, 185.6425018310547, -18.524751663208008, -117.31462860107422, -93.3746109008789, 47.349693298339844, 3.9240100383758545, 83.80733489990234, 123.988037109375, -114.54590606689453, -53.73194122314453, -148.89173889160156, -94.85286712646484, 98.46878814697266, 31.015653610229492, -29.44802474975586, 15.13863468170166, -212.30392456054688, 25.8613338470459, -337.72772216796875, 20.409936904907227, 234.99253845214844, -37.318885803222656, 21.665761947631836, 88.1593017578125, -42.96289825439453, 37.70274353027344, 118.86478424072266, -133.25302124023438, -45.752723693847656, 2.7474277019500732, 82.53760528564453, 88.86121368408203, 97.2734603881836, 219.24180603027344, 61.34247589111328, -96.0424575805664, 80.05040740966797, 73.37799835205078, -82.8704605102539, 162.75148010253906, 89.90473175048828, 19.489500045776367, -117.61749267578125, 32.322723388671875, 79.86418914794922, 52.82646942138672, 203.9403533935547, 188.71646118164062, 19.52122688293457, -72.47303009033203, -43.28807067871094, 17.177766799926758, 5.215620994567871, -49.74416732788086, 51.504554748535156, 27.67035484313965, 202.93191528320312, 36.606475830078125, -52.74268341064453, 229.50173950195312, 265.5201110839844, 29.183408737182617, -29.45731544494629, -149.64378356933594, 42.92566680908203, 245.7309112548828, 97.9447250366211, 290.0603332519531, -16.02390480041504, 183.8670196533203, -131.10733032226562, 50.93107604980469, -279.1498718261719, 55.943458557128906, 195.9427947998047, 12.13089656829834, 9.545462608337402, 60.978233337402344, -52.92327117919922, 80.97754669189453, 13.243460655212402, -35.80234909057617, 37.765220642089844, 43.440040588378906, -196.70620727539062, 29.736406326293945, 112.46894073486328, -302.1496276855469, -48.565895080566406, -15.714104652404785, -62.33845520019531, 46.384674072265625, 14.688632011413574, -61.482948303222656, 321.63726806640625, 172.09788513183594, -35.697593688964844, 45.53469467163086, -151.4870147705078, 194.86349487304688, 27.889989852905273, 117.8747329711914, 49.81899642944336, 60.93659210205078, -99.62433624267578, 55.68762969970703, 72.79068756103516, 185.9281005859375, 160.89637756347656, 1.4652470350265503, -64.01902770996094, 299.0071716308594, -209.00930786132812, -37.662109375, 93.60723876953125, 86.34133911132812, 144.34219360351562, 103.75914764404297, -183.9960479736328, 99.09881591796875, 53.62006378173828, -103.92567443847656, -40.67417907714844, 47.948829650878906, 65.17132568359375, -10.587708473205566, -45.42725372314453, -105.16027069091797, -2.7020046710968018, -57.37534713745117, 84.03528594970703, -2.7466671466827393, 127.43656158447266, -21.614694595336914, 75.74410247802734, 57.814186096191406, -95.30155181884766, 142.73463439941406, -32.97997283935547, -8.98844051361084, 24.073881149291992, 129.40928649902344, -51.73407745361328, 17.041982650756836, -118.48644256591797, -29.729326248168945, 7.405570030212402, -19.617631912231445, -81.50289154052734, -20.173288345336914, -46.745460510253906, 101.09085845947266, 166.83309936523438, -156.9649658203125, -109.40091705322266, 211.96539306640625, 3.4453647136688232, 98.55545806884766, 154.76596069335938, 245.30882263183594, -149.26417541503906, -25.017663955688477, 33.68077087402344, -90.34069061279297, -88.79476928710938, 139.2720489501953, -103.3250503540039, 173.0133056640625, 11.153075218200684, 74.3057861328125, 44.83186721801758, -59.48944854736328, -20.98149299621582, -89.23778533935547, 42.77031707763672, 78.95048522949219, -17.043184280395508, 39.46354675292969, -17.001741409301758, -99.84168243408203, -118.02928924560547, 129.46546936035156, 217.89505004882812, 52.038414001464844, -12.446091651916504, 82.87963104248047, 305.8804626464844, 88.45972442626953, -30.336801528930664, -77.03011322021484, 154.8140411376953, 13.313437461853027, -205.46116638183594, 73.63196563720703, -66.15198516845703, -200.18284606933594, 20.61708641052246, -11.991394996643066, -76.34590911865234, 34.26569747924805, -89.47003936767578, 97.00584411621094, 210.2672576904297, 47.20684051513672, -124.31807708740234, 92.7692642211914, 38.140411376953125, -195.45285034179688, -160.6160125732422, 294.6709899902344, -38.851951599121094, -114.66605377197266, -388.90667724609375, -310.3041076660156, 47.863243103027344, 173.47911071777344, -117.78545379638672, 218.80743408203125, 0.7571781277656555, -22.6390323638916, -10.726433753967285, 5.705649375915527, 244.96693420410156, 29.47773551940918, 60.25506591796875, 49.94561767578125, 15.108534812927246, 234.17527770996094, -288.2239685058594, -98.26049041748047, 4.32175350189209, -87.9515151977539, -39.38407897949219, 14.74375057220459, -78.1994400024414, -85.55609893798828, 188.1248779296875, 31.681163787841797, -53.10016632080078, -59.83420181274414, -168.60423278808594, -49.63707733154297, 15.631590843200684, -9.068222999572754, 149.13072204589844, 111.58470153808594, 166.4844207763672, 129.25758361816406, -159.96060180664062, 15.000067710876465, 75.38555908203125, -84.74057006835938, 218.75357055664062, -158.72299194335938, 59.82069396972656, -122.8615951538086, 62.704345703125, 108.40801239013672, 228.59593200683594, 33.335426330566406, 58.210670471191406, 114.59130096435547, 297.0185546875, -59.821346282958984, 17.199758529663086, -112.90087127685547, 69.6632080078125, 173.46263122558594, -43.09961700439453, 33.95691680908203, -70.4176025390625, -39.16474914550781, -24.116769790649414, 236.51405334472656, 15.196707725524902], 'y_pred_int': [0, 44, 344, 0, 0, 75, 109, 123, 125, 0, 111, 0, 0, 59, 272, 45, 0, 0, 75, 0, 115, 192, 63, 0, 0, 0, 0, 0, 192, 142, 0, 0, 163, 96, 0, 0, 23, 0, 0, 7, 0, 0, 5, 0, 11, 0, 0, 0, 70, 121, 74, 31, 96, 256, 144, 0, 195, 333, 145, 86, 0, 76, 0, 0, 6, 186, 0, 0, 0, 47, 4, 84, 124, 0, 0, 0, 0, 98, 31, 0, 15, 0, 26, 0, 20, 235, 0, 22, 88, 0, 38, 119, 0, 0, 3, 83, 89, 97, 219, 61, 0, 80, 73, 0, 163, 90, 19, 0, 32, 80, 53, 204, 189, 20, 0, 0, 17, 5, 0, 52, 28, 203, 37, 0, 230, 266, 29, 0, 0, 43, 246, 98, 290, 0, 184, 0, 51, 0, 56, 196, 12, 10, 61, 0, 81, 13, 0, 38, 43, 0, 30, 112, 0, 0, 0, 0, 46, 15, 0, 322, 172, 0, 46, 0, 195, 28, 118, 50, 61, 0, 56, 73, 186, 161, 1, 0, 299, 0, 0, 94, 86, 144, 104, 0, 99, 54, 0, 0, 48, 65, 0, 0, 0, 0, 0, 84, 0, 127, 0, 76, 58, 0, 143, 0, 0, 24, 129, 0, 17, 0, 0, 7, 0, 0, 0, 0, 101, 167, 0, 0, 212, 3, 99, 155, 245, 0, 0, 34, 0, 0, 139, 0, 173, 11, 74, 45, 0, 0, 0, 43, 79, 0, 39, 0, 0, 0, 129, 218, 52, 0, 83, 306, 88, 0, 0, 155, 13, 0, 74, 0, 0, 21, 0, 0, 34, 0, 97, 210, 47, 0, 93, 38, 0, 0, 295, 0, 0, 0, 0, 48, 173, 0, 219, 1, 0, 0, 6, 245, 29, 60, 50, 15, 234, 0, 0, 4, 0, 0, 15, 0, 0, 188, 32, 0, 0, 0, 0, 16, 0, 149, 112, 166, 129, 0, 15, 75, 0, 219, 0, 60, 0, 63, 108, 229, 33, 58, 115, 297, 0, 17, 0, 70, 173, 0, 34, 0, 0, 0, 237, 15], 'y_true': [23, 16, 31, 5, 11, 6, 6, 41, 8, 14, 21, 7, 13, 15, 9, 34, 14, 11, 19, 60, 12, 7, 37, 15, 12, 5, 44, 10, 15, 6, 5, 8, 9, 22, 9, 19, 9, 9, 34, 5, 2, 6, 28, 19, 6, 9, 10, 6, 20, 12, 9, 9, 3, 10, 14, 13, 14, 13, 25, 8, 3, 15, 8, 2, 6, 9, 13, 23, 14, 3, 31, 23, 8, 8, 10, 11, 17, 9, 10, 36, 17, 18, 7, 15, 17, 23, 19, 43, 28, 31, 16, 4, 17, 5, 10, 38, 15, 11, 8, 14, 8, 6, 12, 11, 26, 11, 8, 14, 4, 9, 46, 8, 7, 5, 23, 6, 16, 15, 13, 14, 3, 24, 16, 7, 13, 15, 32, 11, 17, 10, 13, 9, 10, 25, 10, 7, 17, 20, 100, 4, 4, 5, 4, 14, 5, 14, 10, 10, 6, 43, 76, 12, 17, 6, 18, 5, 3, 13, 15, 13, 4, 9, 15, 12, 9, 4, 46, 4, 40, 8, 12, 6, 19, 4, 2, 8, 10, 5, 6, 29, 9, 9, 15, 13, 20, 7, 13, 6, 7, 29, 6, 14, 7, 5, 26, 2, 6, 22, 6, 45, 10, 21, 14, 13, 11, 7, 12, 5, 10, 5, 21, 32, 4, 6, 37, 11, 13, 18, 11, 35, 14, 4, 11, 15, 8, 12, 42, 17, 9, 22, 7, 6, 20, 3, 15, 46, 8, 6, 6, 25, 22, 12, 19, 16, 13, 13, 7, 11, 11, 11, 9, 13, 39, 35, 7, 10, 14, 18, 13, 7, 6, 9, 3, 4, 28, 28, 11, 9, 18, 20, 8, 8, 7, 23, 18, 16, 16, 9, 10, 8, 6, 40, 13, 15, 5, 20, 5, 21, 11, 8, 29, 13, 35, 26, 22, 6, 4, 8, 12, 15, 7, 39, 6, 1, 21, 21, 2, 8, 13, 25, 6, 14, 11, 8, 15, 12, 17, 10, 5, 16, 7, 49, 21, 27, 22, 4, 17, 13, 19, 9, 10, 19, 9, 3, 4, 6, 4, 4, 11, 5], 'train_acc': 1.0, 'test_acc': 0.004368512110726644, 'X': 'X_TANL_layer_10_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_11_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 940111.4375, Test loss: 219755.8594
In this epoch 100/5000, Training loss: 5747.3267, Test loss: 27950.9531
In this epoch 150/5000, Training loss: 39.3349, Test loss: 27339.4238
In this epoch 200/5000, Training loss: 0.2279, Test loss: 27325.6055
In this epoch 250/5000, Training loss: 0.0025, Test loss: 27327.0176
In this epoch 300/5000, Training loss: 0.0009, Test loss: 27326.7539
In this epoch 350/5000, Training loss: 0.0007, Test loss: 27326.5527
In this epoch 400/5000, Training loss: 0.0006, Test loss: 27326.3691
In this epoch 450/5000, Training loss: 0.0004, Test loss: 27326.1777
In this epoch 500/5000, Training loss: 0.0003, Test loss: 27326.0098
In this epoch 550/5000, Training loss: 0.0002, Test loss: 27325.8613
In this epoch 600/5000, Training loss: 0.0002, Test loss: 27325.7188
In this epoch 650/5000, Training loss: 0.0001, Test loss: 27325.5762
In this epoch 700/5000, Training loss: 0.0001, Test loss: 27325.4648
In this epoch 750/5000, Training loss: 0.0001, Test loss: 27325.3652
In this epoch 800/5000, Training loss: 0.0000, Test loss: 27325.2832
In this epoch 850/5000, Training loss: 0.0000, Test loss: 27325.2129
In this epoch 900/5000, Training loss: 0.0000, Test loss: 27325.1309
In this epoch 950/5000, Training loss: 0.0000, Test loss: 27325.0703
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 27325.0176
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 27324.9883
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 27324.9648
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 27324.9531
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 27324.9297
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 27324.9121
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 27324.9062
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 27324.8945
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 27324.8926
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 27324.8809
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 27324.8770
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 27324.8652
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 27324.8691
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 27324.8711
In this epoch 3150/5000, Training loss: 0.0003, Test loss: 27324.4297
In this epoch 3200/5000, Training loss: 1780856.8750, Test loss: 1358633.8750
In this epoch 3250/5000, Training loss: 1634.0210, Test loss: 57005.3906
In this epoch 3300/5000, Training loss: 186.8746, Test loss: 27409.0410
In this epoch 3350/5000, Training loss: 0.4415, Test loss: 27355.4980
In this epoch 3400/5000, Training loss: 0.0014, Test loss: 27358.5293
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 27358.2305
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 27358.2383
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 27358.2305
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 27358.2324
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 27358.2305
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 27358.2188
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 27358.2129
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 27358.2090
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 27358.2070
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 27358.2012
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 27358.1973
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 27358.1953
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 27358.1895
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 27358.1895
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 27358.1855
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 27358.1836
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 27358.1836
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 27358.1836
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 27358.1836
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 27358.1797
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 27358.1777
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 27358.1777
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 27358.1797
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 27358.1777
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 27358.1777
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 27358.1777
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 27358.1777
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 27358.1797
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 27358.1777
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 27358.1738
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 27358.1797
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 27358.1777
train_acc 1.0
test_acc 0.00342560553633218
{'y_pred': [18.648038864135742, -634.2177734375, -61.36861038208008, 538.2017211914062, -156.74020385742188, 4.866589546203613, -137.74513244628906, 176.6380157470703, -75.71823120117188, 87.15440368652344, -56.71236038208008, -107.71670532226562, -28.60106086730957, 80.74313354492188, -3.2955195903778076, 131.240478515625, 208.79359436035156, -41.81471633911133, 89.79933166503906, 6.891156196594238, -62.93828201293945, -89.66429138183594, 178.0353240966797, 23.584012985229492, 117.36515808105469, -52.32986831665039, -25.12071418762207, 87.9881591796875, 216.3283233642578, 242.5586700439453, 65.97300720214844, -91.97390747070312, -49.7381477355957, -40.94649124145508, 4.032757759094238, 49.088436126708984, -77.2537841796875, -2.0830562114715576, -236.60025024414062, 27.606325149536133, 82.24395751953125, 33.97709274291992, -3.7378108501434326, -38.26445388793945, 95.57597351074219, 48.711544036865234, -242.74606323242188, 74.66007995605469, -39.82228469848633, 20.15522575378418, -61.151737213134766, 211.2989959716797, -52.21171951293945, 25.41346549987793, 261.5785827636719, 25.08253288269043, 276.8909912109375, 36.51292037963867, -62.06731033325195, 31.520795822143555, 422.63299560546875, 528.367431640625, 65.60636901855469, -162.8907470703125, 174.5471954345703, 154.82972717285156, 210.05079650878906, -21.68144416809082, -99.54529571533203, 114.85527038574219, 12.322216987609863, 99.41520690917969, -127.45660400390625, 244.2294158935547, -269.3356628417969, -83.79524230957031, 27.332502365112305, -61.55130386352539, 94.88360595703125, -88.56834411621094, -44.0734748840332, 534.0740966796875, 10.700268745422363, -25.594926834106445, -70.04440307617188, 59.96902084350586, 118.88319396972656, 199.63540649414062, -345.737548828125, -64.96078491210938, 13.9944429397583, -37.64224624633789, -138.7688446044922, -352.7604064941406, -19.161211013793945, 167.02976989746094, 1.1019717454910278, 20.186674118041992, -117.05091857910156, -28.615114212036133, 275.9635314941406, 177.2115478515625, -52.58845901489258, -47.21876907348633, 424.3765563964844, -149.08035278320312, -91.69947814941406, -254.41685485839844, 128.57305908203125, 42.712337493896484, -36.24795150756836, 195.85479736328125, -58.39017105102539, -61.930850982666016, -28.762529373168945, -46.94595718383789, -153.4016876220703, -26.200685501098633, -127.85408020019531, -385.28167724609375, 148.26016235351562, -155.03208923339844, -5.131400108337402, -94.39845275878906, -97.44627380371094, -170.6807098388672, -66.86058807373047, -235.28968811035156, 77.82322692871094, 49.41169357299805, 121.86955261230469, 139.74560546875, -20.044336318969727, 69.42842102050781, 227.2690887451172, 111.08283996582031, -52.54921340942383, 88.42030334472656, -21.77482795715332, 23.35645866394043, -153.74021911621094, -61.898990631103516, 89.29609680175781, 60.50907516479492, -24.864213943481445, 177.6081085205078, 22.450986862182617, -49.031375885009766, -216.5304412841797, 113.73646545410156, -67.04348754882812, 29.000959396362305, -79.873779296875, 22.197111129760742, -83.35934448242188, 489.33935546875, 32.46610641479492, 155.03346252441406, 300.1126708984375, 279.67132568359375, -136.75779724121094, -284.0257873535156, 28.15321159362793, -216.5032196044922, 156.76573181152344, -12.960436820983887, 113.937255859375, 4.963803291320801, -122.59370422363281, 203.5264129638672, -0.7082852125167847, -197.84349060058594, -52.33693313598633, 37.8164176940918, 46.99923324584961, -141.5808563232422, 140.5543670654297, 187.46849060058594, 245.09535217285156, 205.0183868408203, -59.21867752075195, 25.365705490112305, 91.82633972167969, -95.92790222167969, 2.9485905170440674, -17.49644660949707, 5.319256782531738, 164.99110412597656, 47.7120475769043, -60.3907585144043, -62.29362869262695, 280.8099670410156, -165.87957763671875, -180.91099548339844, -86.02772521972656, 12.441937446594238, 324.9700012207031, -247.31553649902344, -10.198580741882324, 81.91145324707031, 40.71195602416992, -37.04399490356445, 22.60542106628418, -77.71647644042969, -7.396853446960449, 356.4260559082031, 105.75880432128906, -148.8101043701172, 94.89140319824219, -50.99679946899414, 231.1534881591797, 196.4076385498047, -288.3310241699219, 92.87014770507812, -426.1067199707031, -70.08651733398438, -36.2150764465332, -31.018083572387695, -85.27240753173828, -236.99134826660156, 45.70261764526367, -34.6180305480957, 86.00094604492188, -18.90165901184082, 177.2168426513672, -158.04913330078125, 31.82050895690918, -399.997314453125, 137.3916473388672, 266.6955261230469, -56.56514358520508, -107.39675903320312, 89.04574584960938, 71.86024475097656, -272.1875915527344, -66.07464599609375, 48.50388717651367, -151.4047393798828, 84.25439453125, 265.9114685058594, -120.21514892578125, -117.46772766113281, 77.73947143554688, -122.92721557617188, -34.8035774230957, -132.26235961914062, -171.7950439453125, 108.34086608886719, 399.5899353027344, -136.77891540527344, 233.69947814941406, 268.0986633300781, 113.14778137207031, -73.59556579589844, 374.3223571777344, -105.92547607421875, 5.756253242492676, 3.2371037006378174, 236.1473846435547, -205.29551696777344, 19.923627853393555, -117.12443542480469, -314.2136535644531, 204.7171173095703, 424.5885925292969, 1.2132083177566528, -216.21678161621094, -99.67225646972656, 55.9172477722168, -71.25440979003906, -208.85418701171875, -7.207873344421387, -42.40141677856445, 56.40378952026367, 35.60923385620117, 109.76225280761719, 97.96238708496094, -62.9362678527832, 13.678021430969238, -173.96412658691406, 31.989057540893555, 44.5772819519043, 200.74244689941406, 9.007519721984863, 111.67903137207031, 22.72108268737793, 75.67190551757812, 7.432202339172363, 182.30540466308594, 56.4647331237793, 2.9874393939971924, -122.19570922851562, -323.07421875, -129.13253784179688, -135.45973205566406, 667.2823486328125, -39.16048049926758, 161.83412170410156, 12.423565864562988, -20.965295791625977, 50.80356979370117, -22.623140335083008, 143.98814392089844, -20.841203689575195, 247.9617156982422, 25.978925704956055, 109.57017517089844, -234.90211486816406, -141.17462158203125, 51.71638107299805, -99.82417297363281, 148.37669372558594, 31.652631759643555, 142.5813446044922, 7.246930122375488, 176.99754333496094, 278.6614074707031, 271.2707824707031, -222.37998962402344, -197.80824279785156, -182.8637237548828, 123.71760559082031, 148.3504180908203, -233.973388671875, -264.67431640625, 82.99186706542969, -110.84754943847656, -111.81290435791016, -68.10372161865234, -161.968994140625, -75.92408752441406, 45.32640075683594, 501.0646667480469, -144.04090881347656, 41.8613395690918, 115.31407165527344, 109.09294128417969, 236.9503936767578, -43.29294204711914, -66.70146179199219], 'y_pred_int': [19, 0, 0, 538, 0, 5, 0, 177, 0, 87, 0, 0, 0, 81, 0, 131, 209, 0, 90, 7, 0, 0, 178, 24, 117, 0, 0, 88, 216, 243, 66, 0, 0, 0, 4, 49, 0, 0, 0, 28, 82, 34, 0, 0, 96, 49, 0, 75, 0, 20, 0, 211, 0, 25, 262, 25, 277, 37, 0, 32, 423, 528, 66, 0, 175, 155, 210, 0, 0, 115, 12, 99, 0, 244, 0, 0, 27, 0, 95, 0, 0, 534, 11, 0, 0, 60, 119, 200, 0, 0, 14, 0, 0, 0, 0, 167, 1, 20, 0, 0, 276, 177, 0, 0, 424, 0, 0, 0, 129, 43, 0, 196, 0, 0, 0, 0, 0, 0, 0, 0, 148, 0, 0, 0, 0, 0, 0, 0, 78, 49, 122, 140, 0, 69, 227, 111, 0, 88, 0, 23, 0, 0, 89, 61, 0, 178, 22, 0, 0, 114, 0, 29, 0, 22, 0, 489, 32, 155, 300, 280, 0, 0, 28, 0, 157, 0, 114, 5, 0, 204, 0, 0, 0, 38, 47, 0, 141, 187, 245, 205, 0, 25, 92, 0, 3, 0, 5, 165, 48, 0, 0, 281, 0, 0, 0, 12, 325, 0, 0, 82, 41, 0, 23, 0, 0, 356, 106, 0, 95, 0, 231, 196, 0, 93, 0, 0, 0, 0, 0, 0, 46, 0, 86, 0, 177, 0, 32, 0, 137, 267, 0, 0, 89, 72, 0, 0, 49, 0, 84, 266, 0, 0, 78, 0, 0, 0, 0, 108, 400, 0, 234, 268, 113, 0, 374, 0, 6, 3, 236, 0, 20, 0, 0, 205, 425, 1, 0, 0, 56, 0, 0, 0, 0, 56, 36, 110, 98, 0, 14, 0, 32, 45, 201, 9, 112, 23, 76, 7, 182, 56, 3, 0, 0, 0, 0, 667, 0, 162, 12, 0, 51, 0, 144, 0, 248, 26, 110, 0, 0, 52, 0, 148, 32, 143, 7, 177, 279, 271, 0, 0, 0, 124, 148, 0, 0, 83, 0, 0, 0, 0, 0, 45, 501, 0, 42, 115, 109, 237, 0, 0], 'y_true': [12, 13, 11, 27, 19, 11, 26, 12, 8, 9, 5, 3, 16, 29, 14, 7, 10, 36, 28, 3, 7, 8, 44, 33, 8, 10, 17, 11, 6, 20, 4, 7, 5, 40, 9, 6, 6, 3, 13, 15, 13, 7, 8, 23, 14, 12, 26, 17, 5, 8, 5, 15, 4, 9, 20, 9, 7, 7, 6, 8, 13, 17, 8, 3, 7, 30, 10, 4, 22, 13, 4, 16, 7, 34, 15, 4, 29, 33, 10, 42, 10, 22, 9, 5, 10, 19, 17, 20, 11, 9, 5, 4, 4, 20, 2, 15, 5, 7, 22, 15, 76, 8, 19, 4, 17, 16, 9, 12, 10, 5, 9, 8, 10, 10, 4, 3, 7, 15, 10, 53, 4, 8, 12, 16, 10, 29, 15, 10, 18, 5, 33, 8, 5, 3, 5, 5, 8, 24, 20, 11, 21, 14, 5, 16, 5, 13, 30, 6, 21, 12, 8, 29, 3, 14, 5, 6, 29, 8, 17, 9, 21, 20, 7, 24, 6, 5, 17, 6, 16, 15, 13, 6, 6, 6, 12, 33, 7, 13, 19, 9, 1, 10, 14, 2, 46, 15, 23, 17, 8, 5, 21, 20, 24, 39, 16, 7, 15, 7, 13, 3, 9, 11, 10, 7, 9, 28, 5, 15, 5, 7, 14, 12, 11, 19, 13, 14, 22, 10, 29, 9, 28, 6, 9, 3, 13, 8, 9, 24, 7, 25, 5, 16, 17, 23, 23, 3, 12, 31, 26, 40, 8, 9, 15, 14, 49, 15, 8, 7, 47, 10, 20, 38, 7, 15, 21, 7, 18, 5, 4, 8, 14, 3, 25, 16, 9, 8, 17, 11, 27, 5, 9, 26, 7, 50, 2, 25, 4, 6, 17, 3, 18, 4, 35, 9, 2, 13, 23, 12, 14, 15, 11, 14, 16, 10, 6, 13, 14, 12, 5, 13, 14, 9, 19, 10, 14, 14, 22, 21, 21, 3, 14, 3, 30, 7, 8, 17, 9, 9, 13, 22, 26, 14, 22, 8, 2, 6, 6, 11, 25, 10, 19, 35, 11, 3, 16, 18, 48, 6, 31, 5], 'train_acc': 1.0, 'test_acc': 0.00342560553633218, 'X': 'X_TANL_layer_11_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 0.4351, Test loss: 1.4124
In this epoch 100/5000, Training loss: 0.0010, Test loss: 1.2770
In this epoch 150/5000, Training loss: 0.0000, Test loss: 1.2575
In this epoch 200/5000, Training loss: 0.0000, Test loss: 1.2582
In this epoch 250/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 300/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 350/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 400/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 450/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 500/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 550/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 600/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 650/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 700/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 750/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 800/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 850/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 900/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 950/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 1.2583
In this epoch 3800/5000, Training loss: 63.0846, Test loss: 57.1361
In this epoch 3850/5000, Training loss: 0.5195, Test loss: 2.0286
In this epoch 3900/5000, Training loss: 0.0012, Test loss: 1.2710
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 1.2623
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 1.2623
train_acc 0.34935661764705883
test_acc 0.40407439446366783
{'y_pred': [1.282119870185852, 0.07245111465454102, 0.6625472903251648, 0.9075327515602112, 0.19424043595790863, 1.187548041343689, 0.7093815207481384, 2.384857654571533, 0.7251556515693665, 0.5869593024253845, -0.5549315214157104, 2.0388286113739014, 0.9600122570991516, 1.5202802419662476, 0.5778475403785706, 1.2828606367111206, -0.09585990011692047, 1.224390983581543, 1.2751184701919556, 0.952753484249115, 1.6922951936721802, 1.187192678451538, 0.475032240152359, 0.7444731593132019, 0.4827791154384613, 1.0528115034103394, 1.5038609504699707, 1.3647265434265137, 0.9317885637283325, 1.3034446239471436, 0.0326026976108551, 0.9919185638427734, 0.0427924245595932, 0.9230951070785522, 0.2167937308549881, 0.9379470944404602, 0.4207342863082886, 0.5576117634773254, 0.49425116181373596, 0.5489423274993896, 1.1360985040664673, 0.6623983979225159, 0.6043375134468079, 0.7868804335594177, 0.4176342785358429, 1.4331904649734497, 1.1057101488113403, 0.8957528471946716, 0.46121975779533386, 0.6601783037185669, 1.2728465795516968, 1.0314170122146606, 0.48562273383140564, 1.5605030059814453, 0.8753622174263, 0.7711009979248047, 0.9597136974334717, 1.630483627319336, 1.592760443687439, 0.505957841873169, 0.8853153586387634, 1.2347418069839478, 0.8294082283973694, 0.6432915925979614, 0.39164912700653076, 0.40405893325805664, 0.1717483401298523, 0.7681776285171509, 0.9496774077415466, 0.7992477416992188, 1.8349772691726685, 0.9691831469535828, 0.27670231461524963, 0.8336586952209473, 0.4633145034313202, 1.1138006448745728, 0.6540514826774597, 0.15738904476165771, 1.6169124841690063, 0.9050218462944031, 0.6925116181373596, 1.1497596502304077, 0.44319674372673035, 0.03853598237037659, 0.77437424659729, 1.3686022758483887, 1.1061002016067505, 0.6569340229034424, 0.59992516040802, 0.34440502524375916, 0.7687807679176331, 0.6713019013404846, -0.033124569803476334, 0.17698030173778534, 0.4505949318408966, 0.7732394933700562, -0.16633263230323792, 0.6628819704055786, 0.11482445150613785, 2.2781472206115723, 1.745664358139038, 0.4844526946544647, 0.7782579064369202, 0.9824959635734558, 0.7420477867126465, 0.17178785800933838, 1.6547340154647827, 1.53568434715271, 0.8125396370887756, 0.6355051398277283, 0.6739006042480469, 1.353732705116272, 0.2997366786003113, 0.6790351271629333, 0.8969287276268005, -0.4558998644351959, 0.17827166616916656, 1.2918988466262817, 1.0240799188613892, 1.0225721597671509, 1.9109091758728027, 1.0276079177856445, 0.613727331161499, 0.4413672089576721, 1.1850807666778564, 0.8330737352371216, 2.441833972930908, 1.3547089099884033, 1.2602792978286743, 1.5496431589126587, 1.8943482637405396, 1.092763066291809, 0.7365037202835083, 0.5393397212028503, 0.4660334289073944, 0.7810893654823303, 1.3078453540802002, 0.37952283024787903, 1.2808893918991089, 0.6361370086669922, 1.5191150903701782, 0.7305770516395569, 0.9827955365180969, 1.5628300905227661, 0.45498034358024597, 1.7307915687561035, 1.2634161710739136, 1.2593625783920288, 0.7916340827941895, 1.1473829746246338, 1.0580486059188843, 0.18850721418857574, 1.3810997009277344, 0.9494752287864685, 0.7785547375679016, 0.6266236305236816, 1.4523810148239136, 0.7286035418510437, 0.31537914276123047, 1.5900388956069946, 0.483204185962677, 1.5006487369537354, -0.0534978061914444, 1.5993703603744507, 0.8466053009033203, -0.8331950306892395, 1.515974998474121, 1.2150102853775024, 1.1017096042633057, 0.22968672215938568, 0.5675133466720581, 1.3303649425506592, 1.1590977907180786, 2.0653183460235596, 0.8409609198570251, 0.9299881458282471, 1.231953740119934, 2.26906418800354, 1.0092555284500122, 0.5946435332298279, 0.043026864528656006, 0.6317419409751892, 0.9385267496109009, 1.0376174449920654, 0.8086283206939697, 0.7594401836395264, 1.958884835243225, 0.6821864247322083, 1.97634756565094, -0.21042518317699432, 6.263726390898228e-07, 0.8565587997436523, 1.014694333076477, 1.6378272771835327, 0.6118685007095337, 0.7272914052009583, 1.1775256395339966, 0.4808991849422455, 0.23381826281547546, 0.5305010676383972, 1.3026868104934692, 0.396538108587265, 0.43606510758399963, 1.8673070669174194, 1.0726704597473145, 0.14202497899532318, 1.273270845413208, 1.2012265920639038, 0.48132094740867615, 0.6821319460868835, 1.3227471113204956, 0.44257429242134094, 1.236831545829773, 0.36540186405181885, 1.261235237121582, 1.246629238128662, 1.0391939878463745, 1.1763818264007568, 0.9510822892189026, -0.0672345757484436, 0.9263438582420349, 0.7400010228157043, -0.16329699754714966, 1.6884478330612183, 1.9345731735229492, 1.9409003257751465, 1.2120929956436157, -0.007450975012034178, 1.516408085823059, 0.5887748599052429, 0.6481327414512634, 0.37598225474357605, 1.7033265829086304, 0.18852631747722626, 0.6195591688156128, 0.461929053068161, 0.33492714166641235, 1.4037057161331177, 0.9073200225830078, 0.2984847128391266, 0.36238840222358704, -0.005075513850897551, 0.30912768840789795, 1.0341413021087646, 0.8487677574157715, 1.0922726392745972, 1.423412799835205, 0.937893807888031, 0.9131858348846436, 0.5640984177589417, 0.7661448121070862, 0.9637042880058289, 0.5920857787132263, 1.2010869979858398, 0.5426345467567444, 1.0757826566696167, 0.05366013944149017, 0.3005673289299011, 0.8204509019851685, 0.9089816808700562, 0.2123950868844986, 1.413227915763855, 0.4811752736568451, 0.8496101498603821, 1.504194736480713, 1.7126017808914185, 0.2831246256828308, 1.0011225938796997, -0.008058018051087856, 1.784257173538208, -0.36206403374671936, 0.83620285987854, -0.5779781341552734, 0.5726072788238525, 1.2351826429367065, 0.4863879680633545, -0.03542746603488922, 0.443204402923584, 0.5678969621658325, 0.38607701659202576, 1.1386924982070923, 0.9878619313240051, 0.0982363373041153, 0.20100735127925873, 1.315446376800537, 0.41201356053352356, 1.0093659162521362, -0.04942917078733444, 1.2022913694381714, 1.4693207740783691, 0.449172705411911, 2.137563467025757, 0.9792609810829163, 0.5828079581260681, 1.3395283222198486, 1.0537700653076172, 1.4111664295196533, -0.03204943239688873, 0.8939834237098694, 1.4137994050979614, 1.0991077423095703, 1.5056935548782349, 0.6679859757423401, 0.5190154910087585, 0.5874068140983582, 0.3023020923137665, 0.5701970458030701, 1.3172496557235718, 0.5675238966941833, 0.6448301672935486, 0.01629616692662239, 1.6294280290603638, 1.2413774728775024, 1.0487452745437622, 0.43610483407974243, 0.7644959092140198, 1.5192245244979858, 0.9717819094657898, 0.9887128472328186, 0.7213490605354309, 0.19331149756908417, 0.6210558414459229, 0.9776474833488464, 1.0007671117782593, 1.4156826734542847, 1.0651135444641113, 0.9799644947052002, 0.6662460565567017, 0.8970429301261902, 1.2908921241760254, 1.349043369293213, 1.6657291650772095, 0.7817091345787048, 0.9493770003318787, 1.0868054628372192, 1.140554666519165, 0.5793769955635071, 0.8722206950187683, 0.5516182780265808, 0.7926943898200989], 'y_pred_int': [1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 0, 2, 1, 2, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 1, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 2, 2, 0, 1, 1, 1, 0, 2, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 2, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 2, 0, 2, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 2, 2, 1, 0, 2, 1, 1, 0, 2, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 2, 2, 0, 1, 0, 2, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'y_true': [1, 1, 0, 1, 3, 1, 1, 3, 0, 0, 0, 0, 1, 0, 1, 1, 1, 2, 2, 0, 1, 2, 1, 0, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 4, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 2, 1, 1, 0, 3, 0, 1, 0, 0, 1, 0, 1, 0, 1, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 5, 0, 0, 0, 0, 0, 3, 1, 0, 0, 4, 1, 1, 0, 0, 0, 0, 1, 0, 1, 2, 1, 1, 0, 1, 2, 0, 1, 3, 0, 1, 1, 0, 1, 0, 1, 1, 3, 0, 2, 1, 1, 0, 1, 3, 0, 0, 0, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 2, 0, 1, 1, 7, 0, 0, 2, 1, 0, 1, 0, 1, 1, 3, 0, 1, 0, 1, 3, 0, 0, 0, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 0, 8, 1, 0, 1, 0, 0, 0, 1, 0, 2, 3, 0, 0, 5, 1, 0, 3, 2, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 3, 1, 0, 0, 1, 0, 0, 1, 1, 2, 1, 6, 0, 1, 3, 2, 1, 1, 0, 1, 0, 0, 1, 1, 1, 4, 0, 0, 0, 0, 0, 0, 5, 0, 2, 1, 1, 1, 1, 2, 1, 1, 0, 1, 2, 0, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2, 0, 1, 1, 0, 2, 1, 2, 3, 0, 1, 4, 1, 0, 2, 0, 1, 0, 0, 4, 2, 3, 0, 0, 0, 2, 2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 1, 1, 0, 0, 3, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 1, 2, 0, 3, 0, 2], 'train_acc': 0.34935661764705883, 'test_acc': 0.40407439446366783, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 0.5985, Test loss: 37.6508
In this epoch 100/5000, Training loss: 0.0063, Test loss: 37.7107
In this epoch 150/5000, Training loss: 0.0000, Test loss: 37.6540
In this epoch 200/5000, Training loss: 0.0000, Test loss: 37.6494
In this epoch 250/5000, Training loss: 0.0000, Test loss: 37.6491
In this epoch 300/5000, Training loss: 0.0000, Test loss: 37.6493
In this epoch 350/5000, Training loss: 0.0000, Test loss: 37.6496
In this epoch 400/5000, Training loss: 0.0000, Test loss: 37.6498
In this epoch 450/5000, Training loss: 0.0000, Test loss: 37.6500
In this epoch 500/5000, Training loss: 0.0000, Test loss: 37.6502
In this epoch 550/5000, Training loss: 0.0000, Test loss: 37.6504
In this epoch 600/5000, Training loss: 0.0000, Test loss: 37.6506
In this epoch 650/5000, Training loss: 0.0000, Test loss: 37.6508
In this epoch 700/5000, Training loss: 0.0000, Test loss: 37.6510
In this epoch 750/5000, Training loss: 0.0000, Test loss: 37.6511
In this epoch 800/5000, Training loss: 0.0000, Test loss: 37.6512
In this epoch 850/5000, Training loss: 0.0000, Test loss: 37.6513
In this epoch 900/5000, Training loss: 0.0000, Test loss: 37.6514
In this epoch 950/5000, Training loss: 0.0000, Test loss: 37.6515
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 37.6516
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 37.6517
In this epoch 1100/5000, Training loss: 19.6798, Test loss: 57.7912
In this epoch 1150/5000, Training loss: 0.0122, Test loss: 37.8184
In this epoch 1200/5000, Training loss: 0.0006, Test loss: 37.6570
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 37.6513
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 37.6519
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 37.6519
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 37.6519
In this epoch 1450/5000, Training loss: 0.0001, Test loss: 37.6473
In this epoch 1500/5000, Training loss: 0.5533, Test loss: 37.7628
In this epoch 1550/5000, Training loss: 0.0013, Test loss: 37.6497
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 37.6513
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 37.6519
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 37.6519
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 37.6519
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 37.6519
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 37.6519
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 37.6519
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 37.6519
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 37.6519
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 37.6520
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 37.6520
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 37.6520
In this epoch 2200/5000, Training loss: 45.7107, Test loss: 113.5670
In this epoch 2250/5000, Training loss: 0.2654, Test loss: 37.8688
In this epoch 2300/5000, Training loss: 0.0015, Test loss: 37.6666
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 37.6533
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 37.6528
In this epoch 4600/5000, Training loss: 11.3291, Test loss: 48.8713
In this epoch 4650/5000, Training loss: 0.0745, Test loss: 37.6480
In this epoch 4700/5000, Training loss: 0.0003, Test loss: 37.6525
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 37.6566
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 37.6562
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 37.6563
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 37.6563
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 37.6563
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 37.6562
train_acc 1.0
test_acc 0.03575259515570934
{'y_pred': [3.5839264392852783, 25.052478790283203, 7.993086814880371, 7.01720666885376, 2.075864791870117, 16.209598541259766, 17.114831924438477, 18.64006996154785, 5.766636848449707, 21.305028915405273, 6.601990699768066, 16.057861328125, 21.629980087280273, 9.782827377319336, 10.335589408874512, 22.121580123901367, 27.229530334472656, 20.858070373535156, 7.724218368530273, 7.834133625030518, 8.562456130981445, 7.247846603393555, 6.626726150512695, 27.013254165649414, 13.943561553955078, 20.100133895874023, 8.130233764648438, 7.347550392150879, 17.65959358215332, 11.659543991088867, 5.569337844848633, 10.45504093170166, 4.8312273025512695, 9.73484992980957, 7.183122634887695, 6.491304874420166, 7.210758209228516, 25.891399383544922, 18.734655380249023, 7.798064231872559, 5.667666435241699, 18.58004379272461, 8.741381645202637, 24.75922203063965, 26.94025230407715, 21.154356002807617, 18.5208797454834, 9.680851936340332, 23.143659591674805, 19.12433433532715, 9.734733581542969, 3.7030885219573975, 2.57106614112854, 21.311542510986328, 23.593324661254883, 6.088072776794434, 5.678271293640137, 18.588050842285156, 23.57326316833496, 18.870840072631836, 15.207819938659668, 15.032824516296387, 0.9303041696548462, 16.25730323791504, 18.214200973510742, 21.09855842590332, 24.634658813476562, 18.603572845458984, 3.133010149002075, 25.338991165161133, 25.667051315307617, 1.568021535873413, 8.313261985778809, 8.320442199707031, 18.717079162597656, 21.866430282592773, 16.17443084716797, 23.414749145507812, 3.7927722930908203, 25.897472381591797, 3.1964223384857178, 20.423734664916992, 20.611650466918945, 12.578471183776855, 7.308503150939941, 17.66262435913086, 9.90522575378418, 19.072847366333008, 22.540639877319336, 19.108125686645508, 5.996180534362793, 19.640649795532227, 10.966463088989258, 8.776285171508789, 8.430739402770996, 20.92933464050293, 19.600603103637695, 15.828559875488281, 22.652734756469727, 9.011816024780273, 5.359987258911133, 9.629535675048828, 16.30290412902832, 11.207807540893555, 10.514233589172363, 8.686121940612793, 21.87082290649414, 20.71893882751465, 19.444164276123047, 7.355456829071045, 7.821220397949219, 22.80062484741211, 7.011580467224121, 18.45607566833496, 4.392458438873291, 13.672307968139648, 9.214775085449219, 18.06281089782715, 22.599563598632812, 11.974441528320312, 16.70551300048828, 13.870828628540039, 5.926360130310059, 17.73651885986328, 19.491426467895508, 5.643913745880127, 8.334989547729492, 9.58773422241211, 1.7668243646621704, 17.974475860595703, 21.374265670776367, 20.713655471801758, 19.542428970336914, 20.83072853088379, 15.374246597290039, 8.85310173034668, 16.32813835144043, 20.967866897583008, 7.861205101013184, 19.915239334106445, 17.83109474182129, 8.450559616088867, 10.532304763793945, 24.905149459838867, 17.765117645263672, 2.3844330310821533, 3.1776180267333984, 11.956422805786133, 25.83199119567871, 5.96212100982666, 23.767253875732422, 5.3533172607421875, 25.453947067260742, 11.29774284362793, 24.51488494873047, 21.30845832824707, 21.155475616455078, 5.6923322677612305, 3.8646843433380127, 8.638595581054688, 10.212087631225586, 6.5620198249816895, 19.015262603759766, 25.850202560424805, 8.100043296813965, 4.544358253479004, 6.400995254516602, 22.746370315551758, 9.418695449829102, 26.11970329284668, 8.390731811523438, 23.426271438598633, 3.5147829055786133, 23.19719696044922, 6.325695991516113, 12.267901420593262, 4.476550102233887, 6.850068092346191, 4.495763778686523, 28.34079360961914, 4.615684509277344, 8.396909713745117, 17.43947410583496, 6.852347373962402, 3.0171496868133545, 19.620771408081055, 12.29858684539795, 7.888068675994873, 18.904863357543945, 15.520020484924316, 5.806013584136963, 16.643918991088867, 14.08748722076416, 21.75733184814453, 24.013885498046875, 7.514181613922119, 4.634777069091797, 24.400468826293945, 17.6937313079834, 8.29084587097168, 16.73552703857422, 9.35844612121582, 17.831499099731445, 9.536764144897461, 7.031067371368408, 20.868724822998047, 7.8384270668029785, 8.965462684631348, 11.50084400177002, 2.8870368003845215, 28.71099853515625, 29.000001907348633, 16.89166831970215, 7.427663803100586, 5.7889604568481445, 8.256958961486816, 2.6602070331573486, 26.208229064941406, 6.437880039215088, 18.158531188964844, 13.649264335632324, 8.546048164367676, 6.879578113555908, 3.1837003231048584, 18.804040908813477, 17.381084442138672, 8.958956718444824, 16.863351821899414, 5.905137062072754, 19.254072189331055, 13.999039649963379, 11.66978645324707, 9.352375984191895, 18.85190773010254, 4.324220657348633, 2.929328203201294, 12.905900955200195, 9.255178451538086, 17.549928665161133, 5.768277645111084, 3.3159823417663574, 11.708406448364258, 22.06963539123535, 21.893186569213867, 23.978208541870117, 24.081899642944336, 4.134856700897217, 2.87774395942688, 4.600603103637695, 4.446942329406738, 10.942951202392578, 5.377655982971191, 24.974336624145508, 26.860614776611328, 4.160274028778076, 6.219158172607422, 5.489321708679199, 14.733004570007324, 11.375683784484863, 18.902395248413086, 7.749554634094238, 3.8779377937316895, 21.243202209472656, 20.181886672973633, 25.204404830932617, 23.313295364379883, 2.86452317237854, 11.277944564819336, 5.555279731750488, 6.264023780822754, 18.941068649291992, 24.589632034301758, 16.447250366210938, 11.160615921020508, 2.893167018890381, 22.09610366821289, 3.9531190395355225, 11.817301750183105, 11.725434303283691, 12.072783470153809, 20.531187057495117, 24.716705322265625, 7.073841094970703, 7.834343910217285, 0.7636919617652893, 17.430429458618164, 15.760114669799805, 14.767364501953125, 4.656642436981201, 24.613075256347656, 12.25979995727539, 9.964860916137695, 8.955613136291504, 17.89431381225586, 2.3765408992767334, 17.158252716064453, 16.02458953857422, 21.10420799255371, 25.16583824157715, 20.303922653198242, 18.76767921447754, 7.5936102867126465, 15.802072525024414, 8.921259880065918, 19.413047790527344, 5.591362953186035, 20.125436782836914, 2.457646608352661, 7.771040916442871, 18.519821166992188, 3.3750879764556885, 3.483593702316284, 13.81769847869873, 16.878856658935547, 17.226011276245117, 10.635834693908691, 18.065813064575195, 12.181221008300781, 5.601228713989258, 3.566063165664673, 22.041593551635742, 2.872102737426758, 6.384700775146484, 18.243818283081055, 4.750470161437988, 5.0675530433654785, 15.649442672729492, 8.16476058959961, 16.466360092163086, 24.533842086791992, 8.565728187561035, 2.712881565093994, 17.74985122680664, 14.15318489074707, 19.94947624206543, 0.96763014793396, 20.01661491394043, 22.592802047729492, 27.428424835205078, 5.880494117736816], 'y_pred_int': [4, 25, 8, 7, 2, 16, 17, 19, 6, 21, 7, 16, 22, 10, 10, 22, 27, 21, 8, 8, 9, 7, 7, 27, 14, 20, 8, 7, 18, 12, 6, 10, 5, 10, 7, 6, 7, 26, 19, 8, 6, 19, 9, 25, 27, 21, 19, 10, 23, 19, 10, 4, 3, 21, 24, 6, 6, 19, 24, 19, 15, 15, 1, 16, 18, 21, 25, 19, 3, 25, 26, 2, 8, 8, 19, 22, 16, 23, 4, 26, 3, 20, 21, 13, 7, 18, 10, 19, 23, 19, 6, 20, 11, 9, 8, 21, 20, 16, 23, 9, 5, 10, 16, 11, 11, 9, 22, 21, 19, 7, 8, 23, 7, 18, 4, 14, 9, 18, 23, 12, 17, 14, 6, 18, 19, 6, 8, 10, 2, 18, 21, 21, 20, 21, 15, 9, 16, 21, 8, 20, 18, 8, 11, 25, 18, 2, 3, 12, 26, 6, 24, 5, 25, 11, 25, 21, 21, 6, 4, 9, 10, 7, 19, 26, 8, 5, 6, 23, 9, 26, 8, 23, 4, 23, 6, 12, 4, 7, 4, 28, 5, 8, 17, 7, 3, 20, 12, 8, 19, 16, 6, 17, 14, 22, 24, 8, 5, 24, 18, 8, 17, 9, 18, 10, 7, 21, 8, 9, 12, 3, 29, 29, 17, 7, 6, 8, 3, 26, 6, 18, 14, 9, 7, 3, 19, 17, 9, 17, 6, 19, 14, 12, 9, 19, 4, 3, 13, 9, 18, 6, 3, 12, 22, 22, 24, 24, 4, 3, 5, 4, 11, 5, 25, 27, 4, 6, 5, 15, 11, 19, 8, 4, 21, 20, 25, 23, 3, 11, 6, 6, 19, 25, 16, 11, 3, 22, 4, 12, 12, 12, 21, 25, 7, 8, 1, 17, 16, 15, 5, 25, 12, 10, 9, 18, 2, 17, 16, 21, 25, 20, 19, 8, 16, 9, 19, 6, 20, 2, 8, 19, 3, 3, 14, 17, 17, 11, 18, 12, 6, 4, 22, 3, 6, 18, 5, 5, 16, 8, 16, 25, 9, 3, 18, 14, 20, 1, 20, 23, 27, 6], 'y_true': [3, 44, 6, 8, 2, 17, 13, 20, 6, 22, 5, 10, 16, 7, 6, 18, 34, 24, 8, 11, 11, 8, 10, 35, 16, 35, 11, 8, 22, 8, 7, 9, 4, 8, 7, 10, 5, 39, 37, 6, 3, 22, 8, 30, 18, 19, 18, 12, 25, 19, 10, 5, 4, 24, 23, 8, 6, 22, 33, 16, 13, 30, 2, 9, 15, 26, 26, 15, 3, 33, 32, 3, 1, 9, 21, 31, 13, 14, 4, 22, 5, 32, 18, 16, 5, 17, 10, 11, 27, 24, 6, 25, 8, 8, 6, 33, 9, 17, 22, 13, 8, 7, 10, 7, 13, 7, 48, 37, 15, 5, 6, 20, 6, 19, 7, 7, 6, 13, 19, 12, 19, 10, 4, 26, 13, 3, 6, 6, 1, 14, 25, 12, 10, 19, 11, 7, 11, 25, 6, 16, 34, 14, 8, 21, 12, 2, 3, 15, 39, 3, 23, 4, 46, 9, 22, 17, 11, 7, 7, 6, 10, 6, 12, 42, 8, 9, 10, 14, 14, 23, 5, 22, 6, 57, 3, 9, 3, 9, 4, 31, 3, 10, 13, 7, 4, 18, 10, 6, 15, 10, 4, 14, 10, 33, 34, 8, 4, 32, 12, 6, 20, 7, 13, 8, 12, 11, 5, 7, 11, 3, 22, 29, 10, 7, 8, 4, 2, 37, 5, 14, 9, 6, 8, 5, 9, 10, 6, 15, 10, 32, 8, 11, 11, 12, 2, 5, 12, 9, 13, 7, 3, 14, 20, 23, 31, 36, 5, 5, 5, 2, 6, 7, 23, 39, 6, 10, 4, 31, 11, 47, 4, 3, 15, 9, 31, 30, 6, 10, 6, 7, 15, 39, 11, 9, 2, 18, 5, 11, 9, 9, 13, 14, 4, 9, 1, 15, 14, 18, 6, 29, 14, 9, 5, 11, 6, 15, 15, 34, 54, 22, 24, 5, 10, 5, 15, 8, 19, 3, 9, 14, 2, 5, 6, 10, 17, 15, 15, 17, 8, 5, 35, 4, 5, 23, 2, 6, 13, 9, 12, 23, 9, 2, 15, 11, 28, 1, 29, 21, 21, 6], 'train_acc': 1.0, 'test_acc': 0.03575259515570934, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 13993.5430, Test loss: 15585.8018
In this epoch 100/5000, Training loss: 7851.5449, Test loss: 13434.8662
In this epoch 150/5000, Training loss: 4526.3047, Test loss: 12910.9121
In this epoch 200/5000, Training loss: 2489.2549, Test loss: 12680.6328
In this epoch 250/5000, Training loss: 1311.7358, Test loss: 12572.3301
In this epoch 300/5000, Training loss: 666.1628, Test loss: 12520.2588
In this epoch 350/5000, Training loss: 327.9753, Test loss: 12494.6133
In this epoch 400/5000, Training loss: 157.6144, Test loss: 12481.0518
In this epoch 450/5000, Training loss: 74.5737, Test loss: 12472.7549
In this epoch 500/5000, Training loss: 35.1190, Test loss: 12466.7197
In this epoch 550/5000, Training loss: 16.6757, Test loss: 12461.7500
In this epoch 600/5000, Training loss: 8.0924, Test loss: 12457.4443
In this epoch 650/5000, Training loss: 4.0597, Test loss: 12453.7002
In this epoch 700/5000, Training loss: 2.1185, Test loss: 12450.4785
In this epoch 750/5000, Training loss: 1.1495, Test loss: 12447.7637
In this epoch 800/5000, Training loss: 0.6445, Test loss: 12445.5176
In this epoch 850/5000, Training loss: 0.3700, Test loss: 12443.6914
In this epoch 900/5000, Training loss: 0.2155, Test loss: 12442.2314
In this epoch 950/5000, Training loss: 0.1263, Test loss: 12441.0840
In this epoch 1000/5000, Training loss: 0.0741, Test loss: 12440.1963
In this epoch 1050/5000, Training loss: 0.0433, Test loss: 12439.5205
In this epoch 1100/5000, Training loss: 0.0251, Test loss: 12439.0059
In this epoch 1150/5000, Training loss: 0.0144, Test loss: 12438.6338
In this epoch 1200/5000, Training loss: 0.0082, Test loss: 12438.3604
In this epoch 1250/5000, Training loss: 0.0046, Test loss: 12438.1621
In this epoch 1300/5000, Training loss: 0.0025, Test loss: 12438.0234
In this epoch 1350/5000, Training loss: 0.0014, Test loss: 12437.9268
In this epoch 1400/5000, Training loss: 0.0007, Test loss: 12437.8594
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 12437.8135
In this epoch 1500/5000, Training loss: 0.0002, Test loss: 12437.7871
In this epoch 1550/5000, Training loss: 0.0001, Test loss: 12437.7676
In this epoch 1600/5000, Training loss: 0.0001, Test loss: 12437.7529
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 12437.7490
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 12437.7432
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 12437.7383
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 12437.7383
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 12437.7344
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 12437.7344
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 12437.7344
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 12437.7344
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 12437.7344
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 12437.7344
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 12437.7344
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 12437.7344
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 12437.7344
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 12437.7354
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 12437.7354
train_acc 1.0
test_acc 0.008823529411764706
{'y_pred': [494.5762634277344, 200.54220581054688, 563.7230834960938, 222.74583435058594, 223.39157104492188, 210.24302673339844, 181.18801879882812, 493.2995910644531, 502.7482604980469, 453.97509765625, 251.071533203125, 332.98370361328125, 179.4159698486328, 519.9708251953125, 619.5540161132812, 595.2252807617188, 130.97494506835938, 528.5369873046875, 572.6026611328125, 41.25841522216797, 197.12237548828125, 489.8157958984375, 517.9854125976562, 168.80044555664062, 144.61920166015625, 552.7155151367188, 484.25872802734375, 505.6301574707031, 176.5954132080078, 182.87335205078125, 328.43841552734375, 567.4389038085938, 529.9014282226562, 493.87939453125, 292.58709716796875, 258.3480224609375, 365.87945556640625, 73.34873962402344, 536.9223022460938, 567.2427368164062, 557.4996948242188, 569.1165161132812, 114.63892364501953, 227.25193786621094, 514.6465454101562, 164.00997924804688, 523.1029052734375, 121.42257690429688, 179.6553955078125, 503.80078125, 616.3158569335938, 553.1373291015625, 145.1463165283203, 524.9697875976562, 508.8880310058594, 509.36407470703125, 618.9534301757812, 232.22683715820312, 116.26078796386719, 589.8209838867188, 94.53069305419922, 278.40875244140625, 149.91993713378906, 840.0, 450.24029541015625, 481.5025329589844, 520.5001220703125, 226.9747772216797, 150.36361694335938, 163.716796875, 100.63436126708984, 135.36387634277344, 65.443115234375, 459.55438232421875, 98.8386001586914, 261.63720703125, 163.24436950683594, 508.63714599609375, 165.99005126953125, 245.77386474609375, 231.5867156982422, 159.40638732910156, 146.57928466796875, 293.334716796875, 215.85202026367188, 25.086078643798828, 305.85491943359375, 513.8912353515625, 126.17686462402344, 452.1296691894531, 164.4784698486328, 539.345703125, 260.53546142578125, 228.09414672851562, 344.97021484375, 537.2229614257812, 533.7871704101562, 512.6770629882812, 106.6993408203125, 303.0437316894531, 459.0325927734375, 231.36294555664062, 586.8046875, 214.28506469726562, 484.061279296875, 331.4655456542969, 631.7982788085938, 492.9960632324219, 632.2364501953125, 262.9322814941406, 455.34307861328125, 509.09515380859375, 554.8888549804688, 301.4993896484375, 593.5391845703125, 488.7604675292969, 545.9168701171875, 143.17233276367188, 549.6510009765625, 171.1110382080078, 577.6569213867188, 208.44761657714844, 549.4467163085938, 201.11456298828125, 570.8126220703125, 180.51657104492188, 570.7579956054688, 214.39678955078125, 499.24786376953125, 165.1124267578125, 460.86529541015625, 611.044921875, 534.5636596679688, 522.4347534179688, 414.4259033203125, 416.20831298828125, 173.83767700195312, 465.6904296875, 64.82008361816406, 356.8314514160156, 486.2022705078125, 448.9606628417969, 219.8622589111328, 526.2448120117188, 542.0337524414062, 177.7190704345703, 158.1337127685547, 233.48141479492188, 524.3086547851562, 564.1924438476562, 208.61495971679688, 137.42379760742188, 424.5408935546875, 503.838623046875, 495.931640625, 610.7101440429688, 570.8115844726562, 554.5579223632812, 490.0854797363281, 89.82412719726562, 490.78997802734375, 628.2953491210938, 513.9521484375, 101.37499237060547, 291.0533142089844, 515.3821411132812, 90.00965118408203, 67.40676879882812, 131.62158203125, 579.0921020507812, 105.71844482421875, 331.60302734375, 597.0422973632812, 575.4140014648438, 559.0570068359375, 548.0044555664062, 276.2504577636719, 479.15411376953125, 556.3466796875, 463.46636962890625, 395.645263671875, 128.60768127441406, 150.78843688964844, 211.04953002929688, 158.22988891601562, 312.1507263183594, 553.2157592773438, 68.56149291992188, 272.9259033203125, 461.924072265625, 132.30870056152344, 535.6768188476562, 505.517333984375, 187.3075714111328, 191.46487426757812, 578.6067504882812, 441.74798583984375, 514.5156860351562, 266.6156005859375, 133.3441162109375, 533.3682250976562, 248.7730255126953, 530.1795043945312, 258.1634521484375, 130.95172119140625, 567.46044921875, 538.5897827148438, 22.46458625793457, 541.8898315429688, 186.47735595703125, 429.0067443847656, 211.4115447998047, 530.0897216796875, 542.0690307617188, 175.47061157226562, 469.9862060546875, 483.41668701171875, 273.65032958984375, 521.3643188476562, 178.4296875, 619.2396240234375, 323.89190673828125, 236.4227752685547, 535.7189331054688, 52.77203369140625, 125.53309631347656, 394.7528381347656, 505.1849060058594, 231.3688201904297, 112.88777923583984, 518.9298706054688, 550.0860595703125, 283.2336730957031, 549.0728149414062, 479.1740417480469, 114.38346862792969, 498.15277099609375, 224.98883056640625, 547.1641235351562, 537.6791381835938, 136.21372985839844, 374.82208251953125, 273.2275390625, 174.4163055419922, 129.3974151611328, 572.0033569335938, 495.0553894042969, 262.2471618652344, 172.75457763671875, 77.37538146972656, 244.9060516357422, 568.7650756835938, 568.8753051757812, 591.4476928710938, 537.493408203125, 530.6228637695312, 497.78448486328125, 514.0216064453125, 394.94207763671875, 200.8743896484375, 339.15435791015625, 265.7970886230469, 500.9202880859375, 494.3939208984375, 268.3243408203125, 232.22695922851562, 17.41224479675293, 218.27224731445312, 187.61264038085938, 59.2006721496582, 602.1472778320312, 127.36289978027344, 537.6658325195312, 243.30926513671875, 77.1923828125, 127.6742935180664, 511.0763244628906, 564.1934814453125, 219.9276123046875, 650.3270263671875, 585.9489135742188, 477.6928405761719, 566.7706909179688, 402.70654296875, 117.86405944824219, 337.6580810546875, 524.7235717773438, 416.59637451171875, 461.383544921875, 211.9251251220703, 181.72796630859375, 286.051513671875, 214.75439453125, 629.6378784179688, 465.1717529296875, 485.02667236328125, 175.1273193359375, 167.07281494140625, 541.7636108398438, 284.2435302734375, 515.4639892578125, 309.0240173339844, 432.96673583984375, 542.7059936523438, 251.33718872070312, 188.66065979003906, 489.9482421875, 572.3158569335938, 509.6546325683594, 214.63706970214844, 380.56390380859375, 219.2918701171875, 99.51042175292969, 679.578857421875, 570.5820922851562, 546.8294067382812, 299.9501037597656, 480.7854309082031, 594.2296752929688, 97.40512084960938, 233.91372680664062, 86.40855407714844, 508.5591125488281, 523.7301635742188, 515.3622436523438, 169.17868041992188, 340.4782409667969, 403.5169982910156, 177.90643310546875, 499.26654052734375, 508.47381591796875, 248.91018676757812, 463.0283203125, 92.17960357666016, 467.45819091796875, 165.4032440185547, 183.90164184570312, 104.87385559082031, 476.8002624511719, 11.085987091064453], 'y_pred_int': [495, 201, 564, 223, 223, 210, 181, 493, 503, 454, 251, 333, 179, 520, 620, 595, 131, 529, 573, 41, 197, 490, 518, 169, 145, 553, 484, 506, 177, 183, 328, 567, 530, 494, 293, 258, 366, 73, 537, 567, 557, 569, 115, 227, 515, 164, 523, 121, 180, 504, 616, 553, 145, 525, 509, 509, 619, 232, 116, 590, 95, 278, 150, 840, 450, 482, 521, 227, 150, 164, 101, 135, 65, 460, 99, 262, 163, 509, 166, 246, 232, 159, 147, 293, 216, 25, 306, 514, 126, 452, 164, 539, 261, 228, 345, 537, 534, 513, 107, 303, 459, 231, 587, 214, 484, 331, 632, 493, 632, 263, 455, 509, 555, 301, 594, 489, 546, 143, 550, 171, 578, 208, 549, 201, 571, 181, 571, 214, 499, 165, 461, 611, 535, 522, 414, 416, 174, 466, 65, 357, 486, 449, 220, 526, 542, 178, 158, 233, 524, 564, 209, 137, 425, 504, 496, 611, 571, 555, 490, 90, 491, 628, 514, 101, 291, 515, 90, 67, 132, 579, 106, 332, 597, 575, 559, 548, 276, 479, 556, 463, 396, 129, 151, 211, 158, 312, 553, 69, 273, 462, 132, 536, 506, 187, 191, 579, 442, 515, 267, 133, 533, 249, 530, 258, 131, 567, 539, 22, 542, 186, 429, 211, 530, 542, 175, 470, 483, 274, 521, 178, 619, 324, 236, 536, 53, 126, 395, 505, 231, 113, 519, 550, 283, 549, 479, 114, 498, 225, 547, 538, 136, 375, 273, 174, 129, 572, 495, 262, 173, 77, 245, 569, 569, 591, 537, 531, 498, 514, 395, 201, 339, 266, 501, 494, 268, 232, 17, 218, 188, 59, 602, 127, 538, 243, 77, 128, 511, 564, 220, 650, 586, 478, 567, 403, 118, 338, 525, 417, 461, 212, 182, 286, 215, 630, 465, 485, 175, 167, 542, 284, 515, 309, 433, 543, 251, 189, 490, 572, 510, 215, 381, 219, 100, 680, 571, 547, 300, 481, 594, 97, 234, 86, 509, 524, 515, 169, 340, 404, 178, 499, 508, 249, 463, 92, 467, 165, 184, 105, 477, 11], 'y_true': [661, 189, 662, 207, 242, 237, 209, 392, 679, 462, 212, 350, 196, 409, 743, 658, 121, 340, 681, 45, 189, 371, 412, 226, 104, 901, 376, 336, 167, 176, 321, 934, 456, 638, 298, 285, 321, 83, 593, 878, 901, 935, 112, 237, 458, 186, 591, 134, 209, 407, 490, 406, 140, 552, 729, 334, 624, 243, 154, 433, 39, 288, 150, 841, 373, 328, 491, 262, 180, 143, 103, 110, 87, 422, 74, 240, 163, 417, 144, 239, 191, 213, 177, 254, 209, 69, 292, 500, 95, 490, 227, 433, 244, 260, 372, 441, 397, 378, 139, 315, 633, 245, 796, 221, 579, 295, 426, 545, 430, 264, 332, 332, 431, 312, 511, 488, 422, 128, 768, 204, 523, 201, 507, 167, 655, 199, 585, 235, 505, 185, 731, 466, 372, 690, 330, 339, 137, 409, 56, 342, 372, 370, 220, 583, 521, 131, 176, 209, 1342, 762, 244, 115, 401, 395, 482, 758, 542, 590, 492, 96, 727, 723, 354, 109, 270, 396, 69, 78, 155, 514, 92, 312, 724, 556, 842, 676, 291, 450, 911, 299, 314, 150, 132, 159, 200, 294, 607, 78, 280, 348, 143, 452, 446, 197, 197, 771, 331, 621, 307, 123, 456, 217, 556, 262, 160, 449, 463, 32, 523, 155, 446, 257, 385, 560, 181, 350, 530, 248, 390, 300, 898, 335, 240, 358, 81, 119, 302, 827, 247, 82, 634, 548, 234, 541, 310, 130, 343, 149, 880, 361, 128, 351, 279, 181, 111, 447, 474, 226, 161, 60, 276, 583, 748, 643, 747, 440, 404, 455, 348, 206, 302, 258, 519, 378, 228, 241, 41, 169, 185, 90, 513, 103, 631, 206, 54, 93, 562, 447, 202, 461, 617, 286, 921, 339, 150, 297, 534, 409, 431, 279, 220, 250, 222, 464, 367, 600, 136, 168, 481, 268, 361, 294, 365, 459, 208, 169, 687, 493, 376, 261, 325, 237, 111, 863, 753, 603, 211, 327, 431, 97, 216, 79, 443, 399, 385, 144, 341, 309, 157, 380, 599, 251, 391, 100, 639, 130, 200, 118, 427, 35], 'train_acc': 1.0, 'test_acc': 0.008823529411764706, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 27.8021, Test loss: 9.2104
In this epoch 100/5000, Training loss: 0.3313, Test loss: 0.6830
In this epoch 150/5000, Training loss: 0.0800, Test loss: 0.6193
In this epoch 200/5000, Training loss: 0.0488, Test loss: 0.6251
In this epoch 250/5000, Training loss: 0.0307, Test loss: 0.6315
In this epoch 300/5000, Training loss: 0.0197, Test loss: 0.6373
In this epoch 350/5000, Training loss: 0.0128, Test loss: 0.6422
In this epoch 400/5000, Training loss: 0.0083, Test loss: 0.6463
In this epoch 450/5000, Training loss: 0.0055, Test loss: 0.6497
In this epoch 500/5000, Training loss: 0.0036, Test loss: 0.6526
In this epoch 550/5000, Training loss: 0.0023, Test loss: 0.6549
In this epoch 600/5000, Training loss: 0.0015, Test loss: 0.6569
In this epoch 650/5000, Training loss: 0.0010, Test loss: 0.6586
In this epoch 700/5000, Training loss: 0.0006, Test loss: 0.6600
In this epoch 750/5000, Training loss: 0.0004, Test loss: 0.6612
In this epoch 800/5000, Training loss: 0.0003, Test loss: 0.6621
In this epoch 850/5000, Training loss: 0.0002, Test loss: 0.6629
In this epoch 900/5000, Training loss: 0.0001, Test loss: 0.6636
In this epoch 950/5000, Training loss: 0.0001, Test loss: 0.6641
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 0.6645
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 0.6649
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 0.6651
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 0.6653
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 0.6655
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 0.6656
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 0.6657
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 0.6658
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 0.6660
In this epoch 3400/5000, Training loss: 1206.6802, Test loss: 4.7095
In this epoch 3450/5000, Training loss: 2.4026, Test loss: 2.3241
In this epoch 3500/5000, Training loss: 0.0048, Test loss: 0.7124
In this epoch 3550/5000, Training loss: 0.0002, Test loss: 0.6660
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 0.6659
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 0.6659
train_acc 0.3526394896193772
test_acc 0.36083044982698964
{'y_pred': [0.0011430507292971015, -0.02281857468187809, 3.3088057041168213, 0.9417091012001038, 0.8006422519683838, -0.1306513398885727, 2.0742952823638916, -0.14655925333499908, -0.03150254860520363, 1.4598538875579834, 1.7661997079849243, -0.212742879986763, 2.4165143966674805, 0.9222442507743835, 1.6594263315200806, -0.4150031805038452, 1.0969363451004028, 0.43141308426856995, 0.6176093816757202, 0.48293110728263855, -0.0007054495508782566, 1.275665521621704, 1.4724946022033691, 0.9017462730407715, 0.4600561857223511, -0.9292580485343933, 1.4734185934066772, 0.005448935087770224, -0.4312228560447693, 1.3016248941421509, -0.23409731686115265, 1.0214961767196655, 2.023893356323242, 0.7571111917495728, 0.5982905030250549, 0.39533182978630066, 0.1922636479139328, -0.15058457851409912, 1.3592655658721924, 0.5816124081611633, 1.4108829498291016, 0.5091876983642578, 1.6039108037948608, -0.4176611304283142, -0.28169378638267517, 2.837930202484131, 1.9903322458267212, 2.0799527168273926, 0.9180664420127869, 1.2095422744750977, 0.004144986625760794, 0.6400558352470398, -0.2974123954772949, -0.5885201096534729, 2.296147584915161, 0.6314141154289246, 1.3645923137664795, 0.9745252132415771, 0.10218677669763565, 4.2452263832092285, 2.8028652667999268, 0.11238116770982742, 4.4455373426899314e-07, 0.4178593158721924, 0.42666685581207275, 1.428728461265564, 0.2951124608516693, 1.5775150060653687, 1.1225913763046265, -0.9003213047981262, 0.9652124047279358, 2.210576057434082, 2.070120334625244, 2.472054958343506, 1.7192366123199463, 1.6232781410217285, 1.246692419052124, 0.539863646030426, -0.39054858684539795, 1.3558274507522583, -0.01696074940264225, 2.2280499935150146, 0.2508714199066162, 4.306166172027588, -0.10616100579500198, -0.2517600655555725, 0.4567200541496277, 0.650771975517273, 0.15991578996181488, 0.3756307065486908, 0.1535518318414688, 0.3849904239177704, 1.6581710577011108, -0.07841039448976517, 2.9757585525512695, 0.03682081773877144, 2.5629138946533203, 2.547304153442383, 0.2617986500263214, 0.09963317960500717, -0.6863005757331848, -0.08696827292442322, 3.9747838973999023, -0.2531028389930725, 1.1203975677490234, 1.0121877193450928, 0.6215156316757202, 0.7709129452705383, 2.9392571449279785, -0.11029444634914398, 0.3502233624458313, 1.0192798376083374, 1.1631276607513428, -0.4935837388038635, 1.4707509279251099, 0.8513900637626648, -0.03132861480116844, 1.4728105068206787, 0.02889268659055233, 2.485041856765747, 0.14105738699436188, 1.6911333799362183, 3.2444660663604736, 2.796769618988037, 1.9474915266036987, 0.34789708256721497, 0.6551052927970886, 0.41466936469078064, 0.6843215227127075, 0.279323011636734, -0.5526393055915833, -0.16134636104106903, 1.3554284572601318, 0.7725117206573486, 2.0216665267944336, 1.7217761278152466, 0.5870798230171204, 0.3585889935493469, 0.4647694528102875, 1.798274278640747, 0.2143947035074234, 1.5709716081619263, 1.8369492292404175, -0.07191019505262375, -0.24270372092723846, 2.05802321434021, -0.04592589661478996, 1.708854079246521, -0.4718996286392212, 1.3756437301635742, 1.078538179397583, 0.08927398175001144, 0.9595204591751099, 1.7452341318130493, 3.3234033584594727, 2.43373966217041, -0.09768757969141006, 1.2408438920974731, 2.044442653656006, 2.009321451187134, 1.8898656368255615, 0.25658589601516724, 1.2684468030929565, 0.8144592046737671, 1.6124540567398071, -0.6796257495880127, 1.1955853700637817, 0.28427988290786743, 2.2202816009521484, 0.5488136410713196, 0.3692971169948578, -0.327370822429657, 0.05287482216954231, -0.6400074362754822, 0.9668853878974915, 1.9619725942611694, -0.42259272933006287, -0.09910108894109726, 0.049316681921482086, 0.3316354751586914, 1.0927096605300903, 1.5235053300857544, 0.2831793427467346, 3.327950253151357e-07, 1.1077202558517456, 0.88217693567276, 1.582985281944275, 0.004866169299930334, 2.274691104888916, 0.47937139868736267, 1.2390923500061035, -0.2951539158821106, 0.3046826720237732, -0.17515628039836884, 2.3615307807922363, 0.9834501147270203, 1.0858677625656128, 0.968646228313446, 0.6008803844451904, 0.3401166796684265, 0.04779893532395363, 1.510698914527893, 1.9365432262420654, 0.3003087639808655, 0.04858717694878578, 0.012662489898502827, 0.3248305916786194, 0.5952790379524231, 0.15310414135456085, 2.2824130058288574, -0.5389235019683838, 0.6580724120140076, 1.347900629043579, 3.196251153945923, 0.03332769498229027, -0.5571617484092712, 2.7313950061798096, -0.09941869974136353, 1.3509529829025269, 0.14475341141223907, -0.0790344700217247, 0.49159467220306396, -0.24282220005989075, 2.5123791694641113, 1.5070682764053345, 0.8384369611740112, 1.694043517112732, 0.2854043245315552, 0.8791152834892273, -0.09226717799901962, 0.13648177683353424, 2.3202524185180664, 2.7050700187683105, 2.0088891983032227, 1.69773268699646, 2.0978944301605225, 0.0811677798628807, 1.2978318929672241, 1.8022366762161255, 2.3662824630737305, -0.051462750881910324, 2.87099027633667, 0.406740665435791, 0.1549321860074997, 1.5708235502243042, 0.5791749358177185, 1.367820143699646, 1.654333472251892, 2.2607975006103516, 0.9905838370323181, 1.6302489042282104, 0.5389740467071533, 2.6402854919433594, 1.5375537872314453, 1.6399571895599365, -0.09503646194934845, 0.5454408526420593, 0.7268239855766296, 0.778920590877533, 0.39128953218460083, 1.480058193206787, 0.18825431168079376, 1.0937509536743164, 0.5291197299957275, 0.2561318576335907, 0.02806050144135952, 0.9631444215774536, 3.26309871673584, 1.0500812530517578, 3.9618780612945557, 0.3624303936958313, 0.01452066469937563, 2.6065001487731934, 2.5393567085266113, 1.0783277750015259, 1.5244897603988647, 1.0536140203475952, 1.8319206237792969, 3.0397391319274902, 0.02751065604388714, -0.02473006583750248, 0.4621904790401459, 0.4944218099117279, 2.3787622451782227, 0.4732169508934021, 2.1974565982818604, 0.14449889957904816, 0.5463430285453796, -0.24672500789165497, 0.9414548277854919, -0.023657487705349922, -0.590874433517456, 1.2476328611373901, 1.2545469999313354, 0.09185557812452316, 0.08340951055288315, 1.3237680196762085, 1.8397523164749146, 0.7641514539718628, -0.8659122586250305, 0.8564140796661377, 1.4273757934570312, -0.09260875731706619, -0.0418664924800396, 0.45395082235336304, 0.24241885542869568, -0.041894640773534775, 0.10725849121809006, 0.35565853118896484, 0.7361687421798706, -0.058115433901548386, 2.755997657775879, 0.22937598824501038, -0.2575025260448456, 1.5786329507827759, 3.200547218322754, 0.5595546364784241, 1.228895902633667, 0.5803980827331543, 0.2551139295101166, 0.5887637138366699, 1.811852216720581, 1.7430013418197632, 1.435843586921692, 2.5786523818969727, 0.8880277872085571, 0.9046444296836853, 2.124021530151367, 2.0921313762664795, 2.5618114471435547, 0.15819448232650757, -0.5888540744781494, 0.2056218832731247, 2.806452751159668, 1.8574291467666626, 0.38674795627593994, -0.6733927130699158, 0.20624344050884247, 1.2137686014175415, 3.2983968257904053], 'y_pred_int': [0, 0, 3, 1, 1, 0, 2, 0, 0, 1, 2, 0, 2, 1, 2, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 0, 0, 0, 1, 1, 1, 1, 2, 0, 0, 3, 2, 2, 1, 1, 0, 1, 0, 0, 2, 1, 1, 1, 0, 4, 3, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1, 2, 2, 2, 2, 2, 1, 1, 0, 1, 0, 2, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 3, 0, 3, 3, 0, 0, 0, 0, 4, 0, 1, 1, 1, 1, 3, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 2, 0, 2, 3, 3, 2, 0, 1, 0, 1, 0, 0, 0, 1, 1, 2, 2, 1, 0, 0, 2, 0, 2, 2, 0, 0, 2, 0, 2, 0, 1, 1, 0, 1, 2, 3, 2, 0, 1, 2, 2, 2, 0, 1, 1, 2, 0, 1, 0, 2, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 2, 0, 2, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 2, 2, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 3, 0, 0, 3, 0, 1, 0, 0, 0, 0, 3, 2, 1, 2, 0, 1, 0, 0, 2, 3, 2, 2, 2, 0, 1, 2, 2, 0, 3, 0, 0, 2, 1, 1, 2, 2, 1, 2, 1, 3, 2, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 3, 1, 4, 0, 0, 3, 3, 1, 2, 1, 2, 3, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 2, 3, 1, 1, 1, 0, 1, 2, 2, 1, 3, 1, 1, 2, 2, 3, 0, 0, 0, 3, 2, 0, 0, 0, 1, 3], 'y_true': [0, 0, 4, 0, 1, 0, 3, 0, 0, 1, 2, 0, 2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 3, 1, 1, 0, 0, 0, 1, 1, 2, 1, 1, 0, 1, 4, 1, 2, 1, 2, 0, 1, 0, 0, 4, 0, 1, 0, 0, 2, 3, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 1, 2, 2, 2, 1, 1, 0, 2, 0, 2, 1, 8, 1, 0, 0, 1, 0, 0, 0, 1, 3, 0, 2, 0, 2, 3, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 6, 1, 1, 2, 3, 2, 0, 0, 1, 0, 1, 0, 0, 1, 1, 2, 1, 0, 0, 1, 2, 0, 1, 5, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1, 2, 7, 2, 0, 1, 1, 2, 2, 0, 0, 1, 3, 0, 2, 0, 2, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 1, 0, 1, 1, 2, 0, 0, 0, 3, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 3, 1, 0, 2, 0, 1, 0, 0, 1, 0, 4, 1, 1, 1, 1, 0, 0, 0, 3, 1, 1, 1, 2, 0, 1, 2, 1, 0, 3, 0, 0, 1, 0, 2, 1, 2, 1, 0, 0, 2, 1, 5, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 4, 2, 1, 7, 0, 0, 3, 3, 1, 1, 2, 2, 2, 0, 0, 1, 0, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 4, 0, 0, 1, 1, 1, 1, 1, 0, 0, 2, 2, 3, 1, 1, 3, 4, 3, 1, 0, 0, 0, 6, 2, 0, 0, 0, 1, 3], 'train_acc': 0.3526394896193772, 'test_acc': 0.36083044982698964, 'X': 'X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 8.9350, Test loss: 15.3093
In this epoch 100/5000, Training loss: 1.1825, Test loss: 14.4043
In this epoch 150/5000, Training loss: 0.4700, Test loss: 14.0664
In this epoch 200/5000, Training loss: 0.2234, Test loss: 14.0228
In this epoch 250/5000, Training loss: 0.1126, Test loss: 14.0053
In this epoch 300/5000, Training loss: 0.0588, Test loss: 13.9985
In this epoch 350/5000, Training loss: 0.0315, Test loss: 13.9982
In this epoch 400/5000, Training loss: 0.0171, Test loss: 14.0024
In this epoch 450/5000, Training loss: 0.0094, Test loss: 14.0094
In this epoch 500/5000, Training loss: 0.0052, Test loss: 14.0175
In this epoch 550/5000, Training loss: 0.0029, Test loss: 14.0257
In this epoch 600/5000, Training loss: 0.0016, Test loss: 14.0333
In this epoch 650/5000, Training loss: 0.0009, Test loss: 14.0398
In this epoch 700/5000, Training loss: 0.0005, Test loss: 14.0452
In this epoch 750/5000, Training loss: 0.0003, Test loss: 14.0496
In this epoch 800/5000, Training loss: 0.0002, Test loss: 14.0529
In this epoch 850/5000, Training loss: 0.0001, Test loss: 14.0555
In this epoch 900/5000, Training loss: 0.0001, Test loss: 14.0574
In this epoch 950/5000, Training loss: 0.0000, Test loss: 14.0588
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 14.0599
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 14.0606
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 14.0612
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 14.0616
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 14.0619
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 14.0621
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 14.0622
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 14.0623
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 14.0624
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 14.0625
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 14.0626
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 14.0602
In this epoch 1600/5000, Training loss: 7.3999, Test loss: 28.6157
In this epoch 1650/5000, Training loss: 0.0001, Test loss: 14.1907
In this epoch 1700/5000, Training loss: 0.0005, Test loss: 14.0589
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 14.0629
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 14.0627
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 14.0627
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 14.0627
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 14.0618
In this epoch 2000/5000, Training loss: 2.0255, Test loss: 35.2283
In this epoch 2050/5000, Training loss: 0.0981, Test loss: 14.0884
In this epoch 2100/5000, Training loss: 0.0005, Test loss: 14.0744
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 14.0633
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 14.0627
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 14.0629
In this epoch 2400/5000, Training loss: 59.8058, Test loss: 87.9504
In this epoch 2450/5000, Training loss: 0.2897, Test loss: 14.6842
In this epoch 2500/5000, Training loss: 0.0024, Test loss: 14.0481
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 14.0614
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 14.0627
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 2800/5000, Training loss: 0.7146, Test loss: 55.5701
In this epoch 2850/5000, Training loss: 0.0325, Test loss: 14.4026
In this epoch 2900/5000, Training loss: 0.0019, Test loss: 14.0882
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 14.0614
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 14.0627
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 14.0627
In this epoch 3250/5000, Training loss: 115.6455, Test loss: 128.9015
In this epoch 3300/5000, Training loss: 0.0358, Test loss: 14.7175
In this epoch 3350/5000, Training loss: 0.0005, Test loss: 14.0460
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 14.0621
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 14.0626
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 14.0631
In this epoch 3650/5000, Training loss: 17.2841, Test loss: 14.4890
In this epoch 3700/5000, Training loss: 0.1448, Test loss: 14.5439
In this epoch 3750/5000, Training loss: 0.0010, Test loss: 14.0663
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 14.0632
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 14.0627
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 4000/5000, Training loss: 0.0120, Test loss: 14.0283
In this epoch 4050/5000, Training loss: 6.8678, Test loss: 18.2795
In this epoch 4100/5000, Training loss: 0.0271, Test loss: 14.0439
In this epoch 4150/5000, Training loss: 0.0001, Test loss: 14.0615
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 14.0632
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 4600/5000, Training loss: 51.0336, Test loss: 45.5931
In this epoch 4650/5000, Training loss: 0.2546, Test loss: 14.0897
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 14.0513
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 14.0638
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 14.0629
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 14.0628
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 14.0628
train_acc 1.0
test_acc 0.17058823529411765
{'y_pred': [7.273593902587891, 11.940227508544922, 6.212973117828369, 15.72420883178711, 7.848358631134033, 4.700690746307373, 10.179121971130371, 24.60417366027832, 11.335358619689941, 8.803961753845215, 14.631013870239258, 6.744757175445557, 14.406214714050293, 15.709563255310059, 38.63841247558594, 5.99273157119751, 19.855693817138672, 75.11238861083984, 8.021303176879883, 5.6210222244262695, 10.388714790344238, 6.163095951080322, 13.172483444213867, 7.431823253631592, 3.874786376953125, 3.7313284873962402, 20.84171485900879, 3.9764931201934814, 19.18089485168457, 5.481553554534912, 23.309951782226562, 9.362342834472656, 7.5806708335876465, 23.9058780670166, 15.589303016662598, 6.574826717376709, 15.891674995422363, 27.71808624267578, 17.82671356201172, 10.476305961608887, 27.778146743774414, 29.549997329711914, 6.746037006378174, 7.439342021942139, 4.349207878112793, 24.50779151916504, 33.485557556152344, 13.402493476867676, 51.136627197265625, 11.949675559997559, 37.592262268066406, 8.011938095092773, 4.6111063957214355, 10.717662811279297, 9.546348571777344, 13.699955940246582, 6.500772953033447, 21.56744384765625, 8.928199768066406, 8.59546947479248, 25.09925651550293, 15.105856895446777, 39.66104507446289, 15.482857704162598, 15.813007354736328, 12.857276916503906, 14.434513092041016, 6.774697303771973, 15.621838569641113, 9.32407283782959, 14.675780296325684, 7.720992565155029, 31.61562728881836, 7.6066975593566895, 7.292084693908691, 3.677159070968628, 8.029717445373535, 7.391600131988525, 21.811580657958984, 16.589645385742188, 18.476112365722656, 15.891892433166504, 13.354766845703125, 10.545217514038086, 19.233901977539062, 19.596282958984375, 20.892017364501953, 10.2620849609375, 14.09371280670166, 25.123920440673828, 7.216830730438232, 4.727353096008301, 4.599410533905029, 5.003459453582764, 25.957141876220703, 15.432788848876953, 2.6317837238311768, 6.001029968261719, 1.9617745876312256, 9.82542896270752, 10.870888710021973, 14.310675621032715, 6.759876728057861, 13.079169273376465, 23.814998626708984, 13.710370063781738, 20.393638610839844, 4.9621806144714355, 16.302757263183594, 11.090842247009277, 6.2830705642700195, 6.050485610961914, 19.35015869140625, 21.582252502441406, 7.28900671005249, 7.430430889129639, 7.103930950164795, 8.750473022460938, 36.08353805541992, 9.796479225158691, 11.008342742919922, 12.821825981140137, 16.655147552490234, 27.267242431640625, 6.5525641441345215, 13.414145469665527, 19.005298614501953, 19.107646942138672, 6.638759136199951, 8.931793212890625, 20.33249855041504, 4.309894561767578, 12.238631248474121, 8.213007926940918, 15.503713607788086, 1.343328595161438, 17.5839786529541, 31.05054473876953, 4.057175159454346, 7.734416961669922, 12.870738983154297, 20.31396484375, 8.70148754119873, 17.592369079589844, 32.18709182739258, 10.563175201416016, 19.928300857543945, 23.634214401245117, 10.570685386657715, 12.330255508422852, 5.3535614013671875, 4.845715045928955, 5.354524612426758, 11.714594841003418, 1.9669785499572754, 8.126091957092285, 24.47612190246582, 10.0782470703125, 1.4914485216140747, 18.598073959350586, 2.9044485092163086, 6.126288414001465, 37.76951217651367, 2.8019859790802, 32.66567611694336, 33.9808349609375, 1.8931916952133179, 17.596860885620117, 18.258045196533203, 8.118769645690918, 5.527856349945068, 5.48970365524292, 6.327836990356445, 5.445831775665283, 14.203008651733398, 9.497305870056152, 7.156215190887451, 3.151496648788452, 9.229300498962402, 17.845369338989258, 11.307029724121094, 3.8875439167022705, 18.494834899902344, 6.355513095855713, 17.01185417175293, 12.675930976867676, 11.16472053527832, 21.907081604003906, 13.924142837524414, 24.549224853515625, 8.000202178955078, 15.209861755371094, 9.714713096618652, 4.682966232299805, 17.163005828857422, 13.110176086425781, 17.862577438354492, 5.109309196472168, 5.3935675621032715, 10.255631446838379, 14.883557319641113, 18.410762786865234, 26.933170318603516, 6.341640949249268, 12.112637519836426, 25.956695556640625, 17.68109893798828, 14.569985389709473, 10.8038911819458, 6.078614234924316, 9.699169158935547, 7.53843879699707, 10.911385536193848, 20.04239845275879, 4.328897476196289, 25.317546844482422, 4.523200511932373, 11.209080696105957, 25.08218765258789, 8.659555435180664, 22.68166732788086, 16.915315628051758, 43.400638580322266, 13.374584197998047, 7.565429210662842, 3.7498843669891357, 6.3507914543151855, 11.642212867736816, 15.460857391357422, 16.190465927124023, 7.68480920791626, 10.147507667541504, 13.534724235534668, 12.005632400512695, 10.071402549743652, 22.764495849609375, 21.818492889404297, 17.761741638183594, 9.6044921875, 9.444025993347168, 18.839462280273438, 6.56372594833374, 29.67087173461914, 19.011642456054688, 9.187256813049316, 12.537186622619629, 13.846759796142578, 13.357537269592285, 6.108661651611328, 6.069187641143799, 3.2666049003601074, 6.161144256591797, 16.232757568359375, 15.337817192077637, 6.413255214691162, 13.697953224182129, 15.059547424316406, 24.182039260864258, 21.44483757019043, 10.03589916229248, 15.626763343811035, 9.699947357177734, 15.991311073303223, 6.59135103225708, 4.127079963684082, 23.193777084350586, 31.21765899658203, 4.885796070098877, 35.851253509521484, 7.063535213470459, 15.60246467590332, 3.5042433738708496, 25.531572341918945, 25.789806365966797, 24.86114501953125, 7.010021209716797, 6.36126184463501, 16.2954044342041, 19.96285057067871, 10.948151588439941, 41.06940841674805, 7.425117015838623, 19.505338668823242, 28.66244125366211, 20.337324142456055, 35.41771697998047, 3.9011001586914062, 3.916806697845459, 17.430191040039062, 5.879757404327393, 1.5141448974609375, 16.291948318481445, 22.833721160888672, 8.858602523803711, 24.90882110595703, 13.73459529876709, 1.1983327865600586, 7.160740852355957, 29.135841369628906, 15.954256057739258, 12.526956558227539, 10.706010818481445, 20.234773635864258, 12.985913276672363, 24.462448120117188, 5.269252300262451, 3.3835866451263428, 17.7529296875, 5.128728866577148, 16.846118927001953, 16.2818603515625, 12.009428977966309, 35.40606689453125, 2.7347090244293213, 4.656139850616455, 11.055556297302246, 4.8310370445251465, 12.926688194274902, 52.232704162597656, 15.482903480529785, 15.781148910522461, 8.297030448913574, 18.616947174072266, 38.97707748413086, 12.160211563110352, 20.402982711791992, 18.909381866455078, 6.762856960296631, 22.694211959838867, 6.6365437507629395, 6.902463436126709, 2.4659645557403564, 9.693528175354004, 10.565579414367676, 9.101407051086426, 5.545572280883789, 4.084384918212891, 4.7626471519470215, 7.300380229949951, 15.221570014953613], 'y_pred_int': [7, 12, 6, 16, 8, 5, 10, 25, 11, 9, 15, 7, 14, 16, 39, 6, 20, 75, 8, 6, 10, 6, 13, 7, 4, 4, 21, 4, 19, 5, 23, 9, 8, 24, 16, 7, 16, 28, 18, 10, 28, 30, 7, 7, 4, 25, 33, 13, 51, 12, 38, 8, 5, 11, 10, 14, 7, 22, 9, 9, 25, 15, 40, 15, 16, 13, 14, 7, 16, 9, 15, 8, 32, 8, 7, 4, 8, 7, 22, 17, 18, 16, 13, 11, 19, 20, 21, 10, 14, 25, 7, 5, 5, 5, 26, 15, 3, 6, 2, 10, 11, 14, 7, 13, 24, 14, 20, 5, 16, 11, 6, 6, 19, 22, 7, 7, 7, 9, 36, 10, 11, 13, 17, 27, 7, 13, 19, 19, 7, 9, 20, 4, 12, 8, 16, 1, 18, 31, 4, 8, 13, 20, 9, 18, 32, 11, 20, 24, 11, 12, 5, 5, 5, 12, 2, 8, 24, 10, 1, 19, 3, 6, 38, 3, 33, 34, 2, 18, 18, 8, 6, 5, 6, 5, 14, 9, 7, 3, 9, 18, 11, 4, 18, 6, 17, 13, 11, 22, 14, 25, 8, 15, 10, 5, 17, 13, 18, 5, 5, 10, 15, 18, 27, 6, 12, 26, 18, 15, 11, 6, 10, 8, 11, 20, 4, 25, 5, 11, 25, 9, 23, 17, 43, 13, 8, 4, 6, 12, 15, 16, 8, 10, 14, 12, 10, 23, 22, 18, 10, 9, 19, 7, 30, 19, 9, 13, 14, 13, 6, 6, 3, 6, 16, 15, 6, 14, 15, 24, 21, 10, 16, 10, 16, 7, 4, 23, 31, 5, 36, 7, 16, 4, 26, 26, 25, 7, 6, 16, 20, 11, 41, 7, 20, 29, 20, 35, 4, 4, 17, 6, 2, 16, 23, 9, 25, 14, 1, 7, 29, 16, 13, 11, 20, 13, 24, 5, 3, 18, 5, 17, 16, 12, 35, 3, 5, 11, 5, 13, 52, 15, 16, 8, 19, 39, 12, 20, 19, 7, 23, 7, 7, 2, 10, 11, 9, 6, 4, 5, 7, 15], 'y_true': [8, 17, 10, 14, 9, 4, 9, 14, 16, 8, 15, 6, 19, 23, 40, 5, 26, 100, 6, 7, 10, 4, 10, 4, 4, 3, 17, 7, 14, 5, 21, 10, 6, 19, 10, 5, 19, 30, 16, 14, 21, 29, 9, 7, 4, 29, 39, 13, 40, 13, 39, 10, 5, 9, 9, 16, 7, 25, 7, 10, 19, 13, 36, 11, 16, 16, 13, 8, 13, 8, 13, 1, 28, 7, 11, 6, 9, 8, 25, 20, 15, 14, 13, 11, 20, 19, 14, 11, 18, 25, 8, 5, 4, 3, 20, 13, 2, 4, 2, 9, 13, 12, 6, 6, 29, 12, 19, 6, 15, 12, 6, 5, 19, 35, 9, 9, 9, 10, 42, 6, 13, 12, 13, 25, 8, 14, 15, 28, 6, 6, 23, 4, 13, 11, 14, 3, 19, 31, 5, 4, 11, 23, 7, 16, 37, 10, 23, 20, 14, 10, 8, 4, 6, 11, 2, 8, 26, 14, 2, 28, 7, 5, 37, 4, 34, 31, 3, 21, 31, 6, 6, 5, 6, 6, 16, 10, 8, 4, 11, 19, 16, 3, 17, 7, 22, 11, 14, 22, 14, 23, 10, 11, 10, 6, 17, 14, 16, 4, 6, 10, 13, 14, 29, 8, 24, 21, 19, 14, 13, 7, 13, 7, 7, 15, 4, 30, 4, 16, 33, 8, 19, 14, 42, 15, 6, 3, 5, 10, 13, 20, 6, 11, 15, 16, 13, 19, 32, 18, 11, 8, 21, 6, 24, 21, 10, 14, 12, 17, 3, 5, 5, 6, 16, 17, 8, 10, 17, 22, 22, 12, 11, 6, 20, 8, 3, 22, 25, 4, 38, 11, 19, 3, 26, 27, 16, 8, 5, 16, 23, 13, 49, 6, 17, 28, 18, 39, 3, 6, 18, 6, 3, 14, 15, 4, 19, 15, 2, 6, 23, 11, 12, 11, 15, 12, 35, 6, 3, 15, 6, 15, 9, 19, 34, 3, 4, 9, 6, 16, 47, 12, 19, 7, 16, 60, 8, 13, 15, 7, 23, 6, 8, 3, 9, 9, 12, 5, 5, 4, 11, 14], 'train_acc': 1.0, 'test_acc': 0.17058823529411765, 'X': 'X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 793.2181, Test loss: 1023.3595
In this epoch 100/5000, Training loss: 144.1643, Test loss: 236.5930
In this epoch 150/5000, Training loss: 81.2703, Test loss: 185.9186
In this epoch 200/5000, Training loss: 54.0564, Test loss: 164.5514
In this epoch 250/5000, Training loss: 38.9407, Test loss: 154.5275
In this epoch 300/5000, Training loss: 29.1646, Test loss: 148.9475
In this epoch 350/5000, Training loss: 22.3576, Test loss: 145.3980
In this epoch 400/5000, Training loss: 17.4274, Test loss: 142.9100
In this epoch 450/5000, Training loss: 13.7650, Test loss: 141.0493
In this epoch 500/5000, Training loss: 10.9913, Test loss: 139.5964
In this epoch 550/5000, Training loss: 8.8563, Test loss: 138.4284
In this epoch 600/5000, Training loss: 7.1895, Test loss: 137.4693
In this epoch 650/5000, Training loss: 5.8720, Test loss: 136.6686
In this epoch 700/5000, Training loss: 4.8195, Test loss: 135.9916
In this epoch 750/5000, Training loss: 3.9710, Test loss: 135.4125
In this epoch 800/5000, Training loss: 3.2820, Test loss: 134.9130
In this epoch 850/5000, Training loss: 2.7189, Test loss: 134.4785
In this epoch 900/5000, Training loss: 2.2565, Test loss: 134.0989
In this epoch 950/5000, Training loss: 1.8754, Test loss: 133.7654
In this epoch 1000/5000, Training loss: 1.5601, Test loss: 133.4711
In this epoch 1050/5000, Training loss: 1.2988, Test loss: 133.2107
In this epoch 1100/5000, Training loss: 1.0817, Test loss: 132.9801
In this epoch 1150/5000, Training loss: 0.9012, Test loss: 132.7756
In this epoch 1200/5000, Training loss: 0.7509, Test loss: 132.5942
In this epoch 1250/5000, Training loss: 0.6256, Test loss: 132.4332
In this epoch 1300/5000, Training loss: 0.5212, Test loss: 132.2908
In this epoch 1350/5000, Training loss: 0.4342, Test loss: 132.1645
In this epoch 1400/5000, Training loss: 0.3616, Test loss: 132.0529
In this epoch 1450/5000, Training loss: 0.3010, Test loss: 131.9549
In this epoch 1500/5000, Training loss: 0.2506, Test loss: 131.8687
In this epoch 1550/5000, Training loss: 0.2085, Test loss: 131.7929
In this epoch 1600/5000, Training loss: 0.1734, Test loss: 131.7268
In this epoch 1650/5000, Training loss: 0.1442, Test loss: 131.6689
In this epoch 1700/5000, Training loss: 0.1198, Test loss: 131.6183
In this epoch 1750/5000, Training loss: 0.0996, Test loss: 131.5745
In this epoch 1800/5000, Training loss: 0.0827, Test loss: 131.5364
In this epoch 1850/5000, Training loss: 0.0687, Test loss: 131.5031
In this epoch 1900/5000, Training loss: 0.0570, Test loss: 131.4743
In this epoch 1950/5000, Training loss: 0.0473, Test loss: 131.4493
In this epoch 2000/5000, Training loss: 0.0393, Test loss: 131.4274
In this epoch 2050/5000, Training loss: 0.0326, Test loss: 131.4082
In this epoch 2100/5000, Training loss: 0.0270, Test loss: 131.3918
In this epoch 2150/5000, Training loss: 0.0224, Test loss: 131.3772
In this epoch 2200/5000, Training loss: 0.0185, Test loss: 131.3645
In this epoch 2250/5000, Training loss: 0.0154, Test loss: 131.3532
In this epoch 2300/5000, Training loss: 0.0127, Test loss: 131.3432
In this epoch 2350/5000, Training loss: 0.0105, Test loss: 131.3346
In this epoch 2400/5000, Training loss: 0.0087, Test loss: 131.3269
In this epoch 2450/5000, Training loss: 0.0072, Test loss: 131.3200
In this epoch 2500/5000, Training loss: 0.0060, Test loss: 131.3140
In this epoch 2550/5000, Training loss: 0.0050, Test loss: 131.3086
In this epoch 2600/5000, Training loss: 0.0041, Test loss: 131.3036
In this epoch 2650/5000, Training loss: 0.0034, Test loss: 131.2994
In this epoch 2700/5000, Training loss: 0.0028, Test loss: 131.2958
In this epoch 2750/5000, Training loss: 0.0023, Test loss: 131.2923
In this epoch 2800/5000, Training loss: 0.0019, Test loss: 131.2895
In this epoch 2850/5000, Training loss: 0.0016, Test loss: 131.2867
In this epoch 2900/5000, Training loss: 0.0014, Test loss: 131.2847
In this epoch 2950/5000, Training loss: 0.0011, Test loss: 131.2825
In this epoch 3000/5000, Training loss: 0.0009, Test loss: 131.2809
In this epoch 3050/5000, Training loss: 0.0008, Test loss: 131.2793
In this epoch 3100/5000, Training loss: 0.0007, Test loss: 131.2783
In this epoch 3150/5000, Training loss: 0.0006, Test loss: 131.2772
In this epoch 3200/5000, Training loss: 0.0005, Test loss: 131.2763
In this epoch 3250/5000, Training loss: 0.0004, Test loss: 131.2756
In this epoch 3300/5000, Training loss: 0.0004, Test loss: 131.2751
In this epoch 3350/5000, Training loss: 0.0003, Test loss: 131.2746
In this epoch 3400/5000, Training loss: 0.0003, Test loss: 131.2742
In this epoch 3450/5000, Training loss: 0.0003, Test loss: 131.2737
In this epoch 3500/5000, Training loss: 0.0002, Test loss: 131.2736
In this epoch 3550/5000, Training loss: 0.0002, Test loss: 131.2734
In this epoch 3600/5000, Training loss: 0.0002, Test loss: 131.2733
In this epoch 3650/5000, Training loss: 0.0002, Test loss: 131.2734
In this epoch 3700/5000, Training loss: 0.0002, Test loss: 131.2732
In this epoch 3750/5000, Training loss: 0.0002, Test loss: 131.2731
In this epoch 3800/5000, Training loss: 0.0002, Test loss: 131.2731
In this epoch 3850/5000, Training loss: 0.0001, Test loss: 131.2731
In this epoch 3900/5000, Training loss: 0.0001, Test loss: 131.2731
In this epoch 3950/5000, Training loss: 0.0001, Test loss: 131.2732
In this epoch 4000/5000, Training loss: 0.0001, Test loss: 131.2732
In this epoch 4050/5000, Training loss: 0.0001, Test loss: 131.2732
In this epoch 4100/5000, Training loss: 0.0001, Test loss: 131.2733
In this epoch 4150/5000, Training loss: 0.0001, Test loss: 131.2733
In this epoch 4200/5000, Training loss: 0.0001, Test loss: 131.2732
In this epoch 4250/5000, Training loss: 0.0001, Test loss: 131.2732
In this epoch 4300/5000, Training loss: 0.0001, Test loss: 131.2733
In this epoch 4350/5000, Training loss: 0.0001, Test loss: 131.2732
In this epoch 4400/5000, Training loss: 0.0001, Test loss: 131.2732
In this epoch 4450/5000, Training loss: 0.0001, Test loss: 131.2733
In this epoch 4500/5000, Training loss: 0.0001, Test loss: 131.2733
In this epoch 4550/5000, Training loss: 0.0001, Test loss: 131.2734
In this epoch 4600/5000, Training loss: 0.0001, Test loss: 131.2732
In this epoch 4650/5000, Training loss: 0.0001, Test loss: 131.2732
In this epoch 4700/5000, Training loss: 0.0001, Test loss: 131.2733
In this epoch 4750/5000, Training loss: 0.0001, Test loss: 131.2731
In this epoch 4800/5000, Training loss: 0.0001, Test loss: 131.2731
In this epoch 4850/5000, Training loss: 0.0001, Test loss: 131.2730
In this epoch 4900/5000, Training loss: 0.0001, Test loss: 131.2729
In this epoch 4950/5000, Training loss: 0.0001, Test loss: 131.2729
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 131.2727
train_acc 1.0
test_acc 0.058823529411764705
{'y_pred': [418.32757568359375, 801.1666870117188, 304.7928161621094, 609.9642944335938, 208.11146545410156, 72.53105163574219, 166.24534606933594, 102.84577941894531, 521.8863525390625, 393.0705871582031, 633.8897094726562, 194.13026428222656, 817.2374877929688, 206.6698760986328, 305.5477294921875, 39.3239860534668, 198.3071746826172, 417.8205871582031, 607.3118896484375, 455.86981201171875, 340.4787292480469, 491.01177978515625, 664.6675415039062, 208.38107299804688, 105.67537689208984, 236.3798828125, 435.5079650878906, 158.94699096679688, 753.083740234375, 359.1365661621094, 566.46044921875, 80.55632019042969, 135.969970703125, 673.548095703125, 552.4811401367188, 198.62168884277344, 176.4364013671875, 229.05982971191406, 887.701171875, 544.7567138671875, 361.3740234375, 678.8916015625, 195.8484649658203, 265.17926025390625, 170.85394287109375, 102.04080200195312, 62.621238708496094, 690.8356323242188, 444.672119140625, 250.37734985351562, 59.129127502441406, 175.65159606933594, 449.89044189453125, 515.8318481445312, 394.517822265625, 602.712646484375, 233.73568725585938, 451.2153625488281, 353.48297119140625, 237.22708129882812, 150.02684020996094, 149.58755493164062, 135.3853759765625, 498.2937927246094, 138.73129272460938, 440.03076171875, 376.65234375, 258.4065246582031, 296.2264404296875, 150.962646484375, 207.93421936035156, 427.07952880859375, 316.510498046875, 562.9667358398438, 520.522705078125, 175.01905822753906, 697.9450073242188, 1035.288330078125, 80.59151458740234, 360.040283203125, 336.47882080078125, 194.5037384033203, 626.3414916992188, 177.9963836669922, 242.26173400878906, 68.39999389648438, 119.29570007324219, 357.88031005859375, 687.0339965820312, 233.0048370361328, 475.0818176269531, 351.25537109375, 136.6708984375, 335.4178161621094, 75.46296691894531, 206.4723663330078, 617.7825317382812, 150.82456970214844, 122.61058044433594, 413.555419921875, 294.4419860839844, 346.552490234375, 162.3522186279297, 540.0435791015625, 250.73411560058594, 154.61105346679688, 203.0523681640625, 390.4727783203125, 158.64508056640625, 329.7605285644531, 230.4062957763672, 215.63490295410156, 147.0367889404297, 676.4566040039062, 413.0706787109375, 240.48959350585938, 805.5087890625, 139.12953186035156, 788.1837768554688, 164.15728759765625, 116.65553283691406, 526.7816772460938, 256.28228759765625, 585.4540405273438, 684.7066040039062, 219.16038513183594, 527.5252685546875, 120.26573181152344, 249.96434020996094, 515.1455688476562, 587.4291381835938, 384.4913330078125, 290.1114501953125, 365.62896728515625, 797.9951171875, 495.2271423339844, 594.9074096679688, 417.862548828125, 634.1438598632812, 128.94871520996094, 495.2510986328125, 436.5167236328125, 189.6980438232422, 620.9973754882812, 143.86708068847656, 202.58697509765625, 134.77993774414062, 403.02239990234375, 77.96923828125, 162.02130126953125, 101.13761138916016, 95.83602142333984, 251.9685821533203, 271.0357666015625, 598.3602905273438, 480.9609680175781, 337.42901611328125, 892.4736938476562, 269.2984619140625, 461.50146484375, 117.4898452758789, 185.71417236328125, 733.9953002929688, 1125.6220703125, 203.15640258789062, 337.3284912109375, 844.177490234375, 208.24514770507812, 232.4495391845703, 507.10906982421875, 293.19580078125, 680.1752319335938, 310.21771240234375, 339.1682434082031, 279.7026062011719, 568.6494140625, 338.064208984375, 680.9615478515625, 619.6110229492188, 491.359375, 434.7801208496094, 126.54122924804688, 357.4865417480469, 1066.014892578125, 403.677978515625, 597.097412109375, 176.8411102294922, 119.89425659179688, 769.0248413085938, 604.7774047851562, 290.15679931640625, 441.4962158203125, 598.4615478515625, 538.1044921875, 280.95013427734375, 543.6331176757812, 344.8000183105469, 180.13973999023438, 396.6976318359375, 347.6406555175781, 457.41387939453125, 160.99842834472656, 660.7916870117188, 204.3764190673828, 751.2553100585938, 210.34378051757812, 388.6558837890625, 296.73431396484375, 442.083740234375, 367.15435791015625, 403.51556396484375, 313.4170227050781, 380.71630859375, 764.0823364257812, 264.5179748535156, 383.448486328125, 242.75547790527344, 292.29986572265625, 187.72018432617188, 485.1148986816406, 360.86737060546875, 297.93267822265625, 180.4781036376953, 230.6721649169922, 79.14128112792969, 189.9079132080078, 831.6560668945312, 575.4613037109375, 311.41009521484375, 423.95751953125, 426.4649963378906, 246.51287841796875, 114.54522705078125, 159.98509216308594, 1002.0011596679688, 921.1769409179688, 228.2864227294922, 541.1018676757812, 453.1087646484375, 718.19287109375, 170.46340942382812, 362.8014221191406, 414.13140869140625, 304.5888366699219, 386.596435546875, 644.2081909179688, 504.7013854980469, 141.51846313476562, 404.7628173828125, 494.28692626953125, 310.3271789550781, 787.105224609375, 88.8070068359375, 93.10177612304688, 157.234130859375, 243.28689575195312, 84.11935424804688, 584.6737670898438, 383.56109619140625, 506.0625915527344, 96.00413513183594, 655.3453979492188, 73.91017150878906, 438.0140686035156, 902.5716552734375, 656.7220458984375, 235.8867645263672, 472.2091064453125, 247.20375061035156, 176.23756408691406, 191.1929931640625, 263.98663330078125, 169.38999938964844, 797.2915649414062, 604.82470703125, 430.15673828125, 204.34092712402344, 373.0546875, 228.57701110839844, 517.8589477539062, 538.1603393554688, 170.13169860839844, 257.093994140625, 159.1082305908203, 843.48095703125, 481.6026611328125, 245.87245178222656, 390.8043212890625, 373.2554626464844, 134.5387420654297, 322.63885498046875, 509.68682861328125, 157.20912170410156, 243.6995086669922, 362.0845642089844, 594.6551513671875, 561.0523071289062, 316.9254455566406, 437.2986145019531, 783.4081420898438, 460.9841003417969, 637.4414672851562, 185.8550262451172, 468.27130126953125, 757.5506591796875, 116.59962463378906, 340.87164306640625, 878.6237182617188, 61.845001220703125, 104.6303482055664, 400.4376525878906, 210.47091674804688, 213.6079864501953, 310.6261291503906, 164.2102813720703, 156.80418395996094, 86.0862808227539, 310.69769287109375, 598.9227294921875, 162.9176788330078, 402.2359619140625, 358.79376220703125, 773.901123046875, 84.92469787597656, 191.65992736816406, 346.14288330078125, 647.311767578125, 442.9281921386719, 255.1729278564453, 594.124755859375, 459.4130859375, 486.6195373535156, 820.0421752929688, 432.15960693359375, 165.71224975585938, 641.3226318359375, 342.7115783691406, 303.6771545410156, 227.1429443359375, 896.8964233398438], 'y_pred_int': [418, 801, 305, 610, 208, 73, 166, 103, 522, 393, 634, 194, 817, 207, 306, 39, 198, 418, 607, 456, 340, 491, 665, 208, 106, 236, 436, 159, 753, 359, 566, 81, 136, 674, 552, 199, 176, 229, 888, 545, 361, 679, 196, 265, 171, 102, 63, 691, 445, 250, 59, 176, 450, 516, 395, 603, 234, 451, 353, 237, 150, 150, 135, 498, 139, 440, 377, 258, 296, 151, 208, 427, 317, 563, 521, 175, 698, 1035, 81, 360, 336, 195, 626, 178, 242, 68, 119, 358, 687, 233, 475, 351, 137, 335, 75, 206, 618, 151, 123, 414, 294, 347, 162, 540, 251, 155, 203, 390, 159, 330, 230, 216, 147, 676, 413, 240, 806, 139, 788, 164, 117, 527, 256, 585, 685, 219, 528, 120, 250, 515, 587, 384, 290, 366, 798, 495, 595, 418, 634, 129, 495, 437, 190, 621, 144, 203, 135, 403, 78, 162, 101, 96, 252, 271, 598, 481, 337, 892, 269, 462, 117, 186, 734, 1126, 203, 337, 844, 208, 232, 507, 293, 680, 310, 339, 280, 569, 338, 681, 620, 491, 435, 127, 357, 1066, 404, 597, 177, 120, 769, 605, 290, 441, 598, 538, 281, 544, 345, 180, 397, 348, 457, 161, 661, 204, 751, 210, 389, 297, 442, 367, 404, 313, 381, 764, 265, 383, 243, 292, 188, 485, 361, 298, 180, 231, 79, 190, 832, 575, 311, 424, 426, 247, 115, 160, 1002, 921, 228, 541, 453, 718, 170, 363, 414, 305, 387, 644, 505, 142, 405, 494, 310, 787, 89, 93, 157, 243, 84, 585, 384, 506, 96, 655, 74, 438, 903, 657, 236, 472, 247, 176, 191, 264, 169, 797, 605, 430, 204, 373, 229, 518, 538, 170, 257, 159, 843, 482, 246, 391, 373, 135, 323, 510, 157, 244, 362, 595, 561, 317, 437, 783, 461, 637, 186, 468, 758, 117, 341, 879, 62, 105, 400, 210, 214, 311, 164, 157, 86, 311, 599, 163, 402, 359, 774, 85, 192, 346, 647, 443, 255, 594, 459, 487, 820, 432, 166, 641, 343, 304, 227, 897], 'y_true': [431, 801, 297, 614, 209, 74, 166, 102, 538, 391, 611, 193, 867, 214, 286, 41, 202, 423, 617, 474, 351, 487, 686, 217, 105, 240, 437, 167, 762, 367, 579, 81, 138, 659, 550, 199, 175, 220, 898, 541, 370, 686, 207, 270, 168, 103, 68, 680, 436, 247, 62, 182, 458, 516, 389, 601, 239, 445, 339, 235, 148, 151, 135, 515, 144, 440, 382, 263, 291, 149, 201, 422, 319, 544, 523, 180, 765, 1027, 78, 367, 337, 199, 621, 172, 243, 69, 121, 367, 681, 238, 476, 351, 135, 351, 80, 212, 612, 154, 118, 380, 300, 350, 154, 539, 256, 154, 197, 385, 160, 328, 230, 207, 148, 726, 415, 249, 833, 147, 783, 165, 123, 521, 258, 585, 679, 226, 521, 122, 237, 513, 588, 393, 286, 367, 796, 491, 586, 434, 621, 129, 490, 442, 193, 590, 146, 200, 141, 392, 82, 164, 99, 95, 258, 276, 598, 470, 327, 877, 272, 477, 117, 192, 743, 1099, 205, 334, 835, 209, 234, 500, 273, 665, 309, 334, 284, 572, 316, 697, 635, 488, 452, 131, 360, 1042, 421, 588, 173, 116, 773, 590, 288, 443, 582, 530, 286, 529, 346, 173, 387, 336, 455, 160, 660, 200, 738, 211, 402, 299, 430, 373, 401, 328, 379, 756, 248, 391, 256, 301, 192, 465, 358, 296, 184, 236, 78, 194, 823, 602, 318, 440, 436, 258, 110, 170, 1016, 912, 228, 552, 456, 698, 172, 352, 425, 312, 394, 659, 505, 143, 406, 507, 315, 799, 98, 96, 162, 257, 89, 606, 390, 507, 98, 663, 74, 432, 878, 634, 235, 494, 244, 186, 189, 255, 174, 802, 598, 443, 202, 361, 237, 506, 500, 173, 253, 169, 814, 502, 239, 389, 370, 125, 334, 523, 161, 238, 371, 603, 572, 314, 460, 768, 457, 629, 182, 480, 778, 118, 340, 899, 64, 108, 407, 221, 216, 317, 164, 161, 87, 292, 633, 158, 413, 361, 775, 90, 195, 327, 613, 452, 269, 608, 433, 478, 824, 439, 158, 644, 342, 296, 235, 903], 'train_acc': 1.0, 'test_acc': 0.058823529411764705, 'X': 'X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700', 'Y': 'Y_num_tokens_muc1700'}
