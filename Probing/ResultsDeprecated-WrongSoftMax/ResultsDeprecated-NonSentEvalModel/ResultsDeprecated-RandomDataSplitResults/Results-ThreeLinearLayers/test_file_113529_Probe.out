Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 0.2166, Test loss: 12.5350
In this epoch 100/5000, Training loss: 0.0033, Test loss: 11.2178
In this epoch 150/5000, Training loss: 0.0008, Test loss: 11.1756
In this epoch 200/5000, Training loss: 0.0007, Test loss: 11.1684
In this epoch 250/5000, Training loss: 0.0007, Test loss: 11.1692
In this epoch 300/5000, Training loss: 0.0007, Test loss: 11.1692
In this epoch 350/5000, Training loss: 0.0007, Test loss: 11.1692
In this epoch 400/5000, Training loss: 0.0007, Test loss: 11.1692
In this epoch 450/5000, Training loss: 0.0007, Test loss: 11.1692
In this epoch 500/5000, Training loss: 0.0007, Test loss: 11.1691
In this epoch 550/5000, Training loss: 0.0007, Test loss: 11.1691
In this epoch 600/5000, Training loss: 0.0007, Test loss: 11.1691
In this epoch 650/5000, Training loss: 0.0007, Test loss: 11.1691
In this epoch 700/5000, Training loss: 0.0007, Test loss: 11.1691
In this epoch 750/5000, Training loss: 0.0007, Test loss: 11.1691
In this epoch 800/5000, Training loss: 0.0007, Test loss: 11.1691
In this epoch 850/5000, Training loss: 0.0007, Test loss: 11.1691
In this epoch 900/5000, Training loss: 0.0007, Test loss: 11.1690
In this epoch 950/5000, Training loss: 987.8289, Test loss: 691.9893
In this epoch 1000/5000, Training loss: 0.8245, Test loss: 9.7939
In this epoch 1050/5000, Training loss: 0.0671, Test loss: 9.2199
In this epoch 1100/5000, Training loss: 0.0615, Test loss: 9.2986
In this epoch 1150/5000, Training loss: 0.0600, Test loss: 9.2988
In this epoch 1200/5000, Training loss: 0.0583, Test loss: 9.3012
In this epoch 1250/5000, Training loss: 0.0566, Test loss: 9.3035
In this epoch 1300/5000, Training loss: 0.0549, Test loss: 9.3055
In this epoch 1350/5000, Training loss: 0.0532, Test loss: 9.3078
In this epoch 1400/5000, Training loss: 0.0516, Test loss: 9.3107
In this epoch 1450/5000, Training loss: 0.0502, Test loss: 9.3136
In this epoch 1500/5000, Training loss: 0.0491, Test loss: 9.3168
In this epoch 1550/5000, Training loss: 0.0483, Test loss: 9.3200
In this epoch 1600/5000, Training loss: 0.0479, Test loss: 9.3225
In this epoch 1650/5000, Training loss: 0.0476, Test loss: 9.3240
In this epoch 1700/5000, Training loss: 0.0475, Test loss: 9.3249
In this epoch 1750/5000, Training loss: 0.0474, Test loss: 9.3251
In this epoch 1800/5000, Training loss: 0.0474, Test loss: 9.3250
In this epoch 1850/5000, Training loss: 0.0473, Test loss: 9.3250
In this epoch 1900/5000, Training loss: 0.0473, Test loss: 9.3248
In this epoch 1950/5000, Training loss: 0.0473, Test loss: 9.3246
In this epoch 2000/5000, Training loss: 0.0473, Test loss: 9.3245
In this epoch 2050/5000, Training loss: 0.0473, Test loss: 9.3243
In this epoch 2100/5000, Training loss: 0.0473, Test loss: 9.3241
In this epoch 2150/5000, Training loss: 0.0472, Test loss: 9.3239
In this epoch 2200/5000, Training loss: 0.0472, Test loss: 9.3237
In this epoch 2250/5000, Training loss: 0.0472, Test loss: 9.3235
In this epoch 2300/5000, Training loss: 0.0472, Test loss: 9.3232
In this epoch 2350/5000, Training loss: 0.0472, Test loss: 9.3230
In this epoch 2400/5000, Training loss: 0.0472, Test loss: 9.3228
In this epoch 2450/5000, Training loss: 0.0471, Test loss: 9.3226
In this epoch 2500/5000, Training loss: 0.0471, Test loss: 9.3227
In this epoch 2550/5000, Training loss: 0.4375, Test loss: 7.2058
In this epoch 2600/5000, Training loss: 0.0644, Test loss: 6.8002
In this epoch 2650/5000, Training loss: 0.0600, Test loss: 6.8038
In this epoch 2700/5000, Training loss: 0.0577, Test loss: 6.8017
In this epoch 2750/5000, Training loss: 0.0568, Test loss: 6.8001
In this epoch 2800/5000, Training loss: 0.0564, Test loss: 6.7994
In this epoch 2850/5000, Training loss: 0.0562, Test loss: 6.7990
In this epoch 2900/5000, Training loss: 0.0561, Test loss: 6.7989
In this epoch 2950/5000, Training loss: 0.0561, Test loss: 6.7988
In this epoch 3000/5000, Training loss: 0.0561, Test loss: 6.7986
In this epoch 3050/5000, Training loss: 0.0560, Test loss: 6.7984
In this epoch 3100/5000, Training loss: 0.0560, Test loss: 6.7981
In this epoch 3150/5000, Training loss: 0.0560, Test loss: 6.7978
In this epoch 3200/5000, Training loss: 0.0559, Test loss: 6.7974
In this epoch 3250/5000, Training loss: 0.0559, Test loss: 6.7971
In this epoch 3300/5000, Training loss: 0.0559, Test loss: 6.7967
In this epoch 3350/5000, Training loss: 0.0559, Test loss: 6.7963
In this epoch 3400/5000, Training loss: 0.0558, Test loss: 6.7960
In this epoch 3450/5000, Training loss: 0.0558, Test loss: 6.7956
In this epoch 3500/5000, Training loss: 0.0558, Test loss: 6.7952
In this epoch 3550/5000, Training loss: 0.0558, Test loss: 6.7948
In this epoch 3600/5000, Training loss: 0.0558, Test loss: 6.7944
In this epoch 3650/5000, Training loss: 0.0557, Test loss: 6.7940
In this epoch 3700/5000, Training loss: 0.0557, Test loss: 6.7937
In this epoch 3750/5000, Training loss: 0.0557, Test loss: 6.7933
In this epoch 3800/5000, Training loss: 0.0557, Test loss: 6.7929
In this epoch 3850/5000, Training loss: 0.0557, Test loss: 6.7925
In this epoch 3900/5000, Training loss: 0.0556, Test loss: 6.7922
In this epoch 3950/5000, Training loss: 0.0556, Test loss: 6.7918
In this epoch 4000/5000, Training loss: 0.0556, Test loss: 6.7914
In this epoch 4050/5000, Training loss: 0.0556, Test loss: 6.7911
In this epoch 4100/5000, Training loss: 0.0556, Test loss: 6.7907
In this epoch 4150/5000, Training loss: 16.4270, Test loss: 24.0616
In this epoch 4200/5000, Training loss: 0.0809, Test loss: 4.0149
In this epoch 4250/5000, Training loss: 0.0146, Test loss: 3.9052
In this epoch 4300/5000, Training loss: 0.0099, Test loss: 3.9004
In this epoch 4350/5000, Training loss: 0.0096, Test loss: 3.8994
In this epoch 4400/5000, Training loss: 0.0278, Test loss: 3.9015
In this epoch 4450/5000, Training loss: 0.0094, Test loss: 3.9270
In this epoch 4500/5000, Training loss: 0.0093, Test loss: 3.9264
In this epoch 4550/5000, Training loss: 0.0092, Test loss: 3.9271
In this epoch 4600/5000, Training loss: 0.0091, Test loss: 3.9279
In this epoch 4650/5000, Training loss: 0.0090, Test loss: 3.9286
In this epoch 4700/5000, Training loss: 0.0089, Test loss: 3.9293
In this epoch 4750/5000, Training loss: 0.0088, Test loss: 3.9300
In this epoch 4800/5000, Training loss: 0.0087, Test loss: 3.9307
In this epoch 4850/5000, Training loss: 0.0086, Test loss: 3.9313
In this epoch 4900/5000, Training loss: 0.0086, Test loss: 3.9319
In this epoch 4950/5000, Training loss: 0.0085, Test loss: 3.9326
In this epoch 5000/5000, Training loss: 0.0085, Test loss: 3.9333
train_acc 0.34478806228373704
test_acc 0.41377162629757785
{'y_pred': [0.1089819073677063, 0.6178110241889954, -1.3521349430084229, 1.0799314975738525, 0.5629788041114807, -0.0019731074571609497, 0.18601658940315247, 0.6718245148658752, 0.036430180072784424, 1.681603193283081, -0.19920748472213745, -1.0848305225372314, 4.028421401977539, 0.7322841286659241, 0.2685839831829071, 0.291215717792511, 0.563727855682373, 0.44001084566116333, 0.08474518358707428, 0.12521964311599731, 0.7927228212356567, 1.6001644134521484, 0.16330957412719727, 1.1148736476898193, 0.9651511311531067, 2.3833024501800537, 0.15151387453079224, 0.5965808033943176, 0.9986055493354797, -2.304311990737915, -0.19642004370689392, 0.9339008927345276, -0.07091730833053589, 0.14596304297447205, -0.5549470782279968, -0.4370366930961609, -0.8170625567436218, 0.9832685589790344, 0.45172080397605896, -0.33861225843429565, 0.3063805401325226, 0.6776590347290039, 1.2053306102752686, -0.2537176012992859, -0.5059759020805359, 0.2566741108894348, 1.7548730373382568, 0.07470303773880005, 0.36887437105178833, 0.6832351088523865, 0.5363710522651672, 1.4814341068267822, 0.07470303773880005, 1.0745184421539307, -0.3241693377494812, 0.11132222414016724, -0.22597426176071167, 3.6345632076263428, 1.2692484855651855, -0.13792437314987183, 1.2330024242401123, 0.3719804584980011, 0.5850508809089661, 1.2608683109283447, -0.5378267168998718, 0.7453923225402832, -0.9025804400444031, -7.560559272766113, -1.0822203159332275, 0.7629104256629944, 0.6037977337837219, 0.554949164390564, 0.6136159896850586, 1.124021053314209, -2.311441659927368, 1.477609395980835, 0.07470303773880005, 0.4175451397895813, 5.311542510986328, 1.172269582748413, -1.357454538345337, 0.3696945309638977, 0.37214070558547974, 0.14272278547286987, 1.0561435222625732, 0.8637373447418213, 0.3074471652507782, 0.3778223991394043, -0.10562437772750854, 0.7780897617340088, -0.2299901843070984, 0.8406501412391663, 0.6943097114562988, 0.37234407663345337, -0.6157132983207703, 0.9283373355865479, 2.9920480251312256, -0.06068417429924011, 0.7615733742713928, 1.0106056928634644, 1.1940667629241943, 1.0326504707336426, 0.5968870520591736, 3.6728131771087646, 0.9556868672370911, -0.02578943967819214, -0.07133692502975464, 1.3533966541290283, -0.48008131980895996, -0.321790874004364, 0.1821879744529724, 1.955432653427124, 0.46573716402053833, 1.2927100658416748, 0.8768831491470337, 0.07470303773880005, 0.09857838600873947, 0.8492230176925659, 2.3887617588043213, -0.513161838054657, 0.34027811884880066, 0.49253350496292114, 0.7590830326080322, 0.4497465193271637, 0.5530109405517578, 0.5339761972427368, 0.07470303773880005, 0.8719929456710815, 0.33291658759117126, 1.7159154415130615, 0.20766526460647583, 0.22925129532814026, -0.011232554912567139, 0.12370054423809052, 0.4615972638130188, 1.8307511806488037, 0.6196585893630981, -1.796478509902954, 0.9588472247123718, 0.07470303773880005, 0.8241406083106995, -0.8347646594047546, 1.4550855159759521, 0.3326795697212219, 0.64460289478302, 0.49570581316947937, 0.5178214311599731, 0.21088391542434692, 1.0207669734954834, -0.192487895488739, 0.7843256592750549, 1.0658047199249268, 7.481964111328125, 1.3674523830413818, 0.9114508628845215, 2.0373342037200928, 0.3217161297798157, 1.1812798976898193, -0.31633108854293823, -0.06034867465496063, 1.6270978450775146, 0.07904797792434692, 0.2672193646430969, 0.24001199007034302, -0.44830721616744995, 0.30928975343704224, 0.263902485370636, -2.6483118534088135, 0.9943773150444031, -1.430635690689087, 0.609995424747467, -0.16160887479782104, 0.252702534198761, 0.44579488039016724, 2.8520731925964355, -0.4788009524345398, 0.07470303773880005, 0.3696134686470032, 8.86947250366211, 1.198603630065918, 1.688506841659546, 0.2981031537055969, 0.4331780970096588, -0.5919877886772156, -0.3828107714653015, 0.5743565559387207, 0.3760107159614563, 8.555334091186523, 0.08522206544876099, 1.7765233516693115, 0.4210217595100403, 0.054374516010284424, -0.5599042773246765, -1.2734997272491455, -0.26322096586227417, 1.7252328395843506, 2.676774740219116, -3.658968210220337, 0.764373779296875, -2.054121255874634, 0.5223755836486816, 1.578568696975708, 0.6887306571006775, 0.7748315930366516, 1.1802401542663574, -0.09356039762496948, 0.5692336559295654, 0.7513290643692017, 3.641191244125366, -0.7056428790092468, 0.7911937236785889, 0.08420540392398834, 0.2708168029785156, 0.7553176879882812, 0.3302367925643921, 0.7589938044548035, -7.332025527954102, -0.6414015889167786, 1.5888941287994385, 0.7969854474067688, 0.9374462366104126, 1.0563640594482422, 0.17077946662902832, 0.6954978108406067, 0.5054676532745361, 0.6566265225410461, 0.6397749185562134, -0.2059881091117859, 0.8703692555427551, 0.9644518494606018, -0.03310126066207886, 1.1045358180999756, 0.30558639764785767, 0.8502905368804932, 0.7397440075874329, 0.7414053082466125, -2.1845953464508057, -1.1302416324615479, 0.2237260937690735, 0.502223789691925, 0.8813988566398621, 0.7477028369903564, -3.899270534515381, -4.828216552734375, 1.0458734035491943, 3.7789525985717773, 1.122556447982788, -7.320438861846924, 0.8390305042266846, 0.6878144145011902, 0.8830491900444031, 0.2864900827407837, 1.1170012950897217, 0.46853429079055786, 0.41730090975761414, 0.4240390956401825, 1.0476596355438232, 0.9215341210365295, -0.40715357661247253, 0.41907140612602234, 0.9280571937561035, 0.8938459157943726, 0.8618238568305969, 0.4425978660583496, 0.7313133478164673, 0.7514917850494385, -0.38693445920944214, 0.6216450929641724, 0.33637845516204834, 0.1784842610359192, 0.7610999345779419, -5.282488822937012, 0.05406075716018677, 0.24347668886184692, 0.04289132356643677, -0.7527830004692078, 0.5823675990104675, 0.27462148666381836, 0.33103659749031067, -0.3930971026420593, -2.3368723392486572, 0.9614904522895813, -3.6158430576324463, 0.3156755864620209, 0.4986065626144409, 1.0402215719223022, 1.4687554836273193, 0.857490062713623, 0.654578685760498, -0.6170637011528015, 0.7829044461250305, 0.35144010186195374, 0.07470303773880005, 0.3472312390804291, 1.843585729598999, 0.6894973516464233, 7.441509246826172, -0.6171819567680359, 4.087825775146484, 0.3884277939796448, -1.6556618213653564, 1.0072743892669678, 0.8163735866546631, 0.7320895791053772, 0.7243068814277649, 0.8207792043685913, 0.07470303773880005, 2.6251676082611084, 0.6056658029556274, 2.158681631088257, 1.000457763671875, -0.2144758105278015, -0.10205096006393433, 0.04981786012649536, -0.6346341967582703, 2.568366765975952, 0.498704731464386, 0.9458149671554565, 2.3633458614349365, 0.8125466108322144, 0.7744654417037964, 0.7075956463813782, 1.039409875869751, 1.081761121749878, 0.41306573152542114, -0.3568763732910156, 1.9134767055511475, 0.17045167088508606, 3.082794666290283, 0.6637580394744873, -0.206800639629364, -0.12914294004440308, 2.367784261703491, 0.6808775067329407, 1.8496363162994385, 0.42384251952171326, 2.039069890975952, 0.3413839340209961, -0.7941839098930359, 0.5946730971336365], 'y_pred_int': [0, 1, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0, 4, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 4, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 5, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 3, 0, 1, 1, 1, 1, 1, 4, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 7, 1, 1, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 3, 0, 0, 0, 9, 1, 2, 0, 0, 0, 0, 1, 0, 9, 0, 2, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 1, 4, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 4, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 2, 1, 7, 0, 4, 0, 0, 1, 1, 1, 1, 1, 0, 3, 1, 2, 1, 0, 0, 0, 0, 3, 0, 1, 2, 1, 1, 1, 1, 1, 0, 0, 2, 0, 3, 1, 0, 0, 2, 1, 2, 0, 2, 0, 0, 1], 'y_true': [1, 0, 0, 2, 1, 1, 0, 0, 1, 1, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 5, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 0, 0, 3, 0, 2, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 3, 1, 2, 1, 0, 4, 0, 1, 2, 2, 5, 2, 0, 1, 1, 2, 2, 0, 0, 1, 0, 1, 2, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 3, 0, 2, 0, 1, 0, 14, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 2, 1, 0, 1, 1, 2, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1, 0, 1, 5, 4, 2, 1, 0, 0, 2, 2, 2, 2, 1, 1, 0, 5, 1, 1, 0, 2, 0, 1, 3, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 3, 0, 0, 2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 2, 0, 2, 2, 2, 1, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 3, 0, 6, 0, 0, 0, 1, 2, 1, 2, 1, 1, 0, 1, 3, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 3, 0, 2, 1, 1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 2, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 1, 0, 0, 2, 1, 1, 0, 1, 0, 0, 3, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 1, 1, 2, 2, 0, 0, 1, 0, 0, 0, 6, 0, 1, 0], 'train_acc': 0.34478806228373704, 'test_acc': 0.41377162629757785, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 1.7393, Test loss: 124.9358
In this epoch 100/5000, Training loss: 0.1623, Test loss: 126.9713
In this epoch 150/5000, Training loss: 0.0565, Test loss: 127.0447
In this epoch 200/5000, Training loss: 4.3486, Test loss: 160.6376
In this epoch 250/5000, Training loss: 0.1496, Test loss: 127.1271
In this epoch 300/5000, Training loss: 0.0158, Test loss: 126.8722
In this epoch 350/5000, Training loss: 0.0148, Test loss: 126.9905
In this epoch 400/5000, Training loss: 0.0147, Test loss: 126.9931
In this epoch 450/5000, Training loss: 10.1577, Test loss: 168.0705
In this epoch 500/5000, Training loss: 0.0408, Test loss: 128.2254
In this epoch 550/5000, Training loss: 0.0151, Test loss: 129.8844
In this epoch 600/5000, Training loss: 0.0147, Test loss: 129.7938
In this epoch 650/5000, Training loss: 0.0147, Test loss: 129.8066
In this epoch 700/5000, Training loss: 0.0147, Test loss: 129.8086
In this epoch 750/5000, Training loss: 0.0147, Test loss: 129.8089
In this epoch 800/5000, Training loss: 0.0147, Test loss: 129.8089
In this epoch 850/5000, Training loss: 0.0147, Test loss: 129.8089
In this epoch 900/5000, Training loss: 0.0147, Test loss: 129.8089
In this epoch 950/5000, Training loss: 0.0147, Test loss: 129.8089
In this epoch 1000/5000, Training loss: 0.0147, Test loss: 129.8089
In this epoch 1050/5000, Training loss: 0.0147, Test loss: 129.8089
In this epoch 1100/5000, Training loss: 0.0147, Test loss: 129.8089
In this epoch 1150/5000, Training loss: 0.0147, Test loss: 129.8089
In this epoch 1200/5000, Training loss: 0.0147, Test loss: 129.8089
In this epoch 1250/5000, Training loss: 38.4550, Test loss: 161.0910
In this epoch 1300/5000, Training loss: 0.2170, Test loss: 137.6525
In this epoch 1350/5000, Training loss: 0.0571, Test loss: 137.7518
In this epoch 1400/5000, Training loss: 0.0428, Test loss: 137.7865
In this epoch 1450/5000, Training loss: 0.0337, Test loss: 137.8170
In this epoch 1500/5000, Training loss: 0.0277, Test loss: 137.8419
In this epoch 1550/5000, Training loss: 0.0239, Test loss: 137.8618
In this epoch 1600/5000, Training loss: 0.0215, Test loss: 137.8789
In this epoch 1650/5000, Training loss: 0.0200, Test loss: 137.8929
In this epoch 1700/5000, Training loss: 0.0191, Test loss: 137.9040
In this epoch 1750/5000, Training loss: 0.0186, Test loss: 137.9127
In this epoch 1800/5000, Training loss: 0.0184, Test loss: 137.9193
In this epoch 1850/5000, Training loss: 0.0182, Test loss: 137.9247
In this epoch 1900/5000, Training loss: 0.0181, Test loss: 137.9285
In this epoch 1950/5000, Training loss: 0.4959, Test loss: 137.5525
In this epoch 2000/5000, Training loss: 0.0185, Test loss: 137.8297
In this epoch 2050/5000, Training loss: 0.0181, Test loss: 137.7732
In this epoch 2100/5000, Training loss: 0.0180, Test loss: 137.7716
In this epoch 2150/5000, Training loss: 0.0180, Test loss: 137.7727
In this epoch 2200/5000, Training loss: 0.0182, Test loss: 137.8028
In this epoch 2250/5000, Training loss: 0.0647, Test loss: 138.0660
In this epoch 2300/5000, Training loss: 0.0185, Test loss: 137.6856
In this epoch 2350/5000, Training loss: 0.0181, Test loss: 137.6235
In this epoch 2400/5000, Training loss: 0.1446, Test loss: 136.7681
In this epoch 2450/5000, Training loss: 0.0190, Test loss: 137.3437
In this epoch 2500/5000, Training loss: 0.0180, Test loss: 137.4435
In this epoch 2550/5000, Training loss: 0.6972, Test loss: 139.9201
In this epoch 2600/5000, Training loss: 0.0184, Test loss: 137.8948
In this epoch 2650/5000, Training loss: 0.0181, Test loss: 137.7608
In this epoch 2700/5000, Training loss: 0.0180, Test loss: 137.7471
In this epoch 2750/5000, Training loss: 0.0180, Test loss: 137.7483
In this epoch 2800/5000, Training loss: 0.0180, Test loss: 137.7482
In this epoch 2850/5000, Training loss: 0.0180, Test loss: 137.7482
In this epoch 2900/5000, Training loss: 0.0180, Test loss: 137.7481
In this epoch 2950/5000, Training loss: 0.0180, Test loss: 137.7480
In this epoch 3000/5000, Training loss: 3.2114, Test loss: 151.1511
In this epoch 3050/5000, Training loss: 0.1172, Test loss: 140.1763
In this epoch 3100/5000, Training loss: 0.0182, Test loss: 140.5880
In this epoch 3150/5000, Training loss: 0.0180, Test loss: 140.5479
In this epoch 3200/5000, Training loss: 0.0180, Test loss: 140.5493
In this epoch 3250/5000, Training loss: 0.0180, Test loss: 140.5497
In this epoch 3300/5000, Training loss: 0.0180, Test loss: 140.5498
In this epoch 3350/5000, Training loss: 0.0180, Test loss: 140.5498
In this epoch 3400/5000, Training loss: 0.0180, Test loss: 140.5498
In this epoch 3450/5000, Training loss: 0.0180, Test loss: 140.5497
In this epoch 3500/5000, Training loss: 0.0180, Test loss: 140.5496
In this epoch 3550/5000, Training loss: 0.0180, Test loss: 140.5495
In this epoch 3600/5000, Training loss: 0.0180, Test loss: 140.5494
In this epoch 3650/5000, Training loss: 0.0180, Test loss: 140.5492
In this epoch 3700/5000, Training loss: 0.0180, Test loss: 140.5491
In this epoch 3750/5000, Training loss: 0.0180, Test loss: 140.5490
In this epoch 3800/5000, Training loss: 0.0180, Test loss: 140.5489
In this epoch 3850/5000, Training loss: 0.0180, Test loss: 140.5487
In this epoch 3900/5000, Training loss: 0.0180, Test loss: 140.5486
In this epoch 3950/5000, Training loss: 0.0180, Test loss: 140.5484
In this epoch 4000/5000, Training loss: 0.0180, Test loss: 140.5483
In this epoch 4050/5000, Training loss: 0.0180, Test loss: 140.5481
In this epoch 4100/5000, Training loss: 0.0180, Test loss: 140.5480
In this epoch 4150/5000, Training loss: 0.0180, Test loss: 140.5478
In this epoch 4200/5000, Training loss: 0.0180, Test loss: 140.5477
In this epoch 4250/5000, Training loss: 0.0180, Test loss: 140.5475
In this epoch 4300/5000, Training loss: 0.0180, Test loss: 140.5473
In this epoch 4350/5000, Training loss: 0.0180, Test loss: 140.5472
In this epoch 4400/5000, Training loss: 0.0180, Test loss: 140.5470
In this epoch 4450/5000, Training loss: 0.0180, Test loss: 140.5475
In this epoch 4500/5000, Training loss: 0.2227, Test loss: 141.4217
In this epoch 4550/5000, Training loss: 0.0197, Test loss: 140.0774
In this epoch 4600/5000, Training loss: 0.0180, Test loss: 139.9908
In this epoch 4650/5000, Training loss: 0.0180, Test loss: 139.9925
In this epoch 4700/5000, Training loss: 0.0180, Test loss: 139.9921
In this epoch 4750/5000, Training loss: 0.0180, Test loss: 139.9919
In this epoch 4800/5000, Training loss: 0.0180, Test loss: 139.9918
In this epoch 4850/5000, Training loss: 0.0180, Test loss: 139.9916
In this epoch 4900/5000, Training loss: 0.0180, Test loss: 139.9916
In this epoch 4950/5000, Training loss: 0.0866, Test loss: 139.1759
In this epoch 5000/5000, Training loss: 0.0228, Test loss: 139.0398
train_acc 0.04069798875432526
test_acc 0.046764705882352944
{'y_pred': [10.524629592895508, 14.200133323669434, 13.781201362609863, 10.042183876037598, 11.880281448364258, 14.317645072937012, 7.390699863433838, 12.08466625213623, 15.457844734191895, 4.566235065460205, 6.003632545471191, 10.123883247375488, 13.224227905273438, 8.85344123840332, 10.993287086486816, 11.578203201293945, 7.167335510253906, 10.85876178741455, 15.892191886901855, 16.77734375, 8.587028503417969, 9.547938346862793, 11.459969520568848, 12.804980278015137, 9.225881576538086, 11.838860511779785, 10.36830997467041, 9.536693572998047, 10.058985710144043, 7.141921043395996, 7.575723648071289, 12.024186134338379, 8.63021183013916, 14.551300048828125, 11.618704795837402, 16.03421401977539, 13.110644340515137, 15.28563404083252, 13.817215919494629, 7.030539512634277, 12.491496086120605, 14.810874938964844, 12.586750984191895, 4.20103645324707, 11.864025115966797, 3.8421993255615234, 6.894651412963867, 8.014634132385254, 13.318560600280762, 13.218231201171875, 14.450173377990723, 10.781240463256836, 10.039687156677246, 5.260622024536133, 10.156268119812012, 12.82610034942627, 13.583205223083496, 8.630074501037598, 11.735260963439941, 14.995667457580566, 7.596756458282471, 15.604710578918457, 13.466826438903809, 12.976373672485352, 16.137758255004883, 12.360279083251953, 9.9475736618042, 10.080221176147461, 7.138271331787109, 17.109224319458008, 9.274033546447754, 8.217371940612793, 5.616757869720459, 9.744242668151855, 11.597131729125977, 3.25596284866333, 3.5152535438537598, 5.0502729415893555, 14.699719429016113, 8.251606941223145, 5.927557468414307, 10.623525619506836, 13.813149452209473, 10.698287963867188, 16.895645141601562, 14.998028755187988, 14.466547966003418, 10.579265594482422, 17.03944969177246, 13.57967472076416, 11.89696979522705, 8.043957710266113, 13.98929214477539, 13.32706356048584, 13.058207511901855, 10.034768104553223, 15.510294914245605, 12.877030372619629, 4.041515827178955, 13.611263275146484, 9.018048286437988, 6.382216453552246, 14.907309532165527, 13.533049583435059, 11.544066429138184, 9.639894485473633, 8.441046714782715, 11.387171745300293, 10.555377006530762, 11.28646183013916, 15.013032913208008, 7.608464241027832, 11.049912452697754, 14.186052322387695, 18.398950576782227, 8.747139930725098, 14.078730583190918, 15.485932350158691, 9.54137134552002, 9.287466049194336, 9.426636695861816, 12.609293937683105, 13.251272201538086, 2.672621250152588, 9.35568618774414, 6.056190490722656, 8.71030044555664, 15.317183494567871, 15.76795482635498, 7.5520734786987305, 2.763072967529297, 9.015090942382812, 11.926970481872559, 7.34163761138916, 14.121861457824707, 11.39006233215332, 12.236823081970215, 10.505898475646973, 12.857206344604492, 9.264161109924316, 11.031195640563965, 10.585102081298828, 8.859309196472168, 10.621917724609375, 14.973979949951172, 9.437281608581543, 11.324441909790039, 13.954221725463867, 6.920441150665283, 15.893219947814941, 13.162772178649902, 15.416274070739746, 12.5077486038208, 11.104903221130371, 13.714079856872559, 14.339564323425293, 8.098956108093262, 6.916234493255615, 11.915493965148926, 11.479650497436523, 12.514978408813477, 12.256640434265137, 13.768464088439941, 13.32048511505127, 11.865549087524414, 5.200476169586182, 7.453787803649902, 5.440989971160889, 13.973174095153809, 12.677979469299316, 6.423236846923828, 8.637145042419434, 14.00784969329834, 13.43859577178955, 8.906668663024902, 8.454023361206055, 7.875589370727539, 12.713963508605957, 13.595925331115723, 13.40882396697998, 8.643251419067383, 12.146927833557129, 11.253271102905273, 14.62293815612793, 12.428762435913086, 11.721543312072754, 14.97396183013916, 11.71446704864502, 12.863166809082031, 1.9351086616516113, 10.757031440734863, 15.52514934539795, 12.48806095123291, 11.381448745727539, 12.217000007629395, 8.085453987121582, 14.36474895477295, 16.74283218383789, 8.313145637512207, 16.074047088623047, 13.389355659484863, 9.218183517456055, 11.79971694946289, 15.218792915344238, 12.227361679077148, 11.123703002929688, 8.103489875793457, 0.2067364752292633, 9.732218742370605, 15.175500869750977, 14.787476539611816, 7.570867538452148, 10.400139808654785, 12.248371124267578, 16.459333419799805, 8.92353343963623, 14.795380592346191, 14.499491691589355, 14.67641544342041, 10.97889518737793, 7.044130802154541, 9.570582389831543, 15.099272727966309, 10.193552017211914, 13.058331489562988, 9.35964584350586, 13.771536827087402, 4.371781349182129, 6.486570835113525, 10.362792015075684, 9.04931640625, 6.881343364715576, 10.69986629486084, 10.921639442443848, 8.680964469909668, 11.30714225769043, 8.336256980895996, 14.3666353225708, 10.970698356628418, 4.473993301391602, 11.885931968688965, 5.252814292907715, 7.257988452911377, 1.128183364868164, 13.629557609558105, 11.249382019042969, 5.402619361877441, 10.946152687072754, 19.141403198242188, 5.624329090118408, 9.023551940917969, 13.16034984588623, 14.596542358398438, 14.318730354309082, 14.942770957946777, 12.36276912689209, 4.5569257736206055, 14.31632137298584, 9.209227561950684, 15.156098365783691, 15.20129108428955, 12.72948932647705, 5.049545764923096, 9.678149223327637, 12.601847648620605, 10.371858596801758, 14.298164367675781, 6.893551826477051, 5.365531921386719, 6.472743034362793, 9.36419677734375, 9.030654907226562, 10.195969581604004, 3.564493179321289, 13.274981498718262, 5.42799186706543, 6.078352928161621, 8.42980670928955, 17.123205184936523, 14.087689399719238, 13.188109397888184, 6.5387444496154785, 5.898991107940674, 14.341379165649414, 6.769115447998047, 6.2122578620910645, 17.177003860473633, 10.78144359588623, 10.9221773147583, 10.388497352600098, 13.597685813903809, 12.924646377563477, 12.390873908996582, 8.03080940246582, 9.651579856872559, 14.346797943115234, 13.034712791442871, 6.960434913635254, 12.194184303283691, 12.586800575256348, 9.1456298828125, 14.350077629089355, 7.879886150360107, 9.56566333770752, 11.939048767089844, 5.696184158325195, 3.078511953353882, 11.527583122253418, 14.701058387756348, 6.154295921325684, 6.785615921020508, 13.878067016601562, 11.965941429138184, 12.773789405822754, 11.475841522216797, 9.760826110839844, 13.359042167663574, 9.024250030517578, 9.21005630493164, 8.586180686950684, 14.908479690551758, 15.028307914733887, 7.736111640930176, 10.492559432983398, 8.847105026245117, 11.930663108825684, 9.111238479614258, 4.816291332244873, 9.247145652770996, 15.517980575561523, 12.16247272491455, 10.140422821044922, 13.377409934997559, 4.62686824798584, 14.392253875732422, 9.378552436828613, 13.412188529968262, 8.016011238098145, 11.743451118469238, 11.568224906921387], 'y_pred_int': [11, 14, 14, 10, 12, 14, 7, 12, 15, 5, 6, 10, 13, 9, 11, 12, 7, 11, 16, 17, 9, 10, 11, 13, 9, 12, 10, 10, 10, 7, 8, 12, 9, 15, 12, 16, 13, 15, 14, 7, 12, 15, 13, 4, 12, 4, 7, 8, 13, 13, 14, 11, 10, 5, 10, 13, 14, 9, 12, 15, 8, 16, 13, 13, 16, 12, 10, 10, 7, 17, 9, 8, 6, 10, 12, 3, 4, 5, 15, 8, 6, 11, 14, 11, 17, 15, 14, 11, 17, 14, 12, 8, 14, 13, 13, 10, 16, 13, 4, 14, 9, 6, 15, 14, 12, 10, 8, 11, 11, 11, 15, 8, 11, 14, 18, 9, 14, 15, 10, 9, 9, 13, 13, 3, 9, 6, 9, 15, 16, 8, 3, 9, 12, 7, 14, 11, 12, 11, 13, 9, 11, 11, 9, 11, 15, 9, 11, 14, 7, 16, 13, 15, 13, 11, 14, 14, 8, 7, 12, 11, 13, 12, 14, 13, 12, 5, 7, 5, 14, 13, 6, 9, 14, 13, 9, 8, 8, 13, 14, 13, 9, 12, 11, 15, 12, 12, 15, 12, 13, 2, 11, 16, 12, 11, 12, 8, 14, 17, 8, 16, 13, 9, 12, 15, 12, 11, 8, 0, 10, 15, 15, 8, 10, 12, 16, 9, 15, 14, 15, 11, 7, 10, 15, 10, 13, 9, 14, 4, 6, 10, 9, 7, 11, 11, 9, 11, 8, 14, 11, 4, 12, 5, 7, 1, 14, 11, 5, 11, 19, 6, 9, 13, 15, 14, 15, 12, 5, 14, 9, 15, 15, 13, 5, 10, 13, 10, 14, 7, 5, 6, 9, 9, 10, 4, 13, 5, 6, 8, 17, 14, 13, 7, 6, 14, 7, 6, 17, 11, 11, 10, 14, 13, 12, 8, 10, 14, 13, 7, 12, 13, 9, 14, 8, 10, 12, 6, 3, 12, 15, 6, 7, 14, 12, 13, 11, 10, 13, 9, 9, 9, 15, 15, 8, 10, 9, 12, 9, 5, 9, 16, 12, 10, 13, 5, 14, 9, 13, 8, 12, 12], 'y_true': [11, 50, 5, 18, 7, 7, 8, 9, 42, 6, 16, 10, 9, 6, 14, 12, 8, 13, 12, 27, 5, 2, 19, 4, 37, 34, 3, 9, 26, 8, 25, 11, 49, 6, 10, 7, 5, 6, 10, 9, 22, 9, 11, 25, 21, 10, 9, 2, 4, 29, 22, 28, 14, 10, 5, 1, 4, 5, 18, 13, 19, 17, 5, 5, 35, 6, 10, 17, 15, 6, 7, 15, 47, 6, 18, 6, 5, 10, 9, 20, 9, 10, 13, 6, 13, 6, 5, 17, 5, 17, 15, 12, 20, 3, 11, 28, 10, 15, 20, 9, 15, 7, 27, 7, 27, 6, 7, 11, 8, 7, 7, 10, 17, 4, 20, 4, 6, 35, 10, 23, 14, 7, 18, 3, 14, 63, 35, 14, 20, 11, 37, 21, 28, 19, 18, 4, 9, 4, 15, 21, 19, 11, 8, 7, 11, 16, 6, 15, 9, 33, 7, 7, 5, 4, 3, 12, 18, 5, 13, 20, 12, 7, 43, 20, 7, 31, 26, 40, 9, 14, 10, 5, 5, 8, 27, 12, 6, 19, 5, 9, 5, 1, 5, 15, 4, 29, 11, 16, 13, 14, 21, 2, 27, 14, 10, 9, 6, 20, 29, 46, 4, 13, 19, 8, 11, 11, 22, 46, 13, 18, 15, 21, 10, 7, 9, 5, 17, 12, 33, 7, 7, 10, 3, 7, 14, 7, 9, 33, 2, 3, 5, 15, 5, 3, 13, 12, 6, 26, 5, 19, 15, 26, 6, 26, 13, 6, 20, 60, 4, 5, 1, 9, 14, 16, 3, 6, 18, 39, 17, 17, 8, 3, 9, 16, 6, 11, 21, 3, 15, 5, 9, 5, 7, 41, 20, 13, 16, 28, 3, 3, 28, 15, 9, 12, 16, 7, 14, 18, 3, 6, 29, 14, 17, 13, 9, 12, 2, 20, 14, 13, 6, 9, 23, 9, 6, 18, 6, 9, 15, 19, 2, 7, 18, 14, 17, 12, 4, 14, 18, 21, 29, 10, 46, 47, 10, 29, 6, 8, 4, 12, 36, 25, 7, 18, 12, 6, 3, 11, 15, 37], 'train_acc': 0.04069798875432526, 'test_acc': 0.046764705882352944, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_GTT_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 391.6395, Test loss: 42907.0625
In this epoch 100/5000, Training loss: 16.0869, Test loss: 42891.5938
In this epoch 150/5000, Training loss: 13.5297, Test loss: 42887.0000
In this epoch 200/5000, Training loss: 13.5109, Test loss: 42885.8906
In this epoch 250/5000, Training loss: 13.5107, Test loss: 42885.8750
In this epoch 300/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 350/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 400/5000, Training loss: 13.5107, Test loss: 42885.9023
In this epoch 450/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 500/5000, Training loss: 13.5107, Test loss: 42885.8984
In this epoch 550/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 600/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 650/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 700/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 750/5000, Training loss: 13.5107, Test loss: 42885.8984
In this epoch 800/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 850/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 900/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 950/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1000/5000, Training loss: 13.5107, Test loss: 42885.8984
In this epoch 1050/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1100/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1150/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1200/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1250/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1300/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1350/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1400/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1450/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1500/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1550/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1600/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1650/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1700/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1750/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1800/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1850/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 1900/5000, Training loss: 13.5107, Test loss: 42885.8984
In this epoch 1950/5000, Training loss: 13.5107, Test loss: 42885.8984
In this epoch 2000/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 2050/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 2100/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 2150/5000, Training loss: 13.5107, Test loss: 42885.8984
In this epoch 2200/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 2250/5000, Training loss: 13.5107, Test loss: 42885.8984
In this epoch 2300/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 2350/5000, Training loss: 13.5107, Test loss: 42885.8945
In this epoch 2400/5000, Training loss: 2031.7180, Test loss: 44979.1992
In this epoch 2450/5000, Training loss: 20.4774, Test loss: 43199.0898
In this epoch 2500/5000, Training loss: 13.5493, Test loss: 43180.2344
In this epoch 2550/5000, Training loss: 13.5115, Test loss: 43180.3086
In this epoch 2600/5000, Training loss: 13.5107, Test loss: 43180.1562
In this epoch 2650/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 2700/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 2750/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 2800/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 2850/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 2900/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 2950/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3000/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3050/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3100/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3150/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3200/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 3250/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 3300/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3350/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3400/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 3450/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3500/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3550/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3600/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3650/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3700/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3750/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3800/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3850/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3900/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 3950/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 4000/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 4050/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 4100/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 4150/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 4200/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 4250/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 4300/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 4350/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 4400/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 4450/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 4500/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 4550/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 4600/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 4650/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 4700/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 4750/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 4800/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 4850/5000, Training loss: 13.5107, Test loss: 43180.1289
In this epoch 4900/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 4950/5000, Training loss: 13.5107, Test loss: 43180.1250
In this epoch 5000/5000, Training loss: 13.5107, Test loss: 43180.1250
train_acc 0.9970588235294118
test_acc 0.0029411764705882353
{'y_pred': [364.4599304199219, 372.5943298339844, 390.5853271484375, 330.52099609375, 362.6680908203125, 337.95123291015625, 305.11199951171875, 349.1094970703125, 413.1189270019531, 382.7413330078125, 373.6199645996094, 338.6412658691406, 367.4614562988281, 257.284912109375, 278.3591003417969, 410.75738525390625, 351.8642578125, 361.6387939453125, 384.52313232421875, 418.377685546875, 258.94708251953125, 283.2394714355469, 304.0790100097656, 366.4052734375, 253.60842895507812, 353.42938232421875, 327.1877746582031, 369.673583984375, 388.64959716796875, 322.6212158203125, 339.64990234375, 344.2153625488281, 257.4698486328125, 372.631103515625, 333.9581298828125, 356.0657653808594, 341.279541015625, 325.1563720703125, 359.30499267578125, 307.48748779296875, 385.8410949707031, 377.35736083984375, 269.1394958496094, 325.3984375, 343.8070068359375, 297.9112548828125, 339.60791015625, 330.31414794921875, 328.7951965332031, 271.0418701171875, 327.990234375, 321.5343322753906, 356.68133544921875, 318.8162841796875, 303.8846435546875, 314.7694396972656, 369.6867980957031, 360.94012451171875, 246.68162536621094, 297.5756530761719, 321.0746154785156, 369.2945251464844, 345.0570373535156, 279.2174377441406, 364.53741455078125, 360.4068298339844, 472.81292724609375, 360.57232666015625, 353.0343322753906, 314.1641845703125, 356.7093811035156, 314.4858703613281, 361.79327392578125, 314.62591552734375, 351.57647705078125, 272.9614562988281, 377.35943603515625, 376.58575439453125, 295.7474670410156, 378.67999267578125, 429.1157531738281, 352.2120361328125, 353.0584716796875, 296.07513427734375, 354.0366516113281, 293.14910888671875, 287.36553955078125, 408.48614501953125, 327.91876220703125, 280.13189697265625, 386.7444763183594, 314.47003173828125, 323.9089660644531, 304.1025390625, 338.68695068359375, 251.62030029296875, 339.46661376953125, 361.48577880859375, 418.8200378417969, 348.73370361328125, 382.19024658203125, 250.05828857421875, 406.511474609375, 352.3027648925781, 324.22674560546875, 345.178466796875, 343.5044860839844, 354.16107177734375, 344.65655517578125, 366.18634033203125, 336.0101623535156, 320.7562561035156, 351.881591796875, 340.11846923828125, 464.47137451171875, 269.9180908203125, 389.022705078125, 316.40032958984375, 353.78106689453125, 350.8288269042969, 272.65374755859375, 267.462158203125, 319.1854248046875, 342.64605712890625, 294.90838623046875, 317.0644226074219, 397.3439636230469, 365.0329895019531, 362.6090393066406, 361.614501953125, 296.0672302246094, 353.2966003417969, 356.78326416015625, 414.71417236328125, 331.3935546875, 333.6094665527344, 322.50592041015625, 312.6370849609375, 303.14398193359375, 301.7442626953125, 378.52044677734375, 338.603515625, 327.943359375, 310.4727783203125, 311.4195251464844, 380.3489685058594, 357.9896240234375, 290.5098571777344, 350.00390625, 332.9360046386719, 287.34503173828125, 318.0277404785156, 420.3141784667969, 419.2030029296875, 361.2803649902344, 343.132080078125, 363.0862121582031, 322.71319580078125, 373.5543518066406, 335.5262451171875, 362.50341796875, 324.3748474121094, 256.79638671875, 323.79315185546875, 334.94134521484375, 288.6175537109375, 276.6054992675781, 356.9393005371094, 359.6246337890625, 265.39141845703125, 242.06736755371094, 384.8154296875, 359.6979064941406, 373.09674072265625, 339.65185546875, 274.93890380859375, 296.7629089355469, 340.0411071777344, 324.5391540527344, 335.8130798339844, 313.04779052734375, 380.9036865234375, 319.642822265625, 346.03753662109375, 363.40057373046875, 269.69354248046875, 334.9688415527344, 306.3206787109375, 364.396484375, 321.7702331542969, 336.7183532714844, 320.9237365722656, 401.5747375488281, 269.87127685546875, 265.1158752441406, 372.90289306640625, 318.4141540527344, 302.6376647949219, 287.9451904296875, 425.3468017578125, 366.68206787109375, 325.7286682128906, 341.30657958984375, 342.387939453125, 357.4576416015625, 373.5703125, 402.4510498046875, 360.5139465332031, 346.4697570800781, 284.7588195800781, 280.670166015625, 343.13165283203125, 289.2060546875, 377.1199645996094, 329.70501708984375, 319.0130310058594, 325.0772705078125, 356.8680114746094, 371.33251953125, 314.15740966796875, 335.73748779296875, 323.8318786621094, 288.5758361816406, 293.35369873046875, 340.19866943359375, 332.70648193359375, 318.18646240234375, 383.886962890625, 330.0894470214844, 374.44744873046875, 390.38177490234375, 318.1214599609375, 397.3965148925781, 345.8651123046875, 337.7908020019531, 372.9145202636719, 345.3375549316406, 312.087646484375, 307.8658447265625, 288.87420654296875, 320.0067443847656, 300.0231018066406, 365.19085693359375, 371.22686767578125, 333.653076171875, 273.13958740234375, 375.8272705078125, 350.82171630859375, 375.43377685546875, 340.7115478515625, 282.4726257324219, 360.72186279296875, 384.60736083984375, 371.3099670410156, 382.8716735839844, 323.2912292480469, 298.9627990722656, 345.2086181640625, 240.63473510742188, 340.2391052246094, 349.0840759277344, 326.7786560058594, 312.0697326660156, 276.580810546875, 340.3941650390625, 263.08526611328125, 383.7064208984375, 319.219482421875, 356.0293884277344, 372.3423767089844, 324.09088134765625, 336.18670654296875, 236.70741271972656, 352.0766296386719, 349.7382507324219, 351.5008239746094, 369.2616271972656, 292.05419921875, 323.31292724609375, 328.19573974609375, 325.78302001953125, 429.3728332519531, 269.991455078125, 373.120849609375, 354.6693420410156, 350.6614990234375, 347.42938232421875, 358.73773193359375, 355.1980285644531, 345.18817138671875, 272.0669250488281, 365.92236328125, 345.7356262207031, 286.02178955078125, 350.30078125, 275.7764892578125, 318.8042907714844, 357.3067626953125, 339.6029052734375, 320.780029296875, 358.87408447265625, 283.1176452636719, 312.5044860839844, 360.58062744140625, 318.7115478515625, 284.17498779296875, 322.72528076171875, 402.97540283203125, 310.56475830078125, 375.8606872558594, 308.89166259765625, 290.2779846191406, 405.2163391113281, 308.982666015625, 335.7442932128906, 391.67828369140625, 364.4935302734375, 330.3880920410156, 348.6697998046875, 331.13421630859375, 350.5589904785156, 370.75579833984375, 298.63726806640625, 256.1246032714844, 306.2799377441406, 335.7930908203125, 370.5904541015625, 341.8085021972656, 349.74798583984375, 305.022705078125, 288.6340026855469, 326.6605529785156, 333.244140625, 289.12567138671875, 289.4183044433594, 286.3606262207031, 349.7145690917969, 348.3167724609375, 328.2831726074219, 345.6396789550781], 'y_pred_int': [364, 373, 391, 331, 363, 338, 305, 349, 413, 383, 374, 339, 367, 257, 278, 411, 352, 362, 385, 418, 259, 283, 304, 366, 254, 353, 327, 370, 389, 323, 340, 344, 257, 373, 334, 356, 341, 325, 359, 307, 386, 377, 269, 325, 344, 298, 340, 330, 329, 271, 328, 322, 357, 319, 304, 315, 370, 361, 247, 298, 321, 369, 345, 279, 365, 360, 473, 361, 353, 314, 357, 314, 362, 315, 352, 273, 377, 377, 296, 379, 429, 352, 353, 296, 354, 293, 287, 408, 328, 280, 387, 314, 324, 304, 339, 252, 339, 361, 419, 349, 382, 250, 407, 352, 324, 345, 344, 354, 345, 366, 336, 321, 352, 340, 464, 270, 389, 316, 354, 351, 273, 267, 319, 343, 295, 317, 397, 365, 363, 362, 296, 353, 357, 415, 331, 334, 323, 313, 303, 302, 379, 339, 328, 310, 311, 380, 358, 291, 350, 333, 287, 318, 420, 419, 361, 343, 363, 323, 374, 336, 363, 324, 257, 324, 335, 289, 277, 357, 360, 265, 242, 385, 360, 373, 340, 275, 297, 340, 325, 336, 313, 381, 320, 346, 363, 270, 335, 306, 364, 322, 337, 321, 402, 270, 265, 373, 318, 303, 288, 425, 367, 326, 341, 342, 357, 374, 402, 361, 346, 285, 281, 343, 289, 377, 330, 319, 325, 357, 371, 314, 336, 324, 289, 293, 340, 333, 318, 384, 330, 374, 390, 318, 397, 346, 338, 373, 345, 312, 308, 289, 320, 300, 365, 371, 334, 273, 376, 351, 375, 341, 282, 361, 385, 371, 383, 323, 299, 345, 241, 340, 349, 327, 312, 277, 340, 263, 384, 319, 356, 372, 324, 336, 237, 352, 350, 352, 369, 292, 323, 328, 326, 429, 270, 373, 355, 351, 347, 359, 355, 345, 272, 366, 346, 286, 350, 276, 319, 357, 340, 321, 359, 283, 313, 361, 319, 284, 323, 403, 311, 376, 309, 290, 405, 309, 336, 392, 364, 330, 349, 331, 351, 371, 299, 256, 306, 336, 371, 342, 350, 305, 289, 327, 333, 289, 289, 286, 350, 348, 328, 346], 'y_true': [305, 487, 608, 324, 291, 212, 141, 183, 527, 682, 523, 231, 427, 168, 82, 136, 395, 90, 308, 213, 129, 787, 468, 786, 286, 827, 386, 401, 113, 521, 203, 319, 95, 258, 428, 276, 443, 221, 350, 905, 382, 89, 236, 161, 151, 181, 164, 490, 459, 518, 119, 298, 412, 633, 509, 191, 205, 456, 392, 543, 247, 316, 93, 224, 164, 258, 375, 187, 494, 284, 149, 96, 929, 206, 319, 561, 118, 194, 371, 832, 126, 237, 226, 424, 672, 439, 200, 105, 137, 318, 110, 254, 227, 447, 163, 100, 176, 389, 444, 163, 252, 498, 605, 475, 343, 224, 145, 741, 356, 92, 265, 547, 562, 133, 286, 169, 285, 148, 321, 104, 174, 656, 373, 155, 317, 134, 370, 228, 212, 261, 214, 73, 86, 68, 338, 376, 203, 582, 109, 156, 166, 212, 300, 216, 450, 440, 146, 467, 376, 133, 256, 175, 173, 278, 193, 507, 363, 599, 663, 82, 244, 574, 635, 312, 333, 158, 293, 258, 100, 190, 124, 259, 260, 448, 191, 116, 113, 329, 823, 372, 497, 670, 199, 197, 290, 108, 262, 198, 196, 663, 912, 180, 572, 898, 102, 778, 167, 443, 589, 230, 471, 301, 619, 385, 433, 397, 355, 765, 225, 166, 93, 478, 328, 681, 296, 624, 161, 343, 244, 577, 446, 529, 201, 376, 215, 579, 444, 31, 45, 390, 367, 148, 664, 292, 266, 82, 375, 252, 443, 899, 186, 89, 433, 266, 75, 495, 209, 89, 366, 163, 191, 470, 66, 176, 168, 200, 824, 189, 319, 955, 588, 436, 392, 291, 628, 80, 511, 264, 153, 292, 403, 700, 880, 463, 785, 155, 130, 317, 344, 428, 308, 686, 339, 530, 348, 211, 881, 599, 600, 346, 455, 355, 183, 181, 458, 258, 388, 105, 340, 332, 244, 39, 343, 350, 331, 113, 154, 601, 643, 569, 921, 197, 487, 527, 391, 269, 371, 76, 464, 279, 397, 115, 486, 935, 407, 657, 171, 353, 379, 139, 546, 373, 340, 111, 311, 431, 162, 438, 140, 155], 'train_acc': 0.9970588235294118, 'test_acc': 0.0029411764705882353, 'X': 'X_GTT_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 1.5848, Test loss: 7.9344
In this epoch 100/5000, Training loss: 0.5907, Test loss: 7.2510
In this epoch 150/5000, Training loss: 5.2336, Test loss: 9.3371
In this epoch 200/5000, Training loss: 0.8641, Test loss: 9.0207
In this epoch 250/5000, Training loss: 0.1647, Test loss: 6.8831
In this epoch 300/5000, Training loss: 11.1294, Test loss: 7.5394
In this epoch 350/5000, Training loss: 0.1419, Test loss: 5.9467
In this epoch 400/5000, Training loss: 0.1481, Test loss: 6.4611
In this epoch 450/5000, Training loss: 2.7558, Test loss: 8.9038
In this epoch 500/5000, Training loss: 0.0509, Test loss: 5.5666
In this epoch 550/5000, Training loss: 0.0046, Test loss: 5.6663
In this epoch 600/5000, Training loss: 0.8757, Test loss: 8.1163
In this epoch 650/5000, Training loss: 0.4056, Test loss: 4.7664
In this epoch 700/5000, Training loss: 0.0035, Test loss: 4.7764
In this epoch 750/5000, Training loss: 0.0018, Test loss: 4.7676
In this epoch 800/5000, Training loss: 0.0018, Test loss: 4.7670
In this epoch 850/5000, Training loss: 1.5564, Test loss: 5.6039
In this epoch 900/5000, Training loss: 0.0018, Test loss: 4.7305
In this epoch 950/5000, Training loss: 0.0018, Test loss: 4.7357
In this epoch 1000/5000, Training loss: 0.0018, Test loss: 4.7358
In this epoch 1050/5000, Training loss: 1.5059, Test loss: 6.8982
In this epoch 1100/5000, Training loss: 0.0159, Test loss: 3.4415
In this epoch 1150/5000, Training loss: 0.0007, Test loss: 3.4080
In this epoch 1200/5000, Training loss: 0.0006, Test loss: 3.4079
In this epoch 1250/5000, Training loss: 0.0006, Test loss: 3.4074
In this epoch 1300/5000, Training loss: 0.0005, Test loss: 3.4073
In this epoch 1350/5000, Training loss: 0.0005, Test loss: 3.4072
In this epoch 1400/5000, Training loss: 0.0005, Test loss: 3.4071
In this epoch 1450/5000, Training loss: 0.0005, Test loss: 3.4070
In this epoch 1500/5000, Training loss: 0.0005, Test loss: 3.4069
In this epoch 1550/5000, Training loss: 0.0005, Test loss: 3.4069
In this epoch 1600/5000, Training loss: 0.0005, Test loss: 3.4068
In this epoch 1650/5000, Training loss: 0.0005, Test loss: 3.4068
In this epoch 1700/5000, Training loss: 0.0005, Test loss: 3.4068
In this epoch 1750/5000, Training loss: 0.0005, Test loss: 3.4067
In this epoch 1800/5000, Training loss: 0.0005, Test loss: 3.4067
In this epoch 1850/5000, Training loss: 55.7104, Test loss: 453.1440
In this epoch 1900/5000, Training loss: 2.3830, Test loss: 3.6422
In this epoch 1950/5000, Training loss: 0.8695, Test loss: 2.4418
In this epoch 2000/5000, Training loss: 0.8428, Test loss: 2.4378
In this epoch 2050/5000, Training loss: 0.8243, Test loss: 2.4297
In this epoch 2100/5000, Training loss: 0.8087, Test loss: 2.4216
In this epoch 2150/5000, Training loss: 0.7951, Test loss: 2.4137
In this epoch 2200/5000, Training loss: 0.7830, Test loss: 2.4058
In this epoch 2250/5000, Training loss: 0.7722, Test loss: 2.3980
In this epoch 2300/5000, Training loss: 0.7621, Test loss: 2.3902
In this epoch 2350/5000, Training loss: 0.7528, Test loss: 2.3824
In this epoch 2400/5000, Training loss: 0.7440, Test loss: 2.3746
In this epoch 2450/5000, Training loss: 0.7355, Test loss: 2.3667
In this epoch 2500/5000, Training loss: 0.7275, Test loss: 2.3589
In this epoch 2550/5000, Training loss: 0.7197, Test loss: 2.3511
In this epoch 2600/5000, Training loss: 0.7121, Test loss: 2.3434
In this epoch 2650/5000, Training loss: 0.7048, Test loss: 2.3357
In this epoch 2700/5000, Training loss: 0.6976, Test loss: 2.3280
In this epoch 2750/5000, Training loss: 0.6907, Test loss: 2.3204
In this epoch 2800/5000, Training loss: 0.6839, Test loss: 2.3130
In this epoch 2850/5000, Training loss: 0.6773, Test loss: 2.3056
In this epoch 2900/5000, Training loss: 0.6709, Test loss: 2.2983
In this epoch 2950/5000, Training loss: 0.6646, Test loss: 2.2911
In this epoch 3000/5000, Training loss: 0.6585, Test loss: 2.2841
In this epoch 3050/5000, Training loss: 0.6526, Test loss: 2.2771
In this epoch 3100/5000, Training loss: 0.6469, Test loss: 2.2703
In this epoch 3150/5000, Training loss: 0.6413, Test loss: 2.2637
In this epoch 3200/5000, Training loss: 0.6360, Test loss: 2.2572
In this epoch 3250/5000, Training loss: 1.3092, Test loss: 2.2309
In this epoch 3300/5000, Training loss: 0.6313, Test loss: 2.2276
In this epoch 3350/5000, Training loss: 0.6245, Test loss: 2.2265
In this epoch 3400/5000, Training loss: 0.6198, Test loss: 2.2224
In this epoch 3450/5000, Training loss: 0.6154, Test loss: 2.2166
In this epoch 3500/5000, Training loss: 0.6111, Test loss: 2.2111
In this epoch 3550/5000, Training loss: 0.6070, Test loss: 2.2057
In this epoch 3600/5000, Training loss: 0.6031, Test loss: 2.2005
In this epoch 3650/5000, Training loss: 0.5993, Test loss: 2.1955
In this epoch 3700/5000, Training loss: 0.5956, Test loss: 2.1904
In this epoch 3750/5000, Training loss: 0.5922, Test loss: 2.1858
In this epoch 3800/5000, Training loss: 0.5890, Test loss: 2.1806
In this epoch 3850/5000, Training loss: 0.7626, Test loss: 2.1211
In this epoch 3900/5000, Training loss: 0.5901, Test loss: 2.1140
In this epoch 3950/5000, Training loss: 0.5870, Test loss: 2.1125
In this epoch 4000/5000, Training loss: 0.5843, Test loss: 2.1085
In this epoch 4050/5000, Training loss: 0.5818, Test loss: 2.1047
In this epoch 4100/5000, Training loss: 0.5795, Test loss: 2.1011
In this epoch 4150/5000, Training loss: 0.5773, Test loss: 2.0977
In this epoch 4200/5000, Training loss: 0.5753, Test loss: 2.0944
In this epoch 4250/5000, Training loss: 0.5734, Test loss: 2.0913
In this epoch 4300/5000, Training loss: 0.5717, Test loss: 2.0884
In this epoch 4350/5000, Training loss: 0.5701, Test loss: 2.0856
In this epoch 4400/5000, Training loss: 0.5686, Test loss: 2.0829
In this epoch 4450/5000, Training loss: 0.5672, Test loss: 2.0804
In this epoch 4500/5000, Training loss: 0.5660, Test loss: 2.0781
In this epoch 4550/5000, Training loss: 0.5650, Test loss: 2.0736
In this epoch 4600/5000, Training loss: 0.6498, Test loss: 2.0340
In this epoch 4650/5000, Training loss: 0.5670, Test loss: 2.0172
In this epoch 4700/5000, Training loss: 0.5662, Test loss: 2.0128
In this epoch 4750/5000, Training loss: 0.5655, Test loss: 2.0112
In this epoch 4800/5000, Training loss: 0.5649, Test loss: 2.0099
In this epoch 4850/5000, Training loss: 0.5643, Test loss: 2.0086
In this epoch 4900/5000, Training loss: 0.5638, Test loss: 2.0074
In this epoch 4950/5000, Training loss: 0.5633, Test loss: 2.0063
In this epoch 5000/5000, Training loss: 0.5629, Test loss: 2.0053
train_acc 0.3528844074394464
test_acc 0.4028546712802768
{'y_pred': [0.7250490188598633, 1.1356202363967896, 0.9144506454467773, 1.0461933612823486, 0.7250490188598633, 2.5581960678100586, 1.0003328323364258, 0.7874138355255127, 0.7250490188598633, 0.7250490188598633, 2.717484474182129, 0.880786120891571, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.6206992268562317, 1.1680822372436523, 0.5451593399047852, 0.8130226135253906, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 1.028280258178711, 1.0787407159805298, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.9459476470947266, 0.7250490188598633, 4.179020881652832, 1.5026912689208984, 0.6590108871459961, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7805051207542419, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7320241928100586, 0.7250490188598633, 0.7250490188598633, 1.0635831356048584, 0.7250490188598633, 0.5090780258178711, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 1.0659013986587524, 0.9734722375869751, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 1.0076971054077148, 1.346817970275879, 0.9057954549789429, 0.9152469635009766, 0.7250490188598633, -1.84828782081604, 0.7250490188598633, 0.2937641143798828, 0.7250490188598633, 1.6635503768920898, 0.7250490188598633, 1.0451183319091797, 0.7250490188598633, 1.0716142654418945, 0.9999570846557617, 0.7250490188598633, 0.7250490188598633, -0.61590576171875, 3.4217185974121094, 0.7250490188598633, 0.8399366140365601, 0.2929716110229492, 1.037390947341919, 0.2297506332397461, 0.7250490188598633, 1.3238515853881836, 0.7614288330078125, 1.0058507919311523, 1.5204951763153076, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 2.8813438415527344, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.8486395478248596, 0.7250490188598633, 0.6498966217041016, 0.7250490188598633, 0.7250490188598633, 2.5123074054718018, 1.040123701095581, 0.7126026153564453, 2.0077261924743652, 0.7250490188598633, 3.1416175365448, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 1.5874218940734863, 1.1375455856323242, 0.7250490188598633, 0.7250490188598633, 1.1775908470153809, 0.7250490188598633, 0.12495040893554688, 0.7250490188598633, 1.242295265197754, 1.372307300567627, 1.2405176162719727, 0.7468137145042419, 0.7325625419616699, 0.8043779134750366, 0.6391869187355042, 0.6866426467895508, 0.7250490188598633, 0.6232452392578125, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7267999649047852, 2.0201358795166016, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.9476594924926758, 0.7250490188598633, 0.7250490188598633, -0.7402572631835938, 0.9585460424423218, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.49645137786865234, 0.8824596405029297, 1.2847614288330078, 0.7250490188598633, 1.8716845512390137, 0.7250490188598633, 0.7250490188598633, 1.2540054321289062, -0.024949073791503906, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 1.403264045715332, 1.2383009195327759, 0.7250490188598633, 1.1774156093597412, -0.5211811065673828, 0.7250490188598633, 1.4591710567474365, 1.2338037490844727, 0.7250490188598633, 0.7250490188598633, 2.7295279502868652, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 1.6608901023864746, 0.7250490188598633, 0.7250490188598633, 3.7421467304229736, 0.7250490188598633, 0.7250490188598633, 1.1329823732376099, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.1918182373046875, 0.7250490188598633, 0.27531909942626953, 1.2394638061523438, 1.0777287483215332, 2.774935007095337, 0.7250490188598633, 0.7250490188598633, 1.0067863464355469, 0.9404270648956299, 0.7250490188598633, 0.7250490188598633, 0.8610262870788574, 0.7250490188598633, -0.25470733642578125, 0.9157429337501526, 0.7250490188598633, 0.5445268750190735, 1.5612053871154785, 0.7250490188598633, 0.7454361319541931, 1.424290657043457, 0.7250490188598633, 0.9338016510009766, 2.851938247680664, -0.1343669891357422, 0.7250490188598633, 0.7250490188598633, -1.474560260772705, 1.1816911697387695, 1.0196490287780762, 0.727330207824707, 0.7250490188598633, 0.9066495895385742, 0.20822429656982422, 2.1077961921691895, 0.7250490188598633, 1.1370892524719238, 0.5987300872802734, 1.276710867881775, 0.6427364349365234, 0.7250490188598633, 0.22291278839111328, 0.7250490188598633, 1.3290132284164429, 0.22220897674560547, 0.9075223207473755, -0.05448257923126221, 1.3191590309143066, 0.7250490188598633, 0.6842336654663086, 0.48394107818603516, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, -0.033176422119140625, 2.0381507873535156, 0.7250490188598633, 0.3089714050292969, 0.7250490188598633, 0.7020301818847656, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.8871688842773438, 0.7250490188598633, 1.0941272974014282, -1.8352127075195312, 0.9462614059448242, 0.7250490188598633, 0.7250490188598633, 0.38201427459716797, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 1.6235122680664062, 0.9442138671875, 0.7022619247436523, 1.0801163911819458, 0.7250490188598633, 0.9820431470870972, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7289935946464539, 0.9818281531333923, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 2.2699403762817383, 1.5346612930297852, 0.7250490188598633, 0.6179556846618652, -0.08339405059814453, 0.6980113983154297, 0.8877859115600586, 0.7250490188598633, 1.2181806564331055, 0.7250490188598633, 1.6805920600891113, 0.7250490188598633, -0.03708910942077637, 2.3503808975219727, 0.7250490188598633, 0.5861425399780273, 0.657292366027832, 0.7250490188598633, 0.7250490188598633, 0.8682737350463867, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.8724918365478516, 1.897425651550293, 0.7250490188598633, 0.7250490188598633, 0.6040096879005432, 0.7351531982421875, 0.5028448104858398, 1.2858867645263672, 0.7250490188598633, 0.7250490188598633, 1.6877212524414062, 0.05595874786376953, 0.7250490188598633, 0.7250490188598633, 0.7250490188598633, 0.6738195419311523, 0.7250490188598633, 1.1277799606323242, 0.7250490188598633, 1.4808450937271118, 0.7250490188598633, 0.5769882202148438, 1.6123170852661133, 0.7250490188598633, 0.9675251841545105, 2.79082989692688, 0.7250490188598633, 1.0586700439453125, 0.7250490188598633], 'y_pred_int': [1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 3, 1, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1], 'y_true': [0, 4, 0, 3, 5, 1, 0, 0, 0, 0, 0, 0, 4, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 1, 1, 0, 1, 2, 1, 3, 0, 0, 0, 0, 2, 0, 2, 1, 1, 0, 3, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 14, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 1, 3, 0, 0, 1, 1, 1, 0, 2, 0, 0, 2, 3, 1, 2, 1, 0, 0, 0, 1, 0, 1, 2, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 2, 2, 3, 0, 1, 1, 0, 1, 3, 5, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 3, 1, 2, 0, 0, 0, 2, 1, 0, 0, 0, 4, 0, 2, 1, 0, 1, 0, 1, 1, 3, 0, 0, 0, 0, 3, 0, 1, 2, 0, 0, 0, 2, 3, 1, 0, 0, 0, 1, 1, 0, 3, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 2, 2, 3, 0, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 1, 0, 2, 0, 2, 0, 1, 2, 2, 2, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 3, 0, 0, 3, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 1, 1, 3, 0, 1, 1, 1, 1, 3, 7, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 3, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 3, 1, 0, 0, 1, 1, 0, 1, 3, 0, 2, 1, 1, 3, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 2, 1, 0, 0, 2, 5, 0, 1, 0, 1, 1, 1, 3, 1, 0, 0, 1, 2, 0, 0, 3, 0, 0, 3, 1, 2, 1, 0, 1, 2], 'train_acc': 0.3528844074394464, 'test_acc': 0.4028546712802768, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 7.8142, Test loss: 130.2265
In this epoch 100/5000, Training loss: 0.5461, Test loss: 128.8104
In this epoch 150/5000, Training loss: 14.8118, Test loss: 129.6923
In this epoch 200/5000, Training loss: 34.0742, Test loss: 135.1516
In this epoch 250/5000, Training loss: 0.1547, Test loss: 122.2064
In this epoch 300/5000, Training loss: 20.7191, Test loss: 139.2330
In this epoch 350/5000, Training loss: 0.1526, Test loss: 121.2381
In this epoch 400/5000, Training loss: 0.0641, Test loss: 121.3444
In this epoch 450/5000, Training loss: 0.0453, Test loss: 121.4605
In this epoch 500/5000, Training loss: 18.5432, Test loss: 136.7942
In this epoch 550/5000, Training loss: 0.2766, Test loss: 121.4051
In this epoch 600/5000, Training loss: 0.0218, Test loss: 121.4274
In this epoch 650/5000, Training loss: 0.0155, Test loss: 121.5073
In this epoch 700/5000, Training loss: 0.0117, Test loss: 121.5543
In this epoch 750/5000, Training loss: 0.0090, Test loss: 121.5971
In this epoch 800/5000, Training loss: 0.0070, Test loss: 121.6363
In this epoch 850/5000, Training loss: 0.0055, Test loss: 121.6721
In this epoch 900/5000, Training loss: 0.0044, Test loss: 121.7048
In this epoch 950/5000, Training loss: 1.0764, Test loss: 127.5633
In this epoch 1000/5000, Training loss: 1.4473, Test loss: 121.8048
In this epoch 1050/5000, Training loss: 0.0100, Test loss: 121.7292
In this epoch 1100/5000, Training loss: 0.0039, Test loss: 121.7556
In this epoch 1150/5000, Training loss: 0.0032, Test loss: 121.7845
In this epoch 1200/5000, Training loss: 0.0323, Test loss: 121.7383
In this epoch 1250/5000, Training loss: 0.0046, Test loss: 121.9221
In this epoch 1300/5000, Training loss: 0.0022, Test loss: 121.9290
In this epoch 1350/5000, Training loss: 27.0840, Test loss: 156.2838
In this epoch 1400/5000, Training loss: 0.0091, Test loss: 121.7342
In this epoch 1450/5000, Training loss: 0.0046, Test loss: 121.4879
In this epoch 1500/5000, Training loss: 0.0029, Test loss: 121.3174
In this epoch 1550/5000, Training loss: 0.0026, Test loss: 121.3359
In this epoch 1600/5000, Training loss: 0.0024, Test loss: 121.3492
In this epoch 1650/5000, Training loss: 0.0853, Test loss: 121.3608
In this epoch 1700/5000, Training loss: 0.0025, Test loss: 121.3523
In this epoch 1750/5000, Training loss: 0.0022, Test loss: 121.3883
In this epoch 1800/5000, Training loss: 0.5644, Test loss: 129.1556
In this epoch 1850/5000, Training loss: 0.0107, Test loss: 119.9947
In this epoch 1900/5000, Training loss: 0.0202, Test loss: 120.3840
In this epoch 1950/5000, Training loss: 0.0063, Test loss: 120.2666
In this epoch 2000/5000, Training loss: 0.0026, Test loss: 120.2537
In this epoch 2050/5000, Training loss: 0.0024, Test loss: 120.2794
In this epoch 2100/5000, Training loss: 0.0025, Test loss: 120.2841
In this epoch 2150/5000, Training loss: 0.1060, Test loss: 120.1355
In this epoch 2200/5000, Training loss: 0.0029, Test loss: 120.4900
In this epoch 2250/5000, Training loss: 0.0021, Test loss: 120.4737
In this epoch 2300/5000, Training loss: 0.0033, Test loss: 120.5584
In this epoch 2350/5000, Training loss: 0.6425, Test loss: 119.7770
In this epoch 2400/5000, Training loss: 0.0049, Test loss: 118.1866
In this epoch 2450/5000, Training loss: 0.0024, Test loss: 118.3281
In this epoch 2500/5000, Training loss: 0.0022, Test loss: 118.3436
In this epoch 2550/5000, Training loss: 0.2223, Test loss: 118.6296
In this epoch 2600/5000, Training loss: 0.1010, Test loss: 118.1912
In this epoch 2650/5000, Training loss: 0.0027, Test loss: 118.2300
In this epoch 2700/5000, Training loss: 0.0021, Test loss: 118.2178
In this epoch 2750/5000, Training loss: 19.2451, Test loss: 123.2516
In this epoch 2800/5000, Training loss: 0.0482, Test loss: 116.5231
In this epoch 2850/5000, Training loss: 0.0198, Test loss: 116.4029
In this epoch 2900/5000, Training loss: 0.0025, Test loss: 116.6143
In this epoch 2950/5000, Training loss: 0.0022, Test loss: 116.6268
In this epoch 3000/5000, Training loss: 1.0207, Test loss: 116.1841
In this epoch 3050/5000, Training loss: 0.0037, Test loss: 116.6717
In this epoch 3100/5000, Training loss: 0.0021, Test loss: 116.6318
In this epoch 3150/5000, Training loss: 0.0021, Test loss: 116.6395
In this epoch 3200/5000, Training loss: 9.5401, Test loss: 115.0624
In this epoch 3250/5000, Training loss: 0.0423, Test loss: 115.3345
In this epoch 3300/5000, Training loss: 0.0019, Test loss: 115.2700
In this epoch 3350/5000, Training loss: 0.0017, Test loss: 115.2654
In this epoch 3400/5000, Training loss: 0.0016, Test loss: 115.2766
In this epoch 3450/5000, Training loss: 0.0015, Test loss: 115.2848
In this epoch 3500/5000, Training loss: 0.0015, Test loss: 115.2912
In this epoch 3550/5000, Training loss: 0.0015, Test loss: 115.2961
In this epoch 3600/5000, Training loss: 0.0015, Test loss: 115.3000
In this epoch 3650/5000, Training loss: 0.0015, Test loss: 115.3028
In this epoch 3700/5000, Training loss: 184.3736, Test loss: 205.0782
In this epoch 3750/5000, Training loss: 121.8055, Test loss: 181.0607
In this epoch 3800/5000, Training loss: 114.1427, Test loss: 181.7089
In this epoch 3850/5000, Training loss: 110.3001, Test loss: 181.1226
In this epoch 3900/5000, Training loss: 109.4607, Test loss: 180.3020
In this epoch 3950/5000, Training loss: 108.6725, Test loss: 179.5131
In this epoch 4000/5000, Training loss: 107.9187, Test loss: 178.7583
In this epoch 4050/5000, Training loss: 107.1945, Test loss: 178.0337
In this epoch 4100/5000, Training loss: 106.4960, Test loss: 177.3355
In this epoch 4150/5000, Training loss: 105.8203, Test loss: 176.6613
In this epoch 4200/5000, Training loss: 105.1647, Test loss: 176.0084
In this epoch 4250/5000, Training loss: 104.5273, Test loss: 175.3743
In this epoch 4300/5000, Training loss: 103.9061, Test loss: 174.7565
In this epoch 4350/5000, Training loss: 103.2997, Test loss: 174.1531
In this epoch 4400/5000, Training loss: 102.7068, Test loss: 173.5626
In this epoch 4450/5000, Training loss: 102.1263, Test loss: 172.9874
In this epoch 4500/5000, Training loss: 101.5572, Test loss: 172.4243
In this epoch 4550/5000, Training loss: 100.9987, Test loss: 171.8724
In this epoch 4600/5000, Training loss: 100.4499, Test loss: 171.3309
In this epoch 4650/5000, Training loss: 99.9103, Test loss: 170.7998
In this epoch 4700/5000, Training loss: 99.3792, Test loss: 170.2784
In this epoch 4750/5000, Training loss: 98.8561, Test loss: 169.7659
In this epoch 4800/5000, Training loss: 98.3405, Test loss: 169.2608
In this epoch 4850/5000, Training loss: 97.8319, Test loss: 168.7635
In this epoch 4900/5000, Training loss: 97.3301, Test loss: 168.2746
In this epoch 4950/5000, Training loss: 96.8345, Test loss: 167.7944
In this epoch 5000/5000, Training loss: 96.3451, Test loss: 167.3213
train_acc 0.625
test_acc 0.026470588235294117
{'y_pred': [10.27469253540039, 13.54433822631836, 9.573680877685547, 10.274507522583008, 15.631616592407227, 1.8234376907348633, 17.100383758544922, 29.37009048461914, 1.8234376907348633, 17.508434295654297, 14.86302661895752, 9.919281005859375, 1.8234376907348633, 9.749560356140137, 1.8234376907348633, 1.8234376907348633, 16.26337432861328, 1.8234376907348633, 1.8234376907348633, 12.417160034179688, 14.435206413269043, 1.8234376907348633, 14.121601104736328, 1.8234376907348633, 1.8234376907348633, 15.402836799621582, 14.089483261108398, 18.640399932861328, 1.8234376907348633, 12.174442291259766, 1.8234376907348633, 4.9810991287231445, 12.464479446411133, 12.019340515136719, 13.027518272399902, 1.8234376907348633, 1.8234376907348633, 12.00623893737793, 1.8234376907348633, 13.274365425109863, 13.280315399169922, 4.816690444946289, 17.111236572265625, 11.798408508300781, 10.172428131103516, 12.966057777404785, 12.094792366027832, 1.8234376907348633, 16.76828956604004, 1.8234376907348633, 12.62987232208252, 17.922191619873047, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 15.786935806274414, 13.631657600402832, 1.8234376907348633, 19.046096801757812, 1.8234376907348633, 13.494107246398926, 1.8234376907348633, 7.2087016105651855, 14.7706937789917, 15.810805320739746, 1.8234376907348633, 1.8234376907348633, 13.134045600891113, 14.508204460144043, 16.013565063476562, 16.29006576538086, 9.094544410705566, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 16.631359100341797, 11.90446949005127, 1.8234376907348633, 1.8234376907348633, 10.648859977722168, 10.066971778869629, 18.536624908447266, 11.940971374511719, 12.940642356872559, 7.795807838439941, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 14.624858856201172, 11.545938491821289, 13.257855415344238, 14.250519752502441, 1.8234376907348633, 18.376827239990234, 14.474919319152832, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 12.384888648986816, 11.535913467407227, 16.263080596923828, 1.8234376907348633, 1.8234376907348633, 17.069629669189453, 1.8234376907348633, 18.574813842773438, 1.8234376907348633, 16.01000213623047, 12.779730796813965, 1.8234376907348633, 1.8234376907348633, 17.00286102294922, 1.8234376907348633, 1.8234376907348633, 27.279109954833984, 11.32080364227295, 14.420162200927734, 13.637574195861816, 19.673301696777344, 13.389599800109863, 1.8234376907348633, 15.627663612365723, 7.4095458984375, 1.8234376907348633, 14.43664264678955, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 17.384872436523438, 11.899309158325195, 14.136236190795898, 1.8234376907348633, 12.831883430480957, 1.8234376907348633, 13.966439247131348, 1.8234376907348633, 13.471166610717773, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 12.079351425170898, 1.8234376907348633, 1.8234376907348633, 18.846031188964844, 12.677567481994629, 14.44765567779541, 15.157736778259277, 1.8234376907348633, 1.8234376907348633, 6.391484260559082, 10.63755989074707, 13.926992416381836, 14.38748836517334, 1.8234376907348633, 1.8234376907348633, 9.489856719970703, 19.503822326660156, 1.8234376907348633, 1.8234376907348633, 16.470947265625, 10.520426750183105, 1.8234376907348633, 18.899551391601562, 1.8234376907348633, 1.8234376907348633, 14.773382186889648, 1.8234376907348633, 1.8234376907348633, 16.117992401123047, 19.630725860595703, 1.8234376907348633, 1.8234376907348633, 13.7142915725708, 10.391793251037598, 20.202991485595703, 1.8234376907348633, 10.921679496765137, 1.8234376907348633, 1.8234376907348633, 18.90021514892578, 17.962169647216797, 16.428539276123047, 14.361836433410645, 1.8234376907348633, 14.89806842803955, 1.8234376907348633, 1.8234376907348633, 16.84506607055664, 5.79642915725708, 13.880064964294434, 14.644319534301758, 1.8234376907348633, 11.542093276977539, 1.8234376907348633, 13.852252006530762, 4.093005180358887, 15.724347114562988, 16.190868377685547, 16.48211097717285, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 16.106098175048828, 16.776901245117188, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 13.122227668762207, 1.8234376907348633, 17.160736083984375, 1.8234376907348633, 11.169581413269043, 17.467788696289062, 10.913701057434082, 1.8234376907348633, 13.193880081176758, 1.8234376907348633, 1.8234376907348633, 13.746829986572266, 10.091134071350098, 15.218871116638184, 1.8234376907348633, 15.709054946899414, 8.896697044372559, 16.256526947021484, 1.8234376907348633, 10.872243881225586, 17.33771324157715, 15.594643592834473, 15.553141593933105, 16.984302520751953, 12.165319442749023, 16.946144104003906, 7.431331634521484, 1.8234376907348633, 10.621519088745117, 16.439594268798828, 13.47403335571289, 15.682585716247559, 13.638666152954102, 9.079676628112793, 1.8234376907348633, 1.8234376907348633, 10.00893497467041, 13.743047714233398, 38.375675201416016, 1.8234376907348633, 14.319348335266113, 16.694488525390625, 13.104743003845215, 11.636127471923828, 1.8234376907348633, 1.8234376907348633, 11.149369239807129, 15.56924819946289, 13.17607593536377, 16.470720291137695, 11.675932884216309, 13.346539497375488, 16.04393768310547, 12.736719131469727, 13.98254108428955, 2.265632152557373, 1.8234376907348633, 1.8234376907348633, 1.8234376907348633, 12.485315322875977, 1.8234376907348633, 12.067426681518555, 14.314507484436035, 16.851421356201172, 1.8234376907348633, 13.10448169708252, 12.790358543395996, 16.48250961303711, 11.09567928314209, 12.987325668334961, 1.8234376907348633, 8.960247039794922, 12.656936645507812, 1.8234376907348633, 15.42813491821289, 1.8234376907348633, 13.024506568908691, 1.8234376907348633, 14.407133102416992, 17.079504013061523, 1.8234376907348633, 14.532137870788574, 1.8234376907348633, 17.897441864013672, 14.098835945129395, 12.389657020568848, 1.8234376907348633, 1.8234376907348633, 12.047845840454102, 1.8234376907348633, 8.395721435546875, 17.281898498535156, 14.490484237670898, 16.05887794494629, 15.2297945022583, 1.8234376907348633, 15.442891120910645, 15.821271896362305, 1.8234376907348633, 16.114093780517578, 1.8234376907348633, 13.917487144470215, 1.8234376907348633, 1.8234376907348633, 11.58232307434082, 14.916534423828125, 15.092503547668457, 8.780946731567383, 13.186833381652832, 7.135542392730713, 16.698209762573242, 13.446642875671387, 12.304875373840332, 20.87125015258789, 15.483190536499023, 15.747601509094238, 1.8234376907348633, 11.462281227111816, 1.8234376907348633, 13.972302436828613, 1.8234376907348633, 19.175884246826172, 15.530940055847168, 11.64983081817627, 16.889652252197266], 'y_pred_int': [10, 14, 10, 10, 16, 2, 17, 29, 2, 18, 15, 10, 2, 10, 2, 2, 16, 2, 2, 12, 14, 2, 14, 2, 2, 15, 14, 19, 2, 12, 2, 5, 12, 12, 13, 2, 2, 12, 2, 13, 13, 5, 17, 12, 10, 13, 12, 2, 17, 2, 13, 18, 2, 2, 2, 2, 16, 14, 2, 19, 2, 13, 2, 7, 15, 16, 2, 2, 13, 15, 16, 16, 9, 2, 2, 2, 17, 12, 2, 2, 11, 10, 19, 12, 13, 8, 2, 2, 2, 15, 12, 13, 14, 2, 18, 14, 2, 2, 2, 12, 12, 16, 2, 2, 17, 2, 19, 2, 16, 13, 2, 2, 17, 2, 2, 27, 11, 14, 14, 20, 13, 2, 16, 7, 2, 14, 2, 2, 2, 2, 2, 2, 17, 12, 14, 2, 13, 2, 14, 2, 13, 2, 2, 2, 12, 2, 2, 19, 13, 14, 15, 2, 2, 6, 11, 14, 14, 2, 2, 9, 20, 2, 2, 16, 11, 2, 19, 2, 2, 15, 2, 2, 16, 20, 2, 2, 14, 10, 20, 2, 11, 2, 2, 19, 18, 16, 14, 2, 15, 2, 2, 17, 6, 14, 15, 2, 12, 2, 14, 4, 16, 16, 16, 2, 2, 2, 16, 17, 2, 2, 2, 2, 2, 2, 2, 13, 2, 17, 2, 11, 17, 11, 2, 13, 2, 2, 14, 10, 15, 2, 16, 9, 16, 2, 11, 17, 16, 16, 17, 12, 17, 7, 2, 11, 16, 13, 16, 14, 9, 2, 2, 10, 14, 38, 2, 14, 17, 13, 12, 2, 2, 11, 16, 13, 16, 12, 13, 16, 13, 14, 2, 2, 2, 2, 12, 2, 12, 14, 17, 2, 13, 13, 16, 11, 13, 2, 9, 13, 2, 15, 2, 13, 2, 14, 17, 2, 15, 2, 18, 14, 12, 2, 2, 12, 2, 8, 17, 14, 16, 15, 2, 15, 16, 2, 16, 2, 14, 2, 2, 12, 15, 15, 9, 13, 7, 17, 13, 12, 21, 15, 16, 2, 11, 2, 14, 2, 19, 16, 12, 17], 'y_true': [14, 1, 18, 14, 9, 20, 6, 9, 12, 21, 6, 9, 19, 6, 33, 21, 15, 22, 100, 13, 7, 10, 12, 6, 5, 8, 21, 23, 4, 9, 3, 5, 8, 10, 5, 6, 19, 16, 10, 6, 7, 15, 6, 11, 25, 34, 5, 21, 15, 12, 7, 18, 19, 12, 8, 16, 6, 8, 23, 8, 20, 31, 13, 35, 5, 19, 30, 5, 5, 18, 17, 2, 22, 18, 11, 12, 7, 5, 4, 9, 13, 8, 15, 15, 14, 10, 16, 5, 4, 9, 14, 2, 11, 10, 10, 14, 8, 2, 39, 12, 15, 35, 21, 33, 18, 16, 20, 8, 8, 3, 12, 4, 7, 14, 6, 19, 13, 19, 43, 7, 8, 16, 4, 12, 3, 6, 15, 4, 10, 29, 15, 7, 30, 31, 5, 8, 18, 14, 16, 11, 5, 11, 11, 14, 12, 10, 9, 11, 6, 53, 46, 6, 12, 15, 12, 37, 22, 21, 20, 6, 9, 6, 23, 25, 14, 8, 12, 11, 15, 17, 7, 13, 7, 16, 19, 4, 21, 46, 14, 10, 11, 11, 14, 13, 15, 15, 26, 11, 9, 7, 3, 22, 10, 10, 23, 10, 49, 38, 21, 12, 13, 6, 15, 6, 10, 24, 18, 23, 19, 15, 22, 5, 8, 10, 17, 32, 8, 4, 19, 9, 6, 7, 12, 13, 2, 16, 7, 35, 3, 7, 8, 19, 22, 10, 25, 2, 16, 6, 15, 3, 32, 4, 20, 11, 5, 15, 9, 11, 11, 9, 10, 8, 26, 16, 8, 23, 6, 15, 3, 42, 18, 11, 5, 15, 15, 17, 14, 16, 20, 27, 26, 5, 18, 13, 18, 29, 5, 3, 10, 18, 21, 7, 25, 25, 10, 12, 23, 9, 8, 9, 16, 19, 5, 15, 23, 12, 6, 36, 11, 8, 7, 26, 10, 12, 20, 3, 29, 19, 15, 7, 22, 12, 14, 19, 17, 9, 15, 11, 4, 2, 13, 7, 10, 8, 14, 7, 8, 3, 32, 8, 5, 11, 20, 12, 11, 3, 3, 18, 10, 2], 'train_acc': 0.625, 'test_acc': 0.026470588235294117, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_GTT_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 3028.8242, Test loss: 49389.5547
In this epoch 100/5000, Training loss: 90.0157, Test loss: 49513.8711
In this epoch 150/5000, Training loss: 53.4829, Test loss: 49449.5664
In this epoch 200/5000, Training loss: 35.4545, Test loss: 49400.9141
In this epoch 250/5000, Training loss: 24.0347, Test loss: 49353.1016
In this epoch 300/5000, Training loss: 17.8055, Test loss: 49322.5117
In this epoch 350/5000, Training loss: 14.9906, Test loss: 49302.7539
In this epoch 400/5000, Training loss: 13.9412, Test loss: 49292.7109
In this epoch 450/5000, Training loss: 13.6171, Test loss: 49286.9609
In this epoch 500/5000, Training loss: 13.5333, Test loss: 49284.0820
In this epoch 550/5000, Training loss: 26.4709, Test loss: 49291.5078
In this epoch 600/5000, Training loss: 13.6365, Test loss: 49300.3789
In this epoch 650/5000, Training loss: 13.5115, Test loss: 49308.0312
In this epoch 700/5000, Training loss: 13.5107, Test loss: 49307.7461
In this epoch 750/5000, Training loss: 36.2958, Test loss: 49166.0234
In this epoch 800/5000, Training loss: 13.8501, Test loss: 49228.6406
In this epoch 850/5000, Training loss: 13.5140, Test loss: 49262.4727
In this epoch 900/5000, Training loss: 13.5108, Test loss: 49259.1914
In this epoch 950/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1000/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1050/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1100/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1150/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1200/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1250/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1300/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1350/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1400/5000, Training loss: 13.5107, Test loss: 49259.3867
In this epoch 1450/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1500/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1550/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1600/5000, Training loss: 13.5107, Test loss: 49259.3906
In this epoch 1650/5000, Training loss: 650.4763, Test loss: 49921.9648
In this epoch 1700/5000, Training loss: 20.4012, Test loss: 49171.6172
In this epoch 1750/5000, Training loss: 14.3611, Test loss: 49107.5859
In this epoch 1800/5000, Training loss: 13.6806, Test loss: 49105.3477
In this epoch 1850/5000, Training loss: 13.5415, Test loss: 49102.4609
In this epoch 1900/5000, Training loss: 13.5158, Test loss: 49100.8906
In this epoch 1950/5000, Training loss: 13.5115, Test loss: 49100.3555
In this epoch 2000/5000, Training loss: 13.5108, Test loss: 49100.1172
In this epoch 2050/5000, Training loss: 13.5107, Test loss: 49100.0312
In this epoch 2100/5000, Training loss: 13.5107, Test loss: 49099.9961
In this epoch 2150/5000, Training loss: 13.5107, Test loss: 49099.9883
In this epoch 2200/5000, Training loss: 13.5107, Test loss: 49099.9844
In this epoch 2250/5000, Training loss: 13.5107, Test loss: 49099.9844
In this epoch 2300/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2350/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2400/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2450/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2500/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2550/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2600/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2650/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2700/5000, Training loss: 13.5107, Test loss: 49099.9766
In this epoch 2750/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2800/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2850/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2900/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 2950/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3000/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3050/5000, Training loss: 13.5107, Test loss: 49099.9766
In this epoch 3100/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3150/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3200/5000, Training loss: 13.5107, Test loss: 49099.9766
In this epoch 3250/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3300/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3350/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3400/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3450/5000, Training loss: 13.5107, Test loss: 49099.9766
In this epoch 3500/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3550/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3600/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3650/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3700/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3750/5000, Training loss: 13.5107, Test loss: 49099.9766
In this epoch 3800/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3850/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3900/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 3950/5000, Training loss: 13.5107, Test loss: 49099.9844
In this epoch 4000/5000, Training loss: 13.5107, Test loss: 49099.9844
In this epoch 4050/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 4100/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 4150/5000, Training loss: 13.5107, Test loss: 49099.9805
In this epoch 4200/5000, Training loss: 13.5107, Test loss: 49099.9844
In this epoch 4250/5000, Training loss: 107.3557, Test loss: 49576.5078
In this epoch 4300/5000, Training loss: 14.0781, Test loss: 49270.1133
In this epoch 4350/5000, Training loss: 13.5133, Test loss: 49267.1133
In this epoch 4400/5000, Training loss: 13.5107, Test loss: 49267.1133
In this epoch 4450/5000, Training loss: 13.5107, Test loss: 49267.1055
In this epoch 4500/5000, Training loss: 13.5107, Test loss: 49267.1094
In this epoch 4550/5000, Training loss: 13.5107, Test loss: 49267.1055
In this epoch 4600/5000, Training loss: 13.5107, Test loss: 49267.1094
In this epoch 4650/5000, Training loss: 13.5107, Test loss: 49267.1094
In this epoch 4700/5000, Training loss: 13.5107, Test loss: 49267.1133
In this epoch 4750/5000, Training loss: 13.5107, Test loss: 49267.1055
In this epoch 4800/5000, Training loss: 13.5107, Test loss: 49267.1094
In this epoch 4850/5000, Training loss: 13.5107, Test loss: 49267.1133
In this epoch 4900/5000, Training loss: 13.5107, Test loss: 49267.1133
In this epoch 4950/5000, Training loss: 13.5107, Test loss: 49267.1094
In this epoch 5000/5000, Training loss: 13.5107, Test loss: 49267.1133
train_acc 0.9970588235294118
test_acc 0.0
{'y_pred': [261.1636047363281, 304.74530029296875, 337.7431335449219, 415.0513610839844, 401.1236572265625, 403.5679626464844, 309.4178771972656, 249.46389770507812, 378.702392578125, 388.9059143066406, 443.6553955078125, 418.9819641113281, 276.2718505859375, 263.9585266113281, 357.5287780761719, 317.06719970703125, 295.48797607421875, 448.2163391113281, 339.258544921875, 345.89813232421875, 234.34310913085938, 460.0203552246094, 405.1197509765625, 338.6617126464844, 286.46337890625, 353.347412109375, 400.1266784667969, 415.43328857421875, 549.0035400390625, 369.2449035644531, 402.3480529785156, 429.919677734375, 239.12977600097656, 451.6094055175781, 367.1198425292969, 274.5838623046875, 359.68701171875, 406.3061828613281, 383.95556640625, 405.7526550292969, 274.130859375, 312.2255859375, 266.6326904296875, 425.3597717285156, 353.5240783691406, 331.8683166503906, 331.90582275390625, 474.2732238769531, 343.4075012207031, 333.40557861328125, 329.4917907714844, 378.4068603515625, 305.4655456542969, 360.62298583984375, 346.8730163574219, 404.2981872558594, 226.5305633544922, 386.8432922363281, 256.73382568359375, 187.0653533935547, 260.46405029296875, 414.9513244628906, 333.5263671875, 444.7961120605469, 380.4153747558594, 506.2882995605469, 400.306640625, 244.2871856689453, 300.7373352050781, 405.1106872558594, 433.5876770019531, 361.7025451660156, 361.39337158203125, 321.664794921875, 469.23968505859375, 483.6791687011719, 336.4009094238281, 276.96588134765625, 296.7788391113281, 312.9796447753906, 405.49859619140625, 377.2357177734375, 202.3766326904297, 291.3251037597656, 381.079345703125, 422.0855407714844, 379.0273132324219, 340.5762634277344, 371.2276306152344, 303.49273681640625, 380.4180603027344, 349.3186340332031, 322.68426513671875, 466.9810791015625, 299.6199645996094, 280.0748596191406, 367.0948486328125, 327.8431396484375, 338.2066955566406, 511.8335266113281, 330.2131652832031, 401.5049133300781, 284.75750732421875, 336.7742919921875, 365.0645446777344, 388.8359375, 281.4412536621094, 203.85853576660156, 463.4137268066406, 358.8968200683594, 347.2364501953125, 559.02783203125, 421.7046203613281, 386.8476867675781, 328.86895751953125, 396.7394714355469, 364.77655029296875, 338.9530029296875, 298.9732666015625, 325.80615234375, 456.4164123535156, 314.7178955078125, 344.48553466796875, 416.0416564941406, 381.3304748535156, 259.8022766113281, 260.0534362792969, 368.4481506347656, 352.6183776855469, 373.80682373046875, 323.4262390136719, 293.7408752441406, 351.2055969238281, 557.05419921875, 307.1465148925781, 335.7937316894531, 405.3302307128906, 439.4068298339844, 420.1313781738281, 288.8965759277344, 289.9081726074219, 371.6677551269531, 356.3875427246094, 140.3158416748047, 300.7261047363281, 349.1220703125, 242.8135528564453, 457.6327209472656, 319.2607116699219, 229.58677673339844, 202.67323303222656, 377.3413391113281, 397.9832458496094, 460.517578125, 474.06646728515625, 499.9429626464844, 498.4444274902344, 421.7180480957031, 268.7062072753906, 191.16920471191406, 320.3177185058594, 426.9516296386719, 431.0191650390625, 326.1788635253906, 276.4120788574219, 400.1235656738281, 277.2539978027344, 324.0012512207031, 264.77252197265625, 381.09942626953125, 299.6714172363281, 246.10899353027344, 367.7586364746094, 328.7630920410156, 396.055419921875, 303.4818420410156, 454.2388610839844, 332.9841003417969, 280.9702453613281, 404.0445556640625, 243.05812072753906, 379.4405212402344, 258.985595703125, 316.3523254394531, 361.1065979003906, 258.650146484375, 345.96478271484375, 381.8924255371094, 302.5840759277344, 245.07534790039062, 396.8003234863281, 392.4323425292969, 400.5166015625, 349.5873718261719, 335.70977783203125, 426.33868408203125, 369.9761047363281, 385.7123107910156, 390.77447509765625, 299.54058837890625, 426.58551025390625, 332.0535583496094, 369.9058532714844, 219.69728088378906, 284.3560485839844, 310.97906494140625, 301.8176574707031, 285.8653564453125, 510.828369140625, 355.6063537597656, 374.63372802734375, 370.56976318359375, 392.2229309082031, 282.9183044433594, 250.75880432128906, 292.9135437011719, 344.051513671875, 456.99615478515625, 295.9755554199219, 377.63092041015625, 255.7509307861328, 307.52886962890625, 271.873046875, 377.5196838378906, 320.7845764160156, 289.77447509765625, 441.9491271972656, 468.40618896484375, 262.17291259765625, 338.47802734375, 265.697265625, 403.6238098144531, 502.8946838378906, 397.6356201171875, 343.72418212890625, 468.8774719238281, 407.3988037109375, 420.0704040527344, 316.7753601074219, 333.0799560546875, 374.00848388671875, 358.99169921875, 391.3500671386719, 342.2333679199219, 313.5619812011719, 301.6805419921875, 372.6307373046875, 307.1982727050781, 396.191162109375, 323.3448791503906, 419.5901794433594, 368.9231262207031, 337.99127197265625, 209.22865295410156, 281.5928039550781, 588.8875732421875, 450.0984191894531, 313.8658447265625, 427.6079406738281, 255.21873474121094, 318.2281188964844, 339.5024108886719, 264.32379150390625, 289.9103698730469, 432.82415771484375, 456.0998840332031, 480.9687194824219, 320.7310485839844, 296.3625183105469, 440.1925354003906, 390.7342224121094, 442.0728454589844, 357.4251708984375, 351.9360656738281, 321.77569580078125, 442.0277404785156, 533.252197265625, 401.4402160644531, 444.6042175292969, 389.65643310546875, 396.3597717285156, 381.191162109375, 250.1954345703125, 513.5437622070312, 348.6586608886719, 310.81097412109375, 245.0323486328125, 428.8929443359375, 339.8685302734375, 397.6719055175781, 419.4033508300781, 399.43994140625, 351.8626708984375, 456.1310729980469, 256.928955078125, 291.1841735839844, 293.73602294921875, 422.3227233886719, 300.4250793457031, 280.450927734375, 434.4683532714844, 369.68603515625, 234.13014221191406, 415.0533752441406, 334.1971740722656, 490.7794189453125, 232.8538360595703, 407.3841552734375, 326.8838195800781, 340.8656311035156, 453.2039489746094, 168.71292114257812, 305.90130615234375, 298.97735595703125, 234.27638244628906, 331.7645263671875, 338.2435607910156, 294.0395812988281, 328.19061279296875, 438.50653076171875, 423.5058898925781, 457.13897705078125, 341.4726867675781, 260.51409912109375, 212.8747100830078, 252.53062438964844, 424.2897033691406, 321.7700500488281, 439.9997253417969, 242.5153045654297, 293.5419006347656, 404.5136413574219, 274.65032958984375, 216.4253387451172, 311.8492126464844, 431.1944885253906, 304.0382385253906, 329.0704650878906, 258.38055419921875, 362.7840270996094], 'y_pred_int': [261, 305, 338, 415, 401, 404, 309, 249, 379, 389, 444, 419, 276, 264, 358, 317, 295, 448, 339, 346, 234, 460, 405, 339, 286, 353, 400, 415, 549, 369, 402, 430, 239, 452, 367, 275, 360, 406, 384, 406, 274, 312, 267, 425, 354, 332, 332, 474, 343, 333, 329, 378, 305, 361, 347, 404, 227, 387, 257, 187, 260, 415, 334, 445, 380, 506, 400, 244, 301, 405, 434, 362, 361, 322, 469, 484, 336, 277, 297, 313, 405, 377, 202, 291, 381, 422, 379, 341, 371, 303, 380, 349, 323, 467, 300, 280, 367, 328, 338, 512, 330, 402, 285, 337, 365, 389, 281, 204, 463, 359, 347, 559, 422, 387, 329, 397, 365, 339, 299, 326, 456, 315, 344, 416, 381, 260, 260, 368, 353, 374, 323, 294, 351, 557, 307, 336, 405, 439, 420, 289, 290, 372, 356, 140, 301, 349, 243, 458, 319, 230, 203, 377, 398, 461, 474, 500, 498, 422, 269, 191, 320, 427, 431, 326, 276, 400, 277, 324, 265, 381, 300, 246, 368, 329, 396, 303, 454, 333, 281, 404, 243, 379, 259, 316, 361, 259, 346, 382, 303, 245, 397, 392, 401, 350, 336, 426, 370, 386, 391, 300, 427, 332, 370, 220, 284, 311, 302, 286, 511, 356, 375, 371, 392, 283, 251, 293, 344, 457, 296, 378, 256, 308, 272, 378, 321, 290, 442, 468, 262, 338, 266, 404, 503, 398, 344, 469, 407, 420, 317, 333, 374, 359, 391, 342, 314, 302, 373, 307, 396, 323, 420, 369, 338, 209, 282, 589, 450, 314, 428, 255, 318, 340, 264, 290, 433, 456, 481, 321, 296, 440, 391, 442, 357, 352, 322, 442, 533, 401, 445, 390, 396, 381, 250, 514, 349, 311, 245, 429, 340, 398, 419, 399, 352, 456, 257, 291, 294, 422, 300, 280, 434, 370, 234, 415, 334, 491, 233, 407, 327, 341, 453, 169, 306, 299, 234, 332, 338, 294, 328, 439, 424, 457, 341, 261, 213, 253, 424, 322, 440, 243, 294, 405, 275, 216, 312, 431, 304, 329, 258, 363], 'y_true': [200, 307, 633, 795, 260, 502, 298, 291, 660, 363, 248, 302, 438, 103, 96, 175, 440, 81, 168, 375, 135, 833, 173, 279, 167, 545, 224, 150, 212, 490, 330, 196, 492, 588, 484, 180, 393, 482, 162, 203, 129, 382, 417, 443, 912, 58, 392, 355, 588, 228, 235, 276, 407, 326, 376, 95, 351, 552, 302, 335, 400, 197, 241, 65, 162, 164, 259, 101, 61, 901, 231, 752, 254, 481, 244, 157, 643, 100, 312, 200, 686, 207, 332, 277, 175, 103, 187, 173, 447, 731, 163, 203, 462, 230, 64, 203, 60, 434, 351, 301, 338, 542, 93, 207, 45, 324, 587, 493, 236, 527, 233, 382, 258, 191, 185, 493, 317, 743, 544, 82, 187, 585, 599, 285, 457, 254, 562, 112, 466, 299, 1342, 769, 379, 756, 317, 699, 364, 376, 251, 194, 333, 214, 207, 112, 162, 209, 90, 444, 528, 507, 300, 144, 177, 559, 273, 130, 373, 616, 311, 278, 415, 396, 318, 63, 697, 440, 94, 242, 129, 339, 786, 280, 175, 370, 252, 232, 259, 347, 237, 688, 681, 171, 168, 430, 211, 521, 331, 158, 412, 233, 76, 119, 424, 111, 414, 332, 161, 331, 310, 889, 514, 407, 417, 248, 351, 333, 337, 216, 257, 676, 726, 445, 185, 347, 350, 270, 387, 222, 577, 689, 527, 169, 511, 176, 214, 404, 166, 840, 147, 590, 124, 273, 857, 207, 226, 166, 572, 516, 105, 329, 135, 517, 463, 89, 342, 180, 258, 441, 431, 242, 235, 319, 205, 350, 191, 790, 239, 173, 473, 318, 701, 440, 122, 109, 428, 366, 234, 237, 286, 738, 324, 212, 200, 189, 441, 297, 133, 158, 415, 431, 332, 546, 577, 725, 539, 415, 350, 216, 209, 683, 556, 503, 266, 174, 351, 139, 649, 88, 104, 142, 204, 236, 513, 361, 337, 200, 176, 1016, 445, 714, 156, 244, 446, 88, 156, 240, 316, 62, 230, 709, 527, 935, 192, 1006, 796, 350, 401, 290, 901, 202, 867, 156, 103, 371, 565, 404, 797, 458, 580, 1044], 'train_acc': 0.9970588235294118, 'test_acc': 0.0, 'X': 'X_GTT_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 0.0761, Test loss: 1.7521
In this epoch 100/5000, Training loss: 0.0310, Test loss: 1.7692
In this epoch 150/5000, Training loss: 0.0304, Test loss: 1.7700
In this epoch 200/5000, Training loss: 0.0304, Test loss: 1.7695
In this epoch 250/5000, Training loss: 0.0546, Test loss: 1.7753
In this epoch 300/5000, Training loss: 0.0314, Test loss: 1.7514
In this epoch 350/5000, Training loss: 0.0308, Test loss: 1.7477
In this epoch 400/5000, Training loss: 0.0841, Test loss: 1.7355
In this epoch 450/5000, Training loss: 0.0328, Test loss: 1.7444
In this epoch 500/5000, Training loss: 0.0335, Test loss: 1.7322
In this epoch 550/5000, Training loss: 0.0347, Test loss: 1.7319
In this epoch 600/5000, Training loss: 0.0352, Test loss: 1.7236
In this epoch 650/5000, Training loss: 0.0305, Test loss: 1.7206
In this epoch 700/5000, Training loss: 0.1532, Test loss: 1.6910
In this epoch 750/5000, Training loss: 0.0307, Test loss: 1.7020
In this epoch 800/5000, Training loss: 0.0786, Test loss: 1.7119
In this epoch 850/5000, Training loss: 0.0356, Test loss: 1.6562
In this epoch 900/5000, Training loss: 0.0309, Test loss: 1.6556
In this epoch 950/5000, Training loss: 0.0348, Test loss: 1.6526
In this epoch 1000/5000, Training loss: 0.0702, Test loss: 1.5740
In this epoch 1050/5000, Training loss: 0.0437, Test loss: 1.5703
In this epoch 1100/5000, Training loss: 0.0415, Test loss: 1.5819
In this epoch 1150/5000, Training loss: 0.0393, Test loss: 1.5747
In this epoch 1200/5000, Training loss: 0.0601, Test loss: 1.5514
In this epoch 1250/5000, Training loss: 0.0392, Test loss: 1.5338
In this epoch 1300/5000, Training loss: 0.0390, Test loss: 1.5322
In this epoch 1350/5000, Training loss: 0.0476, Test loss: 1.5296
In this epoch 1400/5000, Training loss: 0.0430, Test loss: 1.5196
In this epoch 1450/5000, Training loss: 0.5642, Test loss: 1.7716
In this epoch 1500/5000, Training loss: 0.0493, Test loss: 1.5422
In this epoch 1550/5000, Training loss: 0.0479, Test loss: 1.5372
In this epoch 1600/5000, Training loss: 0.0528, Test loss: 1.5402
In this epoch 1650/5000, Training loss: 0.1561, Test loss: 1.5758
In this epoch 1700/5000, Training loss: 0.0480, Test loss: 1.5139
In this epoch 1750/5000, Training loss: 0.0511, Test loss: 1.5108
In this epoch 1800/5000, Training loss: 0.0679, Test loss: 1.4969
In this epoch 1850/5000, Training loss: 0.0514, Test loss: 1.4924
In this epoch 1900/5000, Training loss: 0.0474, Test loss: 1.4894
In this epoch 1950/5000, Training loss: 0.0615, Test loss: 1.4887
In this epoch 2000/5000, Training loss: 0.0506, Test loss: 1.4906
In this epoch 2050/5000, Training loss: 0.1014, Test loss: 1.4890
In this epoch 2100/5000, Training loss: 0.0476, Test loss: 1.4900
In this epoch 2150/5000, Training loss: 0.0475, Test loss: 1.4914
In this epoch 2200/5000, Training loss: 0.0495, Test loss: 1.4235
In this epoch 2250/5000, Training loss: 0.0379, Test loss: 1.4083
In this epoch 2300/5000, Training loss: 0.0784, Test loss: 1.4050
In this epoch 2350/5000, Training loss: 0.0364, Test loss: 1.4019
In this epoch 2400/5000, Training loss: 0.0413, Test loss: 1.3958
In this epoch 2450/5000, Training loss: 0.0578, Test loss: 1.3959
In this epoch 2500/5000, Training loss: 0.0353, Test loss: 1.3964
In this epoch 2550/5000, Training loss: 0.1250, Test loss: 1.4459
In this epoch 2600/5000, Training loss: 0.0355, Test loss: 1.3878
In this epoch 2650/5000, Training loss: 0.0539, Test loss: 1.3794
In this epoch 2700/5000, Training loss: 0.0364, Test loss: 1.3800
In this epoch 2750/5000, Training loss: 0.0407, Test loss: 1.3752
In this epoch 2800/5000, Training loss: 0.0351, Test loss: 1.3768
In this epoch 2850/5000, Training loss: 0.0351, Test loss: 1.3773
In this epoch 2900/5000, Training loss: 0.0347, Test loss: 1.3665
In this epoch 2950/5000, Training loss: 0.0610, Test loss: 1.4282
In this epoch 3000/5000, Training loss: 0.0437, Test loss: 1.3726
In this epoch 3050/5000, Training loss: 0.0425, Test loss: 1.3697
In this epoch 3100/5000, Training loss: 0.0455, Test loss: 1.3688
In this epoch 3150/5000, Training loss: 0.0426, Test loss: 1.3554
In this epoch 3200/5000, Training loss: 0.0398, Test loss: 1.3567
In this epoch 3250/5000, Training loss: 0.0449, Test loss: 1.3601
In this epoch 3300/5000, Training loss: 0.0408, Test loss: 1.3711
In this epoch 3350/5000, Training loss: 0.0273, Test loss: 1.3618
In this epoch 3400/5000, Training loss: 0.0261, Test loss: 1.3554
In this epoch 3450/5000, Training loss: 0.0251, Test loss: 1.3680
In this epoch 3500/5000, Training loss: 0.0417, Test loss: 1.3727
In this epoch 3550/5000, Training loss: 0.0290, Test loss: 1.3740
In this epoch 3600/5000, Training loss: 0.0291, Test loss: 1.3719
In this epoch 3650/5000, Training loss: 0.0329, Test loss: 1.3603
In this epoch 3700/5000, Training loss: 0.0291, Test loss: 1.3648
In this epoch 3750/5000, Training loss: 0.0375, Test loss: 1.3689
In this epoch 3800/5000, Training loss: 0.0399, Test loss: 1.3628
In this epoch 3850/5000, Training loss: 0.0312, Test loss: 1.3586
In this epoch 3900/5000, Training loss: 0.0284, Test loss: 1.3605
In this epoch 3950/5000, Training loss: 0.0315, Test loss: 1.3635
In this epoch 4000/5000, Training loss: 0.0685, Test loss: 1.3575
In this epoch 4050/5000, Training loss: 0.0339, Test loss: 1.3519
In this epoch 4100/5000, Training loss: 0.0320, Test loss: 1.3642
In this epoch 4150/5000, Training loss: 0.0367, Test loss: 1.3652
In this epoch 4200/5000, Training loss: 0.0320, Test loss: 1.3671
In this epoch 4250/5000, Training loss: 0.0539, Test loss: 1.3641
In this epoch 4300/5000, Training loss: 0.0314, Test loss: 1.3646
In this epoch 4350/5000, Training loss: 0.0850, Test loss: 1.3639
In this epoch 4400/5000, Training loss: 0.0342, Test loss: 1.3711
In this epoch 4450/5000, Training loss: 0.0431, Test loss: 1.3706
In this epoch 4500/5000, Training loss: 0.0284, Test loss: 1.3409
In this epoch 4550/5000, Training loss: 0.0367, Test loss: 1.3324
In this epoch 4600/5000, Training loss: 0.0338, Test loss: 1.3257
In this epoch 4650/5000, Training loss: 0.0340, Test loss: 1.3398
In this epoch 4700/5000, Training loss: 0.0358, Test loss: 1.3363
In this epoch 4750/5000, Training loss: 0.0304, Test loss: 1.3292
In this epoch 4800/5000, Training loss: 0.0319, Test loss: 1.3197
In this epoch 4850/5000, Training loss: 0.0357, Test loss: 1.3467
In this epoch 4900/5000, Training loss: 0.0271, Test loss: 1.3372
In this epoch 4950/5000, Training loss: 0.0273, Test loss: 1.3374
In this epoch 5000/5000, Training loss: 0.0280, Test loss: 1.3357
train_acc 0.3455298442906574
test_acc 0.44459342560553633
{'y_pred': [0.8774895668029785, 0.8249946236610413, 0.3758816421031952, -0.3439960479736328, 0.8172435760498047, 0.6604199409484863, 0.6897258758544922, 0.5447402596473694, 0.4897860884666443, 0.6478315591812134, 0.6047487258911133, 0.4718306064605713, 0.29404085874557495, 0.7536544799804688, 0.35241439938545227, -0.188368558883667, 0.35627683997154236, 0.7075439095497131, 0.27246150374412537, 0.11604684591293335, 0.4149722754955292, 0.7875560522079468, 0.7542703151702881, 0.40813446044921875, 0.08867763727903366, 0.4309900403022766, -0.2829235792160034, 0.33462095260620117, 0.48876821994781494, 1.2184333801269531, 0.18160930275917053, 0.318678617477417, 0.02782677859067917, 1.0681442022323608, 0.5532546639442444, -0.13447344303131104, 0.760086715221405, -0.11585713177919388, 0.310808390378952, 0.3697656989097595, 0.45125967264175415, -0.2022324800491333, 1.0268158912658691, 0.6741002798080444, 0.16940319538116455, 0.898512601852417, 0.42166319489479065, 0.5164160132408142, 0.3040251135826111, 0.5096710324287415, 0.14961457252502441, 0.45346343517303467, 0.5787159204483032, 0.5682462453842163, 0.0577235147356987, 0.3011748790740967, 0.4667266011238098, 0.6271313428878784, 0.7084329128265381, 0.6860100030899048, -0.003445632755756378, 0.335829496383667, 0.72776198387146, -0.014999635517597198, 0.17796830832958221, 0.2625306248664856, 0.5891226530075073, 0.0577235147356987, 0.35137754678726196, 0.31339335441589355, 1.0573441982269287, 0.14532379806041718, 0.06382846087217331, 1.2163164615631104, 0.6968377828598022, 0.7959659099578857, 0.8412621021270752, 0.0577235147356987, 0.8621010780334473, 0.6065950393676758, -0.07678938657045364, 0.8165199756622314, 0.45319703221321106, 0.5764374732971191, 0.8395683765411377, 0.5703020095825195, 0.3443763852119446, 0.40072765946388245, 0.16655832529067993, 0.8443942070007324, 0.6798807382583618, 0.0577235147356987, 0.21390700340270996, 0.7142635583877563, 0.3311844766139984, 0.49974706768989563, 1.0738961696624756, -0.031409867107868195, 0.21209797263145447, 0.470604807138443, 0.3553991913795471, 0.6588252782821655, 0.35771307349205017, 0.4724779427051544, 0.6822955012321472, 1.3232786655426025, 0.15155082941055298, 0.45794710516929626, 0.9684450030326843, 0.35673850774765015, 0.03070306032896042, 0.5014891624450684, 0.4384574890136719, 0.8354807496070862, 0.6131455302238464, 0.36738330125808716, 0.5529087781906128, 0.5494018197059631, 0.2135445475578308, 0.18245387077331543, 1.055079460144043, 0.5191950798034668, 0.4809689521789551, 0.46783703565597534, 0.0073752328753471375, 0.22631528973579407, 0.21699970960617065, 0.1763138771057129, 0.6265640258789062, 0.6617937088012695, 0.16182667016983032, 0.0577235147356987, 0.8769044876098633, 0.5540628433227539, 0.24160385131835938, 0.0719597265124321, 0.5530996322631836, 0.0577235147356987, 0.6161060333251953, 0.0577235147356987, 0.3436253070831299, 1.0096220970153809, 0.8919540047645569, 0.5142027139663696, 0.5496096611022949, 0.5367863774299622, 0.9556145668029785, 0.6281648278236389, 0.22900474071502686, 0.30510926246643066, 0.29020845890045166, 0.5443898439407349, 0.0577235147356987, 0.42631927132606506, 0.6361227035522461, 0.3363957405090332, 0.8393149375915527, 0.013427965342998505, 0.4791088104248047, 0.4057398736476898, 0.648988664150238, 0.453337162733078, 0.7358913421630859, 0.9007730484008789, 0.860384464263916, 0.9106349945068359, 0.5024173259735107, 0.5958833694458008, 0.2990930676460266, 0.4955759644508362, 0.6668391227722168, 0.24583449959754944, 1.252408742904663, 0.8445353507995605, 0.01928286999464035, 0.5225789546966553, 0.845374345779419, 0.8739428520202637, 0.5563006401062012, 0.9457974433898926, 0.07721065729856491, 0.33228594064712524, 0.5801676511764526, 0.9452872276306152, 0.5586303472518921, 0.009391061961650848, 0.1445634365081787, 0.5419315099716187, 0.4953461289405823, 0.5078065991401672, 0.34354138374328613, 0.403849720954895, 0.654778003692627, 0.5594210624694824, 0.6474545001983643, 0.8079982399940491, 0.5208957195281982, 0.5123786926269531, 0.4519274830818176, 0.08591916412115097, 0.508391797542572, 1.1913890838623047, -0.24758780002593994, 0.285294771194458, 0.6485139727592468, 0.16181862354278564, 0.6893562078475952, 0.0577235147356987, 0.8259456157684326, 0.6268541812896729, 0.8106927871704102, 0.3790009021759033, 0.6841012239456177, 1.0542453527450562, 0.4741847515106201, 0.3884948194026947, 0.0577235147356987, 0.36192217469215393, -0.37534523010253906, 0.3215269148349762, 1.1382107734680176, 0.6702144145965576, 0.46433839201927185, 0.14669111371040344, 0.14739453792572021, 0.5306923389434814, 0.5304892659187317, 0.7144144773483276, 0.4158864915370941, 0.6021852493286133, -0.20783376693725586, 0.07535088807344437, 0.8941054344177246, 0.5688117146492004, 0.2855978310108185, -0.030698545277118683, 0.120023213326931, 1.0799684524536133, -0.11753100901842117, -0.013507373631000519, 0.30176135897636414, 0.48634767532348633, 0.4628758430480957, 0.09204971045255661, 0.3586975932121277, 0.18466949462890625, 0.30039864778518677, 0.006278030574321747, 0.8084206581115723, 0.3396039605140686, 0.3960622549057007, 0.08045672625303268, 0.45253854990005493, 0.345339298248291, 0.42168813943862915, 0.8063515424728394, -0.05982101708650589, 0.19797742366790771, 0.6315765380859375, 0.6504108905792236, 0.32903632521629333, 0.7110601663589478, 0.6918940544128418, 0.4482906758785248, 0.24621644616127014, 0.5166434049606323, 0.40274620056152344, 0.44437113404273987, 0.19222435355186462, 0.6928969025611877, 0.1781565546989441, 0.9089331030845642, 0.473190039396286, 0.2864300012588501, 0.6792281866073608, -0.1786966323852539, 0.33420228958129883, 0.0577235147356987, 0.6783627271652222, 0.8405559062957764, 0.8543043732643127, 0.7433333396911621, 0.14819765090942383, 0.139613538980484, 0.6252217292785645, 0.021365277469158173, 0.6411197185516357, 0.06530997157096863, 0.8200106620788574, 0.15449446439743042, 0.6264951229095459, 1.0157225131988525, 0.3175419867038727, 0.8287410736083984, 0.6340826153755188, 0.38320159912109375, 0.5722941756248474, 0.8300619125366211, 0.6267032027244568, 0.32185789942741394, 0.07502350211143494, 0.3667639195919037, 0.2943735122680664, 0.6296162009239197, 0.40318894386291504, 0.7579473853111267, 0.09288935363292694, 0.5813264846801758, 0.5712722539901733, 0.5705686211585999, -0.3002573251724243, 0.9929198026657104, 0.5174844264984131, 0.6931751370429993, 0.23267030715942383, 0.5563678741455078, 0.3109947443008423, 0.7424864768981934, -0.06690079718828201, 0.09277670085430145, -0.07815045863389969, 0.7560991644859314, -0.02077842503786087, 0.8959949612617493, 0.5519073605537415, 0.8630510568618774, 0.30267035961151123, 0.20408084988594055, 0.0577235147356987, 0.989326000213623, 0.07409830391407013, 0.2751627564430237, 0.5114495754241943, 0.4047277569770813, 0.36610227823257446, -0.20272088050842285, 0.30643805861473083, 0.18745946884155273, 0.7756332755088806, 0.0577235147356987], 'y_pred_int': [1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], 'y_true': [1, 0, 1, 0, 1, 0, 1, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0, 8, 0, 0, 0, 0, 0, 2, 4, 1, 1, 0, 0, 0, 0, 0, 1, 4, 1, 1, 3, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 5, 1, 1, 1, 1, 1, 1, 1, 0, 0, 3, 1, 3, 2, 0, 1, 0, 0, 0, 2, 0, 0, 1, 1, 2, 0, 1, 2, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0, 1, 6, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 1, 2, 3, 1, 0, 0, 4, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 4, 0, 0, 2, 1, 2, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 5, 0, 1, 1, 1, 0, 1, 1, 2, 1, 5, 1, 3, 0, 2, 0, 2, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 2, 2, 2, 1, 0, 3, 0, 1, 1, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 2, 2, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 2, 1, 1, 0, 3, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 3, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 4, 1, 0, 1, 0, 0, 0, 1, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 2, 1, 2, 1, 0, 0, 1, 1, 4, 0, 1, 0, 1, 0, 2, 2, 1, 2, 2, 0, 1, 0, 0], 'train_acc': 0.3455298442906574, 'test_acc': 0.44459342560553633, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 0.0825, Test loss: 48.8848
In this epoch 100/5000, Training loss: 0.0051, Test loss: 47.9789
In this epoch 150/5000, Training loss: 0.0014, Test loss: 48.6954
In this epoch 200/5000, Training loss: 0.9563, Test loss: 53.2011
In this epoch 250/5000, Training loss: 0.0071, Test loss: 48.1647
In this epoch 300/5000, Training loss: 0.0292, Test loss: 47.6291
In this epoch 350/5000, Training loss: 0.0022, Test loss: 48.4233
In this epoch 400/5000, Training loss: 0.6368, Test loss: 47.8238
In this epoch 450/5000, Training loss: 0.0054, Test loss: 48.4536
In this epoch 500/5000, Training loss: 0.0000, Test loss: 48.7626
In this epoch 550/5000, Training loss: 0.3487, Test loss: 52.7406
In this epoch 600/5000, Training loss: 0.0019, Test loss: 48.9572
In this epoch 650/5000, Training loss: 0.0000, Test loss: 48.8882
In this epoch 700/5000, Training loss: 0.0000, Test loss: 48.8995
In this epoch 750/5000, Training loss: 0.7003, Test loss: 53.6544
In this epoch 800/5000, Training loss: 0.0039, Test loss: 48.6064
In this epoch 850/5000, Training loss: 0.0010, Test loss: 48.9894
In this epoch 900/5000, Training loss: 0.1736, Test loss: 50.8206
In this epoch 950/5000, Training loss: 0.0004, Test loss: 48.9757
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 48.9816
In this epoch 1050/5000, Training loss: 0.0483, Test loss: 50.2543
In this epoch 1100/5000, Training loss: 0.0016, Test loss: 48.8662
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 48.9388
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 48.9352
In this epoch 1250/5000, Training loss: 0.0002, Test loss: 49.0129
In this epoch 1300/5000, Training loss: 0.1404, Test loss: 49.1688
In this epoch 1350/5000, Training loss: 0.0003, Test loss: 49.5103
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 49.5426
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 49.5470
In this epoch 1500/5000, Training loss: 0.1131, Test loss: 49.6032
In this epoch 1550/5000, Training loss: 0.0014, Test loss: 49.5543
In this epoch 1600/5000, Training loss: 0.0008, Test loss: 49.5286
In this epoch 1650/5000, Training loss: 0.8829, Test loss: 45.6124
In this epoch 1700/5000, Training loss: 0.0177, Test loss: 49.6792
In this epoch 1750/5000, Training loss: 0.0001, Test loss: 49.2989
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 49.2476
In this epoch 1850/5000, Training loss: 0.1943, Test loss: 49.4477
In this epoch 1900/5000, Training loss: 0.0008, Test loss: 49.4485
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 49.5737
In this epoch 2000/5000, Training loss: 0.0020, Test loss: 49.5443
In this epoch 2050/5000, Training loss: 0.0016, Test loss: 49.5507
In this epoch 2100/5000, Training loss: 0.0100, Test loss: 49.4292
In this epoch 2150/5000, Training loss: 0.0303, Test loss: 48.9278
In this epoch 2200/5000, Training loss: 0.0021, Test loss: 49.8290
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 49.5030
In this epoch 2300/5000, Training loss: 0.0262, Test loss: 49.5712
In this epoch 2350/5000, Training loss: 0.0103, Test loss: 49.0813
In this epoch 2400/5000, Training loss: 0.0001, Test loss: 48.9147
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 48.9040
In this epoch 2500/5000, Training loss: 0.6877, Test loss: 45.8216
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 48.9378
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 48.9924
In this epoch 2650/5000, Training loss: 0.0007, Test loss: 48.9919
In this epoch 2700/5000, Training loss: 0.0437, Test loss: 50.2270
In this epoch 2750/5000, Training loss: 0.0002, Test loss: 48.9717
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 48.9480
In this epoch 2850/5000, Training loss: 0.0438, Test loss: 47.7645
In this epoch 2900/5000, Training loss: 0.0009, Test loss: 49.0925
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 48.9927
In this epoch 3000/5000, Training loss: 0.0320, Test loss: 48.8980
In this epoch 3050/5000, Training loss: 0.0003, Test loss: 49.0814
In this epoch 3100/5000, Training loss: 0.0018, Test loss: 48.9185
In this epoch 3150/5000, Training loss: 0.0280, Test loss: 48.8840
In this epoch 3200/5000, Training loss: 0.0001, Test loss: 48.8458
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 48.8596
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 48.8584
In this epoch 3350/5000, Training loss: 0.0066, Test loss: 48.8054
In this epoch 3400/5000, Training loss: 0.0019, Test loss: 48.9692
In this epoch 3450/5000, Training loss: 0.0090, Test loss: 49.0546
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 48.6475
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 48.6340
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 48.6326
In this epoch 3650/5000, Training loss: 0.1228, Test loss: 50.0235
In this epoch 3700/5000, Training loss: 0.0007, Test loss: 48.8021
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 48.9493
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 48.9379
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 48.9383
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 48.9383
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 48.9383
In this epoch 4000/5000, Training loss: 0.0212, Test loss: 48.9192
In this epoch 4050/5000, Training loss: 0.0002, Test loss: 48.9214
In this epoch 4100/5000, Training loss: 0.0084, Test loss: 48.5395
In this epoch 4150/5000, Training loss: 0.0016, Test loss: 49.0718
In this epoch 4200/5000, Training loss: 0.0001, Test loss: 48.7754
In this epoch 4250/5000, Training loss: 0.0097, Test loss: 50.0020
In this epoch 4300/5000, Training loss: 0.0005, Test loss: 48.6406
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 48.6246
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 48.6244
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 48.6231
In this epoch 4500/5000, Training loss: 0.0603, Test loss: 50.1587
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 48.5947
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 48.6339
In this epoch 4650/5000, Training loss: 0.0270, Test loss: 48.3977
In this epoch 4700/5000, Training loss: 0.1963, Test loss: 51.1466
In this epoch 4750/5000, Training loss: 0.0006, Test loss: 48.9197
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 48.7955
In this epoch 4850/5000, Training loss: 0.0002, Test loss: 48.8674
In this epoch 4900/5000, Training loss: 0.0184, Test loss: 49.3178
In this epoch 4950/5000, Training loss: 0.0106, Test loss: 49.3465
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 49.5549
train_acc 1.0
test_acc 0.03980968858131488
{'y_pred': [5.160292148590088, 4.698354721069336, 15.826937675476074, 4.2822585105896, 10.69755744934082, 16.883888244628906, 5.550394535064697, 15.114558219909668, 5.218621253967285, 16.33356285095215, 7.925980567932129, 15.370518684387207, 3.6059815883636475, 3.167304754257202, 15.963123321533203, 4.740392684936523, 7.145213603973389, 2.4364497661590576, 16.57868766784668, 4.615769386291504, 1.6762940883636475, 2.591066360473633, 15.549771308898926, 15.34086799621582, 16.679134368896484, 4.6077046394348145, 17.834434509277344, 4.070756435394287, 11.738056182861328, 5.182134628295898, 13.850894927978516, 3.0848639011383057, 4.602072238922119, 15.943371772766113, 4.850715637207031, 13.754729270935059, 5.031093597412109, 16.616849899291992, 14.037629127502441, 16.48918342590332, 4.883233547210693, 18.711877822875977, 16.102697372436523, 5.128423690795898, 13.078248023986816, 5.006406784057617, 13.400040626525879, 16.395851135253906, 2.5811188220977783, 13.968480110168457, 10.367796897888184, 4.698038578033447, 14.99741268157959, 11.845694541931152, 15.115386009216309, 1.3757684230804443, 1.3030166625976562, 5.154772758483887, 15.88732624053955, 4.227127552032471, 14.646466255187988, 15.849141120910645, 14.10837459564209, 3.974975824356079, 18.353395462036133, 4.172891616821289, 18.017072677612305, 15.896930694580078, 17.12659454345703, 14.92573356628418, 3.5707972049713135, 5.290106296539307, 3.9572253227233887, 14.39020824432373, 3.394620180130005, 15.060845375061035, 15.674920082092285, 5.222717761993408, 3.950995922088623, 12.98408031463623, 4.286701679229736, 3.5463004112243652, 5.130642414093018, 2.059821128845215, 3.587167263031006, 16.637929916381836, 5.217695713043213, 17.67774772644043, 17.864587783813477, 5.155426025390625, 18.0526065826416, 5.231750011444092, 17.204973220825195, 9.393268585205078, 4.585784912109375, 3.492432117462158, 17.910287857055664, 14.332149505615234, 13.284735679626465, 5.208495616912842, 13.320258140563965, 12.420302391052246, 5.186158180236816, 4.739375591278076, 15.247136116027832, 15.62501049041748, 18.57454490661621, 15.907500267028809, 5.098082065582275, 11.052973747253418, 4.412354469299316, 14.049508094787598, 7.562522888183594, 5.1819000244140625, 14.885420799255371, 13.947885513305664, 19.78620147705078, 5.633474349975586, 4.997926235198975, 15.335278511047363, 3.656622886657715, 13.2911958694458, 12.356837272644043, 14.115626335144043, 3.2920734882354736, 14.559764862060547, 12.818134307861328, 12.457303047180176, 4.852635860443115, 17.753904342651367, 4.196996212005615, 5.08642053604126, 14.533905982971191, 15.599837303161621, 15.774907112121582, 4.729971408843994, 16.25391387939453, 16.277910232543945, 17.44820213317871, 5.147711277008057, 15.825177192687988, 14.991990089416504, 5.1072587966918945, 14.341933250427246, 5.564795017242432, 17.069501876831055, 3.955071210861206, 12.970487594604492, 13.133193016052246, 19.321332931518555, 12.16452693939209, 2.099705457687378, 14.233458518981934, 16.990270614624023, 4.271556377410889, 17.491708755493164, 16.800708770751953, 3.533195972442627, 14.09842586517334, 17.777067184448242, 1.827692985534668, 15.208148002624512, 13.211623191833496, 5.1989617347717285, 5.221559047698975, 11.97197151184082, 5.470114707946777, 4.870084285736084, 14.282612800598145, 15.91285228729248, 15.752923011779785, 3.823138475418091, 2.0870728492736816, 5.360498905181885, 14.276810646057129, 15.154565811157227, 16.42333984375, 16.147310256958008, 1.094908356666565, 3.4168107509613037, 14.100769996643066, 9.643646240234375, 17.827678680419922, 2.438744306564331, 18.672128677368164, 4.573120594024658, 15.338129997253418, 15.30010986328125, 4.1100335121154785, 5.182746887207031, 14.24448299407959, 1.7637789249420166, 2.5427939891815186, 8.700264930725098, 5.013500690460205, 4.79589319229126, 11.887070655822754, 4.231252193450928, 4.371995449066162, 17.184789657592773, 14.911839485168457, 16.341588973999023, 4.951901435852051, 8.278494834899902, 4.048247814178467, 13.866968154907227, 4.1881184577941895, 17.679967880249023, 4.82289457321167, 13.689059257507324, 5.019320011138916, 4.233388423919678, 5.037583351135254, 14.709181785583496, 17.416349411010742, 4.693563461303711, 3.6516003608703613, 17.684816360473633, 14.320094108581543, 4.657934665679932, 12.34348201751709, 4.649348735809326, 3.7594501972198486, 1.4729812145233154, 5.455963611602783, 4.048042297363281, 2.386685371398926, 3.241995334625244, 5.330873489379883, 13.315751075744629, 2.736252546310425, 5.211372375488281, 1.8908816576004028, 15.000494003295898, 3.0964853763580322, 17.134302139282227, 16.40264892578125, 4.7200164794921875, 4.939428806304932, 4.604382514953613, 15.88183307647705, 14.874140739440918, 2.681087017059326, 14.279441833496094, 13.902461051940918, 4.42423152923584, 17.765207290649414, 17.135759353637695, 13.876713752746582, 17.32659912109375, 15.144083976745605, 17.119213104248047, 3.911189317703247, 14.458756446838379, 13.481436729431152, 15.842068672180176, 4.804466247558594, 0.9432151913642883, 22.717166900634766, 15.362154960632324, 4.046132564544678, 14.139567375183105, 12.857625961303711, 4.426816463470459, 20.000505447387695, 0.6233164072036743, 5.214564323425293, 4.6610636711120605, 3.993535041809082, 9.960227966308594, 12.596474647521973, 13.09777545928955, 15.808820724487305, 15.98420524597168, 16.40381622314453, 4.62271785736084, 3.846094846725464, 4.997317790985107, 4.684445381164551, 15.420424461364746, 16.599214553833008, 16.24928855895996, 4.7242960929870605, 19.236703872680664, 4.131153106689453, 5.3849639892578125, 18.147539138793945, 18.24702262878418, 4.355325222015381, 12.881964683532715, 2.5722546577453613, 3.7067620754241943, 15.804367065429688, 3.661618947982788, 1.7263671159744263, 2.0500566959381104, 17.132246017456055, 18.873865127563477, 14.93097972869873, 13.018975257873535, 4.614442825317383, 12.94227123260498, 17.0020694732666, 20.814102172851562, 18.892656326293945, 14.541119575500488, 1.355305552482605, 14.985236167907715, 15.247549057006836, 15.825399398803711, 14.612727165222168, 13.78447437286377, 5.691549777984619, 3.4956986904144287, 3.354910373687744, 4.959106922149658, 12.18541431427002, 16.85593605041504, 4.280946731567383, 5.044643402099609, 1.1753265857696533, 4.170546054840088, 8.02323055267334, 3.804194688796997, 4.430696964263916, 14.000948905944824, 9.771553993225098, 17.179637908935547, 14.436585426330566, 18.1617488861084, 4.266390800476074, 3.282872438430786, 4.666351318359375, 9.582121849060059, 2.1652915477752686, 1.203704595565796, 13.863570213317871, 0.8959020376205444, 4.059510231018066, 2.8610620498657227], 'y_pred_int': [5, 5, 16, 4, 11, 17, 6, 15, 5, 16, 8, 15, 4, 3, 16, 5, 7, 2, 17, 5, 2, 3, 16, 15, 17, 5, 18, 4, 12, 5, 14, 3, 5, 16, 5, 14, 5, 17, 14, 16, 5, 19, 16, 5, 13, 5, 13, 16, 3, 14, 10, 5, 15, 12, 15, 1, 1, 5, 16, 4, 15, 16, 14, 4, 18, 4, 18, 16, 17, 15, 4, 5, 4, 14, 3, 15, 16, 5, 4, 13, 4, 4, 5, 2, 4, 17, 5, 18, 18, 5, 18, 5, 17, 9, 5, 3, 18, 14, 13, 5, 13, 12, 5, 5, 15, 16, 19, 16, 5, 11, 4, 14, 8, 5, 15, 14, 20, 6, 5, 15, 4, 13, 12, 14, 3, 15, 13, 12, 5, 18, 4, 5, 15, 16, 16, 5, 16, 16, 17, 5, 16, 15, 5, 14, 6, 17, 4, 13, 13, 19, 12, 2, 14, 17, 4, 17, 17, 4, 14, 18, 2, 15, 13, 5, 5, 12, 5, 5, 14, 16, 16, 4, 2, 5, 14, 15, 16, 16, 1, 3, 14, 10, 18, 2, 19, 5, 15, 15, 4, 5, 14, 2, 3, 9, 5, 5, 12, 4, 4, 17, 15, 16, 5, 8, 4, 14, 4, 18, 5, 14, 5, 4, 5, 15, 17, 5, 4, 18, 14, 5, 12, 5, 4, 1, 5, 4, 2, 3, 5, 13, 3, 5, 2, 15, 3, 17, 16, 5, 5, 5, 16, 15, 3, 14, 14, 4, 18, 17, 14, 17, 15, 17, 4, 14, 13, 16, 5, 1, 23, 15, 4, 14, 13, 4, 20, 1, 5, 5, 4, 10, 13, 13, 16, 16, 16, 5, 4, 5, 5, 15, 17, 16, 5, 19, 4, 5, 18, 18, 4, 13, 3, 4, 16, 4, 2, 2, 17, 19, 15, 13, 5, 13, 17, 21, 19, 15, 1, 15, 15, 16, 15, 14, 6, 3, 3, 5, 12, 17, 4, 5, 1, 4, 8, 4, 4, 14, 10, 17, 14, 18, 4, 3, 5, 10, 2, 1, 14, 1, 4, 3], 'y_true': [9, 7, 23, 9, 12, 23, 9, 12, 6, 28, 10, 30, 7, 3, 11, 6, 9, 11, 23, 5, 2, 7, 24, 18, 20, 4, 26, 6, 33, 7, 15, 13, 7, 15, 9, 10, 7, 19, 12, 19, 5, 18, 15, 7, 17, 7, 13, 26, 9, 20, 13, 6, 31, 11, 21, 13, 8, 4, 28, 4, 16, 9, 15, 8, 23, 8, 30, 15, 19, 14, 10, 6, 5, 17, 9, 29, 12, 4, 11, 22, 3, 3, 6, 9, 9, 23, 5, 17, 31, 8, 36, 3, 10, 11, 5, 6, 36, 24, 29, 6, 27, 20, 7, 7, 15, 15, 31, 11, 7, 12, 4, 9, 10, 5, 15, 15, 23, 8, 5, 10, 3, 11, 9, 12, 11, 16, 17, 29, 5, 16, 3, 5, 10, 27, 21, 4, 17, 22, 23, 5, 16, 13, 4, 22, 11, 18, 3, 15, 13, 23, 17, 9, 14, 15, 5, 10, 11, 7, 12, 19, 15, 29, 13, 5, 5, 16, 6, 11, 28, 26, 15, 9, 8, 7, 16, 10, 15, 28, 8, 3, 21, 14, 25, 3, 19, 4, 18, 18, 9, 5, 21, 7, 7, 12, 5, 11, 11, 9, 5, 19, 38, 21, 4, 9, 7, 11, 7, 15, 6, 15, 8, 4, 9, 16, 23, 5, 3, 57, 38, 5, 11, 9, 4, 11, 6, 6, 9, 9, 4, 13, 14, 6, 9, 15, 4, 18, 17, 6, 6, 10, 12, 33, 10, 16, 10, 5, 20, 18, 14, 43, 33, 35, 6, 12, 12, 15, 5, 10, 46, 15, 9, 17, 20, 6, 20, 10, 9, 6, 7, 15, 14, 8, 32, 26, 17, 9, 8, 7, 6, 15, 53, 11, 4, 22, 9, 6, 28, 30, 8, 13, 13, 3, 15, 4, 11, 9, 18, 19, 16, 13, 5, 11, 29, 27, 21, 20, 9, 26, 11, 23, 15, 18, 6, 10, 11, 19, 15, 11, 11, 10, 7, 6, 14, 8, 9, 16, 8, 18, 13, 39, 10, 8, 7, 15, 11, 6, 14, 9, 2, 7], 'train_acc': 1.0, 'test_acc': 0.03980968858131488, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_0_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 135.8077, Test loss: 15782.3652
In this epoch 100/5000, Training loss: 2.7138, Test loss: 16376.9912
In this epoch 150/5000, Training loss: 0.0418, Test loss: 16294.3604
In this epoch 200/5000, Training loss: 0.0062, Test loss: 16292.4404
In this epoch 250/5000, Training loss: 0.0013, Test loss: 16292.7441
In this epoch 300/5000, Training loss: 0.0005, Test loss: 16292.7637
In this epoch 350/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 400/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 450/5000, Training loss: 0.0004, Test loss: 16292.7734
In this epoch 500/5000, Training loss: 0.0004, Test loss: 16292.7734
In this epoch 550/5000, Training loss: 0.0004, Test loss: 16292.7734
In this epoch 600/5000, Training loss: 0.0004, Test loss: 16292.7734
In this epoch 650/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 700/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 750/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 800/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 850/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 900/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 950/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1000/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1050/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1100/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1150/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1200/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1250/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1300/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1350/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1400/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1450/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1500/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1550/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1600/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1650/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1700/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1750/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1800/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1850/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1900/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 1950/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2000/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2050/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2100/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2150/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2200/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2250/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2300/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2350/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2400/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2450/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 2500/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 2550/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2600/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 2650/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 2700/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2750/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2800/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2850/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2900/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 2950/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 3000/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3050/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 3100/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3150/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 3200/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3250/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3300/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3350/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3400/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 3450/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3500/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 3550/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3600/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3650/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3700/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3750/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3800/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 3850/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3900/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 3950/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4000/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 4050/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4100/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 4150/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4200/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 4250/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4300/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4350/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4400/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4450/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4500/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4550/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4600/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4650/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 4700/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 4750/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 4800/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 4850/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 4900/5000, Training loss: 0.0004, Test loss: 16292.7725
In this epoch 4950/5000, Training loss: 0.0004, Test loss: 16292.7705
In this epoch 5000/5000, Training loss: 0.0004, Test loss: 16292.7725
train_acc 0.9992647058823529
test_acc 0.008823529411764706
{'y_pred': [444.1790466308594, 507.7124328613281, 185.80123901367188, 414.36151123046875, 480.2164611816406, 481.6791076660156, 455.0090026855469, 292.4241638183594, 438.4053955078125, 492.3861999511719, 518.2609252929688, 466.17913818359375, 520.2129516601562, 502.756591796875, 154.9017333984375, 139.277587890625, 138.6598358154297, 471.5588684082031, 86.07805633544922, 462.2276306152344, 146.5037384033203, 494.6886291503906, 432.42755126953125, 502.3638916015625, 147.39590454101562, 447.2383117675781, 135.1449432373047, 594.5093383789062, 422.8819274902344, 517.3715209960938, 245.65988159179688, 486.5062255859375, 445.5524597167969, 107.98455047607422, 167.35958862304688, 58.47161102294922, 166.7305908203125, 508.53790283203125, 130.71365356445312, 229.5449676513672, 140.18116760253906, 436.40960693359375, 170.9961700439453, 564.6500244140625, 179.66555786132812, 156.32421875, 412.9449157714844, 76.75688171386719, 486.922607421875, 517.36181640625, 133.37158203125, 417.9407043457031, 160.38644409179688, 120.51238250732422, 169.10577392578125, 159.79087829589844, 454.82098388671875, 489.3672180175781, 550.3792114257812, 131.75523376464844, 94.09259796142578, 224.66293334960938, 428.47137451171875, 174.00881958007812, 513.4420776367188, 451.6663818359375, 140.5821075439453, 295.6820373535156, 121.96321868896484, 385.1127624511719, 87.18724822998047, 493.2924499511719, 127.19084167480469, 438.2997741699219, 173.48463439941406, 477.42474365234375, 179.74620056152344, 370.55426025390625, 71.38115692138672, 422.5033874511719, 147.0308380126953, 142.58380126953125, 255.73619079589844, 439.9557189941406, 162.04299926757812, 178.37620544433594, 101.90267181396484, 91.3271713256836, 135.34310913085938, 427.2389221191406, 92.8263168334961, 124.54473114013672, 541.2687377929688, 395.6230163574219, 532.6591186523438, 98.84574127197266, 537.9884033203125, 119.83177947998047, 492.3421325683594, 152.6379852294922, 115.83415222167969, 123.77484130859375, 127.00330352783203, 123.22064208984375, 155.0869140625, 145.23623657226562, 167.395263671875, 412.901123046875, 105.49942016601562, 74.15443420410156, 168.70266723632812, 137.5150909423828, 450.6088562011719, 98.08496856689453, 484.15655517578125, 159.8644256591797, 423.7940673828125, 106.64769744873047, 502.4903259277344, 151.1129150390625, 98.86787414550781, 72.60970306396484, 115.01713562011719, 416.20111083984375, 482.2380676269531, 215.12916564941406, 154.1898956298828, 514.0867309570312, 162.75030517578125, 544.60888671875, 404.7716064453125, 167.3078155517578, 379.7940368652344, 493.8765563964844, 161.00135803222656, 535.582275390625, 527.6317749023438, 144.28646850585938, 106.63540649414062, 481.4229736328125, 116.71440124511719, 430.5757141113281, 607.0639038085938, 440.7343444824219, 174.51190185546875, 167.04025268554688, 300.61083984375, 408.5500183105469, 473.87774658203125, 479.5462341308594, 162.5898895263672, 451.3562316894531, 427.09881591796875, 396.43450927734375, 445.9310607910156, 151.48887634277344, 130.27365112304688, 139.74293518066406, 139.02880859375, 125.14618682861328, 316.9592590332031, 130.3668975830078, 156.703369140625, 493.2795104980469, 99.38138580322266, 551.2260131835938, 530.6005249023438, 490.14898681640625, 450.8094177246094, 529.5587768554688, 492.2916564941406, 75.6444320678711, 130.35003662109375, 542.5575561523438, 414.70733642578125, 131.60935974121094, 156.32928466796875, 480.8524475097656, 451.3527526855469, 147.56661987304688, 484.44891357421875, 86.2629623413086, 533.7977294921875, 518.98681640625, 144.0823516845703, 134.90420532226562, 483.97674560546875, 502.7503356933594, 82.82781982421875, 140.86558532714844, 105.7657699584961, 149.06349182128906, 401.562255859375, 463.3915100097656, 280.2926025390625, 486.0462951660156, 166.12515258789062, 116.42501068115234, 532.8782348632812, 38.881561279296875, 246.72731018066406, 487.2366638183594, 148.28854370117188, 126.91358184814453, 98.26630401611328, 170.79736328125, 539.8594970703125, 421.8865966796875, 105.23237609863281, 502.1285705566406, 94.29452514648438, 468.4925842285156, 480.5210266113281, 423.76641845703125, 434.6340026855469, 123.63812255859375, 489.1376037597656, 167.81802368164062, 467.89105224609375, 517.3322143554688, 159.09596252441406, 110.6759033203125, 319.5998229980469, 182.4055938720703, 416.6676940917969, 516.0706787109375, 418.43017578125, 146.19659423828125, 487.3719482421875, 147.99057006835938, 132.59088134765625, 505.7861633300781, 162.29637145996094, 142.02670288085938, 504.3500061035156, 72.78883361816406, 125.83837127685547, 150.34120178222656, 440.27008056640625, 246.47366333007812, 454.1192321777344, 560.1122436523438, 456.3148498535156, 114.33740997314453, 140.18807983398438, 177.8626708984375, 505.1481018066406, 425.13946533203125, 509.0762023925781, 502.7765808105469, 127.94766998291016, 134.19644165039062, 483.4087219238281, 370.9713134765625, 175.68576049804688, 164.92640686035156, 545.6797485351562, 471.77813720703125, 513.3096313476562, 499.5425109863281, 529.959716796875, 485.2425842285156, 169.5169219970703, 487.26934814453125, 234.20111083984375, 436.675048828125, 127.9679946899414, 345.8287658691406, 395.7204895019531, 158.21424865722656, 114.32328033447266, 194.32183837890625, 544.216064453125, 104.60553741455078, 442.4457092285156, 369.14874267578125, 87.86743927001953, 452.10687255859375, 107.39636993408203, 147.2852020263672, 452.2655944824219, 148.09530639648438, 525.7131958007812, 463.1976013183594, 547.000244140625, 292.3949279785156, 596.5503540039062, 400.1590270996094, 500.35845947265625, 494.6173400878906, 92.78722381591797, 572.5156860351562, 570.1552734375, 507.4700012207031, 131.4901885986328, 434.2234191894531, 358.83551025390625, 86.56592559814453, 146.55438232421875, 147.6667938232422, 424.8873291015625, 188.90211486816406, 387.1653137207031, 512.6465454101562, 92.9098129272461, 172.47499084472656, 170.65760803222656, 124.24138641357422, 125.04637908935547, 398.5861511230469, 172.30555725097656, 481.7632141113281, 549.7645874023438, 500.29986572265625, 113.28617858886719, 425.0489807128906, 531.9541625976562, 484.0555114746094, 544.5859985351562, 510.6968688964844, 509.3113708496094, 460.05572509765625, 547.2889404296875, 248.11630249023438, 66.76006317138672, 80.33466339111328, 445.8194274902344, 118.8354263305664, 438.1156311035156, 64.45639038085938, 463.3347473144531, 159.19822692871094, 471.41357421875, 135.66452026367188, 129.10455322265625, 155.281982421875, 170.59295654296875, 534.378662109375, 84.99623107910156, 438.4355163574219], 'y_pred_int': [444, 508, 186, 414, 480, 482, 455, 292, 438, 492, 518, 466, 520, 503, 155, 139, 139, 472, 86, 462, 147, 495, 432, 502, 147, 447, 135, 595, 423, 517, 246, 487, 446, 108, 167, 58, 167, 509, 131, 230, 140, 436, 171, 565, 180, 156, 413, 77, 487, 517, 133, 418, 160, 121, 169, 160, 455, 489, 550, 132, 94, 225, 428, 174, 513, 452, 141, 296, 122, 385, 87, 493, 127, 438, 173, 477, 180, 371, 71, 423, 147, 143, 256, 440, 162, 178, 102, 91, 135, 427, 93, 125, 541, 396, 533, 99, 538, 120, 492, 153, 116, 124, 127, 123, 155, 145, 167, 413, 105, 74, 169, 138, 451, 98, 484, 160, 424, 107, 502, 151, 99, 73, 115, 416, 482, 215, 154, 514, 163, 545, 405, 167, 380, 494, 161, 536, 528, 144, 107, 481, 117, 431, 607, 441, 175, 167, 301, 409, 474, 480, 163, 451, 427, 396, 446, 151, 130, 140, 139, 125, 317, 130, 157, 493, 99, 551, 531, 490, 451, 530, 492, 76, 130, 543, 415, 132, 156, 481, 451, 148, 484, 86, 534, 519, 144, 135, 484, 503, 83, 141, 106, 149, 402, 463, 280, 486, 166, 116, 533, 39, 247, 487, 148, 127, 98, 171, 540, 422, 105, 502, 94, 468, 481, 424, 435, 124, 489, 168, 468, 517, 159, 111, 320, 182, 417, 516, 418, 146, 487, 148, 133, 506, 162, 142, 504, 73, 126, 150, 440, 246, 454, 560, 456, 114, 140, 178, 505, 425, 509, 503, 128, 134, 483, 371, 176, 165, 546, 472, 513, 500, 530, 485, 170, 487, 234, 437, 128, 346, 396, 158, 114, 194, 544, 105, 442, 369, 88, 452, 107, 147, 452, 148, 526, 463, 547, 292, 597, 400, 500, 495, 93, 573, 570, 507, 131, 434, 359, 87, 147, 148, 425, 189, 387, 513, 93, 172, 171, 124, 125, 399, 172, 482, 550, 500, 113, 425, 532, 484, 545, 511, 509, 460, 547, 248, 67, 80, 446, 119, 438, 64, 463, 159, 471, 136, 129, 155, 171, 534, 85, 438], 'y_true': [527, 407, 180, 510, 614, 664, 453, 337, 346, 373, 424, 669, 643, 484, 169, 149, 135, 590, 80, 545, 217, 376, 476, 641, 213, 404, 167, 433, 837, 445, 321, 523, 469, 300, 217, 62, 187, 442, 302, 313, 157, 440, 200, 407, 240, 212, 361, 260, 351, 540, 133, 391, 150, 112, 237, 173, 726, 679, 459, 147, 87, 327, 395, 232, 354, 832, 231, 329, 131, 380, 76, 527, 279, 330, 224, 599, 185, 342, 69, 340, 188, 163, 353, 369, 281, 170, 99, 76, 138, 394, 97, 135, 552, 312, 773, 296, 770, 113, 370, 226, 108, 135, 133, 109, 187, 181, 223, 414, 239, 256, 209, 140, 376, 88, 616, 180, 999, 103, 498, 235, 109, 78, 264, 420, 449, 308, 168, 579, 232, 430, 371, 233, 299, 372, 211, 466, 787, 146, 108, 507, 99, 436, 517, 565, 206, 239, 296, 385, 786, 338, 209, 457, 557, 350, 326, 160, 136, 239, 264, 318, 286, 130, 154, 490, 100, 904, 705, 358, 373, 783, 831, 61, 258, 513, 700, 250, 216, 680, 396, 144, 392, 87, 732, 428, 164, 139, 384, 660, 74, 234, 102, 280, 667, 445, 294, 372, 208, 126, 366, 32, 331, 328, 284, 141, 81, 181, 733, 522, 258, 605, 249, 569, 544, 347, 550, 105, 502, 207, 452, 693, 193, 94, 292, 201, 405, 478, 666, 172, 794, 148, 143, 572, 204, 156, 489, 73, 131, 165, 379, 351, 443, 459, 381, 117, 161, 175, 361, 448, 495, 487, 139, 122, 334, 331, 212, 185, 726, 485, 432, 548, 835, 340, 203, 446, 296, 661, 135, 401, 521, 200, 273, 351, 433, 100, 392, 339, 88, 391, 272, 150, 358, 237, 537, 490, 680, 326, 567, 324, 901, 607, 85, 1062, 779, 611, 232, 292, 356, 69, 173, 165, 1042, 233, 390, 403, 291, 200, 181, 285, 125, 857, 199, 629, 785, 707, 104, 370, 596, 874, 406, 439, 791, 427, 676, 349, 55, 253, 565, 305, 663, 62, 562, 191, 778, 154, 158, 234, 212, 768, 250, 566], 'train_acc': 0.9992647058823529, 'test_acc': 0.008823529411764706, 'X': 'X_TANL_layer_0_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 0.7690, Test loss: 4.8159
In this epoch 100/5000, Training loss: 0.0015, Test loss: 4.5902
In this epoch 150/5000, Training loss: 0.0000, Test loss: 4.6037
In this epoch 200/5000, Training loss: 0.0000, Test loss: 4.6019
In this epoch 250/5000, Training loss: 0.0000, Test loss: 4.6023
In this epoch 300/5000, Training loss: 0.2175, Test loss: 5.6428
In this epoch 350/5000, Training loss: 0.0279, Test loss: 4.3849
In this epoch 400/5000, Training loss: 0.0007, Test loss: 4.5265
In this epoch 450/5000, Training loss: 0.0000, Test loss: 4.5333
In this epoch 500/5000, Training loss: 0.0000, Test loss: 4.5333
In this epoch 550/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 600/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 650/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 700/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 750/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 800/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 850/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 900/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 950/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 4.5334
In this epoch 1150/5000, Training loss: 11.6498, Test loss: 6.3244
In this epoch 1200/5000, Training loss: 1.4602, Test loss: 3.1473
In this epoch 1250/5000, Training loss: 1.3949, Test loss: 3.1304
In this epoch 1300/5000, Training loss: 1.3642, Test loss: 3.0964
In this epoch 1350/5000, Training loss: 1.3444, Test loss: 3.0749
In this epoch 1400/5000, Training loss: 1.3257, Test loss: 3.0530
In this epoch 1450/5000, Training loss: 1.3075, Test loss: 3.0317
In this epoch 1500/5000, Training loss: 1.2899, Test loss: 3.0111
In this epoch 1550/5000, Training loss: 1.2729, Test loss: 2.9909
In this epoch 1600/5000, Training loss: 1.2564, Test loss: 2.9714
In this epoch 1650/5000, Training loss: 1.2406, Test loss: 2.9525
In this epoch 1700/5000, Training loss: 1.2253, Test loss: 2.9343
In this epoch 1750/5000, Training loss: 1.2107, Test loss: 2.9168
In this epoch 1800/5000, Training loss: 1.1967, Test loss: 2.8999
In this epoch 1850/5000, Training loss: 1.1833, Test loss: 2.8837
In this epoch 1900/5000, Training loss: 1.1706, Test loss: 2.8681
In this epoch 1950/5000, Training loss: 1.1584, Test loss: 2.8533
In this epoch 2000/5000, Training loss: 1.1469, Test loss: 2.8390
In this epoch 2050/5000, Training loss: 1.1360, Test loss: 2.8255
In this epoch 2100/5000, Training loss: 1.1257, Test loss: 2.8125
In this epoch 2150/5000, Training loss: 1.1159, Test loss: 2.8002
In this epoch 2200/5000, Training loss: 1.1068, Test loss: 2.7886
In this epoch 2250/5000, Training loss: 1.0982, Test loss: 2.7775
In this epoch 2300/5000, Training loss: 1.0901, Test loss: 2.7671
In this epoch 2350/5000, Training loss: 1.0826, Test loss: 2.7573
In this epoch 2400/5000, Training loss: 1.0755, Test loss: 2.7481
In this epoch 2450/5000, Training loss: 1.0690, Test loss: 2.7394
In this epoch 2500/5000, Training loss: 1.0629, Test loss: 2.7312
In this epoch 2550/5000, Training loss: 1.0573, Test loss: 2.7236
In this epoch 2600/5000, Training loss: 1.0521, Test loss: 2.7165
In this epoch 2650/5000, Training loss: 1.0473, Test loss: 2.7099
In this epoch 2700/5000, Training loss: 1.0430, Test loss: 2.7037
In this epoch 2750/5000, Training loss: 1.0389, Test loss: 2.6980
In this epoch 2800/5000, Training loss: 1.0353, Test loss: 2.6927
In this epoch 2850/5000, Training loss: 1.0319, Test loss: 2.6877
In this epoch 2900/5000, Training loss: 1.0289, Test loss: 2.6832
In this epoch 2950/5000, Training loss: 1.0262, Test loss: 2.6790
In this epoch 3000/5000, Training loss: 1.0288, Test loss: 2.6759
In this epoch 3050/5000, Training loss: 1.0245, Test loss: 2.6715
In this epoch 3100/5000, Training loss: 1.0223, Test loss: 2.6681
In this epoch 3150/5000, Training loss: 1.0204, Test loss: 2.6650
In this epoch 3200/5000, Training loss: 1.0187, Test loss: 2.6622
In this epoch 3250/5000, Training loss: 1.0171, Test loss: 2.6597
In this epoch 3300/5000, Training loss: 1.0158, Test loss: 2.6573
In this epoch 3350/5000, Training loss: 1.0146, Test loss: 2.6552
In this epoch 3400/5000, Training loss: 1.0186, Test loss: 2.6554
In this epoch 3450/5000, Training loss: 1.0128, Test loss: 2.6300
In this epoch 3500/5000, Training loss: 1.0120, Test loss: 2.6226
In this epoch 3550/5000, Training loss: 1.0113, Test loss: 2.6213
In this epoch 3600/5000, Training loss: 1.0107, Test loss: 2.6212
In this epoch 3650/5000, Training loss: 1.0103, Test loss: 2.5712
In this epoch 3700/5000, Training loss: 1.0098, Test loss: 2.5916
In this epoch 3750/5000, Training loss: 1.0095, Test loss: 2.5896
In this epoch 3800/5000, Training loss: 1.0092, Test loss: 2.5890
In this epoch 3850/5000, Training loss: 1.0089, Test loss: 2.5882
In this epoch 3900/5000, Training loss: 1.0089, Test loss: 2.5973
In this epoch 3950/5000, Training loss: 1.0138, Test loss: 2.4892
In this epoch 4000/5000, Training loss: 1.0084, Test loss: 2.5261
In this epoch 4050/5000, Training loss: 1.0083, Test loss: 2.5229
In this epoch 4100/5000, Training loss: 1.0082, Test loss: 2.5225
In this epoch 4150/5000, Training loss: 1.0081, Test loss: 2.5220
In this epoch 4200/5000, Training loss: 1.0081, Test loss: 2.5216
In this epoch 4250/5000, Training loss: 1.0080, Test loss: 2.5213
In this epoch 4300/5000, Training loss: 1.0080, Test loss: 2.5210
In this epoch 4350/5000, Training loss: 1.0084, Test loss: 2.5028
In this epoch 4400/5000, Training loss: 1.0093, Test loss: 2.4535
In this epoch 4450/5000, Training loss: 1.0079, Test loss: 2.4352
In this epoch 4500/5000, Training loss: 1.0079, Test loss: 2.4375
In this epoch 4550/5000, Training loss: 1.0079, Test loss: 2.4371
In this epoch 4600/5000, Training loss: 1.0079, Test loss: 2.4369
In this epoch 4650/5000, Training loss: 1.0079, Test loss: 2.4368
In this epoch 4700/5000, Training loss: 1.0079, Test loss: 2.4367
In this epoch 4750/5000, Training loss: 1.0079, Test loss: 2.4366
In this epoch 4800/5000, Training loss: 1.0079, Test loss: 2.4365
In this epoch 4850/5000, Training loss: 1.0079, Test loss: 2.4364
In this epoch 4900/5000, Training loss: 1.0079, Test loss: 2.4364
In this epoch 4950/5000, Training loss: 1.0491, Test loss: 2.1655
In this epoch 5000/5000, Training loss: 1.0144, Test loss: 2.1279
train_acc 0.3436370025951557
test_acc 0.35378892733564016
{'y_pred': [0.7587050199508667, 3.3178515434265137, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.1872576475143433, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 4.5614423751831055, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 6.438563346862793, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 2.1738672256469727, 0.7587050199508667, 2.3063223361968994, 0.7796992063522339, 0.7587050199508667, 3.3934245109558105, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.7562676668167114, 0.7587050199508667, 0.7587050199508667, 0.7513322830200195, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, -0.5736026763916016, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 4.922208786010742, 1.1520389318466187, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 5.114569664001465, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.5255601406097412, 1.6891396045684814, 0.7587050199508667, 0.7587050199508667, 3.579977512359619, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.5465941429138184, 2.029127597808838, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 2.150602340698242, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 5.297638893127441, 0.7587050199508667, 0.7587050199508667, 3.1603426933288574, 0.7587050199508667, 0.09126412868499756, 0.7587050199508667, 1.4602751731872559, 4.077056884765625, 0.7587050199508667, 0.7587050199508667, 1.9718643426895142, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 3.3509559631347656, 0.7587050199508667, 0.7587050199508667, -0.31497514247894287, 0.7587050199508667, 3.6978039741516113, 1.1427136659622192, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.9208545684814453, 0.7587050199508667, 0.7587050199508667, 4.7165846824646, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 4.101481914520264, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, -1.294394612312317, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 2.8268208503723145, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 6.394598960876465, 4.322911262512207, 0.7587050199508667, 0.7587050199508667, -1.3967233896255493, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.9463531970977783, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 2.467282772064209, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 5.00560998916626, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.5670908689498901, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.4790855646133423, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.5371488332748413, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 2.3517723083496094, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.2483981847763062, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 2.3754310607910156, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 5.143771648406982, 6.187348365783691, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.3073053359985352, 0.7587050199508667, 1.9070119857788086, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.1321029663085938, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 3.255406379699707, 1.8497287034988403, 0.7587050199508667, 2.7676138877868652, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.8285413980484009, 2.304802417755127, 2.810800075531006, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.9860514402389526, 0.7587050199508667, 1.6380385160446167, 0.7587050199508667, 0.7587050199508667, 4.593408107757568, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.7389867305755615, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.4415254592895508, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.8191729784011841, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 3.413041114807129, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.9705837965011597, 0.7587050199508667, 1.266107439994812, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 1.6782376766204834, 0.7587050199508667, 5.352120399475098, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667, 0.7587050199508667], 'y_pred_int': [1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 2, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 2, 2, 1, 1, 4, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 5, 1, 1, 3, 1, 0, 1, 1, 4, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 0, 1, 4, 1, 1, 1, 1, 2, 1, 1, 5, 1, 1, 1, 1, 4, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 6, 4, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 3, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 5, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 5, 1, 1, 1, 1, 1, 1], 'y_true': [1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 1, 0, 0, 1, 3, 1, 0, 0, 1, 2, 2, 2, 1, 1, 2, 1, 0, 3, 1, 3, 0, 1, 2, 0, 1, 2, 1, 0, 4, 0, 1, 0, 3, 1, 1, 1, 1, 0, 3, 3, 1, 0, 1, 0, 1, 0, 0, 3, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 2, 2, 2, 2, 1, 1, 1, 3, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 1, 1, 6, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 2, 0, 0, 1, 2, 1, 2, 1, 2, 8, 1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 3, 3, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 4, 1, 1, 2, 1, 0, 0, 0, 1, 2, 1, 1, 1, 0, 1, 0, 2, 1, 0, 0, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 0, 0, 3, 0, 3, 1, 3, 3, 1, 2, 1, 3, 4, 1, 1, 0, 0, 1, 1, 3, 1, 1, 3, 1, 0, 0, 3, 1, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 3, 0, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0, 0, 1, 4, 2, 1, 0, 1, 3, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 0, 1, 0, 7, 1, 2, 1, 1, 0, 0, 0, 5, 0, 2, 1, 1, 1, 0, 0, 1, 1, 2, 0, 2, 1, 0, 0, 1, 1, 2, 1, 1, 0, 0, 2, 1, 2, 0, 1, 1, 1, 0, 1, 1, 0, 3, 1, 0, 1, 2, 0, 1, 0, 2, 1, 0, 3], 'train_acc': 0.3436370025951557, 'test_acc': 0.35378892733564016, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_muc_1700_num_events'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_sent_muc1700.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 0.2140, Test loss: 44.2877
In this epoch 100/5000, Training loss: 0.0039, Test loss: 44.2981
In this epoch 150/5000, Training loss: 0.0573, Test loss: 44.4846
In this epoch 200/5000, Training loss: 0.0014, Test loss: 44.4363
In this epoch 250/5000, Training loss: 0.0156, Test loss: 44.8905
In this epoch 300/5000, Training loss: 0.0471, Test loss: 44.2980
In this epoch 350/5000, Training loss: 0.0010, Test loss: 44.3781
In this epoch 400/5000, Training loss: 1.2642, Test loss: 46.9272
In this epoch 450/5000, Training loss: 0.0359, Test loss: 44.4451
In this epoch 500/5000, Training loss: 0.0022, Test loss: 44.3024
In this epoch 550/5000, Training loss: 0.0648, Test loss: 44.0633
In this epoch 600/5000, Training loss: 0.0113, Test loss: 44.8106
In this epoch 650/5000, Training loss: 0.0045, Test loss: 44.1763
In this epoch 700/5000, Training loss: 0.0094, Test loss: 44.2826
In this epoch 750/5000, Training loss: 2.0154, Test loss: 44.0243
In this epoch 800/5000, Training loss: 0.0036, Test loss: 44.2504
In this epoch 850/5000, Training loss: 0.3348, Test loss: 45.1091
In this epoch 900/5000, Training loss: 0.0020, Test loss: 44.2427
In this epoch 950/5000, Training loss: 1.6478, Test loss: 44.2651
In this epoch 1000/5000, Training loss: 0.1941, Test loss: 44.1414
In this epoch 1050/5000, Training loss: 0.0012, Test loss: 44.0366
In this epoch 1100/5000, Training loss: 0.0001, Test loss: 44.0418
In this epoch 1150/5000, Training loss: 0.0147, Test loss: 44.0109
In this epoch 1200/5000, Training loss: 0.0028, Test loss: 43.9472
In this epoch 1250/5000, Training loss: 0.0027, Test loss: 43.9486
In this epoch 1300/5000, Training loss: 0.6823, Test loss: 48.8228
In this epoch 1350/5000, Training loss: 0.0083, Test loss: 43.7394
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 43.4413
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 43.4294
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 43.4287
In this epoch 1550/5000, Training loss: 0.5218, Test loss: 43.2852
In this epoch 1600/5000, Training loss: 0.0012, Test loss: 43.6549
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 43.8167
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 43.8207
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 43.8202
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 43.8203
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 43.8203
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 43.8203
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 43.8203
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 43.8203
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 43.8203
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 43.8203
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 43.8203
In this epoch 2200/5000, Training loss: 20.8075, Test loss: 75.6678
In this epoch 2250/5000, Training loss: 12.6350, Test loss: 56.1284
In this epoch 2300/5000, Training loss: 12.2388, Test loss: 55.5924
In this epoch 2350/5000, Training loss: 11.9100, Test loss: 55.1737
In this epoch 2400/5000, Training loss: 11.6085, Test loss: 54.7993
In this epoch 2450/5000, Training loss: 11.3285, Test loss: 54.4519
In this epoch 2500/5000, Training loss: 11.0662, Test loss: 54.1279
In this epoch 2550/5000, Training loss: 10.8187, Test loss: 53.8217
In this epoch 2600/5000, Training loss: 10.5840, Test loss: 53.5299
In this epoch 2650/5000, Training loss: 10.3603, Test loss: 53.2517
In this epoch 2700/5000, Training loss: 10.1463, Test loss: 52.9855
In this epoch 2750/5000, Training loss: 9.9410, Test loss: 52.7293
In this epoch 2800/5000, Training loss: 9.7435, Test loss: 52.4822
In this epoch 2850/5000, Training loss: 9.5530, Test loss: 52.2438
In this epoch 2900/5000, Training loss: 9.3689, Test loss: 52.0132
In this epoch 2950/5000, Training loss: 9.1907, Test loss: 51.7899
In this epoch 3000/5000, Training loss: 9.0180, Test loss: 51.5733
In this epoch 3050/5000, Training loss: 8.8503, Test loss: 51.3628
In this epoch 3100/5000, Training loss: 8.6873, Test loss: 51.1581
In this epoch 3150/5000, Training loss: 8.5286, Test loss: 50.9588
In this epoch 3200/5000, Training loss: 8.3741, Test loss: 50.7646
In this epoch 3250/5000, Training loss: 8.2235, Test loss: 50.5751
In this epoch 3300/5000, Training loss: 8.0765, Test loss: 50.3901
In this epoch 3350/5000, Training loss: 7.9331, Test loss: 50.2091
In this epoch 3400/5000, Training loss: 7.7929, Test loss: 50.0318
In this epoch 3450/5000, Training loss: 7.6559, Test loss: 49.8584
In this epoch 3500/5000, Training loss: 7.5219, Test loss: 49.6887
In this epoch 3550/5000, Training loss: 7.3908, Test loss: 49.5225
In this epoch 3600/5000, Training loss: 7.2625, Test loss: 49.3598
In this epoch 3650/5000, Training loss: 7.1368, Test loss: 49.2002
In this epoch 3700/5000, Training loss: 7.0137, Test loss: 49.0433
In this epoch 3750/5000, Training loss: 6.8931, Test loss: 48.8881
In this epoch 3800/5000, Training loss: 6.7749, Test loss: 48.7358
In this epoch 3850/5000, Training loss: 6.6590, Test loss: 48.5864
In this epoch 3900/5000, Training loss: 6.5453, Test loss: 48.4397
In this epoch 3950/5000, Training loss: 6.4338, Test loss: 48.2957
In this epoch 4000/5000, Training loss: 6.3245, Test loss: 48.1542
In this epoch 4050/5000, Training loss: 6.2172, Test loss: 48.0149
In this epoch 4100/5000, Training loss: 6.1127, Test loss: 47.8645
In this epoch 4150/5000, Training loss: 6.0086, Test loss: 47.7290
In this epoch 4200/5000, Training loss: 5.9072, Test loss: 47.5984
In this epoch 4250/5000, Training loss: 5.8077, Test loss: 47.4695
In this epoch 4300/5000, Training loss: 5.7154, Test loss: 47.3426
In this epoch 4350/5000, Training loss: 5.6142, Test loss: 47.1743
In this epoch 4400/5000, Training loss: 5.5199, Test loss: 47.0521
In this epoch 4450/5000, Training loss: 5.4275, Test loss: 46.9336
In this epoch 4500/5000, Training loss: 5.3367, Test loss: 46.8169
In this epoch 4550/5000, Training loss: 5.2519, Test loss: 46.5904
In this epoch 4600/5000, Training loss: 5.1602, Test loss: 46.4655
In this epoch 4650/5000, Training loss: 5.0743, Test loss: 46.3555
In this epoch 4700/5000, Training loss: 4.9901, Test loss: 46.2472
In this epoch 4750/5000, Training loss: 4.9073, Test loss: 46.1408
In this epoch 4800/5000, Training loss: 4.8261, Test loss: 46.0363
In this epoch 4850/5000, Training loss: 4.7583, Test loss: 45.9325
In this epoch 4900/5000, Training loss: 4.6694, Test loss: 45.5036
In this epoch 4950/5000, Training loss: 4.5915, Test loss: 45.3882
In this epoch 5000/5000, Training loss: 4.5161, Test loss: 45.2905
train_acc 0.7029411764705882
test_acc 0.02726643598615917
{'y_pred': [24.80974578857422, 19.770877838134766, 3.1008691787719727, 19.783466339111328, 15.264762878417969, 17.21813201904297, 16.74716567993164, 3.1008691787719727, 3.1008691787719727, 14.415657043457031, 3.1008691787719727, 23.313037872314453, 3.1008691787719727, 15.299397468566895, 3.1008691787719727, 18.122207641601562, 26.67780303955078, 24.648258209228516, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 10.59485912322998, 25.43527603149414, 14.558977127075195, 20.395767211914062, 17.321077346801758, 20.457592010498047, 3.1008691787719727, 22.795791625976562, 17.333776473999023, 18.473323822021484, 3.1008691787719727, 18.41046905517578, 3.1008691787719727, 3.1008691787719727, 15.042978286743164, 6.858664512634277, 25.0970458984375, 3.1008691787719727, 3.1008691787719727, 25.32799530029297, 3.1008691787719727, 16.191600799560547, 3.1008691787719727, 7.119144916534424, 15.003260612487793, 3.1008691787719727, 12.708035469055176, 17.288787841796875, 3.1008691787719727, 3.1008691787719727, 2.1970088481903076, 3.1008691787719727, 3.1008691787719727, 20.912017822265625, 3.1008691787719727, 21.48162078857422, 23.760448455810547, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 19.246047973632812, 17.105934143066406, 20.778671264648438, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 28.75888442993164, 3.1008691787719727, 23.94973373413086, 3.1008691787719727, 9.724386215209961, 19.22429656982422, 17.697650909423828, 19.652847290039062, 19.281291961669922, 20.392536163330078, 18.917076110839844, 20.525970458984375, 3.1008691787719727, 24.07815170288086, 11.465128898620605, 20.975624084472656, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 18.442340850830078, 3.1008691787719727, 13.641404151916504, 3.1008691787719727, 17.104103088378906, 25.521747589111328, 17.697650909423828, 3.1008691787719727, 3.1008691787719727, 6.741223335266113, 21.176727294921875, 19.73011016845703, 18.68354034423828, 3.1008691787719727, 3.1008691787719727, 24.75592041015625, 3.1008691787719727, 23.952342987060547, 14.724048614501953, 10.393613815307617, 3.1008691787719727, 21.25455093383789, 21.81287384033203, 2.245178699493408, 17.66063690185547, 26.876052856445312, 16.93138885498047, 23.728187561035156, 3.1008691787719727, 3.1008691787719727, 22.480422973632812, 19.42056655883789, 3.1008691787719727, 3.1008691787719727, 22.31177520751953, 3.1008691787719727, 3.1008691787719727, -2.2666492462158203, 11.46993637084961, 3.1008691787719727, 21.47433090209961, 3.1008691787719727, 17.830810546875, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 22.473297119140625, 3.1008691787719727, 32.222896575927734, 18.306278228759766, 3.1008691787719727, 3.1008691787719727, 16.50467872619629, 16.07390022277832, 3.1008691787719727, 12.760247230529785, 3.1008691787719727, 21.804363250732422, 13.059903144836426, 3.1008691787719727, 22.214401245117188, 21.918834686279297, 17.948932647705078, 18.67607879638672, 11.117810249328613, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 22.968608856201172, 3.1008691787719727, 17.69719696044922, 3.1008691787719727, 18.960084915161133, 3.1008691787719727, 22.233875274658203, 19.830902099609375, 17.54131317138672, 7.679863452911377, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 21.780078887939453, 19.056259155273438, 20.812599182128906, 21.073081970214844, 3.1008691787719727, 3.1008691787719727, 7.951462745666504, 3.1008691787719727, 12.713627815246582, 3.1008691787719727, 18.971656799316406, 3.1008691787719727, 28.78164291381836, 3.1008691787719727, 20.690261840820312, 3.1008691787719727, 17.289133071899414, 19.241069793701172, 21.162487030029297, 14.463193893432617, 22.157520294189453, 22.586536407470703, 21.001995086669922, 15.335790634155273, 3.1008691787719727, 15.150092124938965, 3.1008691787719727, 8.322416305541992, 24.13520050048828, 21.907363891601562, 22.44256591796875, 3.1008691787719727, 23.638534545898438, 16.538284301757812, 17.563541412353516, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 11.120747566223145, 5.339998245239258, 16.62605094909668, 10.127510070800781, 4.496878623962402, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 23.930091857910156, 17.57034683227539, 25.90557861328125, 24.520328521728516, 21.42864227294922, 22.278202056884766, 3.1008691787719727, 3.1008691787719727, 26.012142181396484, 19.62347412109375, 12.765220642089844, 3.1008691787719727, 7.702565670013428, 15.398802757263184, 3.1008691787719727, 9.383235931396484, 8.46778678894043, 18.117263793945312, 3.1008691787719727, 14.895177841186523, 20.177871704101562, 3.1008691787719727, 3.1008691787719727, 3.0885422229766846, 20.360668182373047, 3.1008691787719727, 4.563098907470703, 19.038776397705078, 5.705486297607422, 3.1008691787719727, 16.28048324584961, 18.410991668701172, 20.850086212158203, 3.1008691787719727, 8.890260696411133, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 9.009002685546875, 25.466327667236328, 3.1008691787719727, 19.570152282714844, 3.1008691787719727, 29.450336456298828, 1.0574214458465576, 3.1008691787719727, 3.1008691787719727, 18.202171325683594, 24.203445434570312, 3.1008691787719727, 11.500184059143066, 3.1008691787719727, 19.03144073486328, 18.311172485351562, 19.49367904663086, 20.203594207763672, 3.1008691787719727, 21.57016372680664, 3.1008691787719727, 18.970996856689453, 3.1008691787719727, 27.221858978271484, 3.1008691787719727, 22.971240997314453, 8.471855163574219, 8.808586120605469, 6.877011299133301, 18.556663513183594, 17.903175354003906, 9.3145751953125, 3.1008691787719727, 17.67714500427246, 6.050457954406738, 19.148162841796875, 23.798316955566406, 22.248275756835938, 18.235347747802734, 12.552074432373047, 14.19138240814209, 21.35103988647461, 17.960071563720703, 3.1008691787719727, 3.1008691787719727, 3.1008691787719727, 18.581348419189453, 2.45052170753479, 3.1008691787719727, 24.181163787841797, 28.065738677978516, 26.425498962402344, 17.282804489135742, 22.941219329833984, 18.097492218017578, 18.542299270629883, 12.798460006713867, 13.575542449951172, 18.61132049560547, 27.13011932373047, 10.705265045166016, 3.1008691787719727, 22.208770751953125, 14.3363037109375, 27.538654327392578, 17.352680206298828, 3.1008691787719727, 16.696666717529297, 16.844942092895508, 3.1008691787719727, 9.816787719726562, 3.1008691787719727, 3.1008691787719727, 23.156253814697266, 19.767433166503906, 3.1008691787719727, 6.917932987213135, 5.123994827270508, 26.33751678466797, 3.1008691787719727, 28.29302978515625], 'y_pred_int': [25, 20, 3, 20, 15, 17, 17, 3, 3, 14, 3, 23, 3, 15, 3, 18, 27, 25, 3, 3, 3, 3, 3, 3, 3, 11, 25, 15, 20, 17, 20, 3, 23, 17, 18, 3, 18, 3, 3, 15, 7, 25, 3, 3, 25, 3, 16, 3, 7, 15, 3, 13, 17, 3, 3, 2, 3, 3, 21, 3, 21, 24, 3, 3, 3, 19, 17, 21, 3, 3, 3, 3, 3, 29, 3, 24, 3, 10, 19, 18, 20, 19, 20, 19, 21, 3, 24, 11, 21, 3, 3, 3, 18, 3, 14, 3, 17, 26, 18, 3, 3, 7, 21, 20, 19, 3, 3, 25, 3, 24, 15, 10, 3, 21, 22, 2, 18, 27, 17, 24, 3, 3, 22, 19, 3, 3, 22, 3, 3, 0, 11, 3, 21, 3, 18, 3, 3, 3, 22, 3, 32, 18, 3, 3, 17, 16, 3, 13, 3, 22, 13, 3, 22, 22, 18, 19, 11, 3, 3, 3, 23, 3, 18, 3, 19, 3, 22, 20, 18, 8, 3, 3, 3, 22, 19, 21, 21, 3, 3, 8, 3, 13, 3, 19, 3, 29, 3, 21, 3, 17, 19, 21, 14, 22, 23, 21, 15, 3, 15, 3, 8, 24, 22, 22, 3, 24, 17, 18, 3, 3, 3, 11, 5, 17, 10, 4, 3, 3, 3, 24, 18, 26, 25, 21, 22, 3, 3, 26, 20, 13, 3, 8, 15, 3, 9, 8, 18, 3, 15, 20, 3, 3, 3, 20, 3, 5, 19, 6, 3, 16, 18, 21, 3, 9, 3, 3, 3, 3, 3, 9, 25, 3, 20, 3, 29, 1, 3, 3, 18, 24, 3, 12, 3, 19, 18, 19, 20, 3, 22, 3, 19, 3, 27, 3, 23, 8, 9, 7, 19, 18, 9, 3, 18, 6, 19, 24, 22, 18, 13, 14, 21, 18, 3, 3, 3, 19, 2, 3, 24, 28, 26, 17, 23, 18, 19, 13, 14, 19, 27, 11, 3, 22, 14, 28, 17, 3, 17, 17, 3, 10, 3, 3, 23, 20, 3, 7, 5, 26, 3, 28], 'y_true': [10, 21, 11, 19, 7, 14, 27, 10, 8, 8, 2, 19, 7, 15, 8, 11, 26, 29, 6, 3, 5, 6, 7, 5, 8, 7, 25, 14, 35, 13, 12, 2, 13, 12, 22, 2, 21, 2, 5, 13, 9, 42, 9, 7, 36, 11, 9, 8, 8, 20, 4, 11, 14, 5, 9, 10, 4, 6, 13, 5, 18, 30, 7, 10, 6, 13, 14, 19, 10, 5, 5, 2, 14, 46, 11, 10, 6, 9, 14, 29, 14, 12, 14, 19, 12, 3, 30, 7, 21, 4, 4, 10, 11, 4, 10, 11, 10, 18, 29, 3, 2, 9, 11, 18, 15, 2, 7, 25, 10, 40, 9, 8, 7, 13, 20, 9, 23, 35, 14, 28, 14, 7, 15, 21, 2, 6, 19, 4, 10, 7, 9, 4, 16, 5, 10, 8, 10, 5, 17, 4, 39, 13, 6, 9, 16, 15, 7, 12, 9, 17, 6, 5, 13, 38, 19, 17, 22, 2, 9, 7, 25, 5, 17, 5, 14, 5, 19, 20, 16, 8, 5, 2, 6, 17, 17, 11, 26, 9, 5, 9, 6, 9, 5, 18, 2, 34, 4, 46, 2, 34, 16, 29, 21, 18, 21, 21, 16, 8, 13, 3, 7, 33, 23, 16, 5, 31, 12, 21, 3, 4, 3, 8, 10, 8, 11, 9, 5, 6, 6, 41, 11, 26, 50, 24, 25, 7, 6, 37, 15, 10, 5, 9, 13, 6, 9, 10, 10, 6, 11, 19, 6, 4, 10, 16, 3, 11, 15, 12, 12, 17, 11, 37, 4, 12, 7, 7, 8, 7, 6, 7, 30, 7, 15, 4, 31, 11, 8, 4, 22, 11, 7, 10, 8, 25, 10, 16, 19, 7, 16, 6, 24, 6, 18, 9, 51, 12, 10, 7, 13, 22, 11, 3, 14, 16, 27, 2, 20, 19, 12, 17, 23, 15, 9, 5, 3, 15, 9, 6, 30, 23, 32, 16, 19, 18, 11, 5, 9, 15, 76, 8, 5, 21, 25, 27, 15, 10, 15, 10, 5, 13, 7, 6, 27, 34, 5, 9, 8, 40, 9, 39], 'train_acc': 0.7029411764705882, 'test_acc': 0.02726643598615917, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_sent_muc1700'}
Running on: X_TANL_layer_last_bert-uncased_epoch20_muc1700.npy Y_num_tokens_muc1700.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
In this epoch 50/5000, Training loss: 672.8917, Test loss: 12803.2559
In this epoch 100/5000, Training loss: 6.0111, Test loss: 12090.5605
In this epoch 150/5000, Training loss: 0.9700, Test loss: 12102.4268
In this epoch 200/5000, Training loss: 0.3299, Test loss: 12102.7910
In this epoch 250/5000, Training loss: 0.1137, Test loss: 12102.3291
In this epoch 300/5000, Training loss: 0.0385, Test loss: 12102.1182
In this epoch 350/5000, Training loss: 0.0129, Test loss: 12102.0107
In this epoch 400/5000, Training loss: 0.0043, Test loss: 12101.9619
In this epoch 450/5000, Training loss: 0.0014, Test loss: 12101.9404
In this epoch 500/5000, Training loss: 0.0004, Test loss: 12101.9326
In this epoch 550/5000, Training loss: 0.0001, Test loss: 12101.9297
In this epoch 600/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 650/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 700/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 750/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 800/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 850/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 900/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 950/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 1000/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 1050/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 1100/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 1150/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 1200/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 1250/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 1300/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 1350/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 1400/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 1450/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 1500/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 1550/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 1600/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 1650/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 1700/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 1750/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 1800/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 1850/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 1900/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 1950/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 2000/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 2050/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 2100/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 2150/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 2200/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 2250/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 2300/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 2350/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 2400/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 2450/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 2500/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 2550/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 2600/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 2650/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 2700/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 2750/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 2800/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 2850/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 2900/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 2950/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 3000/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 3050/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 3100/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 3150/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 3200/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 3250/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 3300/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 3350/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 3400/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 3450/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 3500/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 3550/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 3600/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 3650/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 3700/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 3750/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 3800/5000, Training loss: 0.0000, Test loss: 12101.9307
In this epoch 3850/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 3900/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 3950/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 4000/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 4050/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 4100/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 4150/5000, Training loss: 0.0000, Test loss: 12101.9277
In this epoch 4200/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 4250/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 4300/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 4350/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 4400/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 4450/5000, Training loss: 0.0000, Test loss: 12101.9307
In this epoch 4500/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 4550/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 4600/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 4650/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 4700/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 4750/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 4800/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 4850/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 4900/5000, Training loss: 0.0000, Test loss: 12101.9287
In this epoch 4950/5000, Training loss: 0.0000, Test loss: 12101.9297
In this epoch 5000/5000, Training loss: 0.0000, Test loss: 12101.9287
train_acc 1.0
test_acc 0.011764705882352941
{'y_pred': [534.0170288085938, 182.1783447265625, 431.70721435546875, 172.92779541015625, 506.5348815917969, 196.35780334472656, 234.92709350585938, 268.86431884765625, 205.19305419921875, 52.92414855957031, 569.9144287109375, 210.10214233398438, 219.66847229003906, 93.31140899658203, 113.82502746582031, 214.16976928710938, 141.8291015625, 445.7535400390625, 527.7531127929688, 621.548583984375, 204.3763885498047, 205.77914428710938, 159.54420471191406, 451.7329406738281, 127.80166625976562, 137.2392578125, 393.4027099609375, 161.08558654785156, 176.56924438476562, 151.08468627929688, 389.779296875, 209.4405517578125, 546.5490112304688, 389.41949462890625, 217.9261016845703, 392.6719055175781, 227.27276611328125, 508.54595947265625, 98.94889831542969, 424.28240966796875, 443.7995910644531, 644.0162353515625, 82.97792053222656, 213.48741149902344, 189.1887664794922, 502.1614990234375, 152.9169158935547, 529.3345947265625, 217.71624755859375, 153.73931884765625, 553.8816528320312, 203.30751037597656, 474.6787109375, 196.1154327392578, 284.61102294921875, 624.77880859375, 200.79510498046875, 118.94789123535156, 568.3311767578125, 527.7105712890625, 373.00006103515625, 433.648193359375, 540.1882934570312, 480.3201904296875, 136.72288513183594, 593.4683837890625, 551.766845703125, 484.61505126953125, 205.33462524414062, 475.305419921875, 147.82652282714844, 128.80076599121094, 218.3157196044922, 693.5105590820312, 542.7013549804688, 181.80364990234375, 146.28094482421875, 221.84072875976562, 613.14990234375, 562.6434936523438, 602.9453735351562, 196.0625762939453, 141.29437255859375, 602.828369140625, 457.03076171875, 112.12896728515625, 161.26954650878906, 104.6944580078125, 590.2460327148438, 131.31631469726562, 613.9209594726562, 468.94244384765625, 593.6192016601562, 492.3140869140625, 213.2560272216797, 684.806396484375, 209.8842010498047, 150.99440002441406, 547.9535522460938, 140.2119140625, 511.2742919921875, 495.73870849609375, 201.2329559326172, 478.36956787109375, 526.9256591796875, 431.06549072265625, 509.417724609375, 488.9522705078125, 414.1446533203125, 211.19529724121094, 388.5564880371094, 539.8200073242188, 99.68778228759766, 670.6611328125, 443.1922302246094, 173.82533264160156, 562.6498413085938, 467.7872619628906, 134.55661010742188, 445.9329833984375, 474.41754150390625, 43.83076858520508, 523.3405151367188, 480.3030090332031, 185.34153747558594, 186.02969360351562, 636.27685546875, 159.1047821044922, 197.04444885253906, 507.15252685546875, 473.19488525390625, 586.0188598632812, 488.8084411621094, 463.6973876953125, 90.42753601074219, 191.5439910888672, 154.53846740722656, 142.00897216796875, 511.38140869140625, 622.5736083984375, 94.03868103027344, 69.79024505615234, 202.69911193847656, 219.594970703125, 575.184814453125, 626.3717651367188, 195.24168395996094, 168.90667724609375, 111.5782470703125, 520.7553100585938, 165.80096435546875, 83.86172485351562, 149.99317932128906, 103.6595458984375, 549.4321899414062, 646.669677734375, 161.94390869140625, 106.34439086914062, 463.53802490234375, 564.0680541992188, 468.3164367675781, 193.5207977294922, 149.59864807128906, 106.99359130859375, 534.9221801757812, 527.2252807617188, 606.2288818359375, 503.43341064453125, 545.2340087890625, 630.4733276367188, 599.5806884765625, 239.52352905273438, 321.4419860839844, 230.0592498779297, 58.469482421875, 471.36029052734375, 440.672119140625, 214.2189178466797, 75.20270538330078, 567.8738403320312, 290.02386474609375, 554.4783325195312, 539.164306640625, 552.4259643554688, 266.6151428222656, 663.4871826171875, 183.6731414794922, 471.9994812011719, 582.4116821289062, 610.6344604492188, 472.6446838378906, 611.075927734375, 560.1947021484375, 224.54537963867188, 585.7777709960938, 557.14892578125, 547.7648315429688, 523.6937255859375, 95.46092987060547, 84.50289154052734, 225.27500915527344, 534.1990966796875, 76.56497192382812, 185.14329528808594, 528.4127197265625, 510.9884948730469, 493.87982177734375, 161.78219604492188, 157.9559783935547, 469.2374267578125, 189.4073028564453, 219.0563507080078, 574.4736938476562, 501.8587341308594, 510.2862243652344, 507.792236328125, 370.8169860839844, 579.4017944335938, 140.16571044921875, 467.7872619628906, 626.6050415039062, 523.4411010742188, 185.26124572753906, 608.1187744140625, 122.1036605834961, 173.37808227539062, 461.7882080078125, 613.8829345703125, 472.2496643066406, 507.8903503417969, 406.2963562011719, 168.56068420410156, 504.5038757324219, 151.89503479003906, 167.1132049560547, 125.51622009277344, 497.36407470703125, 256.6380615234375, 212.11846923828125, 134.3908233642578, 203.1622772216797, 529.1923828125, 152.08047485351562, 561.163818359375, 198.13092041015625, 535.784912109375, 125.94750213623047, 420.6868591308594, 575.0595703125, 144.8687286376953, 212.08152770996094, 475.4712829589844, 298.9826354980469, 218.1095733642578, 287.2543640136719, 370.9396667480469, 181.99765014648438, 654.0632934570312, 328.2009582519531, 601.9342041015625, 160.1786346435547, 192.21888732910156, 375.8621520996094, 409.15899658203125, 437.94873046875, 94.89363861083984, 201.9670867919922, 580.4607543945312, 443.5361633300781, 649.664306640625, 539.3099365234375, 366.69122314453125, 505.63037109375, 141.5950164794922, 224.4137725830078, 352.0641174316406, 477.33148193359375, 620.0391235351562, 584.864013671875, 191.06736755371094, 217.1014862060547, 171.44480895996094, 583.8237915039062, 195.99951171875, 525.0535888671875, 655.9105224609375, 61.66236114501953, 76.77034759521484, 149.5643310546875, 202.75244140625, 174.94151306152344, 428.2347106933594, 589.6893310546875, 631.7190551757812, 611.4822387695312, 161.1527557373047, 363.2159118652344, 578.8446044921875, 663.9583129882812, 473.51092529296875, 489.30340576171875, 199.67283630371094, 486.7772521972656, 475.028564453125, 220.7491912841797, 238.9888153076172, 134.225830078125, 647.0986938476562, 89.376708984375, 95.1934814453125, 464.5010986328125, 454.2020263671875, 584.940185546875, 540.7476806640625, 497.6236877441406, 393.39208984375, 686.4840087890625, 204.88230895996094, 204.36557006835938, 90.40858459472656, 515.201904296875, 486.8318786621094, 203.90847778320312, 191.9532012939453, 215.03150939941406, 501.86248779296875, 496.6471252441406, 273.81610107421875, 173.1597137451172, 223.8929901123047, 503.06036376953125, 574.70751953125, 580.0411376953125, 267.86517333984375, 227.6576385498047, 652.0192260742188, 217.61077880859375, 344.93939208984375, 449.01947021484375, 251.6014404296875], 'y_pred_int': [534, 182, 432, 173, 507, 196, 235, 269, 205, 53, 570, 210, 220, 93, 114, 214, 142, 446, 528, 622, 204, 206, 160, 452, 128, 137, 393, 161, 177, 151, 390, 209, 547, 389, 218, 393, 227, 509, 99, 424, 444, 644, 83, 213, 189, 502, 153, 529, 218, 154, 554, 203, 475, 196, 285, 625, 201, 119, 568, 528, 373, 434, 540, 480, 137, 593, 552, 485, 205, 475, 148, 129, 218, 694, 543, 182, 146, 222, 613, 563, 603, 196, 141, 603, 457, 112, 161, 105, 590, 131, 614, 469, 594, 492, 213, 685, 210, 151, 548, 140, 511, 496, 201, 478, 527, 431, 509, 489, 414, 211, 389, 540, 100, 671, 443, 174, 563, 468, 135, 446, 474, 44, 523, 480, 185, 186, 636, 159, 197, 507, 473, 586, 489, 464, 90, 192, 155, 142, 511, 623, 94, 70, 203, 220, 575, 626, 195, 169, 112, 521, 166, 84, 150, 104, 549, 647, 162, 106, 464, 564, 468, 194, 150, 107, 535, 527, 606, 503, 545, 630, 600, 240, 321, 230, 58, 471, 441, 214, 75, 568, 290, 554, 539, 552, 267, 663, 184, 472, 582, 611, 473, 611, 560, 225, 586, 557, 548, 524, 95, 85, 225, 534, 77, 185, 528, 511, 494, 162, 158, 469, 189, 219, 574, 502, 510, 508, 371, 579, 140, 468, 627, 523, 185, 608, 122, 173, 462, 614, 472, 508, 406, 169, 505, 152, 167, 126, 497, 257, 212, 134, 203, 529, 152, 561, 198, 536, 126, 421, 575, 145, 212, 475, 299, 218, 287, 371, 182, 654, 328, 602, 160, 192, 376, 409, 438, 95, 202, 580, 444, 650, 539, 367, 506, 142, 224, 352, 477, 620, 585, 191, 217, 171, 584, 196, 525, 656, 62, 77, 150, 203, 175, 428, 590, 632, 611, 161, 363, 579, 664, 474, 489, 200, 487, 475, 221, 239, 134, 647, 89, 95, 465, 454, 585, 541, 498, 393, 686, 205, 204, 90, 515, 487, 204, 192, 215, 502, 497, 274, 173, 224, 503, 575, 580, 268, 228, 652, 218, 345, 449, 252], 'y_true': [446, 173, 484, 189, 415, 198, 296, 286, 199, 57, 507, 209, 276, 86, 112, 232, 161, 476, 392, 585, 208, 297, 163, 401, 119, 139, 409, 167, 173, 164, 506, 222, 542, 394, 261, 326, 247, 614, 100, 310, 384, 753, 82, 277, 197, 373, 161, 727, 243, 172, 935, 263, 493, 217, 331, 877, 207, 115, 570, 397, 373, 355, 611, 385, 158, 537, 962, 815, 197, 445, 157, 129, 258, 572, 634, 175, 163, 232, 787, 1044, 659, 202, 143, 415, 438, 119, 154, 108, 459, 132, 701, 436, 544, 343, 259, 489, 222, 162, 468, 154, 446, 350, 213, 629, 487, 333, 507, 417, 330, 200, 299, 824, 95, 477, 420, 171, 434, 840, 132, 312, 598, 45, 521, 367, 191, 191, 796, 160, 216, 390, 397, 446, 492, 334, 81, 185, 149, 149, 515, 544, 94, 68, 224, 258, 681, 624, 207, 172, 112, 395, 172, 86, 189, 110, 778, 743, 227, 108, 635, 378, 310, 185, 187, 102, 367, 552, 770, 414, 370, 585, 724, 245, 316, 256, 58, 344, 450, 245, 77, 566, 337, 545, 380, 700, 274, 567, 184, 530, 664, 1238, 359, 901, 611, 243, 842, 655, 446, 440, 97, 78, 257, 422, 78, 215, 376, 317, 361, 187, 164, 565, 195, 258, 846, 587, 670, 756, 348, 686, 142, 841, 617, 500, 300, 503, 124, 185, 572, 401, 437, 498, 686, 183, 547, 163, 181, 130, 387, 305, 298, 136, 266, 456, 163, 378, 209, 434, 122, 663, 486, 162, 248, 616, 321, 233, 335, 311, 185, 588, 317, 828, 158, 227, 296, 351, 606, 96, 237, 656, 455, 527, 656, 337, 392, 157, 227, 341, 469, 526, 431, 193, 291, 169, 685, 184, 350, 1062, 60, 80, 257, 196, 178, 319, 560, 706, 433, 165, 350, 534, 464, 403, 428, 253, 494, 343, 230, 273, 135, 498, 82, 87, 347, 446, 596, 694, 443, 332, 863, 197, 268, 87, 386, 412, 228, 195, 220, 641, 391, 290, 182, 228, 607, 599, 407, 280, 221, 762, 244, 312, 310, 266], 'train_acc': 1.0, 'test_acc': 0.011764705882352941, 'X': 'X_TANL_layer_last_bert-uncased_epoch20_muc1700', 'Y': 'Y_num_tokens_muc1700'}
Running on: X_dygiepp_embedding-unpadded_bert-uncased_epoch21_muc1700.npy Y_muc_1700_num_events.npy
loading X Y
Reshaping X Y
Scale-fitting X Y
Torch-loading X Y
Creating model
Training X Y
